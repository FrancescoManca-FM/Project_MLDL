{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "c9XO0l5B9WDO",
        "uhi2ESn89cml",
        "bSAhwwBr9iZ9",
        "6AaNgr4c9ncG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXZiclrtCLr7"
      },
      "source": [
        "Classification Network (using Resnet32) on CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlNz0nWYCR0L"
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9XO0l5B9WDO"
      },
      "source": [
        "### Data Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMcd8DV4C1nw",
        "outputId": "4eae8542-f442-40d9-86e5-18313d857909"
      },
      "source": [
        "# we build a transform to normalize images: Data normalization is an important step which ensures \n",
        "# each input parameter (pixel, in this case) has a similar data distribution. This makes convergence \n",
        "# faster while training the network.\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, \n",
        "                                         download=True, transform=transform)\n",
        "# DataLoader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
        "# batch_size = how many samples per batch to load\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhi2ESn89cml"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noF4lFZKH_Mp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c0fcf6-dbb1-46a3-fc5b-59f5391da684"
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "\"\"\" instantiate the net \"\"\"\n",
        "resnet34 = models.resnet18(pretrained=False)\n",
        "#resnet34.fc.out_features = 10\n",
        "\n",
        "\"\"\" move the model to the GPU \"\"\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "resnet34.to(device)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSAhwwBr9iZ9"
      },
      "source": [
        "### Define the loss and the optimization technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJqiRlCoI-b0"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(resnet34.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AaNgr4c9ncG"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9zwIkLXJPx0",
        "outputId": "a8e328a0-93dd-4c2a-95c3-ebcc645d4ecd"
      },
      "source": [
        "#train the network\n",
        "for epoch in range(3):\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "    # get the inputs; data is a list of  [input,labels]\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad() # Sets the gradients of all optimized torch.Tensor to zero.\n",
        "\n",
        "    outputs = resnet34(inputs) # forward: assign weights to each edge in each layer\n",
        "    loss = criterion(outputs,labels) # calculate the loss \n",
        "    loss.backward() # redesign the weights evaluating the performance of the network\n",
        "    optimizer.step() # update parameters \n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 2000 == 1999:    # print every 2000 mini-batches the average value of the loss accumulated in each batch\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 3.901\n",
            "[1,  4000] loss: 3.838\n",
            "[1,  6000] loss: 3.806\n",
            "[1,  8000] loss: 3.754\n",
            "[1, 10000] loss: 3.685\n",
            "[1, 12000] loss: 3.668\n",
            "[2,  2000] loss: 3.539\n",
            "[2,  4000] loss: 3.516\n",
            "[2,  6000] loss: 3.444\n",
            "[2,  8000] loss: 3.439\n",
            "[2, 10000] loss: 3.397\n",
            "[2, 12000] loss: 3.374\n",
            "[3,  2000] loss: 3.245\n",
            "[3,  4000] loss: 3.232\n",
            "[3,  6000] loss: 3.194\n",
            "[3,  8000] loss: 3.184\n",
            "[3, 10000] loss: 3.169\n",
            "[3, 12000] loss: 3.138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCnzW3XCF0JW"
      },
      "source": [
        "### Performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6YoczYtQu90",
        "outputId": "17b8a9f2-8f83-4f3e-8a7a-102e9e414108"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = resnet34(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 23 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDn00aWqQ2FM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5beda586-aaf5-4a16-a5d7-239ce1e7c8e3"
      },
      "source": [
        "# divided our dataset into sample of 10 classes each\n",
        "# train the network on the first 10 classes\n",
        "# evaluate the network on the first 10 classes\n",
        "# train the network on the second 10 classes (adding 10 output layers)\n",
        "# evaluate the network on the first 20 classes\n",
        "iterations=10\n",
        "test_set = [] #initialized here because we test over all the classes not only those one in which I train\n",
        "# for epoch in range(3):\n",
        "for i in range(iterations):\n",
        "  #i = 0\n",
        "  classes_current_iter = range(i*iterations, i*iterations+iterations)\n",
        "  train_iter = [] \n",
        "  for j in range(len(trainset)):\n",
        "    if(trainset[j][-1] in classes_current_iter):\n",
        "      test_set.append(trainset[j]) \n",
        "      train_iter.append(trainset[j])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_iter, shuffle = True, batch_size=batch_size, num_workers=2)\n",
        "  valid_loader = torch.utils.data.DataLoader(test_set, shuffle = True, batch_size = batch_size, num_workers=2) \n",
        "  training(train_loader, i, resnet34) # Train the network with 10 classes at a time\n",
        "  test(valid_loader, i, resnet34) # Test the network with all classes seen until this iteration\n",
        "\n",
        "  # train loader contains at each iteration the new 10 classes used to evaluate the network, while valid loader contains all classes seen so far"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 0 iteration: 62 %\n",
            "Accuracy of the network on the 1 iteration: 18 %\n",
            "Accuracy of the network on the 2 iteration: 16 %\n",
            "Accuracy of the network on the 3 iteration: 10 %\n",
            "Accuracy of the network on the 4 iteration: 9 %\n",
            "Accuracy of the network on the 5 iteration: 7 %\n",
            "Accuracy of the network on the 6 iteration: 7 %\n",
            "Accuracy of the network on the 7 iteration: 6 %\n",
            "Accuracy of the network on the 8 iteration: 4 %\n",
            "Accuracy of the network on the 9 iteration: 5 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi8z0BUyK9tt"
      },
      "source": [
        "# Each time we call training function we must pass a different trainloader, updated with the following 10 classes\n",
        "def training(trainloader, iteration, resnet34):\n",
        "  #if (iteration != 0):\n",
        "    # add 10 output nodes to the network\n",
        "    #num_classes = 10\n",
        "    #addOutputNodes(resnet34, num_classes)\n",
        "\n",
        "  #train the network\n",
        "  for epoch in range(1):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "      # get the inputs; data is a list of  [input,labels]\n",
        "      inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "      optimizer.zero_grad() # Sets the gradients of all optimized torch.Tensor to zero.\n",
        "\n",
        "      outputs = resnet34(inputs) # forward: assign weights to each edge in each layer\n",
        "      loss = criterion(outputs,labels) # calculate the loss \n",
        "      loss.backward() # redesign the weights evaluating the performance of the network\n",
        "      optimizer.step() # update parameters \n",
        "\n",
        "      running_loss += loss.item()\n",
        "      if i % 2000 == 1999:    # print every 2000 mini-batches the average value of the loss accumulated in each batch\n",
        "        print('[%d, %5d] loss: %.3f' %\n",
        "                (epoch + 1, i + 1, running_loss / 2000))\n",
        "        running_loss = 0.0"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv3OQxITMFuM"
      },
      "source": [
        "def addOutputNodes (network, num_classes):\n",
        "  in_features = network.fc.in_features\n",
        "  out_features = network.fc.out_features\n",
        "  weight = network.fc.weight.data\n",
        "\n",
        "  network.fc = nn.Linear(in_features, out_features + num_classes)\n",
        "  network.fc.weight.data[:out_features] = weight"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmbA81iYSSSc"
      },
      "source": [
        "def test(testloader, iteration, resnet34):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          images, labels = data[0].to(device), data[1].to(device)\n",
        "          # calculate outputs by running images through the network\n",
        "          outputs = resnet34(images)\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f'Accuracy of the network on the {iteration} iteration: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}