{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di Copia di LWF funzionante.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-OWtqytyzmMF",
        "KpWv5ZkhxTPJ",
        "qJ-J8aC_xx-c",
        "9nX10znUi6qd",
        "TKBdrJ8Csy2a",
        "F5HRagcujHgM",
        "gy_HVL-tjZcb",
        "DFHiLXyLmaWu",
        "FQKnRMfjEaA-",
        "trLViwIa2_bY",
        "CMyXZhVemn8q",
        "SBFSAonO0mgQ"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoManca-FM/Project_MLDL/blob/main/Copia_di_Copia_di_LWF_funzionante.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAfp19qRVa7l"
      },
      "source": [
        "### GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag2iq0PaVWC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b451cd-badc-4178-e0fc-5d1d6211ed02"
      },
      "source": [
        "# Check GPU assigned\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul 11 14:35:36 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P0    25W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OWtqytyzmMF"
      },
      "source": [
        "## Network, dataset, functions and parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itMjhanGVTUp"
      },
      "source": [
        "### LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6tCA_s2rru"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nHVORYbiZRx"
      },
      "source": [
        "### Resnet32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIdSy6PnibZA"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\"\"\"\n",
        "Credits to @hshustc\n",
        "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from torch.nn import init\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_relu=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.stride = stride\n",
        "        self.use_relu = use_relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "        if self.use_relu:\n",
        "            out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        assert stride == 2\n",
        "        self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat((x, x.mul(0)), 1)\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block=BasicBlock, depth=32, num_classes=0, channels=3):\n",
        "\n",
        "        super(CifarResNet, self).__init__()\n",
        "\n",
        "        # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
        "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
        "        layer_blocks = (depth - 2) // 6\n",
        "        bn = nn.BatchNorm2d\n",
        "        self.inplanes = 16\n",
        "\n",
        "        self.conv_1_3x3 = nn.Conv2d(channels, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn_1 = bn(self.inplanes)\n",
        "\n",
        "        self.stage_1 = self._make_layer(block, 16, layer_blocks, 1)\n",
        "        self.stage_2 = self._make_layer(block, 32, layer_blocks, 2)\n",
        "        self.stage_3 = self._make_layer(block, 64, layer_blocks, 2, last=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.relu = nn.ReLU()\n",
        "        #self.linear = nn.Linear(64, num_classes)\n",
        "        self.fcs = nn.ModuleList([nn.Linear(64, num_classes)])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, math.sqrt(1. / 64.))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, last=False):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = Downsample(self.\n",
        "inplanes, planes * block.expansion, stride)\n",
        "\n",
        "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        if last:\n",
        "            for i in range(1, blocks - 1):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            layers.append(block(self.inplanes, planes, use_relu=False))\n",
        "\n",
        "        else:\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv_1_3x3(x)\n",
        "\n",
        "        x = self.bn_1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.stage_1(x)\n",
        "        x = self.stage_2(x)\n",
        "        x = self.stage_3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        out = []\n",
        "        for fc in self.fcs:\n",
        "            out.append(fc(x))\n",
        "        out = torch.cat(out, dim=1)\n",
        "        return out\n",
        "\n",
        "    def addOutputNodes(self, num_classes):\n",
        "        self.fcs.append(nn.Linear(64, num_classes))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ga-SaCsjkYV"
      },
      "source": [
        "### Cifar100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHAyljuejmkd"
      },
      "source": [
        "# ref:\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py\n",
        "# https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR10\n",
        "# homework2 (caltech)\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# This is an handler class for the Cifar dataset\n",
        "class CIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    `CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "    This is a subclass of the `CIFAR100` Dataset.\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar-100-python'\n",
        "    train_file = 'train'\n",
        "    test_file = 'test'\n",
        "    meta_file = 'meta'\n",
        "\n",
        "    def __init__(self, root, split = 'train', transform = None):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              root (string): Root directory of the dataset where directory\n",
        "                  cifar-100-python exists.\n",
        "              split (string, optional): If 'train', creates dataset from training\n",
        "                  set, otherwise creates from test set.\n",
        "              transform (callable, optional): A function/transform that takes in a\n",
        "                  PIL image and returns a transformed version.\n",
        "        \"\"\"\n",
        "        super(CIFAR100, self).__init__(root, transform=transform)\n",
        "\n",
        "        self.split = split\n",
        "        if split == 'train':\n",
        "            filename = self.train_file\n",
        "        else:\n",
        "            filename = self.test_file\n",
        "\n",
        "        data_path = os.path.join(self.root, self.base_folder, filename)\n",
        "        data = None\n",
        "        labels = None\n",
        "\n",
        "        with open(data_path, 'rb') as f:\n",
        "            entry = pickle.load(f, encoding='latin1')\n",
        "            data = entry['data']\n",
        "            labels = entry['fine_labels']\n",
        "        \n",
        "        data = np.vstack(data).reshape(-1, 3, 32, 32)\n",
        "        data = data.transpose((0, 2, 3, 1))  # Convert to HWC\n",
        "        \n",
        "        labels = np.array(labels)\n",
        "\n",
        "        self.df = pd.DataFrame()\n",
        "        self.df['data'] = pd.Series(list(data))\n",
        "        self.df['labels'] = labels\n",
        "\n",
        "        self.data = self.df['data']\n",
        "        self.labels = self.df['labels']\n",
        "\n",
        "        self._load_meta()\n",
        "\n",
        "    def _load_meta(self):\n",
        "        meta_path = os.path.join(self.root, self.base_folder, self.meta_file)\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f, encoding='latin1')\n",
        "            self.label_names = meta['fine_label_names']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.df.loc[index, 'data'], self.df.loc[index, 'labels']\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img) # Return a PIL image\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return index, img, target\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def getTargets(self):\n",
        "        return set(self.labels)\n",
        "     \n",
        "    # test\n",
        "    def get_indices(self, labels):\n",
        "        return list(self.df[self.df['labels'].isin(labels)].index)\n",
        "\n",
        "    def split_classes(self, n_splits=10, seed=None, dictionary_of='dataframes'):\n",
        "        if dictionary_of not in ['dataframes','indices']:\n",
        "            raise ValueError(\"'dictionary_of' must be equal to 'dataframes' or 'indices'\")\n",
        "\n",
        "        all_classes = list(self.df['labels'].value_counts().index)\n",
        "        dictionary = {}\n",
        "        random.seed(seed)\n",
        "        random.shuffle(all_classes)\n",
        "        split_size = int(len(all_classes)/n_splits)\n",
        "        for j in range(n_splits):\n",
        "            if ((j+1)*split_size < len(all_classes)):\n",
        "                split_end = (j+1)*split_size\n",
        "            else:\n",
        "                split_end = None\n",
        "            subgroup = all_classes[j*split_size:split_end]\n",
        "            if dictionary_of == 'dataframes':\n",
        "                dictionary[j] = self.df[self.df['labels'].isin(subgroup)]\n",
        "            elif dictionary_of == 'indices':\n",
        "                dictionary[j] = list(self.df[self.df['labels'].isin(subgroup)].index)\n",
        "        return dictionary\n",
        "\n",
        "    def split_groups_in_train_validation(self, groups, ratio=0.5, seed=None):\n",
        "        groups_train_val = dict()\n",
        "        for k, subdf in groups.items():\n",
        "            train_indexes = []\n",
        "            val_indexes = []\n",
        "            split_labels = list(subdf['labels'].value_counts().index)\n",
        "            for l in split_labels:\n",
        "                indexes_to_sample = list(subdf[subdf['labels'] == l].index)\n",
        "                random.seed(seed)\n",
        "                train_samples = random.sample(indexes_to_sample, int(len(indexes_to_sample)*ratio))\n",
        "                train_indexes = train_indexes + train_samples\n",
        "                val_indexes = val_indexes + list(set(indexes_to_sample).difference(set(train_samples)))\n",
        "            groups_train_val[k] = {\n",
        "                'train': train_indexes,\n",
        "                'val': val_indexes\n",
        "            }\n",
        "        return groups_train_val\n",
        "    \n",
        "    def split_in_train_val_groups(self, n_splits=10, ratio=0.5, seed=None):\n",
        "        groups = self.split_classes(n_splits=n_splits, seed=seed)\n",
        "        return self.split_groups_in_train_validation(groups, ratio=ratio, seed=seed)\n",
        "\n",
        "    # given a tensors returns an image (used in exemplars)\n",
        "    #def tensorToImg(self, tensor):\n",
        "    #   return Variable(transform(Image.fromarray(img)), volatile=True)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_M1anVYkRnF"
      },
      "source": [
        "### Reverse Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j66mUOXekTlM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "__all__ = ['ReverseIndex']\n",
        "\n",
        "class ReverseIndex():\n",
        "\n",
        "    def __init__(self, dataset, splits, device='cuda'):\n",
        "\n",
        "        self.df = pd.DataFrame(columns=['group', 'labels'])\n",
        "\n",
        "        for k in splits.keys():\n",
        "            labels = list(dataset.df.loc[splits[k]['train'],'labels'].value_counts().index)\n",
        "            group = [k for i in range(len(labels))]\n",
        "            data = pd.DataFrame(np.array([group, labels]).T, columns=['group', 'labels'])\n",
        "            self.df = self.df.append(data, ignore_index=True)\n",
        "\n",
        "        self.df['nodes'] = self.df.index\n",
        "        self.device = device\n",
        "    \n",
        "    def _changeIndex(self, reverse_index, column):\n",
        "        reverse_index = reverse_index.set_index(column)\n",
        "        reverse_index[column] = reverse_index.index\n",
        "        return reverse_index\n",
        "\n",
        "    def getLabels(self, outputs):\n",
        "        outs = outputs.cpu().numpy()\n",
        "        reverse_index = self._changeIndex(self.df, 'nodes')\n",
        "        labels = reverse_index.loc[outs, 'labels']\n",
        "\n",
        "        labels = torch.tensor(list(labels))\n",
        "        return labels.to(self.device)\n",
        "\n",
        "    def getNodes(self, labels):\n",
        "        labels = labels.cpu().numpy()\n",
        "       \n",
        "        reverse_index = self._changeIndex(self.df, 'labels')\n",
        "\n",
        "        nodes = reverse_index.loc[labels, 'nodes']\n",
        "\n",
        "        nodes = torch.tensor(list(nodes))\n",
        "        return nodes.to(self.device)\n",
        "\n",
        "    def getGroups(self, distinct=True):\n",
        "        return self.df['group'].value_counts().index.sort_values()\n",
        "    \n",
        "    def getLabelsOfGroup(self, group):\n",
        "        return self.df.loc[self.df['group'] == group, 'labels']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVfOeOCuknB8"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QpAHzFkoW0"
      },
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Module\n",
        "\n",
        "# These are the default iCaRL hyper-parameters\n",
        "def getHyperparams():\n",
        "\tdictHyperparams = {\n",
        "\t\t\"LR\": 2,\n",
        "\t\t\"MOMENTUM\": 0.9,\n",
        "\t\t\"WEIGHT_DECAY\": 1e-5,\n",
        "\t\t\"NUM_EPOCHS\": 70,\n",
        "\t\t\"MILESTONES\": [49, 63],\n",
        "\t\t\"BATCH_SIZE\": 128,\n",
        "\t\t\"DEVICE\": 'cuda',\n",
        "\t\t\"GAMMA\": 0.2,\n",
        "\t\t\"SEED\": 66, #use 30, 42, 16\n",
        "\t\t\"LOG_FREQUENCY\": 10,\n",
        "\t\t\"NUM_CLASSES\": 100\n",
        "\t}\n",
        "\treturn dictHyperparams\n",
        "\n",
        "def getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize):\n",
        "\toptimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\tscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA, last_epoch=-1) \n",
        "\treturn optimizer, scheduler\n",
        "\n",
        "# the mean and the std have been found on the web as mean and std of cifar100\n",
        "# alternative (realistic): compute mean and std for the dataset\n",
        "def getTransformations():\n",
        "\t# Define transforms for training phase\n",
        "\ttrain_transform = transforms.Compose([transforms.RandomHorizontalFlip(), # Randomly flip the image with probability of 0.5\n",
        "\t                                      transforms.Pad(4), # Add padding\n",
        "\t                                      transforms.RandomCrop(32),# Crops a random squares of the image\n",
        "\t                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\t# Define transforms for the evaluation phase\n",
        "\teval_transform = transforms.Compose([\n",
        "\t                                      transforms.ToTensor(),\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\treturn train_transform, eval_transform\n",
        "\n",
        "# BCEWithLogits = Sigmoid + BCE, is the loss used in iCaRL\n",
        "def getLossCriterion():\n",
        "\tcriterion = nn.BCEWithLogitsLoss(reduction = 'mean') \n",
        "\treturn criterion\n",
        "\n",
        "# CrossEntropyLoss \n",
        "def computeLoss(criterion, outputs, labels):\n",
        "\treturn criterion(outputs, labels)\n",
        " \n",
        "# Loss L2\n",
        "def l2Loss (outputs, labels):\n",
        "  criterion = nn.MSELoss()\n",
        "  return criterion(outputs, labels)\n",
        "\n",
        "# Loss L1\n",
        "def l1Loss(outputs, labels):\n",
        "  criterion = nn.L1Loss()\n",
        "  return criterion(outputs, labels)\n",
        "\n",
        "# support BCE\n",
        "def _one_hot_encode(labels, n_classes, reverse_index, dtype=None, device='cuda'):\n",
        "\tbatch_size = len(labels)\n",
        "\tenconded = torch.zeros(batch_size, n_classes, dtype=dtype, device=device)\n",
        "\tlabels=map_to_outputs(labels, reverse_index)\n",
        "\tfor i, l in enumerate(labels):\n",
        "\t  enconded[i, l] = 1\n",
        "\treturn enconded\n",
        "\n",
        "def map_to_outputs(labels, reverse_index):\n",
        "\tif reverse_index is None:\n",
        "\t  return labels\n",
        "\tif type(labels) == int:\n",
        "\t  return int(reverse_index.getNodes(torch.tensor([labels])))\n",
        "\telif type(labels) == torch.Tensor:\n",
        "\t\treturn reverse_index.getNodes(labels)\n",
        "\n",
        "\n",
        "def plotAccuracyTrend(method, data_plot_line, seed):\n",
        "\tplt.figure(figsize=(20,7))\n",
        "\taccuracyDF=pd.DataFrame(data_plot_line, columns = ['Classes','Accuracy'])\n",
        "\tax = sns.lineplot(x=\"Classes\", y=\"Accuracy\",data=accuracyDF, marker = 'o')\n",
        "\tax.minorticks_on()\n",
        "\tax.set_xticks(np.arange(10,110,10))\n",
        "\tax.set_xlim(xmin=9, xmax=101)\n",
        "\tax.set_ylim(ymin=0, ymax=1)\n",
        "\tplt.legend(['Accuracy {}'.format(method)])\n",
        "\tax.grid(axis='y')\n",
        "\tplt.title(\"Accuracies against seen classes {} - seed: {}\".format(method, seed))\n",
        "\t\n",
        "\tfilename = \"acc_{}_{}.jpg\".format(method, seed) # ex. acc_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "def plotConfusionMatrix(method, confusionMatrixData, seed):\n",
        "\tfig,ax=plt.subplots(figsize=(10,10))\n",
        "\tsns.heatmap(confusionMatrixData,cmap='terrain',ax=ax)\n",
        "\tplt.ylabel('True label')\n",
        "\tplt.xlabel('Predicted label')\n",
        "\tplt.title(\"Confusion Matrix {} - seed: {}\".format(method, seed))\n",
        "\n",
        "\tfilename = \"cm_{}_{}.jpg\".format(method, seed) # ex. cm_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "# Write down the metrics (accuracy trand and confusion matrix)\n",
        "# this method is a shortcut when perfoming multiple tests with different splits (random_seed)\n",
        "# and allow us to plot on the same graph the data from multiple models (accuracy)\n",
        "def writeMetrics(method, seed, accuracies, confusionMatrixData):\n",
        "  data = {}\n",
        "  data['accuracies'] = []\n",
        "  data['cm'] = [] #cm line\n",
        "  i = 0\n",
        "  for classes_seen in range(10, 110, 10): #x axis on the plot\n",
        "    data['accuracies'].append({classes_seen : accuracies[i]}) \n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  for class_num in range(0,len(confusionMatrixData)): #rows of the cm\n",
        "    data['cm'].append({class_num : confusionMatrixData[i].tolist()}) \n",
        "    i += 1\n",
        "  \n",
        "  # dump to file\n",
        "  aus = method + '_' + str(seed)\n",
        "  filename = 'data_{}.json'.format(aus)\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "\n",
        "# Functions\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getResNet32():\n",
        "    net = CifarResNet()\n",
        "    # net.fc = nn.Linear(net.fc.in_features, output_size) # embedded in the class\n",
        "\n",
        "    criterion = getLossCriterion()\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return net, criterion, optimizer, scheduler\n",
        "\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getNet():\n",
        "    return getResNet32()\n",
        "\n",
        "def getSchedulerOptimizer(net):\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return optimizer, scheduler\n",
        "def softmax(x):\n",
        "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
        "    return f_x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfJj1z2LSxp0"
      },
      "source": [
        "### DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysghtAWOPYZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a0b42ee-f3b8-41c7-981a-d705b63c8e87"
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "if not os.path.isdir('./{}'.format(\"$DATA_DIR/cifar-100-python\")):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-11 14:35:40--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  94.7MB/s    in 1.7s    \n",
            "\n",
            "2021-07-11 14:35:42 (94.7 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY10HfpTS1lq"
      },
      "source": [
        "### HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-pqSNg4_Ris"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100\n",
        "BATCH_SIZE = 128 \n",
        "LR = 0.1    \n",
        "MOMENTUM = 0.9     \n",
        "WEIGHT_DECAY = 1e-05\n",
        "NUM_EPOCHS = 70\n",
        "NUM_EPOCHS_FINETUNE = 10  \n",
        "GAMMA = 0.2\n",
        "LOG_FREQUENCY = 10\n",
        "MILESTONES = [49,63]\n",
        "RANDOM_SEED = 31\n",
        "THRESHOLD = 0.5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW7WxV_QS4re"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_jHycn_1kk"
      },
      "source": [
        "train_transform, eval_transform = getTransformations()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPrwhTcIqUbA"
      },
      "source": [
        "### train & test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJKwvGljJj2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "197754fc-3b7a-40f7-96c3-457d47864db6"
      },
      "source": [
        "# Import dataset and apply transformations \n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# Check datasets length \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnooWwcyS_YN"
      },
      "source": [
        "### SPLIT DATA IN CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckn3H69iJj2X"
      },
      "source": [
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpcJvhxhJOLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d36a035f-d4b1-44ae-f77c-518db7054531"
      },
      "source": [
        "# TRAIN / VAL split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "print(outputs_labels_mapping.getGroups())\n",
        "\n",
        "# TEST split\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MSItI0QVpn"
      },
      "source": [
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10):\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpWv5ZkhxTPJ"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4NQa-wNxWL1"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pisadUTPxcRP"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            # if (group_id > 1):\n",
        "            #   old_features = old_net.forward(images)\n",
        "            #   old_outputs = old_net.predict(old_features)\n",
        "            #   labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX_61nSuxh3v"
      },
      "source": [
        "### sequential learning fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM0GysHZxk1B"
      },
      "source": [
        "### Fine tuning\n",
        "def sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    old_net = None\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    #confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2X2IVZ-xrVT"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DGpd8lslxtUW",
        "outputId": "2a7a5171-8688-4b1f-fe52-f88f4b532a6a"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.7173749208450317\n",
            "Train step - Step 10, Loss 0.31387636065483093\n",
            "Train step - Step 20, Loss 0.2874338626861572\n",
            "Train step - Step 30, Loss 0.2654590606689453\n",
            "Train epoch - Accuracy: 0.318989898989899 Loss: 0.34560690457772725 Corrects: 1579\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.26789265871047974\n",
            "Train step - Step 50, Loss 0.258944034576416\n",
            "Train step - Step 60, Loss 0.2350231260061264\n",
            "Train step - Step 70, Loss 0.23865528404712677\n",
            "Train epoch - Accuracy: 0.44363636363636366 Loss: 0.24301170718790305 Corrects: 2196\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.23784291744232178\n",
            "Train step - Step 90, Loss 0.22167158126831055\n",
            "Train step - Step 100, Loss 0.22836807370185852\n",
            "Train step - Step 110, Loss 0.21694694459438324\n",
            "Train epoch - Accuracy: 0.48707070707070704 Loss: 0.22610946089330344 Corrects: 2411\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.21196965873241425\n",
            "Train step - Step 130, Loss 0.20575585961341858\n",
            "Train step - Step 140, Loss 0.22211872041225433\n",
            "Train step - Step 150, Loss 0.20619133114814758\n",
            "Train epoch - Accuracy: 0.5016161616161616 Loss: 0.21912972303954037 Corrects: 2483\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.2055768072605133\n",
            "Train step - Step 170, Loss 0.20553207397460938\n",
            "Train step - Step 180, Loss 0.22383907437324524\n",
            "Train step - Step 190, Loss 0.19269728660583496\n",
            "Train epoch - Accuracy: 0.5418181818181819 Loss: 0.20608459960932685 Corrects: 2682\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.19962547719478607\n",
            "Train step - Step 210, Loss 0.1956397444009781\n",
            "Train step - Step 220, Loss 0.23317204415798187\n",
            "Train step - Step 230, Loss 0.1870502084493637\n",
            "Train epoch - Accuracy: 0.5523232323232323 Loss: 0.19966931800047558 Corrects: 2734\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.19728612899780273\n",
            "Train step - Step 250, Loss 0.17967957258224487\n",
            "Train step - Step 260, Loss 0.1929226964712143\n",
            "Train step - Step 270, Loss 0.19188766181468964\n",
            "Train epoch - Accuracy: 0.5834343434343434 Loss: 0.19277908541939476 Corrects: 2888\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1998911201953888\n",
            "Train step - Step 290, Loss 0.18329913914203644\n",
            "Train step - Step 300, Loss 0.16586630046367645\n",
            "Train step - Step 310, Loss 0.17675089836120605\n",
            "Train epoch - Accuracy: 0.598989898989899 Loss: 0.1864176897809963 Corrects: 2965\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.17738604545593262\n",
            "Train step - Step 330, Loss 0.18055176734924316\n",
            "Train step - Step 340, Loss 0.17641305923461914\n",
            "Train step - Step 350, Loss 0.14969894289970398\n",
            "Train epoch - Accuracy: 0.6155555555555555 Loss: 0.17903075204955207 Corrects: 3047\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.1671830266714096\n",
            "Train step - Step 370, Loss 0.1687958985567093\n",
            "Train step - Step 380, Loss 0.159011110663414\n",
            "Train epoch - Accuracy: 0.6309090909090909 Loss: 0.1738420210462628 Corrects: 3123\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.15419088304042816\n",
            "Train step - Step 400, Loss 0.14775967597961426\n",
            "Train step - Step 410, Loss 0.16979841887950897\n",
            "Train step - Step 420, Loss 0.1525963395833969\n",
            "Train epoch - Accuracy: 0.6539393939393939 Loss: 0.1639658623993999 Corrects: 3237\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.19196446239948273\n",
            "Train step - Step 440, Loss 0.1466265171766281\n",
            "Train step - Step 450, Loss 0.17056730389595032\n",
            "Train step - Step 460, Loss 0.18163080513477325\n",
            "Train epoch - Accuracy: 0.6608080808080808 Loss: 0.16202940798167026 Corrects: 3271\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.176211878657341\n",
            "Train step - Step 480, Loss 0.1588747799396515\n",
            "Train step - Step 490, Loss 0.1342204362154007\n",
            "Train step - Step 500, Loss 0.14058950543403625\n",
            "Train epoch - Accuracy: 0.6767676767676768 Loss: 0.15383696049752862 Corrects: 3350\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.1555892676115036\n",
            "Train step - Step 520, Loss 0.13571906089782715\n",
            "Train step - Step 530, Loss 0.1515876054763794\n",
            "Train step - Step 540, Loss 0.1359780877828598\n",
            "Train epoch - Accuracy: 0.702020202020202 Loss: 0.14594818502363532 Corrects: 3475\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.15486030280590057\n",
            "Train step - Step 560, Loss 0.13412602245807648\n",
            "Train step - Step 570, Loss 0.15869688987731934\n",
            "Train step - Step 580, Loss 0.13754160702228546\n",
            "Train epoch - Accuracy: 0.7034343434343434 Loss: 0.14330182038172326 Corrects: 3482\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.13797789812088013\n",
            "Train step - Step 600, Loss 0.14471013844013214\n",
            "Train step - Step 610, Loss 0.1659015417098999\n",
            "Train step - Step 620, Loss 0.13035106658935547\n",
            "Train epoch - Accuracy: 0.712929292929293 Loss: 0.1402043603164981 Corrects: 3529\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.10688119381666183\n",
            "Train step - Step 640, Loss 0.10551120340824127\n",
            "Train step - Step 650, Loss 0.11231940239667892\n",
            "Train step - Step 660, Loss 0.12159333378076553\n",
            "Train epoch - Accuracy: 0.7434343434343434 Loss: 0.1296956703397963 Corrects: 3680\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.13172531127929688\n",
            "Train step - Step 680, Loss 0.142404243350029\n",
            "Train step - Step 690, Loss 0.12234494835138321\n",
            "Train step - Step 700, Loss 0.14414963126182556\n",
            "Train epoch - Accuracy: 0.756969696969697 Loss: 0.12469814313180519 Corrects: 3747\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.1333446353673935\n",
            "Train step - Step 720, Loss 0.15927483141422272\n",
            "Train step - Step 730, Loss 0.11001887172460556\n",
            "Train step - Step 740, Loss 0.12100914865732193\n",
            "Train epoch - Accuracy: 0.7450505050505051 Loss: 0.12714890332836093 Corrects: 3688\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.11506552994251251\n",
            "Train step - Step 760, Loss 0.10524898767471313\n",
            "Train step - Step 770, Loss 0.0984029471874237\n",
            "Train epoch - Accuracy: 0.7668686868686869 Loss: 0.11733168877435453 Corrects: 3796\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.12441835552453995\n",
            "Train step - Step 790, Loss 0.11353017389774323\n",
            "Train step - Step 800, Loss 0.13449843227863312\n",
            "Train step - Step 810, Loss 0.12670955061912537\n",
            "Train epoch - Accuracy: 0.7739393939393939 Loss: 0.11266988289175611 Corrects: 3831\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.10420570522546768\n",
            "Train step - Step 830, Loss 0.10050588846206665\n",
            "Train step - Step 840, Loss 0.09052333980798721\n",
            "Train step - Step 850, Loss 0.07840898633003235\n",
            "Train epoch - Accuracy: 0.7808080808080808 Loss: 0.11037116950509523 Corrects: 3865\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.13141851127147675\n",
            "Train step - Step 870, Loss 0.11617157608270645\n",
            "Train step - Step 880, Loss 0.11001942306756973\n",
            "Train step - Step 890, Loss 0.08741968870162964\n",
            "Train epoch - Accuracy: 0.7844444444444445 Loss: 0.10794938291564132 Corrects: 3883\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.11431687325239182\n",
            "Train step - Step 910, Loss 0.0919078066945076\n",
            "Train step - Step 920, Loss 0.1131000742316246\n",
            "Train step - Step 930, Loss 0.12487182766199112\n",
            "Train epoch - Accuracy: 0.7935353535353535 Loss: 0.10410453104310566 Corrects: 3928\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.0851389467716217\n",
            "Train step - Step 950, Loss 0.10327544063329697\n",
            "Train step - Step 960, Loss 0.09313042461872101\n",
            "Train step - Step 970, Loss 0.12224413454532623\n",
            "Train epoch - Accuracy: 0.8014141414141415 Loss: 0.10208582940125706 Corrects: 3967\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.10216508060693741\n",
            "Train step - Step 990, Loss 0.10625173896551132\n",
            "Train step - Step 1000, Loss 0.12439626455307007\n",
            "Train step - Step 1010, Loss 0.09492490440607071\n",
            "Train epoch - Accuracy: 0.805050505050505 Loss: 0.10121105768764863 Corrects: 3985\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.1075296625494957\n",
            "Train step - Step 1030, Loss 0.08558668196201324\n",
            "Train step - Step 1040, Loss 0.10448060184717178\n",
            "Train step - Step 1050, Loss 0.0987623855471611\n",
            "Train epoch - Accuracy: 0.8115151515151515 Loss: 0.09598487866647316 Corrects: 4017\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.11706822365522385\n",
            "Train step - Step 1070, Loss 0.08957835286855698\n",
            "Train step - Step 1080, Loss 0.1004997119307518\n",
            "Train step - Step 1090, Loss 0.10269360989332199\n",
            "Train epoch - Accuracy: 0.8151515151515152 Loss: 0.093790872918837 Corrects: 4035\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10780717432498932\n",
            "Train step - Step 1110, Loss 0.0829559937119484\n",
            "Train step - Step 1120, Loss 0.06946247071027756\n",
            "Train step - Step 1130, Loss 0.08679612725973129\n",
            "Train epoch - Accuracy: 0.8232323232323232 Loss: 0.09224864899811118 Corrects: 4075\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.0877915769815445\n",
            "Train step - Step 1150, Loss 0.07081689685583115\n",
            "Train step - Step 1160, Loss 0.09684779495000839\n",
            "Train epoch - Accuracy: 0.8305050505050505 Loss: 0.08737428996298048 Corrects: 4111\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.08674424141645432\n",
            "Train step - Step 1180, Loss 0.08422601222991943\n",
            "Train step - Step 1190, Loss 0.09191498905420303\n",
            "Train step - Step 1200, Loss 0.09191594272851944\n",
            "Train epoch - Accuracy: 0.834949494949495 Loss: 0.08547621402174535 Corrects: 4133\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.07304417341947556\n",
            "Train step - Step 1220, Loss 0.09637052565813065\n",
            "Train step - Step 1230, Loss 0.08423800766468048\n",
            "Train step - Step 1240, Loss 0.09754743427038193\n",
            "Train epoch - Accuracy: 0.8418181818181818 Loss: 0.0811596822106477 Corrects: 4167\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.08752617239952087\n",
            "Train step - Step 1260, Loss 0.08612791448831558\n",
            "Train step - Step 1270, Loss 0.09345199167728424\n",
            "Train step - Step 1280, Loss 0.06988094002008438\n",
            "Train epoch - Accuracy: 0.8432323232323232 Loss: 0.08195995536717501 Corrects: 4174\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.063926100730896\n",
            "Train step - Step 1300, Loss 0.09519979357719421\n",
            "Train step - Step 1310, Loss 0.0794561430811882\n",
            "Train step - Step 1320, Loss 0.09330239146947861\n",
            "Train epoch - Accuracy: 0.8434343434343434 Loss: 0.08210123597973525 Corrects: 4175\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.06706669181585312\n",
            "Train step - Step 1340, Loss 0.07936250418424606\n",
            "Train step - Step 1350, Loss 0.08014490455389023\n",
            "Train step - Step 1360, Loss 0.08957503736019135\n",
            "Train epoch - Accuracy: 0.8488888888888889 Loss: 0.07934777613541093 Corrects: 4202\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.08020991086959839\n",
            "Train step - Step 1380, Loss 0.09724964946508408\n",
            "Train step - Step 1390, Loss 0.06381719559431076\n",
            "Train step - Step 1400, Loss 0.06528539955615997\n",
            "Train epoch - Accuracy: 0.8549494949494949 Loss: 0.07585879642855037 Corrects: 4232\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.07746966183185577\n",
            "Train step - Step 1420, Loss 0.08617367595434189\n",
            "Train step - Step 1430, Loss 0.06970410794019699\n",
            "Train step - Step 1440, Loss 0.08160582929849625\n",
            "Train epoch - Accuracy: 0.8527272727272728 Loss: 0.07876499318715298 Corrects: 4221\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.06916437298059464\n",
            "Train step - Step 1460, Loss 0.06720501184463501\n",
            "Train step - Step 1470, Loss 0.06595558673143387\n",
            "Train step - Step 1480, Loss 0.0707656592130661\n",
            "Train epoch - Accuracy: 0.8654545454545455 Loss: 0.07132082007449082 Corrects: 4284\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.05659304931759834\n",
            "Train step - Step 1500, Loss 0.06974741071462631\n",
            "Train step - Step 1510, Loss 0.0784800574183464\n",
            "Train step - Step 1520, Loss 0.06901217252016068\n",
            "Train epoch - Accuracy: 0.8672727272727273 Loss: 0.07315080337753199 Corrects: 4293\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.07148046046495438\n",
            "Train step - Step 1540, Loss 0.08015324175357819\n",
            "Train step - Step 1550, Loss 0.07519340515136719\n",
            "Train epoch - Accuracy: 0.8682828282828283 Loss: 0.06976768631826748 Corrects: 4298\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.061872996389865875\n",
            "Train step - Step 1570, Loss 0.08952672779560089\n",
            "Train step - Step 1580, Loss 0.0770217701792717\n",
            "Train step - Step 1590, Loss 0.06781128793954849\n",
            "Train epoch - Accuracy: 0.8719191919191919 Loss: 0.06902587638659911 Corrects: 4316\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.0734851062297821\n",
            "Train step - Step 1610, Loss 0.055662430822849274\n",
            "Train step - Step 1620, Loss 0.08112241327762604\n",
            "Train step - Step 1630, Loss 0.07834818959236145\n",
            "Train epoch - Accuracy: 0.8739393939393939 Loss: 0.06590250587222551 Corrects: 4326\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.05307411774992943\n",
            "Train step - Step 1650, Loss 0.04475534334778786\n",
            "Train step - Step 1660, Loss 0.06450628489255905\n",
            "Train step - Step 1670, Loss 0.06469791382551193\n",
            "Train epoch - Accuracy: 0.8838383838383839 Loss: 0.06405000432874217 Corrects: 4375\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.055307772010564804\n",
            "Train step - Step 1690, Loss 0.06018352508544922\n",
            "Train step - Step 1700, Loss 0.05738571286201477\n",
            "Train step - Step 1710, Loss 0.07564640045166016\n",
            "Train epoch - Accuracy: 0.883030303030303 Loss: 0.06291912011124871 Corrects: 4371\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.0579797700047493\n",
            "Train step - Step 1730, Loss 0.05599530413746834\n",
            "Train step - Step 1740, Loss 0.07791357487440109\n",
            "Train step - Step 1750, Loss 0.07570113986730576\n",
            "Train epoch - Accuracy: 0.8860606060606061 Loss: 0.06122048602411241 Corrects: 4386\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.046970587223768234\n",
            "Train step - Step 1770, Loss 0.05448814854025841\n",
            "Train step - Step 1780, Loss 0.0570836178958416\n",
            "Train step - Step 1790, Loss 0.052846431732177734\n",
            "Train epoch - Accuracy: 0.8921212121212121 Loss: 0.05948092536342264 Corrects: 4416\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.0464363619685173\n",
            "Train step - Step 1810, Loss 0.056273628026247025\n",
            "Train step - Step 1820, Loss 0.0658160001039505\n",
            "Train step - Step 1830, Loss 0.0690997764468193\n",
            "Train epoch - Accuracy: 0.8955555555555555 Loss: 0.05688923456151076 Corrects: 4433\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.059149112552404404\n",
            "Train step - Step 1850, Loss 0.05253974348306656\n",
            "Train step - Step 1860, Loss 0.07070215791463852\n",
            "Train step - Step 1870, Loss 0.05269395932555199\n",
            "Train epoch - Accuracy: 0.8927272727272727 Loss: 0.05812157820571553 Corrects: 4419\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.04611649736762047\n",
            "Train step - Step 1890, Loss 0.06521129608154297\n",
            "Train step - Step 1900, Loss 0.0597667470574379\n",
            "Train step - Step 1910, Loss 0.07057886570692062\n",
            "Train epoch - Accuracy: 0.9002020202020202 Loss: 0.05608734115506663 Corrects: 4456\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.042010191828012466\n",
            "Train step - Step 1930, Loss 0.043184537440538406\n",
            "Train step - Step 1940, Loss 0.0397011898458004\n",
            "Train epoch - Accuracy: 0.9242424242424242 Loss: 0.04325935698518849 Corrects: 4575\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.039822064340114594\n",
            "Train step - Step 1960, Loss 0.036546554416418076\n",
            "Train step - Step 1970, Loss 0.04635452851653099\n",
            "Train step - Step 1980, Loss 0.033635109663009644\n",
            "Train epoch - Accuracy: 0.9432323232323232 Loss: 0.037418713830035144 Corrects: 4669\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.03440694510936737\n",
            "Train step - Step 2000, Loss 0.04546314477920532\n",
            "Train step - Step 2010, Loss 0.03715860843658447\n",
            "Train step - Step 2020, Loss 0.050867773592472076\n",
            "Train epoch - Accuracy: 0.9412121212121212 Loss: 0.03600090804876703 Corrects: 4659\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.02928098477423191\n",
            "Train step - Step 2040, Loss 0.03901729732751846\n",
            "Train step - Step 2050, Loss 0.037106212228536606\n",
            "Train step - Step 2060, Loss 0.03669671341776848\n",
            "Train epoch - Accuracy: 0.9452525252525252 Loss: 0.03400145272564406 Corrects: 4679\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.0314776748418808\n",
            "Train step - Step 2080, Loss 0.031674575060606\n",
            "Train step - Step 2090, Loss 0.018039535731077194\n",
            "Train step - Step 2100, Loss 0.0348820686340332\n",
            "Train epoch - Accuracy: 0.9468686868686869 Loss: 0.033380321872354755 Corrects: 4687\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.02144073322415352\n",
            "Train step - Step 2120, Loss 0.029142146930098534\n",
            "Train step - Step 2130, Loss 0.03788841888308525\n",
            "Train step - Step 2140, Loss 0.032097745686769485\n",
            "Train epoch - Accuracy: 0.9496969696969697 Loss: 0.03260654991022264 Corrects: 4701\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.027982238680124283\n",
            "Train step - Step 2160, Loss 0.020777106285095215\n",
            "Train step - Step 2170, Loss 0.030742371454834938\n",
            "Train step - Step 2180, Loss 0.0357779823243618\n",
            "Train epoch - Accuracy: 0.9543434343434344 Loss: 0.03025592699345916 Corrects: 4724\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.031895961612463\n",
            "Train step - Step 2200, Loss 0.025660132989287376\n",
            "Train step - Step 2210, Loss 0.038197994232177734\n",
            "Train step - Step 2220, Loss 0.025359375402331352\n",
            "Train epoch - Accuracy: 0.9525252525252526 Loss: 0.03145596713730783 Corrects: 4715\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.03523494675755501\n",
            "Train step - Step 2240, Loss 0.02871292270720005\n",
            "Train step - Step 2250, Loss 0.028518855571746826\n",
            "Train step - Step 2260, Loss 0.028394445776939392\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.031200148419599342 Corrects: 4703\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.026044586673378944\n",
            "Train step - Step 2280, Loss 0.03078872337937355\n",
            "Train step - Step 2290, Loss 0.03193800523877144\n",
            "Train step - Step 2300, Loss 0.05611391365528107\n",
            "Train epoch - Accuracy: 0.9555555555555556 Loss: 0.030103796654277377 Corrects: 4730\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.02573627233505249\n",
            "Train step - Step 2320, Loss 0.026890838518738747\n",
            "Train step - Step 2330, Loss 0.03133704885840416\n",
            "Train epoch - Accuracy: 0.953939393939394 Loss: 0.0305143504032884 Corrects: 4722\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.029058242216706276\n",
            "Train step - Step 2350, Loss 0.02426040545105934\n",
            "Train step - Step 2360, Loss 0.03160322457551956\n",
            "Train step - Step 2370, Loss 0.04511835798621178\n",
            "Train epoch - Accuracy: 0.9563636363636364 Loss: 0.029337084354324774 Corrects: 4734\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.031107379123568535\n",
            "Train step - Step 2390, Loss 0.029924381524324417\n",
            "Train step - Step 2400, Loss 0.024590641260147095\n",
            "Train step - Step 2410, Loss 0.020830800756812096\n",
            "Train epoch - Accuracy: 0.961010101010101 Loss: 0.02768125159346094 Corrects: 4757\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.02306819148361683\n",
            "Train step - Step 2430, Loss 0.05020679160952568\n",
            "Train step - Step 2440, Loss 0.032019324600696564\n",
            "Train step - Step 2450, Loss 0.035698115825653076\n",
            "Train epoch - Accuracy: 0.9587878787878787 Loss: 0.028257839812172784 Corrects: 4746\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.03360297158360481\n",
            "Train step - Step 2470, Loss 0.02808537892997265\n",
            "Train step - Step 2480, Loss 0.01882505789399147\n",
            "Train step - Step 2490, Loss 0.01958288811147213\n",
            "Train epoch - Accuracy: 0.96 Loss: 0.026811572033347504 Corrects: 4752\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.024240998551249504\n",
            "Train step - Step 2510, Loss 0.015550089068710804\n",
            "Train step - Step 2520, Loss 0.01995779201388359\n",
            "Train step - Step 2530, Loss 0.02513287402689457\n",
            "Train epoch - Accuracy: 0.962020202020202 Loss: 0.025361865840174934 Corrects: 4762\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.021166345104575157\n",
            "Train step - Step 2550, Loss 0.018641367554664612\n",
            "Train step - Step 2560, Loss 0.021410910412669182\n",
            "Train step - Step 2570, Loss 0.03057834319770336\n",
            "Train epoch - Accuracy: 0.9646464646464646 Loss: 0.023534335130167127 Corrects: 4775\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.029833734035491943\n",
            "Train step - Step 2590, Loss 0.018363453447818756\n",
            "Train step - Step 2600, Loss 0.026480138301849365\n",
            "Train step - Step 2610, Loss 0.028178563341498375\n",
            "Train epoch - Accuracy: 0.9660606060606061 Loss: 0.024363452725187695 Corrects: 4782\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.0342327281832695\n",
            "Train step - Step 2630, Loss 0.02055048942565918\n",
            "Train step - Step 2640, Loss 0.020426509901881218\n",
            "Train step - Step 2650, Loss 0.021581510081887245\n",
            "Train epoch - Accuracy: 0.9648484848484848 Loss: 0.025252315486320343 Corrects: 4776\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.03008689358830452\n",
            "Train step - Step 2670, Loss 0.0230241846293211\n",
            "Train step - Step 2680, Loss 0.019735518842935562\n",
            "Train step - Step 2690, Loss 0.01372586376965046\n",
            "Train epoch - Accuracy: 0.9644444444444444 Loss: 0.024579011685769966 Corrects: 4774\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.02005911059677601\n",
            "Train step - Step 2710, Loss 0.023675382137298584\n",
            "Train step - Step 2720, Loss 0.02118927799165249\n",
            "Train epoch - Accuracy: 0.9634343434343434 Loss: 0.024567253339772274 Corrects: 4769\n",
            "Training finished in 203.4930591583252 seconds\n",
            "EVALUATION:  0.78 0.12797264754772186\n",
            "TEST GROUP:  0.836\n",
            "TEST ALL:  0.836\n",
            "GROUP:  2\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.5198947787284851\n",
            "Train step - Step 10, Loss 0.18119554221630096\n",
            "Train step - Step 20, Loss 0.1347174048423767\n",
            "Train step - Step 30, Loss 0.10982789844274521\n",
            "Train epoch - Accuracy: 0.36808080808080806 Loss: 0.17154767145111102 Corrects: 1822\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.09604242444038391\n",
            "Train step - Step 50, Loss 0.09236812591552734\n",
            "Train step - Step 60, Loss 0.08287163078784943\n",
            "Train step - Step 70, Loss 0.08627394586801529\n",
            "Train epoch - Accuracy: 0.6482828282828282 Loss: 0.08684955637563359 Corrects: 3209\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.08100271224975586\n",
            "Train step - Step 90, Loss 0.07603686302900314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-754cea9f07ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequentialLearningFineTuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_subsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-31e6b3e7dcf7>\u001b[0m in \u001b[0;36msequentialLearningFineTuning\u001b[0;34m(train_subsets, val_subsets, test_subsets)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSchedulerOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset learning rate and step_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes_seen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# Validate on current group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-20cff53f0192>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Bring images and labels to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aakVopSdyeGr"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ0VbtGQygRH"
      },
      "source": [
        "method = \"finetuning\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ-J8aC_xx-c"
      },
      "source": [
        "## LWF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZBNivOix1FB"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdfIllqBx59v"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkzFiDVDyAm_"
      },
      "source": [
        "### sequential learning LWF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf7REFkIyEiQ"
      },
      "source": [
        "### LWF\n",
        "def sequentialLearningLWF(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZTwTS_4TcES"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkrMQy2TuUAb"
      },
      "source": [
        "# train\n",
        "# net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningLWF(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2qUxVyMTfB4"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_4xLfwcpDz"
      },
      "source": [
        "method = \"Learning Without Forgetting\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGHBWaLNXGeI"
      },
      "source": [
        "\"\"\"num_classes_seen = 100\n",
        "dif_accuracies=printAccuracyDifference(net,old_accuracies, num_classes_seen)\n",
        "dif_accuracies\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjGRIqDtNQP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nX10znUi6qd"
      },
      "source": [
        "## iCaRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKBdrJ8Csy2a"
      },
      "source": [
        "### classify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAPTOU9bszTY"
      },
      "source": [
        "def classify(batch_img, net, exemplar_sets):\n",
        "  \"\"\" classify images by nearest mean-of-exemplars \"\"\" \n",
        "  \"\"\" \n",
        "  class1: list_of_indices for images that represent class1\n",
        "  class2: list_of_indices for images that represent class2\n",
        "  ...\n",
        "  class100: list_of_indices for images that represent class 100\n",
        "  with Subset I retrieve the images from the train_dataset\n",
        "  iterate over it to calculate mean for each class\n",
        "  \"\"\"\n",
        "  net.eval()\n",
        "  classes_mean = []\n",
        "  for k, exemplars_indices in exemplar_sets.items():\n",
        "    features = []\n",
        "    class_images_set = Subset(train_dataset, exemplars_indices)\n",
        "    class_images = DataLoader(class_images_set, batch_size=BATCH_SIZE, num_workers=2)\n",
        "    for _, images, labels in class_images:\n",
        "      # for each class (paper from y=1...t) calculate features and then mean\n",
        "      feature = net.forward(images)\n",
        "      features.append(feature)\n",
        "    features_s = torch.cat(features)\n",
        "    class_mean = features_s.mean(0)\n",
        "    classes_mean.append(class_mean)\n",
        "    means_exemplars = torch.cat(classes_mean, dim=0)\n",
        "    means_exemplars = torch.stack([means_exemplars] * BATCH_SIZE)\n",
        "    means_exemplars = means_exemplars.transpose(1,2)\n",
        "  feature_images_to_classify = net.forward(batch_img)\n",
        "  # sono da normalizzare?\n",
        "  feature_images_to_classify = feature_images_to_classify.unsqueeze(2)\n",
        "  feature_images_to_classify = feature_images_to_classify.expand_as(means_exemplars) # expand_as to get the same dimension\n",
        "  preds = torch.argmin((feature_images_to_classify - means_exemplars).pow(2).sum(1), dim=1)\n",
        "  return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5HRagcujHgM"
      },
      "source": [
        "### construct exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blwBqG6BjNsM"
      },
      "source": [
        "import sys\n",
        "\n",
        "def constructExemplarSet(net, Xclass, m):\n",
        "#Xclass contiene immagini e label della classe X\n",
        "  exemplars_set = []\n",
        "  feature_exemplars = []\n",
        "  indexes = []\n",
        "  features = [] \n",
        "  class_images = []\n",
        "  net.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    XtrainLoader = torch.utils.data.DataLoader(Xclass, shuffle = True, batch_size=1, num_workers=2)\n",
        "    for _, image, labels in XtrainLoader:\n",
        "      image = image.to(DEVICE)\n",
        "      class_images.append(image)\n",
        "      # per ogni immagine della classe x, prendiamo le rispettive feature e le uniamo nel vettore features, che contiene tutte quelle delle immagini della classe x\n",
        "      feature = net.forward(image)\n",
        "      #feature = net.predict(featuretmp)\n",
        "      feature = feature/np.linalg.norm(feature.cpu())\n",
        "      features.append(feature)\n",
        "\n",
        "    features = torch.cat(features, dim=0) #cat solve the problem of inequal size of tensors \n",
        "    current_class_mean = features.mean(0) # mu = media delle features delle immagini della classe\n",
        "\n",
        "    for k in range(1, m+1):\n",
        "      min = 100000\n",
        "      sum = 0\n",
        "      for j in range(k-1):\n",
        "        sum += feature_exemplars[j]\n",
        "      for x in range(len(Xclass)): \n",
        "        if (x not in indexes):\n",
        "          phiX = features[x]\n",
        "          val = current_class_mean - ((phiX + sum)/k)\n",
        "          val = np.linalg.norm(val.cpu().numpy()) ## NORMA \n",
        "          if (val < min):\n",
        "            min = val\n",
        "            feature_min = phiX\n",
        "            index_min = x\n",
        "      feature_exemplars.append(feature_min)\n",
        "      exemplars_set.append(Xclass[index_min][0])\n",
        "\n",
        "    print(\"lunghezza exemplar set: \", len(exemplars_set))\n",
        "    return exemplars_set"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy_HVL-tjZcb"
      },
      "source": [
        "### reduce exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7kuQ5DnjcCP"
      },
      "source": [
        "# DA CHIAMARE PER OGNI CLASSE k: riduce il numero di exemplars all'interno dell'exemplar_set della classe k \n",
        "# EXEMPLAR SET è una lista di indici che rappresentano la posizione dell'immagine selezionata per l'exemplar della classe corrente nel dataset di partenza\n",
        "def reduceExemplarSet(m, exemplars_set):\n",
        "  exemplars_new = []\n",
        "  for i in range(m):\n",
        "    if (exemplars_set != []):\n",
        "      exemplars_new.append(exemplars_set[i])\n",
        "\n",
        "  return exemplars_new"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMMlEpLsjfaj"
      },
      "source": [
        "### update representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRdA9RtDjhak"
      },
      "source": [
        "def updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot, old_net):\n",
        "  exemplars_subset = []\n",
        "  exemplars_indices = []\n",
        "  total_exemplars = []\n",
        "  labels_tot = []\n",
        "\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "    if (exemplar_set_class_k != []):\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "\n",
        "  if group_id > 1:\n",
        "    train_subset_total = torch.utils.data.ConcatDataset([train_subset, total_exemplars])\n",
        "  else:\n",
        "    train_subset_total = train_subset\n",
        "    \n",
        "  print(\"Len TOTAL train susbset: \", len(train_subset_total))\n",
        "  # train_loader is the concatenation of new images and exemplars of old classes\n",
        "  train_loader = torch.utils.data.DataLoader(train_subset_total, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  print(\"training\")\n",
        "  train(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)\n",
        "  #trainWithOtherLosses(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)\n",
        "  #trainCEandL1(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac65-6QGjm9O"
      },
      "source": [
        "### incremental train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPU_32ArjpPH"
      },
      "source": [
        "import copy\n",
        "def incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_until_now):\n",
        "  print(\"Starting the update representation\")\n",
        "  exemplar_indices = None\n",
        "  num_classes = 10\n",
        "  new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "  print(\"NEW CLASSES: \", new_classes_examined)\n",
        "\n",
        "  updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net)\n",
        "\n",
        "  iteration = group_id - 1\n",
        "  # num_classes received till now\n",
        "  t = (num_classes * iteration) + num_classes  \n",
        "  m = int(K/t)\n",
        "\n",
        "  # REDUCING EXEMPLAR SET FOR EXISTING CLASSES\n",
        "  print(\"reducing exemplars for each class\")\n",
        "  print(total_classes_until_now)\n",
        "  for y in total_classes_until_now: \n",
        "    exemplar_y_new = reduceExemplarSet(m, exemplars_set_tot[y])\n",
        "    print(\"REDUCED EXEMPLAR: \", len(exemplar_y_new))\n",
        "    exemplars_set_tot[y] = exemplar_y_new\n",
        "\n",
        "  \n",
        "  # CONSTRUCTION EXEMPLAR SET FOR NEW CLASSES\n",
        "  for y in new_classes_examined:\n",
        "    images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data']\n",
        "    imgs_idxs = images_current_class.index \n",
        "    class_train_subset = Subset(train_dataset, imgs_idxs)\n",
        "    print(\"class train: \", class_train_subset)\n",
        "    print(\"Constructing exemplars of class\", y)\n",
        "    # exemplar set is a set of indexes \n",
        "    exemplars_set = constructExemplarSet(net, class_train_subset, m) \n",
        "    exemplars_set_tot[y] = exemplars_set\n",
        "    print(\"exemplar set: \", exemplars_set)\n",
        "\n",
        "  # IMPLEMENTATION 'END-to-END Incremental Learning' PAPER\n",
        "  #balancedFinetune(net, group_id, exemplars_set_tot, NUM_EPOCHS_FINETUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDxmsQlWjp5D"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dQznGoEju9J"
      },
      "source": [
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes_till_now, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    print(\"num classes till now: \", num_classes_till_now)\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes_till_now, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            #if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcZUqVCjeMG4"
      },
      "source": [
        "### Train con CE + L1Loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl4zFMYGtAEJ"
      },
      "source": [
        "import copy\n",
        "def trainCEandL1(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    distillation_loss = 0\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "            # Classification LOSS\n",
        "            lr = 0.01\n",
        "            classification_loss = nn.CrossEntropyLoss()(outputs, labels) # BCE\n",
        "\n",
        "            # Distillation LOSS \n",
        "            if (group_id > 1):\n",
        "               old_features = old_net.forward(images)\n",
        "               old_outputs = old_net.predict(old_features)\n",
        "               #labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "               new_labels = torch.sigmoid(old_outputs)\n",
        "               new_outputs = outputs[:, 0:num_classes_till_previous_step]\n",
        "               distillation_loss = l1Loss(new_outputs, new_labels) # L1\n",
        "               print(distillation_loss)\n",
        "\n",
        "            loss = classification_loss + distillation_loss\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrbPsdT2eRho"
      },
      "source": [
        "### Train con CE + KLDiv Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzTrBE-Dea1S"
      },
      "source": [
        "import copy\n",
        "def trainWithOtherLosses(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    distillation_loss = 0\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "            # Classification LOSS\n",
        "            classification_loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "            # Distillation LOSS \n",
        "            if (group_id > 1):\n",
        "               old_features = old_net.forward(images)\n",
        "               old_outputs = old_net.predict(old_features)\n",
        "               T = 2\n",
        "               beta = 0.25\n",
        "               distillation_loss = nn.KLDivLoss()(F.log_softmax(outputs[:, 0:num_classes_till_previous_step]/T, dim = 1), F.softmax(old_outputs.detach()/T, dim = 1)) * T * T * beta * num_classes_till_previous_step\n",
        "\n",
        "            loss = classification_loss + distillation_loss\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHpi410ljzWD"
      },
      "source": [
        "### test and validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQYB0-cLj0sP"
      },
      "source": [
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        #_, preds = classify(images, )\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "246M7j-s4Chq"
      },
      "source": [
        "### Balanced Finetune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtFJY_fB5ymS"
      },
      "source": [
        "def data_augmentation_e2e(img, lab):\n",
        "    \"\"\"\n",
        "        Realize the data augmentation in End-to-End paper\n",
        "        Parameters\n",
        "        ----------\n",
        "        img: the original images, size = (n, c, w, h)\n",
        "        lab: the original labels, size = (n)\n",
        "        Returns\n",
        "        ----------\n",
        "        img_aug: the original images, size = (n * 12, c, w, h)\n",
        "        lab_aug: the original labels, size = (n * 12)\n",
        "    \"\"\"\n",
        "    \n",
        "    shape = np.shape(img)\n",
        "    # print(\"IMG is: \",img)\n",
        "    # print(shape[0], 1, shape[1], shape[2], shape[3])\n",
        "    img_aug = np.zeros((shape[0], 6, shape[1], shape[2], shape[3]))\n",
        "    img_aug[:, 0, :, :, :] = img\n",
        "    lab_aug = np.zeros((shape[0], 6))\n",
        "    # print(\"IMG_AUG is: \", img_aug)\n",
        "\n",
        "    for i in range(shape[0]):\n",
        "        # np.random.seed(int(time.time()) % 1000)\n",
        "\n",
        "        # convert image from tensor to numpy\n",
        "        image=img.numpy()\n",
        "        im = image[i]\n",
        "      \n",
        "        # # brightness\n",
        "        brightness = (np.random.rand(1)-0.5)*2*63\n",
        "        im_temp = im + brightness\n",
        "\n",
        "        img_aug[i, 1] = im_temp\n",
        "\n",
        "\n",
        "        # constrast\n",
        "        constrast = (np.random.rand(1)-0.5)*2*0.8+1\n",
        "        m0 = np.mean(im[0])\n",
        "        m1 = np.mean(im[1])\n",
        "        m2 = np.mean(im[2])\n",
        "        im_temp = im\n",
        "        im_temp[0] = (im_temp[0]-m0)*constrast + m0\n",
        "        im_temp[1] = (im_temp[1]-m1)*constrast + m1\n",
        "        im_temp[2] = (im_temp[2]-m2)*constrast + m2\n",
        "        img_aug[i, 2] = im_temp\n",
        "\n",
        "        # crop\n",
        "        im_temp = img_aug[i, :3]\n",
        "        for j in range(3):\n",
        "            x_ = int(np.random.rand(1)*1000)%8\n",
        "            y_ = int(np.random.rand(1)*1000)%8\n",
        "            im_temp = np.zeros(shape=(shape[1], shape[2]+8, shape[3]+8))\n",
        "            im_temp[:, 4:-4, 4:-4] = img_aug[i, j]\n",
        "            img_aug[i, 3+j] = im_temp[:, x_:x_+shape[2], y_:y_+shape[3]]\n",
        "\n",
        "\n",
        "\n",
        "        # mirror\n",
        "        # for j in range(6):\n",
        "        #     im_temp = img_aug[i, j]\n",
        "        #     img_aug[i, 6 + j] = im_temp[:,-1::-1,:]\n",
        "\n",
        "        lab_aug[i, :] = lab[i]\n",
        "\n",
        "    # idx = np.where(img_aug>255)\n",
        "    # img_aug[idx] = 255\n",
        "    # idx = np.where(img_aug<0)\n",
        "    # img_aug[idx] = 0\n",
        "\n",
        "    img_aug = np.reshape(img_aug, newshape=(shape[0]*6, shape[1], shape[2], shape[3]))\n",
        "    img_aug = np.array(img_aug, dtype=np.float64)\n",
        "    lab_aug = np.reshape(lab_aug, newshape=(shape[0]*6))\n",
        "    lab_aug = np.array(lab_aug, dtype=np.float64)\n",
        "    return img_aug, lab_aug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC_zGBOB4B4J"
      },
      "source": [
        "def balancedFinetune(net, iteration, exemplars_set_tot, NUM_EPOCHS):\n",
        "  num_classes_till_previous_step = iteration * 10 - 10\n",
        "  num_classes = iteration * 10\n",
        "  total_exemplars = []\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  old_net = copy.deepcopy(net)\n",
        "  old_net.eval()\n",
        "\n",
        "  #if iteration > 1:\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    if (exemplar_set_class_k != []):\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "    \n",
        "  reduced_train_loader = torch.utils.data.DataLoader(total_exemplars, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  # reduced train loader is the sets of all exemplars of new and old classes\n",
        "\n",
        "  # finetune\n",
        "  lrc = LR *0.1 # small learning rate for finetune\n",
        "  print('current lr = %f' % (lrc))\n",
        "  softmax = nn.Softmax(dim=-1).cuda()\n",
        "  current_step = 0\n",
        "  acc_finetune_train = []\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS_FINETUNE):\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for _, images, labels in reduced_train_loader:\n",
        "      images, labels = data_augmentation_e2e(images,labels) \n",
        "      images = torch.from_numpy(images) \n",
        "      labels = torch.from_numpy(labels) \n",
        "    # reduced_train_loader contains the same number of images for both new classes and old classes\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "      optimizer = torch.optim.SGD(net.parameters(), lr=lrc, momentum=MOMENTUM,\n",
        "                                    weight_decay= WEIGHT_DECAY, nesterov=True)\n",
        "\n",
        "\n",
        "      # print(\"Outside: input size\", img.size(), \"output_size\", lab.size())\n",
        "      features = net.forward(images)\n",
        "      outputs = net.predict(features)\n",
        "\n",
        "      # classification loss\n",
        "      prob_cls = softmax(outputs)\n",
        "      labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "      labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "    # distillation loss for all classes (maybe the author only distillates for novel classes)\n",
        "      if iteration > 1:\n",
        "        old_features = old_net.forward(images)\n",
        "        old_outputs = old_net.predict(old_features)\n",
        "        labels_enc[:, 0:num_classes_till_previous_step] = torch.sigmoid(old_outputs[:, 0:num_classes_till_previous_step])\n",
        "\n",
        "\n",
        "      loss = computeLoss(criterion, outputs, labels_enc)\n",
        "\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "          \n",
        "    # Update Corrects & Loss\n",
        "      running_loss += loss.item() * images.size(0)\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Log loss\n",
        "      if current_step % LOG_FREQUENCY == 0:\n",
        "        print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss = running_loss / float(len(reduced_train_loader.dataset)*6)\n",
        "      epoch_acc = running_corrects / float(len(reduced_train_loader.dataset)*6)\n",
        "      \n",
        "    print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T96WcHWlIYGR"
      },
      "source": [
        "### CLASSIFIERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZTGxaotIXBI"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def classifierTrain(net, exemplars_set_tot):\n",
        "  torch.no_grad()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  X_train, y_train = [], []\n",
        "  total_exemplars = []\n",
        "  counter = 0\n",
        "\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "    if (exemplar_set_class_k != []):\n",
        "      counter += 1\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "\n",
        "\n",
        "  exemplar_sets = torch.utils.data.DataLoader(total_exemplars, shuffle = True, batch_size=1, num_workers=2)\n",
        "  for _, exemplar, label in exemplar_sets:\n",
        "    exemplar = exemplar.to(DEVICE)\n",
        "    features = net.forward(exemplar)\n",
        "    outputs = net.predict(features)\n",
        "    outputs = outputs.squeeze()\n",
        "    outputs.data = outputs.data / outputs.data.norm()\n",
        "    X_train.append(outputs.cpu().detach().numpy())\n",
        "    y_train.append(label)\n",
        "\n",
        "  K_nn = math.ceil(2000/counter)\n",
        "  model1 = LinearSVC()\n",
        "  model2 = KNeighborsClassifier(n_neighbors = K_nn)\n",
        "  print(\"x train: \", X_train[0])\n",
        "  print(\"y_train: \", y_train)\n",
        "  model = model1.fit(X_train, y_train)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYs6xKy1J-pQ"
      },
      "source": [
        "def classifySVM_KNN(net, images, model):\n",
        "  torch.no_grad()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  X_pred = []\n",
        "  images = images.to(DEVICE)\n",
        "  features = net.forward(images)\n",
        "  outputs = net.predict(features)\n",
        "\n",
        "  for feature in outputs:\n",
        "    feature = feature.squeeze()\n",
        "    feature.data = feature.data / feature.data.norm()\n",
        "    X_pred.append(feature.cpu().detach().numpy())\n",
        "\n",
        "  preds = model.predict(X_pred)\n",
        "  return torch.tensor(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7WwJLGnJcXC"
      },
      "source": [
        "def validateSVM_KNN(net, val_dataloader, criterion, num_classes, model):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        #images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        preds = classifySVM_KNN(net, images, model)\n",
        "        preds = preds.to(DEVICE)\n",
        "\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def testSVM_KNN(net, test_dataloader, num_classes, model):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validateSVM_KNN(net, test_dataloader, None, num_classes, model)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTv6zBetj3Cb"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_cnELYdkap4"
      },
      "source": [
        "def sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 10\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      print(\"TEST SET LENGHT: \", len(test_set))\n",
        "      print(\"TEST CURRENT GROUP SET LENGHT: \", len(test_subset))\n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      test_classes = list(test_dataset.df.loc[test_set.indices, 'labels'].value_counts().index)\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TEST_SET CLASSES: \", test_classes)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen) # Train the network with 10 classes at a time\n",
        "      model = classifierTrain(net, exemplars_set_tot)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      #acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      acc, loss, _, _ = validateSVM_KNN(net, val_loader, criterion, num_classes_seen, model)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      #acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      acc_group, _, _ = testSVM_KNN(net, test_group_loader, num_classes_seen, model)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      #acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      acc_all, all_preds_cm, all_labels_cm = testSVM_KNN(net, test_loader, num_classes_seen, model)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTV1Gxq6yF5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f667f4f-d602-4022-e835-150e18af6cfd"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  1000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 86, 82, 78, 69, 57, 50, 30, 27, 25]\n",
            "TRAIN_SET CLASSES:  [27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "VALIDATION CLASSES:  [57, 50, 97, 30, 27, 25, 86, 82, 78, 69]\n",
            "GROUP:  1\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "Len TOTAL train susbset:  4950\n",
            "training\n",
            "num classes till now:  10\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.6801885962486267\n",
            "Train step - Step 10, Loss 0.31596842408180237\n",
            "Train step - Step 20, Loss 0.2731570303440094\n",
            "Train step - Step 30, Loss 0.2711406648159027\n",
            "Train epoch - Accuracy: 0.32606060606060605 Loss: 0.329973349992675 Corrects: 1614\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.2539721429347992\n",
            "Train step - Step 50, Loss 0.2279220074415207\n",
            "Train step - Step 60, Loss 0.23143987357616425\n",
            "Train step - Step 70, Loss 0.22050456702709198\n",
            "Train epoch - Accuracy: 0.4628282828282828 Loss: 0.23180835402975178 Corrects: 2291\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.21377062797546387\n",
            "Train step - Step 90, Loss 0.20685572922229767\n",
            "Train step - Step 100, Loss 0.21114130318164825\n",
            "Train step - Step 110, Loss 0.218997523188591\n",
            "Train epoch - Accuracy: 0.5226262626262627 Loss: 0.2140810085667504 Corrects: 2587\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.21719515323638916\n",
            "Train step - Step 130, Loss 0.18628831207752228\n",
            "Train step - Step 140, Loss 0.20993471145629883\n",
            "Train step - Step 150, Loss 0.18042322993278503\n",
            "Train epoch - Accuracy: 0.561010101010101 Loss: 0.20115475563689916 Corrects: 2777\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.22087962925434113\n",
            "Train step - Step 170, Loss 0.18273575603961945\n",
            "Train step - Step 180, Loss 0.20491765439510345\n",
            "Train step - Step 190, Loss 0.1794435828924179\n",
            "Train epoch - Accuracy: 0.5747474747474748 Loss: 0.19426715459486452 Corrects: 2845\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.19612903892993927\n",
            "Train step - Step 210, Loss 0.21881070733070374\n",
            "Train step - Step 220, Loss 0.1859501451253891\n",
            "Train step - Step 230, Loss 0.1871321201324463\n",
            "Train epoch - Accuracy: 0.5961616161616161 Loss: 0.18830902715524037 Corrects: 2951\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.18181344866752625\n",
            "Train step - Step 250, Loss 0.17936241626739502\n",
            "Train step - Step 260, Loss 0.19067373871803284\n",
            "Train step - Step 270, Loss 0.2004012167453766\n",
            "Train epoch - Accuracy: 0.6070707070707071 Loss: 0.18266964852809905 Corrects: 3005\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.177969828248024\n",
            "Train step - Step 290, Loss 0.14841704070568085\n",
            "Train step - Step 300, Loss 0.19109229743480682\n",
            "Train step - Step 310, Loss 0.18652692437171936\n",
            "Train epoch - Accuracy: 0.6218181818181818 Loss: 0.17710750207756504 Corrects: 3078\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.1752675622701645\n",
            "Train step - Step 330, Loss 0.1941029131412506\n",
            "Train step - Step 340, Loss 0.17527244985103607\n",
            "Train step - Step 350, Loss 0.14842455089092255\n",
            "Train epoch - Accuracy: 0.6363636363636364 Loss: 0.17002986759248406 Corrects: 3150\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.16549070179462433\n",
            "Train step - Step 370, Loss 0.17320922017097473\n",
            "Train step - Step 380, Loss 0.1689566820859909\n",
            "Train epoch - Accuracy: 0.6531313131313131 Loss: 0.16659907465029244 Corrects: 3233\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.14423613250255585\n",
            "Train step - Step 400, Loss 0.134902685880661\n",
            "Train step - Step 410, Loss 0.15653492510318756\n",
            "Train step - Step 420, Loss 0.1572187840938568\n",
            "Train epoch - Accuracy: 0.656969696969697 Loss: 0.16027003822302577 Corrects: 3252\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.15987712144851685\n",
            "Train step - Step 440, Loss 0.17122653126716614\n",
            "Train step - Step 450, Loss 0.15857088565826416\n",
            "Train step - Step 460, Loss 0.13228221237659454\n",
            "Train epoch - Accuracy: 0.6705050505050505 Loss: 0.15851967622535398 Corrects: 3319\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.15999342501163483\n",
            "Train step - Step 480, Loss 0.11872871965169907\n",
            "Train step - Step 490, Loss 0.16088657081127167\n",
            "Train step - Step 500, Loss 0.157939612865448\n",
            "Train epoch - Accuracy: 0.6882828282828283 Loss: 0.15269361883702903 Corrects: 3407\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.1439782828092575\n",
            "Train step - Step 520, Loss 0.15573200583457947\n",
            "Train step - Step 530, Loss 0.1700887233018875\n",
            "Train step - Step 540, Loss 0.14948436617851257\n",
            "Train epoch - Accuracy: 0.6903030303030303 Loss: 0.1483452897722071 Corrects: 3417\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.15274588763713837\n",
            "Train step - Step 560, Loss 0.14973624050617218\n",
            "Train step - Step 570, Loss 0.1405818909406662\n",
            "Train step - Step 580, Loss 0.15431532263755798\n",
            "Train epoch - Accuracy: 0.7014141414141414 Loss: 0.14633188253701335 Corrects: 3472\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.1661142259836197\n",
            "Train step - Step 600, Loss 0.12461123615503311\n",
            "Train step - Step 610, Loss 0.1441589593887329\n",
            "Train step - Step 620, Loss 0.1398155838251114\n",
            "Train epoch - Accuracy: 0.7143434343434344 Loss: 0.1423917437743659 Corrects: 3536\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.16444554924964905\n",
            "Train step - Step 640, Loss 0.14363978803157806\n",
            "Train step - Step 650, Loss 0.1277310848236084\n",
            "Train step - Step 660, Loss 0.1338883340358734\n",
            "Train epoch - Accuracy: 0.7157575757575757 Loss: 0.1380305762905063 Corrects: 3543\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.13400594890117645\n",
            "Train step - Step 680, Loss 0.1329967975616455\n",
            "Train step - Step 690, Loss 0.15841792523860931\n",
            "Train step - Step 700, Loss 0.13776440918445587\n",
            "Train epoch - Accuracy: 0.7165656565656565 Loss: 0.13903051299278182 Corrects: 3547\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.12796147167682648\n",
            "Train step - Step 720, Loss 0.1258554309606552\n",
            "Train step - Step 730, Loss 0.16898487508296967\n",
            "Train step - Step 740, Loss 0.1332443803548813\n",
            "Train epoch - Accuracy: 0.7341414141414141 Loss: 0.1324704607387986 Corrects: 3634\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.11676754802465439\n",
            "Train step - Step 760, Loss 0.1355283111333847\n",
            "Train step - Step 770, Loss 0.15644250810146332\n",
            "Train epoch - Accuracy: 0.743030303030303 Loss: 0.1291443628012532 Corrects: 3678\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.12238788604736328\n",
            "Train step - Step 790, Loss 0.13207300007343292\n",
            "Train step - Step 800, Loss 0.10969586670398712\n",
            "Train step - Step 810, Loss 0.1393928825855255\n",
            "Train epoch - Accuracy: 0.7496969696969698 Loss: 0.12452204399337673 Corrects: 3711\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.12576986849308014\n",
            "Train step - Step 830, Loss 0.12391433864831924\n",
            "Train step - Step 840, Loss 0.10678648948669434\n",
            "Train step - Step 850, Loss 0.131744846701622\n",
            "Train epoch - Accuracy: 0.7507070707070707 Loss: 0.1241225717404876 Corrects: 3716\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.1098310723900795\n",
            "Train step - Step 870, Loss 0.14549683034420013\n",
            "Train step - Step 880, Loss 0.12707987427711487\n",
            "Train step - Step 890, Loss 0.13417749106884003\n",
            "Train epoch - Accuracy: 0.7616161616161616 Loss: 0.12072531962635541 Corrects: 3770\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.11482100933790207\n",
            "Train step - Step 910, Loss 0.14926491677761078\n",
            "Train step - Step 920, Loss 0.11061177402734756\n",
            "Train step - Step 930, Loss 0.10653563588857651\n",
            "Train epoch - Accuracy: 0.7717171717171717 Loss: 0.11582399561549678 Corrects: 3820\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.09030082076787949\n",
            "Train step - Step 950, Loss 0.10036599636077881\n",
            "Train step - Step 960, Loss 0.11583433300256729\n",
            "Train step - Step 970, Loss 0.1142544150352478\n",
            "Train epoch - Accuracy: 0.7709090909090909 Loss: 0.11373101772081973 Corrects: 3816\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.11963369697332382\n",
            "Train step - Step 990, Loss 0.11331479996442795\n",
            "Train step - Step 1000, Loss 0.10716702789068222\n",
            "Train step - Step 1010, Loss 0.10718295723199844\n",
            "Train epoch - Accuracy: 0.7822222222222223 Loss: 0.11119616711380506 Corrects: 3872\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.12028782814741135\n",
            "Train step - Step 1030, Loss 0.11975691467523575\n",
            "Train step - Step 1040, Loss 0.12254001945257187\n",
            "Train step - Step 1050, Loss 0.11107625812292099\n",
            "Train epoch - Accuracy: 0.7840404040404041 Loss: 0.11038017975260513 Corrects: 3881\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.09505651146173477\n",
            "Train step - Step 1070, Loss 0.10531965643167496\n",
            "Train step - Step 1080, Loss 0.10866502672433853\n",
            "Train step - Step 1090, Loss 0.11696965992450714\n",
            "Train epoch - Accuracy: 0.7903030303030303 Loss: 0.10805643119595268 Corrects: 3912\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.09771980345249176\n",
            "Train step - Step 1110, Loss 0.12008290737867355\n",
            "Train step - Step 1120, Loss 0.11248107254505157\n",
            "Train step - Step 1130, Loss 0.09199295938014984\n",
            "Train epoch - Accuracy: 0.7872727272727272 Loss: 0.10709001317770794 Corrects: 3897\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.10482148081064224\n",
            "Train step - Step 1150, Loss 0.08934979885816574\n",
            "Train step - Step 1160, Loss 0.09189499914646149\n",
            "Train epoch - Accuracy: 0.8054545454545454 Loss: 0.10113225839053741 Corrects: 3987\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.08463390171527863\n",
            "Train step - Step 1180, Loss 0.10870154201984406\n",
            "Train step - Step 1190, Loss 0.09779161214828491\n",
            "Train step - Step 1200, Loss 0.11580055207014084\n",
            "Train epoch - Accuracy: 0.8036363636363636 Loss: 0.09974132014043403 Corrects: 3978\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.08904778957366943\n",
            "Train step - Step 1220, Loss 0.10474862158298492\n",
            "Train step - Step 1230, Loss 0.08565967530012131\n",
            "Train step - Step 1240, Loss 0.09429839998483658\n",
            "Train epoch - Accuracy: 0.8121212121212121 Loss: 0.09718191007171015 Corrects: 4020\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.07452838867902756\n",
            "Train step - Step 1260, Loss 0.08387517184019089\n",
            "Train step - Step 1270, Loss 0.10280704498291016\n",
            "Train step - Step 1280, Loss 0.09020688384771347\n",
            "Train epoch - Accuracy: 0.817979797979798 Loss: 0.09505710119550878 Corrects: 4049\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.09348670393228531\n",
            "Train step - Step 1300, Loss 0.0966816172003746\n",
            "Train step - Step 1310, Loss 0.09426916390657425\n",
            "Train step - Step 1320, Loss 0.07322331517934799\n",
            "Train epoch - Accuracy: 0.8224242424242424 Loss: 0.09241034232004725 Corrects: 4071\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.09675595909357071\n",
            "Train step - Step 1340, Loss 0.10219261795282364\n",
            "Train step - Step 1350, Loss 0.08873657137155533\n",
            "Train step - Step 1360, Loss 0.09818710386753082\n",
            "Train epoch - Accuracy: 0.826060606060606 Loss: 0.09074502785699536 Corrects: 4089\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.10465703159570694\n",
            "Train step - Step 1380, Loss 0.0732247605919838\n",
            "Train step - Step 1390, Loss 0.08496981114149094\n",
            "Train step - Step 1400, Loss 0.08410823345184326\n",
            "Train epoch - Accuracy: 0.8331313131313132 Loss: 0.08770237013848141 Corrects: 4124\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.09651598334312439\n",
            "Train step - Step 1420, Loss 0.09794630110263824\n",
            "Train step - Step 1430, Loss 0.08567681908607483\n",
            "Train step - Step 1440, Loss 0.11767764389514923\n",
            "Train epoch - Accuracy: 0.8347474747474748 Loss: 0.08656945762008128 Corrects: 4132\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.06170625612139702\n",
            "Train step - Step 1460, Loss 0.0803455039858818\n",
            "Train step - Step 1470, Loss 0.11235398054122925\n",
            "Train step - Step 1480, Loss 0.1000329777598381\n",
            "Train epoch - Accuracy: 0.8385858585858585 Loss: 0.0848502096262845 Corrects: 4151\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.09291418641805649\n",
            "Train step - Step 1500, Loss 0.08967498689889908\n",
            "Train step - Step 1510, Loss 0.10292232036590576\n",
            "Train step - Step 1520, Loss 0.08340246230363846\n",
            "Train epoch - Accuracy: 0.8446464646464646 Loss: 0.08444140704894307 Corrects: 4181\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.07348300516605377\n",
            "Train step - Step 1540, Loss 0.09858930110931396\n",
            "Train step - Step 1550, Loss 0.07685663551092148\n",
            "Train epoch - Accuracy: 0.8381818181818181 Loss: 0.08384637111666227 Corrects: 4149\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.06541422754526138\n",
            "Train step - Step 1570, Loss 0.0901155099272728\n",
            "Train step - Step 1580, Loss 0.0884426087141037\n",
            "Train step - Step 1590, Loss 0.07603722810745239\n",
            "Train epoch - Accuracy: 0.8494949494949495 Loss: 0.0779907303777608 Corrects: 4205\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.07098301500082016\n",
            "Train step - Step 1610, Loss 0.08132288604974747\n",
            "Train step - Step 1620, Loss 0.07077030092477798\n",
            "Train step - Step 1630, Loss 0.06654886901378632\n",
            "Train epoch - Accuracy: 0.8478787878787879 Loss: 0.07805610999314472 Corrects: 4197\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.08779653161764145\n",
            "Train step - Step 1650, Loss 0.08308717608451843\n",
            "Train step - Step 1660, Loss 0.09248536825180054\n",
            "Train step - Step 1670, Loss 0.08948539942502975\n",
            "Train epoch - Accuracy: 0.857979797979798 Loss: 0.07542225102583568 Corrects: 4247\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.06140752509236336\n",
            "Train step - Step 1690, Loss 0.0744214877486229\n",
            "Train step - Step 1700, Loss 0.0788266733288765\n",
            "Train step - Step 1710, Loss 0.07667946070432663\n",
            "Train epoch - Accuracy: 0.8591919191919192 Loss: 0.07288990187524545 Corrects: 4253\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.0615917444229126\n",
            "Train step - Step 1730, Loss 0.07789044827222824\n",
            "Train step - Step 1740, Loss 0.055745627731084824\n",
            "Train step - Step 1750, Loss 0.06391704082489014\n",
            "Train epoch - Accuracy: 0.8626262626262626 Loss: 0.07362030220152152 Corrects: 4270\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.06825561076402664\n",
            "Train step - Step 1770, Loss 0.046947479248046875\n",
            "Train step - Step 1780, Loss 0.09384144842624664\n",
            "Train step - Step 1790, Loss 0.06786467134952545\n",
            "Train epoch - Accuracy: 0.8703030303030304 Loss: 0.07108488432686738 Corrects: 4308\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.0781230702996254\n",
            "Train step - Step 1810, Loss 0.07139875739812851\n",
            "Train step - Step 1820, Loss 0.06408446282148361\n",
            "Train step - Step 1830, Loss 0.08346384018659592\n",
            "Train epoch - Accuracy: 0.8684848484848485 Loss: 0.07167251124526515 Corrects: 4299\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.06950754672288895\n",
            "Train step - Step 1850, Loss 0.09237580001354218\n",
            "Train step - Step 1860, Loss 0.07034152001142502\n",
            "Train step - Step 1870, Loss 0.06451306492090225\n",
            "Train epoch - Accuracy: 0.8765656565656565 Loss: 0.06696742118910105 Corrects: 4339\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.05532893165946007\n",
            "Train step - Step 1890, Loss 0.06110068038105965\n",
            "Train step - Step 1900, Loss 0.05307018384337425\n",
            "Train step - Step 1910, Loss 0.06369353830814362\n",
            "Train epoch - Accuracy: 0.8852525252525253 Loss: 0.06329396832470942 Corrects: 4382\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.05373300239443779\n",
            "Train step - Step 1930, Loss 0.0678364634513855\n",
            "Train step - Step 1940, Loss 0.06437301635742188\n",
            "Train epoch - Accuracy: 0.9056565656565656 Loss: 0.053406230236845785 Corrects: 4483\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.05193695053458214\n",
            "Train step - Step 1960, Loss 0.03764570876955986\n",
            "Train step - Step 1970, Loss 0.049049653112888336\n",
            "Train step - Step 1980, Loss 0.06239850074052811\n",
            "Train epoch - Accuracy: 0.9204040404040404 Loss: 0.04694935512512621 Corrects: 4556\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.02669139765202999\n",
            "Train step - Step 2000, Loss 0.047059182077646255\n",
            "Train step - Step 2010, Loss 0.03976645693182945\n",
            "Train step - Step 2020, Loss 0.0529276505112648\n",
            "Train epoch - Accuracy: 0.9272727272727272 Loss: 0.043594326899208204 Corrects: 4590\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.04145532473921776\n",
            "Train step - Step 2040, Loss 0.03855226933956146\n",
            "Train step - Step 2050, Loss 0.03469913825392723\n",
            "Train step - Step 2060, Loss 0.05890791490674019\n",
            "Train epoch - Accuracy: 0.9272727272727272 Loss: 0.042767652399612194 Corrects: 4590\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.034095119684934616\n",
            "Train step - Step 2080, Loss 0.04942398890852928\n",
            "Train step - Step 2090, Loss 0.03676915541291237\n",
            "Train step - Step 2100, Loss 0.04602118581533432\n",
            "Train epoch - Accuracy: 0.9286868686868687 Loss: 0.041698502272066444 Corrects: 4597\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.0503293052315712\n",
            "Train step - Step 2120, Loss 0.0491487942636013\n",
            "Train step - Step 2130, Loss 0.043297238647937775\n",
            "Train step - Step 2140, Loss 0.03433180972933769\n",
            "Train epoch - Accuracy: 0.9323232323232323 Loss: 0.04082575900386078 Corrects: 4615\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.03644726797938347\n",
            "Train step - Step 2160, Loss 0.04270150884985924\n",
            "Train step - Step 2170, Loss 0.03621853515505791\n",
            "Train step - Step 2180, Loss 0.04602242261171341\n",
            "Train epoch - Accuracy: 0.9345454545454546 Loss: 0.039539663969266295 Corrects: 4626\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.052423980087041855\n",
            "Train step - Step 2200, Loss 0.04297227784991264\n",
            "Train step - Step 2210, Loss 0.03800292685627937\n",
            "Train step - Step 2220, Loss 0.04821097478270531\n",
            "Train epoch - Accuracy: 0.9343434343434344 Loss: 0.03884252425697115 Corrects: 4625\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.033987388014793396\n",
            "Train step - Step 2240, Loss 0.03644297644495964\n",
            "Train step - Step 2250, Loss 0.043219175189733505\n",
            "Train step - Step 2260, Loss 0.04017328470945358\n",
            "Train epoch - Accuracy: 0.9345454545454546 Loss: 0.03834717375612018 Corrects: 4626\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.04067723825573921\n",
            "Train step - Step 2280, Loss 0.03248061612248421\n",
            "Train step - Step 2290, Loss 0.03619120642542839\n",
            "Train step - Step 2300, Loss 0.037205666303634644\n",
            "Train epoch - Accuracy: 0.9371717171717172 Loss: 0.038058251383328676 Corrects: 4639\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.0396200567483902\n",
            "Train step - Step 2320, Loss 0.03423497825860977\n",
            "Train step - Step 2330, Loss 0.03277399763464928\n",
            "Train epoch - Accuracy: 0.938989898989899 Loss: 0.03763099920418527 Corrects: 4648\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.04044891521334648\n",
            "Train step - Step 2350, Loss 0.024458957836031914\n",
            "Train step - Step 2360, Loss 0.0401582308113575\n",
            "Train step - Step 2370, Loss 0.03321150690317154\n",
            "Train epoch - Accuracy: 0.9422222222222222 Loss: 0.035753154780226525 Corrects: 4664\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.025066232308745384\n",
            "Train step - Step 2390, Loss 0.030449002981185913\n",
            "Train step - Step 2400, Loss 0.03524748608469963\n",
            "Train step - Step 2410, Loss 0.028080839663743973\n",
            "Train epoch - Accuracy: 0.9414141414141414 Loss: 0.035972266811313054 Corrects: 4660\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.042317841202020645\n",
            "Train step - Step 2430, Loss 0.03648374602198601\n",
            "Train step - Step 2440, Loss 0.031631745398044586\n",
            "Train step - Step 2450, Loss 0.04098673537373543\n",
            "Train epoch - Accuracy: 0.9418181818181818 Loss: 0.03539517698414398 Corrects: 4662\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.02989429607987404\n",
            "Train step - Step 2470, Loss 0.04218396171927452\n",
            "Train step - Step 2480, Loss 0.03715358301997185\n",
            "Train step - Step 2490, Loss 0.031822916120290756\n",
            "Train epoch - Accuracy: 0.9490909090909091 Loss: 0.03330550387727492 Corrects: 4698\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.028939152136445045\n",
            "Train step - Step 2510, Loss 0.02005034312605858\n",
            "Train step - Step 2520, Loss 0.03604603558778763\n",
            "Train step - Step 2530, Loss 0.03163815662264824\n",
            "Train epoch - Accuracy: 0.9482828282828283 Loss: 0.03211410686072677 Corrects: 4694\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.025999635457992554\n",
            "Train step - Step 2550, Loss 0.028922153636813164\n",
            "Train step - Step 2560, Loss 0.03385809436440468\n",
            "Train step - Step 2570, Loss 0.029063407331705093\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.031800762718976146 Corrects: 4703\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.023937581107020378\n",
            "Train step - Step 2590, Loss 0.041477106511592865\n",
            "Train step - Step 2600, Loss 0.03649153187870979\n",
            "Train step - Step 2610, Loss 0.026939142495393753\n",
            "Train epoch - Accuracy: 0.9446464646464646 Loss: 0.03349831054156477 Corrects: 4676\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.03261159360408783\n",
            "Train step - Step 2630, Loss 0.019862627610564232\n",
            "Train step - Step 2640, Loss 0.027196794748306274\n",
            "Train step - Step 2650, Loss 0.03332221135497093\n",
            "Train epoch - Accuracy: 0.9509090909090909 Loss: 0.03196508015647079 Corrects: 4707\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.02246338315308094\n",
            "Train step - Step 2670, Loss 0.035455893725156784\n",
            "Train step - Step 2680, Loss 0.025931669399142265\n",
            "Train step - Step 2690, Loss 0.03257901966571808\n",
            "Train epoch - Accuracy: 0.9551515151515152 Loss: 0.03069974917956073 Corrects: 4728\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.038380254060029984\n",
            "Train step - Step 2710, Loss 0.03411146625876427\n",
            "Train step - Step 2720, Loss 0.02543148212134838\n",
            "Train epoch - Accuracy: 0.9494949494949495 Loss: 0.032284092195708346 Corrects: 4700\n",
            "Training finished in 182.40718293190002 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238249150>\n",
            "Constructing exemplars of class 27\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [499, 47137, 25357, 6296, 36691, 32071, 46338, 32808, 25699, 46334, 2901, 4790, 25823, 36457, 21305, 30375, 47378, 2579, 17563, 38941, 2526, 8348, 13226, 29745, 1631, 17200, 25194, 48775, 15494, 41738, 22683, 27286, 5972, 23090, 8348, 33447, 2519, 37513, 5216, 17655, 22953, 10862, 47901, 21370, 43791, 34577, 35259, 33343, 23880, 37704, 17530, 40413, 28959, 19478, 9170, 12072, 11536, 13843, 5972, 17728, 48257, 38306, 40198, 34390, 15441, 1055, 26692, 34823, 30875, 12730, 47177, 16451, 20510, 15502, 20476, 34807, 13955, 33830, 17728, 44619, 9190, 27513, 30751, 46334, 8693, 13570, 3240, 34084, 40086, 40638, 48296, 48999, 42787, 38941, 32098, 30271, 40198, 26690, 5396, 39439, 49948, 19172, 28763, 17563, 33447, 13843, 39265, 42220, 35737, 23880, 10551, 4949, 13856, 8384, 37456, 20314, 33447, 27775, 12197, 19711, 19172, 22940, 1782, 6640, 40216, 21305, 4296, 19527, 41788, 7170, 47669, 47985, 5790, 15469, 8143, 27239, 8268, 42787, 40513, 32808, 11177, 15502, 21305, 17703, 42910, 20987, 2810, 19771, 29745, 10410, 41932, 15228, 25049, 10875, 1239, 34313, 32691, 30800, 39385, 11353, 37183, 25662, 46211, 45785, 28781, 45258, 13712, 4296, 23132, 33737, 10410, 7167, 8693, 9467, 10875, 39325, 17698, 13712, 38146, 32808, 7215, 48257, 42910, 40513, 1598, 5432, 13570, 16652, 46544, 31719, 36691, 34823, 35737, 26442, 17703, 45681, 37704, 23400, 9910, 32289]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22380f9e90>\n",
            "Constructing exemplars of class 86\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [7345, 31202, 22232, 45299, 42913, 35599, 33691, 5, 10837, 22949, 30570, 6638, 48900, 16815, 8251, 31357, 9113, 45536, 40526, 47631, 33691, 42820, 8802, 31538, 15672, 39043, 27703, 22317, 10299, 42218, 3318, 28461, 21816, 4512, 20852, 23586, 23177, 26401, 33691, 47214, 19624, 45187, 10837, 17533, 39129, 33083, 23177, 21404, 16815, 25436, 32601, 27908, 1342, 23108, 45686, 45263, 22547, 36947, 21762, 23586, 13053, 6649, 4434, 17306, 12635, 23177, 23034, 10176, 23280, 26719, 34863, 5645, 45951, 34052, 39718, 1186, 3334, 15688, 38753, 263, 16656, 23108, 18310, 5629, 45915, 24114, 43938, 21404, 42290, 3045, 18516, 32495, 46650, 39129, 10915, 19443, 31612, 8086, 35599, 32129, 5251, 1186, 1941, 49008, 30757, 32121, 47610, 47855, 496, 37678, 26304, 22033, 3895, 30586, 40963, 37014, 2171, 48978, 11035, 3318, 46357, 29520, 47855, 32129, 34586, 3045, 39718, 20828, 36755, 4651, 10991, 12561, 12375, 34591, 23600, 21679, 14994, 31896, 12755, 32483, 37678, 29326, 37836, 2378, 35457, 46375, 4392, 35090, 48978, 23808, 32483, 41129, 22229, 43676, 40492, 37391, 46559, 16656, 45141, 11892, 19219, 7646, 44794, 2235, 15688, 42707, 39739, 34543, 46967, 40526, 8251, 42913, 5595, 21691, 41129, 18105, 25287, 24004, 16134, 21404, 28461, 21600, 8303, 39699, 2636, 44113, 1342, 8379, 48884, 39576, 11892, 32605, 43948, 2353, 1439, 21600, 23394, 21519, 16656, 39718]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2244394290>\n",
            "Constructing exemplars of class 82\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [41968, 32028, 10309, 18917, 6761, 35967, 40462, 5450, 2753, 31249, 7027, 48289, 43830, 26921, 5656, 24024, 13546, 44260, 34608, 16838, 30468, 1296, 40408, 19060, 10727, 22681, 11501, 12252, 30203, 41895, 49858, 47957, 164, 33382, 2542, 30203, 41895, 47516, 26976, 30450, 1854, 48025, 33709, 49631, 8581, 39637, 15317, 35369, 32847, 8331, 27063, 5244, 43453, 20690, 15113, 36437, 11901, 20686, 32417, 6173, 16467, 40408, 28096, 1136, 2753, 18626, 49708, 36261, 6173, 44928, 5257, 34875, 36799, 274, 164, 30639, 46394, 38486, 32175, 30450, 46951, 39262, 46832, 16897, 2542, 22110, 14057, 47319, 36727, 33729, 25615, 21590, 28101, 32449, 18644, 24387, 14557, 46908, 15358, 15304, 13318, 18644, 35336, 36261, 30639, 3040, 164, 20690, 49832, 46550, 16897, 47737, 25724, 1402, 3876, 32602, 5244, 45064, 39114, 14398, 33382, 8518, 35336, 36799, 10923, 11832, 44523, 47124, 47516, 13318, 43320, 44417, 36492, 44909, 12, 19817, 47319, 1143, 39114, 5454, 28783, 39637, 20686, 47675, 38171, 32858, 21695, 31759, 40952, 5073, 3507, 10381, 6961, 46951, 36194, 27709, 25557, 39637, 164, 45656, 40367, 13318, 21512, 40409, 3438, 23130, 28404, 31775, 23402, 46147, 27053, 15403, 1136, 5586, 11600, 21991, 36705, 44030, 4877, 3221, 35369, 32847, 29478, 34267, 15304, 324, 9512, 21695, 25615, 36301, 28893, 12, 25817, 26921, 30450, 48289, 7471, 14557, 6173, 37064]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238254350>\n",
            "Constructing exemplars of class 78\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [12515, 15944, 22314, 35456, 39142, 42102, 42980, 46141, 13733, 19888, 10326, 44594, 10582, 4495, 31093, 19728, 7351, 10900, 28343, 24529, 165, 29518, 29063, 3984, 21245, 18321, 18228, 47100, 3392, 31473, 14886, 27875, 42102, 15532, 39693, 39066, 19489, 17956, 43936, 5221, 33082, 48964, 47371, 47715, 45048, 37783, 21721, 23172, 46710, 17712, 32597, 12285, 46923, 42875, 2895, 6759, 15532, 33671, 2916, 32972, 18091, 43662, 46292, 36435, 43662, 33321, 38949, 25317, 23046, 18741, 34349, 49541, 37043, 31093, 27624, 30370, 14886, 27925, 29612, 27624, 28872, 30077, 16414, 48341, 23632, 47100, 2630, 33353, 26686, 42439, 1539, 29043, 38585, 15438, 29063, 9258, 23945, 38042, 45244, 16845, 2058, 3976, 6932, 165, 14820, 13147, 33157, 26752, 40090, 46734, 15718, 43557, 29063, 49946, 16367, 14895, 2355, 19381, 33789, 21245, 42980, 31346, 20404, 36516, 29063, 37783, 7022, 16792, 27737, 12615, 24843, 36210, 37001, 47603, 22314, 30672, 49, 24781, 44594, 10326, 3764, 41759, 31346, 22426, 24664, 26752, 22083, 3984, 33322, 5214, 19436, 42281, 28020, 35818, 30868, 47715, 34157, 21721, 3219, 34089, 16801, 17292, 15648, 40087, 12344, 21721, 25646, 13916, 3984, 10053, 19489, 35818, 29612, 1619, 22799, 5916, 7889, 12268, 41337, 44594, 28595, 38439, 13392, 7624, 13925, 16009, 9946, 37998, 16009, 32793, 27624, 24549, 5348, 37998, 37680, 34324, 3422, 19436, 31346, 47760]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a91610>\n",
            "Constructing exemplars of class 50\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [32282, 16788, 9154, 13011, 10353, 22023, 11062, 20483, 37242, 40724, 18525, 2433, 42845, 18858, 8552, 5182, 47346, 25946, 38226, 31166, 716, 4682, 30545, 16229, 49801, 17551, 11964, 10750, 24682, 35011, 14448, 30671, 7152, 29494, 30312, 28002, 6677, 42051, 49701, 23294, 3376, 29452, 3603, 9081, 23187, 29076, 14061, 26655, 17694, 26647, 32947, 44685, 37359, 38927, 1655, 16550, 31448, 29184, 15697, 17925, 14133, 31000, 45042, 49701, 9210, 30956, 12736, 30882, 30312, 43261, 36768, 35530, 9154, 18402, 40724, 35844, 38839, 37711, 35427, 49666, 13686, 39006, 30776, 8640, 28739, 48966, 44661, 16788, 44701, 37563, 19649, 16646, 6379, 40299, 22674, 47377, 9354, 42699, 49701, 42824, 13799, 43122, 24044, 27918, 41252, 34444, 26647, 18119, 19049, 35600, 35085, 42528, 14897, 26647, 9147, 22479, 37713, 24899, 42799, 48077, 1545, 17258, 42268, 10078, 16940, 13534, 10169, 20129, 35011, 19514, 14258, 39986, 49408, 38173, 25622, 20483, 42051, 21468, 10811, 3376, 24044, 28574, 10544, 26624, 11609, 18119, 47071, 44063, 30882, 19649, 19204, 3945, 10413, 35844, 18153, 2070, 41934, 35983, 13677, 11475, 4368, 12607, 37711, 43094, 854, 25649, 36255, 29076, 41217, 19611, 42057, 12563, 14070, 30426, 16940, 39248, 19204, 12663, 10528, 31081, 49913, 6435, 25692, 16911, 39364, 27060, 26723, 19649, 42752, 30882, 10528, 37713, 35530, 20840, 30956, 35085, 42057, 10533, 42528, 748]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a91350>\n",
            "Constructing exemplars of class 30\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [2413, 9913, 12797, 17555, 27245, 48748, 5484, 40798, 20271, 4669, 5737, 41101, 31622, 19509, 30490, 39595, 13670, 37151, 41010, 8243, 26915, 44583, 17858, 47679, 505, 6936, 17424, 42267, 47703, 41685, 2879, 3792, 20885, 18698, 37648, 5737, 29426, 28897, 41381, 6215, 3959, 8216, 19999, 44883, 20823, 8877, 43518, 505, 18447, 48707, 14466, 32520, 20279, 48982, 18584, 20125, 48233, 7413, 26915, 2511, 1012, 36170, 44328, 23465, 35421, 20318, 33698, 42312, 3959, 44772, 8877, 23881, 29360, 36587, 48707, 12161, 34222, 17504, 25585, 6576, 41297, 34418, 46352, 21934, 22281, 24257, 8601, 48982, 2511, 10403, 3864, 36587, 24678, 10526, 34693, 32330, 19039, 5127, 10118, 5460, 49334, 39411, 24104, 12797, 16860, 27269, 21345, 23314, 39595, 6922, 42993, 19225, 15018, 18447, 2041, 33698, 38545, 1522, 35567, 30650, 33600, 45523, 20323, 16088, 29579, 8726, 20318, 25227, 17424, 42267, 2165, 9928, 4352, 41247, 10526, 3394, 3409, 28897, 44755, 28039, 23642, 9760, 15610, 12696, 6103, 36882, 13868, 23385, 24104, 44467, 24298, 43702, 32830, 4883, 8459, 2879, 49334, 5460, 29949, 6343, 13845, 13529, 43838, 21636, 42731, 12929, 24753, 1012, 21484, 4766, 29722, 12768, 9527, 20125, 36716, 37821, 20071, 10667, 46746, 44219, 47703, 41685, 16634, 9760, 3959, 18486, 6568, 44842, 34585, 41381, 10403, 44756, 41635, 20676, 36293, 6215, 32684, 13224, 46184, 20627]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238246350>\n",
            "Constructing exemplars of class 97\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [36343, 44473, 12941, 41517, 8376, 886, 40168, 10935, 26479, 10823, 48808, 38282, 13319, 44415, 29017, 46311, 26147, 25932, 23332, 8417, 2662, 46009, 41545, 42894, 12109, 35940, 903, 45858, 16559, 18971, 34203, 46801, 29077, 43211, 17021, 4720, 37464, 42067, 28256, 36439, 13865, 17460, 27998, 47396, 27738, 15643, 23104, 33838, 4086, 21924, 46225, 14228, 3015, 43767, 48042, 26771, 33305, 45500, 12927, 428, 22851, 33838, 7433, 8317, 45354, 9712, 45500, 21266, 42986, 10935, 12927, 46822, 5211, 9756, 22433, 30754, 9167, 49451, 15542, 40829, 40556, 7629, 23008, 49396, 733, 6826, 9584, 12798, 20592, 22938, 7782, 41638, 10510, 35010, 1720, 40327, 30057, 30709, 42958, 28974, 18032, 3498, 33893, 14484, 21751, 46009, 49451, 28610, 41773, 47513, 44473, 26746, 15643, 11371, 7250, 33058, 40303, 44601, 48426, 41368, 5768, 4086, 11228, 10128, 18032, 5786, 8630, 9756, 39561, 10275, 32465, 5184, 39603, 20662, 15414, 13500, 48673, 17320, 33893, 36270, 3791, 21715, 38414, 16693, 12069, 15542, 13649, 26239, 23008, 45326, 1770, 28161, 49994, 8417, 39501, 32811, 23775, 29017, 886, 14484, 6079, 27949, 12075, 17021, 26479, 45227, 31860, 17488, 28974, 48042, 26771, 7232, 14221, 2976, 45484, 13500, 13319, 24013, 11228, 103, 42286, 44601, 20723, 28371, 42986, 23281, 30855, 7174, 10485, 49396, 2197, 46011, 13894, 1761, 13270, 20902, 5786, 8630, 20662, 46225]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22413f1f10>\n",
            "Constructing exemplars of class 69\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [3129, 6521, 11180, 23762, 38511, 36095, 16411, 6461, 554, 4223, 851, 44301, 17909, 22670, 7165, 31481, 20249, 46927, 1420, 48821, 34929, 17783, 10479, 28088, 10653, 3479, 45680, 11567, 759, 19871, 13167, 19731, 41127, 20310, 31168, 25360, 43227, 33841, 11554, 26511, 12976, 33019, 16594, 23902, 9189, 29900, 12421, 28883, 14210, 14100, 7071, 16686, 22241, 29599, 29508, 10772, 45811, 21015, 16760, 9865, 39450, 41127, 28883, 22241, 16876, 23762, 30706, 19250, 4217, 23902, 2248, 32177, 5581, 30173, 20496, 11567, 43120, 39244, 16297, 29041, 39788, 35296, 41963, 15128, 37267, 10614, 27583, 7966, 16760, 10888, 1465, 13946, 25238, 23510, 39388, 4217, 199, 21528, 31437, 14220, 851, 42228, 17311, 4439, 19510, 4223, 34282, 26576, 45113, 24263, 11387, 9148, 25544, 16434, 6840, 32177, 17911, 4889, 38785, 19180, 4217, 19731, 34753, 20055, 32437, 12043, 42617, 33664, 14186, 8821, 22619, 48594, 11143, 3907, 18152, 24187, 7654, 39814, 3479, 10852, 20310, 29041, 831, 15260, 34161, 14654, 37312, 43120, 29866, 16958, 16084, 17447, 43120, 49128, 19871, 5048, 45495, 18238, 1847, 10607, 36095, 3469, 13906, 37267, 44224, 28563, 19180, 38407, 24066, 32108, 34753, 15128, 39120, 26754, 49128, 41606, 22619, 20291, 28088, 19731, 10614, 40924, 11057, 6131, 7966, 31042, 27877, 32731, 42026, 29900, 21181, 13506, 4889, 34907, 7662, 39814, 34282, 4201, 36664, 31733]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2246382c50>\n",
            "Constructing exemplars of class 57\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [40996, 44930, 45530, 48712, 13188, 46114, 5512, 35347, 42029, 34568, 23264, 45158, 29120, 30242, 21487, 43367, 1495, 30481, 41912, 10165, 46691, 22360, 11701, 20086, 13333, 4718, 23915, 23751, 24969, 2112, 35845, 37550, 9651, 2333, 18100, 15583, 29645, 28228, 47604, 49036, 16219, 15541, 46465, 4828, 4852, 36651, 39552, 331, 23082, 31881, 16512, 1593, 21822, 25197, 49750, 24287, 34969, 685, 24254, 22472, 46114, 5753, 8773, 18039, 48777, 16422, 43288, 31623, 16089, 47383, 40068, 17260, 36234, 40848, 4702, 14528, 5457, 1634, 48649, 4200, 43067, 11497, 44930, 14528, 30384, 36415, 15171, 33657, 14008, 4990, 19198, 32046, 11219, 46797, 49911, 8309, 47398, 23751, 34294, 6981, 11111, 47929, 2453, 22510, 26755, 27522, 44632, 34659, 41906, 9651, 36057, 15678, 24636, 24969, 44770, 43367, 27522, 18643, 14491, 4579, 1684, 6496, 32550, 592, 6066, 24962, 4835, 47871, 4828, 21228, 5545, 12757, 40553, 32456, 5418, 48099, 20438, 12361, 37202, 45056, 6723, 19979, 12757, 8401, 30481, 14755, 47871, 11662, 42194, 590, 36771, 45831, 38758, 49883, 26024, 14716, 4830, 48653, 17529, 38372, 46558, 46114, 32201, 17736, 37512, 7094, 43367, 45127, 19198, 26024, 33426, 2453, 16557, 45413, 15678, 30349, 35877, 45158, 26234, 49036, 23240, 48401, 38648, 19134, 28228, 43877, 10165, 41608, 5571, 27491, 40116, 17806, 31291, 5512, 43707, 32046, 3399, 15920, 16597, 21228]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223801ba10>\n",
            "Constructing exemplars of class 25\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [11945, 21894, 10686, 22972, 35549, 4034, 19870, 33908, 3826, 26764, 17683, 37566, 4789, 4482, 36174, 6831, 20179, 19995, 33604, 37501, 6268, 2438, 41399, 6233, 25564, 39568, 44627, 19623, 21192, 11541, 1382, 39267, 2578, 42829, 37494, 16155, 20613, 30221, 17629, 17440, 4674, 49824, 44585, 46442, 30530, 43954, 11805, 22666, 1487, 8468, 10276, 42138, 9695, 23362, 5013, 33804, 18838, 7808, 3826, 42829, 22339, 43145, 27047, 25653, 25037, 17968, 43798, 38919, 48092, 20965, 10044, 13651, 19995, 35242, 8619, 28675, 25538, 5555, 4570, 6655, 27449, 29723, 40247, 3669, 49467, 47163, 26321, 33436, 43585, 31943, 32196, 15226, 2690, 14997, 40540, 1926, 27047, 25653, 16741, 25441, 1257, 27058, 13276, 38706, 18923, 4565, 35188, 7405, 703, 19995, 30687, 40273, 39266, 15340, 43145, 44689, 47262, 31480, 43118, 29159, 3595, 29886, 8468, 450, 22160, 241, 15340, 4482, 49866, 1954, 9733, 42280, 47736, 46990, 42829, 47061, 24309, 22854, 13175, 2578, 39267, 35866, 42138, 5653, 43954, 18838, 30836, 17937, 19860, 15632, 33303, 21405, 12020, 41623, 47197, 13175, 33254, 44757, 25441, 6268, 20170, 46461, 47540, 35964, 703, 13580, 31965, 22666, 35242, 33231, 31271, 40620, 30836, 46673, 22666, 49070, 18992, 24982, 40368, 39684, 11859, 28453, 40417, 1471, 26664, 41623, 28675, 37566, 18655, 16563, 27055, 15190, 19645, 24244, 11541, 22181, 6268, 10276, 22302, 46713]\n",
            "x train:  [-0.27958414 -0.33382413 -0.31899416 -0.3075703  -0.36007908 -0.37246212\n",
            " -0.37052172 -0.2600769   0.25233024 -0.27758867]\n",
            "y_train:  [tensor([57]), tensor([27]), tensor([78]), tensor([27]), tensor([97]), tensor([78]), tensor([69]), tensor([97]), tensor([57]), tensor([97]), tensor([97]), tensor([27]), tensor([30]), tensor([78]), tensor([97]), tensor([82]), tensor([57]), tensor([69]), tensor([25]), tensor([86]), tensor([25]), tensor([25]), tensor([25]), tensor([78]), tensor([97]), tensor([27]), tensor([82]), tensor([25]), tensor([30]), tensor([82]), tensor([69]), tensor([50]), tensor([69]), tensor([78]), tensor([30]), tensor([27]), tensor([27]), tensor([25]), tensor([27]), tensor([57]), tensor([78]), tensor([97]), tensor([69]), tensor([97]), tensor([30]), tensor([86]), tensor([78]), tensor([82]), tensor([86]), tensor([69]), tensor([25]), tensor([57]), tensor([57]), tensor([27]), tensor([97]), tensor([78]), tensor([97]), tensor([27]), tensor([27]), tensor([50]), tensor([86]), tensor([27]), tensor([25]), tensor([27]), tensor([57]), tensor([57]), tensor([57]), tensor([97]), tensor([78]), tensor([69]), tensor([25]), tensor([69]), tensor([27]), tensor([82]), tensor([25]), tensor([82]), tensor([82]), tensor([50]), tensor([30]), tensor([82]), tensor([97]), tensor([78]), tensor([82]), tensor([25]), tensor([50]), tensor([86]), tensor([27]), tensor([97]), tensor([69]), tensor([50]), tensor([50]), tensor([86]), tensor([86]), tensor([69]), tensor([25]), tensor([57]), tensor([50]), tensor([30]), tensor([50]), tensor([97]), tensor([30]), tensor([27]), tensor([78]), tensor([57]), tensor([50]), tensor([69]), tensor([57]), tensor([78]), tensor([82]), tensor([30]), tensor([30]), tensor([25]), tensor([50]), tensor([25]), tensor([86]), tensor([30]), tensor([78]), tensor([57]), tensor([78]), tensor([25]), tensor([78]), tensor([25]), tensor([78]), tensor([69]), tensor([25]), tensor([30]), tensor([50]), tensor([27]), tensor([30]), tensor([50]), tensor([78]), tensor([57]), tensor([50]), tensor([25]), tensor([86]), tensor([57]), tensor([50]), tensor([69]), tensor([25]), tensor([57]), tensor([30]), tensor([30]), tensor([82]), tensor([86]), tensor([25]), tensor([97]), tensor([78]), tensor([57]), tensor([50]), tensor([50]), tensor([82]), tensor([30]), tensor([57]), tensor([50]), tensor([78]), tensor([97]), tensor([50]), tensor([25]), tensor([82]), tensor([82]), tensor([86]), tensor([86]), tensor([69]), tensor([30]), tensor([86]), tensor([27]), tensor([69]), tensor([27]), tensor([78]), tensor([50]), tensor([27]), tensor([25]), tensor([25]), tensor([50]), tensor([82]), tensor([97]), tensor([25]), tensor([78]), tensor([97]), tensor([86]), tensor([69]), tensor([97]), tensor([57]), tensor([97]), tensor([97]), tensor([86]), tensor([30]), tensor([27]), tensor([25]), tensor([86]), tensor([82]), tensor([27]), tensor([50]), tensor([27]), tensor([27]), tensor([69]), tensor([27]), tensor([27]), tensor([57]), tensor([27]), tensor([30]), tensor([57]), tensor([57]), tensor([25]), tensor([30]), tensor([57]), tensor([78]), tensor([25]), tensor([27]), tensor([86]), tensor([78]), tensor([50]), tensor([50]), tensor([82]), tensor([86]), tensor([30]), tensor([30]), tensor([97]), tensor([30]), tensor([78]), tensor([30]), tensor([82]), tensor([69]), tensor([82]), tensor([69]), tensor([50]), tensor([30]), tensor([30]), tensor([57]), tensor([27]), tensor([30]), tensor([50]), tensor([30]), tensor([50]), tensor([25]), tensor([27]), tensor([82]), tensor([50]), tensor([27]), tensor([50]), tensor([27]), tensor([30]), tensor([82]), tensor([30]), tensor([69]), tensor([69]), tensor([97]), tensor([78]), tensor([57]), tensor([27]), tensor([86]), tensor([57]), tensor([27]), tensor([30]), tensor([82]), tensor([27]), tensor([30]), tensor([97]), tensor([30]), tensor([82]), tensor([25]), tensor([57]), tensor([97]), tensor([97]), tensor([50]), tensor([50]), tensor([30]), tensor([82]), tensor([82]), tensor([57]), tensor([27]), tensor([30]), tensor([27]), tensor([82]), tensor([97]), tensor([82]), tensor([27]), tensor([97]), tensor([78]), tensor([82]), tensor([25]), tensor([69]), tensor([57]), tensor([86]), tensor([50]), tensor([69]), tensor([69]), tensor([97]), tensor([57]), tensor([78]), tensor([27]), tensor([25]), tensor([78]), tensor([25]), tensor([50]), tensor([27]), tensor([82]), tensor([78]), tensor([86]), tensor([86]), tensor([78]), tensor([69]), tensor([50]), tensor([50]), tensor([78]), tensor([57]), tensor([50]), tensor([78]), tensor([82]), tensor([82]), tensor([69]), tensor([30]), tensor([57]), tensor([78]), tensor([30]), tensor([30]), tensor([86]), tensor([50]), tensor([50]), tensor([27]), tensor([57]), tensor([25]), tensor([78]), tensor([78]), tensor([57]), tensor([86]), tensor([25]), tensor([27]), tensor([30]), tensor([57]), tensor([69]), tensor([69]), tensor([86]), tensor([97]), tensor([27]), tensor([30]), tensor([30]), tensor([50]), tensor([69]), tensor([69]), tensor([69]), tensor([69]), tensor([27]), tensor([82]), tensor([57]), tensor([78]), tensor([82]), tensor([50]), tensor([82]), tensor([82]), tensor([82]), tensor([57]), tensor([57]), tensor([78]), tensor([69]), tensor([25]), tensor([86]), tensor([69]), tensor([57]), tensor([25]), tensor([30]), tensor([97]), tensor([57]), tensor([27]), tensor([86]), tensor([97]), tensor([25]), tensor([82]), tensor([82]), tensor([50]), tensor([57]), tensor([78]), tensor([97]), tensor([86]), tensor([86]), tensor([25]), tensor([30]), tensor([69]), tensor([50]), tensor([97]), tensor([97]), tensor([27]), tensor([69]), tensor([82]), tensor([86]), tensor([25]), tensor([30]), tensor([69]), tensor([82]), tensor([82]), tensor([78]), tensor([86]), tensor([30]), tensor([30]), tensor([27]), tensor([78]), tensor([30]), tensor([86]), tensor([57]), tensor([86]), tensor([57]), tensor([82]), tensor([57]), tensor([97]), tensor([97]), tensor([82]), tensor([86]), tensor([97]), tensor([57]), tensor([50]), tensor([97]), tensor([78]), tensor([82]), tensor([30]), tensor([78]), tensor([86]), tensor([82]), tensor([57]), tensor([50]), tensor([97]), tensor([97]), tensor([82]), tensor([25]), tensor([30]), tensor([78]), tensor([78]), tensor([30]), tensor([69]), tensor([27]), tensor([86]), tensor([27]), tensor([78]), tensor([27]), tensor([78]), tensor([97]), tensor([50]), tensor([25]), tensor([97]), tensor([27]), tensor([25]), tensor([27]), tensor([78]), tensor([50]), tensor([30]), tensor([86]), tensor([86]), tensor([82]), tensor([50]), tensor([78]), tensor([30]), tensor([69]), tensor([27]), tensor([78]), tensor([30]), tensor([97]), tensor([69]), tensor([57]), tensor([30]), tensor([86]), tensor([25]), tensor([78]), tensor([78]), tensor([27]), tensor([50]), tensor([82]), tensor([78]), tensor([97]), tensor([50]), tensor([82]), tensor([82]), tensor([82]), tensor([57]), tensor([30]), tensor([30]), tensor([97]), tensor([27]), tensor([86]), tensor([27]), tensor([69]), tensor([69]), tensor([97]), tensor([57]), tensor([97]), tensor([57]), tensor([27]), tensor([57]), tensor([50]), tensor([86]), tensor([69]), tensor([86]), tensor([69]), tensor([27]), tensor([57]), tensor([97]), tensor([86]), tensor([30]), tensor([69]), tensor([86]), tensor([57]), tensor([82]), tensor([78]), tensor([69]), tensor([50]), tensor([27]), tensor([30]), tensor([69]), tensor([78]), tensor([27]), tensor([78]), tensor([86]), tensor([25]), tensor([82]), tensor([27]), tensor([30]), tensor([30]), tensor([82]), tensor([69]), tensor([69]), tensor([82]), tensor([78]), tensor([69]), tensor([50]), tensor([27]), tensor([25]), tensor([57]), tensor([50]), tensor([78]), tensor([86]), tensor([86]), tensor([78]), tensor([30]), tensor([25]), tensor([97]), tensor([57]), tensor([30]), tensor([57]), tensor([82]), tensor([50]), tensor([25]), tensor([27]), tensor([97]), tensor([78]), tensor([27]), tensor([30]), tensor([27]), tensor([86]), tensor([50]), tensor([78]), tensor([27]), tensor([57]), tensor([69]), tensor([69]), tensor([97]), tensor([50]), tensor([97]), tensor([86]), tensor([25]), tensor([50]), tensor([50]), tensor([50]), tensor([27]), tensor([97]), tensor([30]), tensor([78]), tensor([25]), tensor([78]), tensor([57]), tensor([30]), tensor([69]), tensor([86]), tensor([78]), tensor([57]), tensor([25]), tensor([69]), tensor([86]), tensor([27]), tensor([97]), tensor([50]), tensor([30]), tensor([78]), tensor([97]), tensor([50]), tensor([69]), tensor([57]), tensor([57]), tensor([30]), tensor([97]), tensor([82]), tensor([30]), tensor([78]), tensor([78]), tensor([78]), tensor([57]), tensor([82]), tensor([82]), tensor([30]), tensor([82]), tensor([86]), tensor([50]), tensor([86]), tensor([50]), tensor([97]), tensor([27]), tensor([25]), tensor([57]), tensor([25]), tensor([69]), tensor([69]), tensor([50]), tensor([25]), tensor([30]), tensor([30]), tensor([27]), tensor([25]), tensor([78]), tensor([69]), tensor([57]), tensor([78]), tensor([27]), tensor([78]), tensor([25]), tensor([25]), tensor([27]), tensor([97]), tensor([25]), tensor([30]), tensor([97]), tensor([82]), tensor([27]), tensor([57]), tensor([25]), tensor([78]), tensor([57]), tensor([78]), tensor([50]), tensor([57]), tensor([69]), tensor([69]), tensor([27]), tensor([97]), tensor([57]), tensor([78]), tensor([82]), tensor([82]), tensor([57]), tensor([82]), tensor([30]), tensor([50]), tensor([57]), tensor([50]), tensor([30]), tensor([78]), tensor([30]), tensor([97]), tensor([25]), tensor([82]), tensor([82]), tensor([97]), tensor([78]), tensor([69]), tensor([27]), tensor([30]), tensor([57]), tensor([97]), tensor([57]), tensor([78]), tensor([78]), tensor([57]), tensor([57]), tensor([57]), tensor([69]), tensor([86]), tensor([69]), tensor([86]), tensor([57]), tensor([86]), tensor([86]), tensor([69]), tensor([82]), tensor([86]), tensor([50]), tensor([50]), tensor([78]), tensor([86]), tensor([57]), tensor([57]), tensor([50]), tensor([86]), tensor([27]), tensor([27]), tensor([69]), tensor([86]), tensor([86]), tensor([78]), tensor([97]), tensor([78]), tensor([82]), tensor([78]), tensor([25]), tensor([82]), tensor([82]), tensor([25]), tensor([97]), tensor([86]), tensor([57]), tensor([86]), tensor([25]), tensor([50]), tensor([82]), tensor([82]), tensor([82]), tensor([25]), tensor([69]), tensor([57]), tensor([78]), tensor([97]), tensor([82]), tensor([82]), tensor([86]), tensor([30]), tensor([27]), tensor([50]), tensor([97]), tensor([69]), tensor([97]), tensor([78]), tensor([25]), tensor([50]), tensor([86]), tensor([27]), tensor([25]), tensor([69]), tensor([86]), tensor([30]), tensor([25]), tensor([86]), tensor([50]), tensor([30]), tensor([86]), tensor([25]), tensor([86]), tensor([27]), tensor([57]), tensor([27]), tensor([30]), tensor([30]), tensor([27]), tensor([27]), tensor([78]), tensor([69]), tensor([69]), tensor([50]), tensor([50]), tensor([30]), tensor([25]), tensor([82]), tensor([50]), tensor([25]), tensor([30]), tensor([69]), tensor([50]), tensor([97]), tensor([78]), tensor([78]), tensor([86]), tensor([50]), tensor([50]), tensor([78]), tensor([25]), tensor([78]), tensor([27]), tensor([27]), tensor([30]), tensor([50]), tensor([97]), tensor([97]), tensor([50]), tensor([25]), tensor([78]), tensor([30]), tensor([82]), tensor([30]), tensor([78]), tensor([78]), tensor([57]), tensor([82]), tensor([69]), tensor([82]), tensor([97]), tensor([78]), tensor([69]), tensor([27]), tensor([82]), tensor([86]), tensor([78]), tensor([25]), tensor([78]), tensor([30]), tensor([27]), tensor([27]), tensor([78]), tensor([25]), tensor([57]), tensor([57]), tensor([57]), tensor([69]), tensor([78]), tensor([78]), tensor([78]), tensor([30]), tensor([78]), tensor([25]), tensor([27]), tensor([50]), tensor([27]), tensor([25]), tensor([78]), tensor([97]), tensor([78]), tensor([27]), tensor([78]), tensor([69]), tensor([57]), tensor([97]), tensor([86]), tensor([27]), tensor([69]), tensor([57]), tensor([25]), tensor([82]), tensor([50]), tensor([97]), tensor([30]), tensor([57]), tensor([86]), tensor([57]), tensor([78]), tensor([86]), tensor([25]), tensor([50]), tensor([97]), tensor([57]), tensor([27]), tensor([82]), tensor([57]), tensor([27]), tensor([69]), tensor([78]), tensor([86]), tensor([69]), tensor([97]), tensor([25]), tensor([57]), tensor([69]), tensor([27]), tensor([57]), tensor([30]), tensor([97]), tensor([86]), tensor([30]), tensor([97]), tensor([69]), tensor([97]), tensor([50]), tensor([57]), tensor([27]), tensor([27]), tensor([69]), tensor([86]), tensor([78]), tensor([86]), tensor([86]), tensor([69]), tensor([27]), tensor([27]), tensor([30]), tensor([25]), tensor([82]), tensor([82]), tensor([25]), tensor([25]), tensor([57]), tensor([69]), tensor([78]), tensor([82]), tensor([69]), tensor([27]), tensor([82]), tensor([25]), tensor([82]), tensor([30]), tensor([30]), tensor([69]), tensor([86]), tensor([78]), tensor([86]), tensor([86]), tensor([50]), tensor([27]), tensor([30]), tensor([25]), tensor([57]), tensor([97]), tensor([50]), tensor([86]), tensor([97]), tensor([25]), tensor([82]), tensor([86]), tensor([30]), tensor([97]), tensor([50]), tensor([78]), tensor([57]), tensor([86]), tensor([50]), tensor([69]), tensor([57]), tensor([50]), tensor([97]), tensor([86]), tensor([50]), tensor([69]), tensor([50]), tensor([69]), tensor([27]), tensor([57]), tensor([97]), tensor([97]), tensor([97]), tensor([78]), tensor([57]), tensor([82]), tensor([86]), tensor([25]), tensor([27]), tensor([78]), tensor([57]), tensor([97]), tensor([25]), tensor([82]), tensor([97]), tensor([86]), tensor([50]), tensor([25]), tensor([25]), tensor([97]), tensor([30]), tensor([78]), tensor([97]), tensor([78]), tensor([30]), tensor([97]), tensor([69]), tensor([30]), tensor([82]), tensor([27]), tensor([57]), tensor([69]), tensor([25]), tensor([82]), tensor([82]), tensor([82]), tensor([27]), tensor([50]), tensor([97]), tensor([69]), tensor([30]), tensor([86]), tensor([86]), tensor([82]), tensor([82]), tensor([86]), tensor([78]), tensor([50]), tensor([57]), tensor([30]), tensor([30]), tensor([25]), tensor([97]), tensor([57]), tensor([27]), tensor([86]), tensor([25]), tensor([82]), tensor([97]), tensor([25]), tensor([78]), tensor([69]), tensor([78]), tensor([27]), tensor([97]), tensor([50]), tensor([97]), tensor([82]), tensor([97]), tensor([27]), tensor([27]), tensor([27]), tensor([27]), tensor([97]), tensor([25]), tensor([97]), tensor([86]), tensor([25]), tensor([82]), tensor([78]), tensor([78]), tensor([69]), tensor([86]), tensor([30]), tensor([30]), tensor([57]), tensor([82]), tensor([69]), tensor([82]), tensor([78]), tensor([25]), tensor([69]), tensor([30]), tensor([78]), tensor([97]), tensor([25]), tensor([78]), tensor([25]), tensor([25]), tensor([86]), tensor([25]), tensor([50]), tensor([78]), tensor([97]), tensor([78]), tensor([27]), tensor([50]), tensor([25]), tensor([50]), tensor([86]), tensor([30]), tensor([27]), tensor([86]), tensor([50]), tensor([30]), tensor([97]), tensor([57]), tensor([57]), tensor([27]), tensor([69]), tensor([82]), tensor([69]), tensor([25]), tensor([69]), tensor([25]), tensor([69]), tensor([27]), tensor([25]), tensor([78]), tensor([30]), tensor([27]), tensor([30]), tensor([27]), tensor([50]), tensor([27]), tensor([69]), tensor([82]), tensor([69]), tensor([86]), tensor([78]), tensor([27]), tensor([86]), tensor([86]), tensor([27]), tensor([30]), tensor([69]), tensor([30]), tensor([27]), tensor([86]), tensor([50]), tensor([30]), tensor([69]), tensor([97]), tensor([27]), tensor([25]), tensor([78]), tensor([86]), tensor([27]), tensor([78]), tensor([30]), tensor([27]), tensor([30]), tensor([50]), tensor([86]), tensor([30]), tensor([78]), tensor([25]), tensor([97]), tensor([86]), tensor([97]), tensor([30]), tensor([86]), tensor([82]), tensor([69]), tensor([86]), tensor([25]), tensor([57]), tensor([30]), tensor([30]), tensor([25]), tensor([78]), tensor([86]), tensor([86]), tensor([86]), tensor([27]), tensor([78]), tensor([50]), tensor([69]), tensor([86]), tensor([57]), tensor([82]), tensor([50]), tensor([57]), tensor([82]), tensor([97]), tensor([69]), tensor([82]), tensor([25]), tensor([30]), tensor([30]), tensor([86]), tensor([82]), tensor([25]), tensor([97]), tensor([78]), tensor([50]), tensor([30]), tensor([86]), tensor([30]), tensor([86]), tensor([86]), tensor([69]), tensor([25]), tensor([57]), tensor([69]), tensor([82]), tensor([27]), tensor([82]), tensor([78]), tensor([50]), tensor([57]), tensor([25]), tensor([57]), tensor([27]), tensor([86]), tensor([50]), tensor([50]), tensor([82]), tensor([30]), tensor([69]), tensor([25]), tensor([86]), tensor([50]), tensor([78]), tensor([57]), tensor([30]), tensor([25]), tensor([97]), tensor([30]), tensor([82]), tensor([57]), tensor([78]), tensor([78]), tensor([25]), tensor([82]), tensor([25]), tensor([78]), tensor([30]), tensor([25]), tensor([50]), tensor([30]), tensor([82]), tensor([27]), tensor([27]), tensor([86]), tensor([30]), tensor([27]), tensor([82]), tensor([82]), tensor([97]), tensor([50]), tensor([69]), tensor([97]), tensor([69]), tensor([82]), tensor([69]), tensor([86]), tensor([97]), tensor([78]), tensor([57]), tensor([97]), tensor([86]), tensor([97]), tensor([97]), tensor([97]), tensor([57]), tensor([78]), tensor([69]), tensor([57]), tensor([78]), tensor([30]), tensor([86]), tensor([78]), tensor([97]), tensor([86]), tensor([50]), tensor([78]), tensor([86]), tensor([30]), tensor([57]), tensor([82]), tensor([57]), tensor([97]), tensor([97]), tensor([97]), tensor([27]), tensor([69]), tensor([27]), tensor([50]), tensor([97]), tensor([50]), tensor([30]), tensor([78]), tensor([30]), tensor([97]), tensor([86]), tensor([82]), tensor([57]), tensor([97]), tensor([97]), tensor([86]), tensor([86]), tensor([97]), tensor([57]), tensor([25]), tensor([97]), tensor([57]), tensor([82]), tensor([30]), tensor([78]), tensor([27]), tensor([86]), tensor([69]), tensor([30]), tensor([69]), tensor([57]), tensor([25]), tensor([50]), tensor([25]), tensor([78]), tensor([69]), tensor([69]), tensor([30]), tensor([27]), tensor([25]), tensor([69]), tensor([69]), tensor([82]), tensor([97]), tensor([69]), tensor([82]), tensor([25]), tensor([50]), tensor([69]), tensor([82]), tensor([82]), tensor([25]), tensor([78]), tensor([97]), tensor([50]), tensor([69]), tensor([30]), tensor([86]), tensor([25]), tensor([82]), tensor([25]), tensor([50]), tensor([30]), tensor([78]), tensor([25]), tensor([86]), tensor([97]), tensor([50]), tensor([78]), tensor([97]), tensor([25]), tensor([82]), tensor([82]), tensor([57]), tensor([78]), tensor([97]), tensor([30]), tensor([57]), tensor([82]), tensor([57]), tensor([27]), tensor([86]), tensor([50]), tensor([57]), tensor([50]), tensor([82]), tensor([86]), tensor([27]), tensor([69]), tensor([69]), tensor([69]), tensor([27]), tensor([50]), tensor([25]), tensor([97]), tensor([86]), tensor([78]), tensor([27]), tensor([78]), tensor([82]), tensor([69]), tensor([57]), tensor([97]), tensor([57]), tensor([57]), tensor([50]), tensor([82]), tensor([57]), tensor([82]), tensor([78]), tensor([82]), tensor([78]), tensor([82]), tensor([86]), tensor([97]), tensor([82]), tensor([27]), tensor([69]), tensor([86]), tensor([82]), tensor([50]), tensor([57]), tensor([78]), tensor([25]), tensor([27]), tensor([25]), tensor([86]), tensor([69]), tensor([86]), tensor([57]), tensor([78]), tensor([25]), tensor([25]), tensor([50]), tensor([97]), tensor([50]), tensor([27]), tensor([27]), tensor([27]), tensor([97]), tensor([27]), tensor([27]), tensor([86]), tensor([97]), tensor([97]), tensor([97]), tensor([86]), tensor([25]), tensor([50]), tensor([25]), tensor([30]), tensor([69]), tensor([25]), tensor([82]), tensor([50]), tensor([50]), tensor([50]), tensor([69]), tensor([82]), tensor([82]), tensor([25]), tensor([50]), tensor([86]), tensor([30]), tensor([30]), tensor([86]), tensor([30]), tensor([30]), tensor([86]), tensor([50]), tensor([57]), tensor([25]), tensor([30]), tensor([57]), tensor([27]), tensor([50]), tensor([27]), tensor([25]), tensor([27]), tensor([78]), tensor([30]), tensor([69]), tensor([27]), tensor([25]), tensor([57]), tensor([97]), tensor([82]), tensor([86]), tensor([78]), tensor([97]), tensor([27]), tensor([30]), tensor([97]), tensor([86]), tensor([25]), tensor([97]), tensor([69]), tensor([27]), tensor([69]), tensor([82]), tensor([27]), tensor([30]), tensor([86]), tensor([25]), tensor([69]), tensor([57]), tensor([78]), tensor([78]), tensor([25]), tensor([57]), tensor([30]), tensor([78]), tensor([30]), tensor([97]), tensor([69]), tensor([50]), tensor([97]), tensor([27]), tensor([27]), tensor([25]), tensor([82]), tensor([86]), tensor([69]), tensor([69]), tensor([97]), tensor([78]), tensor([57]), tensor([57]), tensor([69]), tensor([82]), tensor([50]), tensor([97]), tensor([82]), tensor([50]), tensor([69]), tensor([25]), tensor([97]), tensor([25]), tensor([78]), tensor([50]), tensor([78]), tensor([86]), tensor([50]), tensor([27]), tensor([57]), tensor([50]), tensor([78]), tensor([57]), tensor([57]), tensor([57]), tensor([69]), tensor([97]), tensor([50]), tensor([27]), tensor([57]), tensor([30]), tensor([57]), tensor([78]), tensor([50]), tensor([82]), tensor([69]), tensor([78]), tensor([86]), tensor([30]), tensor([25]), tensor([57]), tensor([27]), tensor([97]), tensor([69]), tensor([78]), tensor([27]), tensor([86]), tensor([82]), tensor([69]), tensor([97]), tensor([30]), tensor([97]), tensor([50]), tensor([86]), tensor([69]), tensor([86]), tensor([82]), tensor([50]), tensor([97]), tensor([82]), tensor([86]), tensor([69]), tensor([82]), tensor([69]), tensor([78]), tensor([86]), tensor([50]), tensor([30]), tensor([50]), tensor([30]), tensor([86]), tensor([82]), tensor([97]), tensor([86]), tensor([97]), tensor([97]), tensor([25]), tensor([69]), tensor([25]), tensor([97]), tensor([78]), tensor([27]), tensor([97]), tensor([50]), tensor([86]), tensor([30]), tensor([86]), tensor([69]), tensor([78]), tensor([86]), tensor([86]), tensor([78]), tensor([82]), tensor([86]), tensor([86]), tensor([50]), tensor([50]), tensor([50]), tensor([30]), tensor([50]), tensor([25]), tensor([78]), tensor([69]), tensor([97]), tensor([57]), tensor([25]), tensor([82]), tensor([27]), tensor([57]), tensor([30]), tensor([50]), tensor([27]), tensor([82]), tensor([27]), tensor([57]), tensor([27]), tensor([50]), tensor([50]), tensor([78]), tensor([50]), tensor([30]), tensor([78]), tensor([30]), tensor([57]), tensor([57]), tensor([86]), tensor([27]), tensor([78]), tensor([30]), tensor([27]), tensor([30]), tensor([25]), tensor([78]), tensor([69]), tensor([50]), tensor([97]), tensor([97]), tensor([30]), tensor([27]), tensor([27]), tensor([86]), tensor([82]), tensor([86]), tensor([78]), tensor([25]), tensor([78]), tensor([97]), tensor([69]), tensor([86]), tensor([69]), tensor([50]), tensor([82]), tensor([69]), tensor([69]), tensor([78]), tensor([50]), tensor([57]), tensor([30]), tensor([27]), tensor([69]), tensor([82]), tensor([57]), tensor([82]), tensor([57]), tensor([69]), tensor([69]), tensor([27]), tensor([97]), tensor([25]), tensor([50]), tensor([27]), tensor([97]), tensor([27]), tensor([25]), tensor([30]), tensor([50]), tensor([25]), tensor([69]), tensor([97]), tensor([25]), tensor([97]), tensor([25]), tensor([30]), tensor([27]), tensor([57]), tensor([30]), tensor([30]), tensor([86]), tensor([97]), tensor([57]), tensor([97]), tensor([50]), tensor([82]), tensor([82]), tensor([86]), tensor([82]), tensor([78]), tensor([86]), tensor([57]), tensor([50]), tensor([69]), tensor([25]), tensor([86]), tensor([50]), tensor([82]), tensor([69]), tensor([86]), tensor([57]), tensor([78]), tensor([30]), tensor([27]), tensor([30]), tensor([57]), tensor([97]), tensor([30]), tensor([27]), tensor([82]), tensor([97]), tensor([86]), tensor([27]), tensor([69]), tensor([27]), tensor([50]), tensor([57]), tensor([25]), tensor([27]), tensor([57]), tensor([25]), tensor([97]), tensor([27]), tensor([57]), tensor([82]), tensor([69]), tensor([25]), tensor([50]), tensor([78]), tensor([97]), tensor([50]), tensor([86]), tensor([82]), tensor([69]), tensor([57]), tensor([78]), tensor([57]), tensor([50]), tensor([25]), tensor([69]), tensor([86]), tensor([27]), tensor([97]), tensor([69]), tensor([25]), tensor([25]), tensor([82]), tensor([50]), tensor([82]), tensor([27]), tensor([86]), tensor([69]), tensor([27]), tensor([69]), tensor([82]), tensor([30]), tensor([57]), tensor([69]), tensor([82]), tensor([25]), tensor([50]), tensor([97]), tensor([82]), tensor([82]), tensor([50]), tensor([30]), tensor([82]), tensor([97]), tensor([25]), tensor([97]), tensor([57]), tensor([57]), tensor([69]), tensor([57]), tensor([97]), tensor([82]), tensor([50]), tensor([86]), tensor([25]), tensor([97]), tensor([78]), tensor([69]), tensor([82]), tensor([30]), tensor([27]), tensor([82]), tensor([27]), tensor([97]), tensor([69]), tensor([82]), tensor([25]), tensor([69]), tensor([69]), tensor([69]), tensor([97]), tensor([69]), tensor([97]), tensor([30]), tensor([78]), tensor([57]), tensor([25]), tensor([69]), tensor([82]), tensor([25]), tensor([27]), tensor([82]), tensor([57]), tensor([50]), tensor([27]), tensor([30]), tensor([82]), tensor([86]), tensor([86]), tensor([25]), tensor([57]), tensor([57]), tensor([50]), tensor([50]), tensor([86]), tensor([27]), tensor([69]), tensor([82]), tensor([30]), tensor([97]), tensor([50]), tensor([50]), tensor([30]), tensor([78]), tensor([78]), tensor([27]), tensor([30]), tensor([50]), tensor([27]), tensor([25]), tensor([69]), tensor([69]), tensor([57]), tensor([78]), tensor([78]), tensor([27]), tensor([86]), tensor([57]), tensor([78]), tensor([27]), tensor([50]), tensor([50]), tensor([57]), tensor([57]), tensor([50]), tensor([97]), tensor([86]), tensor([97]), tensor([30]), tensor([97]), tensor([30]), tensor([50]), tensor([69]), tensor([86]), tensor([97]), tensor([86]), tensor([25]), tensor([25]), tensor([82]), tensor([30]), tensor([25]), tensor([30]), tensor([97]), tensor([69]), tensor([82]), tensor([86]), tensor([82]), tensor([30]), tensor([57]), tensor([50]), tensor([25]), tensor([78]), tensor([82]), tensor([97]), tensor([50]), tensor([27]), tensor([25]), tensor([25]), tensor([30]), tensor([25]), tensor([30]), tensor([30]), tensor([30]), tensor([50]), tensor([69]), tensor([27]), tensor([69]), tensor([25]), tensor([25]), tensor([69]), tensor([25]), tensor([30]), tensor([69]), tensor([78]), tensor([57]), tensor([78]), tensor([50]), tensor([57]), tensor([50]), tensor([30]), tensor([30]), tensor([25]), tensor([30]), tensor([57]), tensor([69]), tensor([57]), tensor([57]), tensor([78]), tensor([86]), tensor([25]), tensor([82]), tensor([50]), tensor([78]), tensor([57]), tensor([50]), tensor([82]), tensor([82]), tensor([69]), tensor([97]), tensor([27]), tensor([86]), tensor([86]), tensor([27]), tensor([97]), tensor([57]), tensor([78]), tensor([50]), tensor([86]), tensor([97]), tensor([82]), tensor([25]), tensor([97]), tensor([82]), tensor([69]), tensor([69]), tensor([50]), tensor([69]), tensor([86]), tensor([27]), tensor([30]), tensor([57]), tensor([78]), tensor([30]), tensor([97]), tensor([25]), tensor([30]), tensor([82]), tensor([50]), tensor([86]), tensor([27]), tensor([57]), tensor([27]), tensor([30]), tensor([82]), tensor([27]), tensor([57]), tensor([25]), tensor([97]), tensor([69]), tensor([27]), tensor([97]), tensor([27]), tensor([25]), tensor([57]), tensor([30]), tensor([57]), tensor([25]), tensor([86]), tensor([97]), tensor([30]), tensor([57]), tensor([78]), tensor([78]), tensor([50]), tensor([30]), tensor([97]), tensor([25]), tensor([50]), tensor([82]), tensor([25]), tensor([82]), tensor([25]), tensor([69]), tensor([82]), tensor([69]), tensor([50]), tensor([82]), tensor([57]), tensor([69]), tensor([86]), tensor([50]), tensor([25]), tensor([57]), tensor([82]), tensor([27]), tensor([25]), tensor([25]), tensor([57]), tensor([57]), tensor([50]), tensor([82]), tensor([25]), tensor([57]), tensor([78]), tensor([25]), tensor([25]), tensor([30]), tensor([82]), tensor([50]), tensor([25]), tensor([25]), tensor([82]), tensor([69]), tensor([86]), tensor([69]), tensor([86]), tensor([57]), tensor([86]), tensor([86]), tensor([57]), tensor([86]), tensor([86]), tensor([97]), tensor([25]), tensor([78]), tensor([50]), tensor([78]), tensor([50]), tensor([57]), tensor([30]), tensor([30]), tensor([86]), tensor([86]), tensor([50]), tensor([82]), tensor([86]), tensor([69]), tensor([82]), tensor([69]), tensor([97]), tensor([86]), tensor([69])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.78 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.808\n",
            "TEST ALL:  0.808\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  2000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 95, 25, 27, 30, 34, 49, 50, 57, 58, 59, 60, 68, 69, 78, 81, 82, 86, 88, 13]\n",
            "TRAIN_SET CLASSES:  [95, 59, 58, 34, 81, 49, 13, 88, 68, 60]\n",
            "VALIDATION CLASSES:  [60, 59, 58, 49, 34, 95, 88, 81, 13, 68]\n",
            "GROUP:  2\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [95, 59, 58, 34, 81, 49, 13, 88, 68, 60]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  20\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.5137197375297546\n",
            "Train step - Step 10, Loss 0.15527212619781494\n",
            "Train step - Step 20, Loss 0.1620972603559494\n",
            "Train step - Step 30, Loss 0.14635789394378662\n",
            "Train step - Step 40, Loss 0.15947948396205902\n",
            "Train step - Step 50, Loss 0.14003886282444\n",
            "Train epoch - Accuracy: 0.3100719424460432 Loss: 0.17604783110481373 Corrects: 2155\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.135798841714859\n",
            "Train step - Step 70, Loss 0.13709400594234467\n",
            "Train step - Step 80, Loss 0.11814415454864502\n",
            "Train step - Step 90, Loss 0.13351868093013763\n",
            "Train step - Step 100, Loss 0.1228102445602417\n",
            "Train epoch - Accuracy: 0.37237410071942445 Loss: 0.13111206749574744 Corrects: 2588\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11647393554449081\n",
            "Train step - Step 120, Loss 0.11900182068347931\n",
            "Train step - Step 130, Loss 0.11732804775238037\n",
            "Train step - Step 140, Loss 0.12590499222278595\n",
            "Train step - Step 150, Loss 0.11622827500104904\n",
            "Train step - Step 160, Loss 0.1222437173128128\n",
            "Train epoch - Accuracy: 0.40302158273381294 Loss: 0.1238487555140214 Corrects: 2801\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.12576769292354584\n",
            "Train step - Step 180, Loss 0.117875836789608\n",
            "Train step - Step 190, Loss 0.10939595848321915\n",
            "Train step - Step 200, Loss 0.11352682113647461\n",
            "Train step - Step 210, Loss 0.1227969154715538\n",
            "Train epoch - Accuracy: 0.4343884892086331 Loss: 0.11971386516265732 Corrects: 3019\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11465467512607574\n",
            "Train step - Step 230, Loss 0.1235346719622612\n",
            "Train step - Step 240, Loss 0.11164980381727219\n",
            "Train step - Step 250, Loss 0.12655311822891235\n",
            "Train step - Step 260, Loss 0.12204866856336594\n",
            "Train step - Step 270, Loss 0.10593034327030182\n",
            "Train epoch - Accuracy: 0.46388489208633094 Loss: 0.11722736817469699 Corrects: 3224\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1274828016757965\n",
            "Train step - Step 290, Loss 0.11482997238636017\n",
            "Train step - Step 300, Loss 0.1154828816652298\n",
            "Train step - Step 310, Loss 0.10806825011968613\n",
            "Train step - Step 320, Loss 0.10392715036869049\n",
            "Train epoch - Accuracy: 0.48676258992805754 Loss: 0.11355792100695397 Corrects: 3383\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11730537563562393\n",
            "Train step - Step 340, Loss 0.12456243485212326\n",
            "Train step - Step 350, Loss 0.12611590325832367\n",
            "Train step - Step 360, Loss 0.10982400178909302\n",
            "Train step - Step 370, Loss 0.11163318157196045\n",
            "Train step - Step 380, Loss 0.11227124184370041\n",
            "Train epoch - Accuracy: 0.5046043165467626 Loss: 0.11165039151478157 Corrects: 3507\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.11253364384174347\n",
            "Train step - Step 400, Loss 0.11447291821241379\n",
            "Train step - Step 410, Loss 0.10710430145263672\n",
            "Train step - Step 420, Loss 0.11158579587936401\n",
            "Train step - Step 430, Loss 0.11459939926862717\n",
            "Train epoch - Accuracy: 0.5207194244604316 Loss: 0.10996882746331126 Corrects: 3619\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10490003973245621\n",
            "Train step - Step 450, Loss 0.10794421285390854\n",
            "Train step - Step 460, Loss 0.1155400201678276\n",
            "Train step - Step 470, Loss 0.10816049575805664\n",
            "Train step - Step 480, Loss 0.11086416244506836\n",
            "Train step - Step 490, Loss 0.11548230797052383\n",
            "Train epoch - Accuracy: 0.5320863309352518 Loss: 0.10759653664535755 Corrects: 3698\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10682572424411774\n",
            "Train step - Step 510, Loss 0.11029054969549179\n",
            "Train step - Step 520, Loss 0.1085776686668396\n",
            "Train step - Step 530, Loss 0.1104159876704216\n",
            "Train step - Step 540, Loss 0.1039591059088707\n",
            "Train epoch - Accuracy: 0.5461870503597123 Loss: 0.10673047166505306 Corrects: 3796\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.10894034057855606\n",
            "Train step - Step 560, Loss 0.10895290225744247\n",
            "Train step - Step 570, Loss 0.1118004098534584\n",
            "Train step - Step 580, Loss 0.11070596426725388\n",
            "Train step - Step 590, Loss 0.10156487673521042\n",
            "Train step - Step 600, Loss 0.09430040419101715\n",
            "Train epoch - Accuracy: 0.5575539568345323 Loss: 0.10503135061306919 Corrects: 3875\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.1005595251917839\n",
            "Train step - Step 620, Loss 0.10481525957584381\n",
            "Train step - Step 630, Loss 0.10602650791406631\n",
            "Train step - Step 640, Loss 0.10052080452442169\n",
            "Train step - Step 650, Loss 0.10943281650543213\n",
            "Train epoch - Accuracy: 0.5730935251798561 Loss: 0.10282688811957408 Corrects: 3983\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.09834938496351242\n",
            "Train step - Step 670, Loss 0.09769809991121292\n",
            "Train step - Step 680, Loss 0.09821218252182007\n",
            "Train step - Step 690, Loss 0.10797180980443954\n",
            "Train step - Step 700, Loss 0.0941043272614479\n",
            "Train step - Step 710, Loss 0.09698782116174698\n",
            "Train epoch - Accuracy: 0.5781294964028777 Loss: 0.10171778334559296 Corrects: 4018\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10962522029876709\n",
            "Train step - Step 730, Loss 0.10139738768339157\n",
            "Train step - Step 740, Loss 0.09661175310611725\n",
            "Train step - Step 750, Loss 0.10276100784540176\n",
            "Train step - Step 760, Loss 0.09553369879722595\n",
            "Train epoch - Accuracy: 0.5922302158273381 Loss: 0.10114398369042993 Corrects: 4116\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10056030750274658\n",
            "Train step - Step 780, Loss 0.09527494758367538\n",
            "Train step - Step 790, Loss 0.09898130595684052\n",
            "Train step - Step 800, Loss 0.09152210503816605\n",
            "Train step - Step 810, Loss 0.09602291882038116\n",
            "Train step - Step 820, Loss 0.09175743907690048\n",
            "Train epoch - Accuracy: 0.6056115107913669 Loss: 0.09850410102082671 Corrects: 4209\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.09368482977151871\n",
            "Train step - Step 840, Loss 0.08973021805286407\n",
            "Train step - Step 850, Loss 0.08805853128433228\n",
            "Train step - Step 860, Loss 0.1074579730629921\n",
            "Train step - Step 870, Loss 0.10363306850194931\n",
            "Train epoch - Accuracy: 0.6187050359712231 Loss: 0.0979529124069557 Corrects: 4300\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.10176260769367218\n",
            "Train step - Step 890, Loss 0.09938796609640121\n",
            "Train step - Step 900, Loss 0.09452502429485321\n",
            "Train step - Step 910, Loss 0.08878082036972046\n",
            "Train step - Step 920, Loss 0.09494564682245255\n",
            "Train step - Step 930, Loss 0.10756053775548935\n",
            "Train epoch - Accuracy: 0.6146762589928058 Loss: 0.09672370919649549 Corrects: 4272\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.09367267787456512\n",
            "Train step - Step 950, Loss 0.09806215018033981\n",
            "Train step - Step 960, Loss 0.09274570643901825\n",
            "Train step - Step 970, Loss 0.07879046350717545\n",
            "Train step - Step 980, Loss 0.08649956434965134\n",
            "Train epoch - Accuracy: 0.6235971223021582 Loss: 0.0971101653854624 Corrects: 4334\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.09523584693670273\n",
            "Train step - Step 1000, Loss 0.08574391901493073\n",
            "Train step - Step 1010, Loss 0.10065052658319473\n",
            "Train step - Step 1020, Loss 0.09243523329496384\n",
            "Train step - Step 1030, Loss 0.10573585331439972\n",
            "Train step - Step 1040, Loss 0.09262379258871078\n",
            "Train epoch - Accuracy: 0.6378417266187051 Loss: 0.09517672079715797 Corrects: 4433\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.10534518957138062\n",
            "Train step - Step 1060, Loss 0.09342121332883835\n",
            "Train step - Step 1070, Loss 0.10277354717254639\n",
            "Train step - Step 1080, Loss 0.09624594449996948\n",
            "Train step - Step 1090, Loss 0.09371928870677948\n",
            "Train epoch - Accuracy: 0.6450359712230216 Loss: 0.09521175100434598 Corrects: 4483\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.09534984081983566\n",
            "Train step - Step 1110, Loss 0.10372570902109146\n",
            "Train step - Step 1120, Loss 0.09274481981992722\n",
            "Train step - Step 1130, Loss 0.0925784781575203\n",
            "Train step - Step 1140, Loss 0.09150991588830948\n",
            "Train step - Step 1150, Loss 0.11130104213953018\n",
            "Train epoch - Accuracy: 0.6461870503597122 Loss: 0.09433283052641711 Corrects: 4491\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.09895253926515579\n",
            "Train step - Step 1170, Loss 0.09334152936935425\n",
            "Train step - Step 1180, Loss 0.0956624373793602\n",
            "Train step - Step 1190, Loss 0.09620903432369232\n",
            "Train step - Step 1200, Loss 0.10206875950098038\n",
            "Train epoch - Accuracy: 0.6607194244604316 Loss: 0.09341554357422341 Corrects: 4592\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.08560919016599655\n",
            "Train step - Step 1220, Loss 0.09864766150712967\n",
            "Train step - Step 1230, Loss 0.09393327683210373\n",
            "Train step - Step 1240, Loss 0.09488432109355927\n",
            "Train step - Step 1250, Loss 0.11246214061975479\n",
            "Train step - Step 1260, Loss 0.09086866676807404\n",
            "Train epoch - Accuracy: 0.662589928057554 Loss: 0.09228127417804526 Corrects: 4605\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.0799490213394165\n",
            "Train step - Step 1280, Loss 0.08642221987247467\n",
            "Train step - Step 1290, Loss 0.09554249048233032\n",
            "Train step - Step 1300, Loss 0.08790846914052963\n",
            "Train step - Step 1310, Loss 0.09130074828863144\n",
            "Train epoch - Accuracy: 0.6735251798561152 Loss: 0.09153262235492253 Corrects: 4681\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.08753395080566406\n",
            "Train step - Step 1330, Loss 0.09305932372808456\n",
            "Train step - Step 1340, Loss 0.0920829102396965\n",
            "Train step - Step 1350, Loss 0.08910984545946121\n",
            "Train step - Step 1360, Loss 0.09821899235248566\n",
            "Train step - Step 1370, Loss 0.09530873596668243\n",
            "Train epoch - Accuracy: 0.681294964028777 Loss: 0.09002043465701796 Corrects: 4735\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.08706448972225189\n",
            "Train step - Step 1390, Loss 0.08805632591247559\n",
            "Train step - Step 1400, Loss 0.09423217922449112\n",
            "Train step - Step 1410, Loss 0.07923392206430435\n",
            "Train step - Step 1420, Loss 0.09010037779808044\n",
            "Train epoch - Accuracy: 0.6853237410071943 Loss: 0.09072114170455246 Corrects: 4763\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.09863287210464478\n",
            "Train step - Step 1440, Loss 0.08719305694103241\n",
            "Train step - Step 1450, Loss 0.08623050898313522\n",
            "Train step - Step 1460, Loss 0.08364180475473404\n",
            "Train step - Step 1470, Loss 0.0849689394235611\n",
            "Train step - Step 1480, Loss 0.09269847720861435\n",
            "Train epoch - Accuracy: 0.6854676258992806 Loss: 0.08931897576121117 Corrects: 4764\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.09634333848953247\n",
            "Train step - Step 1500, Loss 0.08717544376850128\n",
            "Train step - Step 1510, Loss 0.08811482042074203\n",
            "Train step - Step 1520, Loss 0.08923672884702682\n",
            "Train step - Step 1530, Loss 0.08147948235273361\n",
            "Train epoch - Accuracy: 0.7011510791366906 Loss: 0.08768172647884424 Corrects: 4873\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.07794263958930969\n",
            "Train step - Step 1550, Loss 0.09044281393289566\n",
            "Train step - Step 1560, Loss 0.08175473660230637\n",
            "Train step - Step 1570, Loss 0.0930468961596489\n",
            "Train step - Step 1580, Loss 0.08658254146575928\n",
            "Train step - Step 1590, Loss 0.07934921234846115\n",
            "Train epoch - Accuracy: 0.6992805755395683 Loss: 0.08738481337861191 Corrects: 4860\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.09282620996236801\n",
            "Train step - Step 1610, Loss 0.08408072590827942\n",
            "Train step - Step 1620, Loss 0.09264273941516876\n",
            "Train step - Step 1630, Loss 0.08586331456899643\n",
            "Train step - Step 1640, Loss 0.08752615004777908\n",
            "Train epoch - Accuracy: 0.697841726618705 Loss: 0.08725031874805904 Corrects: 4850\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.07930278033018112\n",
            "Train step - Step 1660, Loss 0.08849040418863297\n",
            "Train step - Step 1670, Loss 0.08729168027639389\n",
            "Train step - Step 1680, Loss 0.08867409825325012\n",
            "Train step - Step 1690, Loss 0.07749109715223312\n",
            "Train step - Step 1700, Loss 0.09094701707363129\n",
            "Train epoch - Accuracy: 0.7106474820143885 Loss: 0.08703734857144116 Corrects: 4939\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.09048929065465927\n",
            "Train step - Step 1720, Loss 0.08356504887342453\n",
            "Train step - Step 1730, Loss 0.08424090594053268\n",
            "Train step - Step 1740, Loss 0.08474788814783096\n",
            "Train step - Step 1750, Loss 0.08735012263059616\n",
            "Train epoch - Accuracy: 0.7136690647482015 Loss: 0.08584581870826886 Corrects: 4960\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.07971066236495972\n",
            "Train step - Step 1770, Loss 0.08698461204767227\n",
            "Train step - Step 1780, Loss 0.08507969230413437\n",
            "Train step - Step 1790, Loss 0.08864626288414001\n",
            "Train step - Step 1800, Loss 0.08239445835351944\n",
            "Train step - Step 1810, Loss 0.07583171129226685\n",
            "Train epoch - Accuracy: 0.7253237410071942 Loss: 0.08448212118886357 Corrects: 5041\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.07364539057016373\n",
            "Train step - Step 1830, Loss 0.08390919119119644\n",
            "Train step - Step 1840, Loss 0.07670706510543823\n",
            "Train step - Step 1850, Loss 0.08950991183519363\n",
            "Train step - Step 1860, Loss 0.08510259538888931\n",
            "Train epoch - Accuracy: 0.7266187050359713 Loss: 0.0840875696749996 Corrects: 5050\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.08284159749746323\n",
            "Train step - Step 1880, Loss 0.08583543449640274\n",
            "Train step - Step 1890, Loss 0.09001516550779343\n",
            "Train step - Step 1900, Loss 0.08738614618778229\n",
            "Train step - Step 1910, Loss 0.08585526794195175\n",
            "Train step - Step 1920, Loss 0.08405327051877975\n",
            "Train epoch - Accuracy: 0.7333812949640288 Loss: 0.0834215130587276 Corrects: 5097\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.07540019601583481\n",
            "Train step - Step 1940, Loss 0.07869986444711685\n",
            "Train step - Step 1950, Loss 0.0729476809501648\n",
            "Train step - Step 1960, Loss 0.08516658842563629\n",
            "Train step - Step 1970, Loss 0.08709259331226349\n",
            "Train epoch - Accuracy: 0.7381294964028777 Loss: 0.08377414759328897 Corrects: 5130\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.07943244278430939\n",
            "Train step - Step 1990, Loss 0.07961037009954453\n",
            "Train step - Step 2000, Loss 0.07295819371938705\n",
            "Train step - Step 2010, Loss 0.08755698055028915\n",
            "Train step - Step 2020, Loss 0.09211256355047226\n",
            "Train step - Step 2030, Loss 0.08329223841428757\n",
            "Train epoch - Accuracy: 0.740431654676259 Loss: 0.08308536692488966 Corrects: 5146\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.08170788735151291\n",
            "Train step - Step 2050, Loss 0.08275917172431946\n",
            "Train step - Step 2060, Loss 0.0858304426074028\n",
            "Train step - Step 2070, Loss 0.07858647406101227\n",
            "Train step - Step 2080, Loss 0.08792652934789658\n",
            "Train epoch - Accuracy: 0.7369784172661871 Loss: 0.08184769358351934 Corrects: 5122\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.08494587242603302\n",
            "Train step - Step 2100, Loss 0.07336889207363129\n",
            "Train step - Step 2110, Loss 0.0828673243522644\n",
            "Train step - Step 2120, Loss 0.07628609240055084\n",
            "Train step - Step 2130, Loss 0.08665778487920761\n",
            "Train step - Step 2140, Loss 0.07487447559833527\n",
            "Train epoch - Accuracy: 0.7541007194244604 Loss: 0.08176011660973803 Corrects: 5241\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.0885625034570694\n",
            "Train step - Step 2160, Loss 0.08166667073965073\n",
            "Train step - Step 2170, Loss 0.08018147945404053\n",
            "Train step - Step 2180, Loss 0.08001047372817993\n",
            "Train step - Step 2190, Loss 0.08148421347141266\n",
            "Train epoch - Accuracy: 0.7515107913669065 Loss: 0.08158057080755989 Corrects: 5223\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.07706590741872787\n",
            "Train step - Step 2210, Loss 0.0797080472111702\n",
            "Train step - Step 2220, Loss 0.08511817455291748\n",
            "Train step - Step 2230, Loss 0.087377168238163\n",
            "Train step - Step 2240, Loss 0.08352716267108917\n",
            "Train step - Step 2250, Loss 0.07630030065774918\n",
            "Train epoch - Accuracy: 0.7571223021582734 Loss: 0.0802971484978422 Corrects: 5262\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.08549987524747849\n",
            "Train step - Step 2270, Loss 0.08611471205949783\n",
            "Train step - Step 2280, Loss 0.07016594707965851\n",
            "Train step - Step 2290, Loss 0.07858879119157791\n",
            "Train step - Step 2300, Loss 0.08604902774095535\n",
            "Train epoch - Accuracy: 0.759136690647482 Loss: 0.08114769571547886 Corrects: 5276\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.08030302822589874\n",
            "Train step - Step 2320, Loss 0.07497315108776093\n",
            "Train step - Step 2330, Loss 0.07517413049936295\n",
            "Train step - Step 2340, Loss 0.08480717986822128\n",
            "Train step - Step 2350, Loss 0.07881420105695724\n",
            "Train step - Step 2360, Loss 0.07360482960939407\n",
            "Train epoch - Accuracy: 0.76431654676259 Loss: 0.08045893929201922 Corrects: 5312\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.08918626606464386\n",
            "Train step - Step 2380, Loss 0.07610037922859192\n",
            "Train step - Step 2390, Loss 0.07993558794260025\n",
            "Train step - Step 2400, Loss 0.07343622297048569\n",
            "Train step - Step 2410, Loss 0.07503028959035873\n",
            "Train epoch - Accuracy: 0.7658992805755396 Loss: 0.08003820805884093 Corrects: 5323\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.08134033530950546\n",
            "Train step - Step 2430, Loss 0.08050155639648438\n",
            "Train step - Step 2440, Loss 0.08664305508136749\n",
            "Train step - Step 2450, Loss 0.07922056317329407\n",
            "Train step - Step 2460, Loss 0.09436456859111786\n",
            "Train step - Step 2470, Loss 0.07809104770421982\n",
            "Train epoch - Accuracy: 0.7706474820143885 Loss: 0.07974407131508958 Corrects: 5356\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.08621776103973389\n",
            "Train step - Step 2490, Loss 0.06996627151966095\n",
            "Train step - Step 2500, Loss 0.07676687836647034\n",
            "Train step - Step 2510, Loss 0.07017969340085983\n",
            "Train step - Step 2520, Loss 0.08751892298460007\n",
            "Train epoch - Accuracy: 0.7730935251798561 Loss: 0.07828936831771041 Corrects: 5373\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.08088164776563644\n",
            "Train step - Step 2540, Loss 0.07686486840248108\n",
            "Train step - Step 2550, Loss 0.08857851475477219\n",
            "Train step - Step 2560, Loss 0.07915546745061874\n",
            "Train step - Step 2570, Loss 0.07420473545789719\n",
            "Train step - Step 2580, Loss 0.08481122553348541\n",
            "Train epoch - Accuracy: 0.7755395683453238 Loss: 0.078529702918135 Corrects: 5390\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.08124800771474838\n",
            "Train step - Step 2600, Loss 0.08041898161172867\n",
            "Train step - Step 2610, Loss 0.08211817592382431\n",
            "Train step - Step 2620, Loss 0.07901903241872787\n",
            "Train step - Step 2630, Loss 0.07663803547620773\n",
            "Train epoch - Accuracy: 0.7728057553956834 Loss: 0.07713004355379145 Corrects: 5371\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.07974462956190109\n",
            "Train step - Step 2650, Loss 0.0830555334687233\n",
            "Train step - Step 2660, Loss 0.08138811588287354\n",
            "Train step - Step 2670, Loss 0.08925288170576096\n",
            "Train step - Step 2680, Loss 0.08188947290182114\n",
            "Train step - Step 2690, Loss 0.07767104357481003\n",
            "Train epoch - Accuracy: 0.7994244604316547 Loss: 0.07578067874522518 Corrects: 5556\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.07252953201532364\n",
            "Train step - Step 2710, Loss 0.05989137291908264\n",
            "Train step - Step 2720, Loss 0.06925400346517563\n",
            "Train step - Step 2730, Loss 0.06764595955610275\n",
            "Train step - Step 2740, Loss 0.07116127014160156\n",
            "Train epoch - Accuracy: 0.8048920863309352 Loss: 0.071172800604388 Corrects: 5594\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.07476639002561569\n",
            "Train step - Step 2760, Loss 0.07601749151945114\n",
            "Train step - Step 2770, Loss 0.06429257988929749\n",
            "Train step - Step 2780, Loss 0.0783190131187439\n",
            "Train step - Step 2790, Loss 0.07094205915927887\n",
            "Train step - Step 2800, Loss 0.06262075155973434\n",
            "Train epoch - Accuracy: 0.8105035971223021 Loss: 0.06938832835542212 Corrects: 5633\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.07437986135482788\n",
            "Train step - Step 2820, Loss 0.07001225650310516\n",
            "Train step - Step 2830, Loss 0.06506583839654922\n",
            "Train step - Step 2840, Loss 0.06593982130289078\n",
            "Train step - Step 2850, Loss 0.07529112696647644\n",
            "Train epoch - Accuracy: 0.8125179856115108 Loss: 0.06980720868213572 Corrects: 5647\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.06447569280862808\n",
            "Train step - Step 2870, Loss 0.07000371068716049\n",
            "Train step - Step 2880, Loss 0.06963162124156952\n",
            "Train step - Step 2890, Loss 0.06636413186788559\n",
            "Train step - Step 2900, Loss 0.06860849261283875\n",
            "Train step - Step 2910, Loss 0.07622908800840378\n",
            "Train epoch - Accuracy: 0.816115107913669 Loss: 0.06936502665924511 Corrects: 5672\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.06749660521745682\n",
            "Train step - Step 2930, Loss 0.06826024502515793\n",
            "Train step - Step 2940, Loss 0.061849117279052734\n",
            "Train step - Step 2950, Loss 0.07017137855291367\n",
            "Train step - Step 2960, Loss 0.06735862046480179\n",
            "Train epoch - Accuracy: 0.8162589928057554 Loss: 0.06874214718667723 Corrects: 5673\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.07207958400249481\n",
            "Train step - Step 2980, Loss 0.0719502866268158\n",
            "Train step - Step 2990, Loss 0.06928153336048126\n",
            "Train step - Step 3000, Loss 0.06428641080856323\n",
            "Train step - Step 3010, Loss 0.06831766664981842\n",
            "Train step - Step 3020, Loss 0.07425983250141144\n",
            "Train epoch - Accuracy: 0.8201438848920863 Loss: 0.06837133668952709 Corrects: 5700\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.07646727561950684\n",
            "Train step - Step 3040, Loss 0.0709725171327591\n",
            "Train step - Step 3050, Loss 0.061700787395238876\n",
            "Train step - Step 3060, Loss 0.06753735989332199\n",
            "Train step - Step 3070, Loss 0.07541318982839584\n",
            "Train epoch - Accuracy: 0.818705035971223 Loss: 0.06870097133538706 Corrects: 5690\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.07458435744047165\n",
            "Train step - Step 3090, Loss 0.06673649698495865\n",
            "Train step - Step 3100, Loss 0.065317802131176\n",
            "Train step - Step 3110, Loss 0.07203247398138046\n",
            "Train step - Step 3120, Loss 0.06310570240020752\n",
            "Train step - Step 3130, Loss 0.06544651836156845\n",
            "Train epoch - Accuracy: 0.8191366906474821 Loss: 0.06800679594707146 Corrects: 5693\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.06858061999082565\n",
            "Train step - Step 3150, Loss 0.06900080293416977\n",
            "Train step - Step 3160, Loss 0.06967704743146896\n",
            "Train step - Step 3170, Loss 0.07758313417434692\n",
            "Train step - Step 3180, Loss 0.06984340399503708\n",
            "Train epoch - Accuracy: 0.820863309352518 Loss: 0.06762577895852302 Corrects: 5705\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.06835265457630157\n",
            "Train step - Step 3200, Loss 0.07019121944904327\n",
            "Train step - Step 3210, Loss 0.07439161837100983\n",
            "Train step - Step 3220, Loss 0.0673198327422142\n",
            "Train step - Step 3230, Loss 0.06697966158390045\n",
            "Train step - Step 3240, Loss 0.06831631064414978\n",
            "Train epoch - Accuracy: 0.8237410071942446 Loss: 0.06768142867645771 Corrects: 5725\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.06446557492017746\n",
            "Train step - Step 3260, Loss 0.07153867930173874\n",
            "Train step - Step 3270, Loss 0.06854569166898727\n",
            "Train step - Step 3280, Loss 0.0750897005200386\n",
            "Train step - Step 3290, Loss 0.06718166172504425\n",
            "Train epoch - Accuracy: 0.8270503597122302 Loss: 0.06745510075160925 Corrects: 5748\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.06075084209442139\n",
            "Train step - Step 3310, Loss 0.06793849915266037\n",
            "Train step - Step 3320, Loss 0.06748151034116745\n",
            "Train step - Step 3330, Loss 0.06385751068592072\n",
            "Train step - Step 3340, Loss 0.06405927985906601\n",
            "Train step - Step 3350, Loss 0.06225118786096573\n",
            "Train epoch - Accuracy: 0.8299280575539568 Loss: 0.06744642545422204 Corrects: 5768\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.06337190419435501\n",
            "Train step - Step 3370, Loss 0.06923902034759521\n",
            "Train step - Step 3380, Loss 0.0682426244020462\n",
            "Train step - Step 3390, Loss 0.0659913420677185\n",
            "Train step - Step 3400, Loss 0.0750669464468956\n",
            "Train epoch - Accuracy: 0.8280575539568346 Loss: 0.06662913817295925 Corrects: 5755\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.06196798011660576\n",
            "Train step - Step 3420, Loss 0.06485550850629807\n",
            "Train step - Step 3430, Loss 0.06348340958356857\n",
            "Train step - Step 3440, Loss 0.06361221522092819\n",
            "Train step - Step 3450, Loss 0.06026061251759529\n",
            "Train step - Step 3460, Loss 0.06681904941797256\n",
            "Train epoch - Accuracy: 0.8302158273381295 Loss: 0.06655740247355948 Corrects: 5770\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.072975292801857\n",
            "Train step - Step 3480, Loss 0.05933220311999321\n",
            "Train step - Step 3490, Loss 0.059193920344114304\n",
            "Train step - Step 3500, Loss 0.06796402484178543\n",
            "Train step - Step 3510, Loss 0.062430489808321\n",
            "Train epoch - Accuracy: 0.8309352517985612 Loss: 0.06638119916049696 Corrects: 5775\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.0608590729534626\n",
            "Train step - Step 3530, Loss 0.07800799608230591\n",
            "Train step - Step 3540, Loss 0.05905953049659729\n",
            "Train step - Step 3550, Loss 0.06221465393900871\n",
            "Train step - Step 3560, Loss 0.07207079231739044\n",
            "Train step - Step 3570, Loss 0.06744766235351562\n",
            "Train epoch - Accuracy: 0.837410071942446 Loss: 0.06568886065440212 Corrects: 5820\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.06388998031616211\n",
            "Train step - Step 3590, Loss 0.057940542697906494\n",
            "Train step - Step 3600, Loss 0.0702398270368576\n",
            "Train step - Step 3610, Loss 0.06380406767129898\n",
            "Train step - Step 3620, Loss 0.06191299483180046\n",
            "Train epoch - Accuracy: 0.8329496402877697 Loss: 0.06560417303507277 Corrects: 5789\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.05899148806929588\n",
            "Train step - Step 3640, Loss 0.060915183275938034\n",
            "Train step - Step 3650, Loss 0.06338294595479965\n",
            "Train step - Step 3660, Loss 0.06035848334431648\n",
            "Train step - Step 3670, Loss 0.06344914436340332\n",
            "Train step - Step 3680, Loss 0.0763983279466629\n",
            "Train epoch - Accuracy: 0.837841726618705 Loss: 0.06511312590871783 Corrects: 5823\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.06606154143810272\n",
            "Train step - Step 3700, Loss 0.06260736286640167\n",
            "Train step - Step 3710, Loss 0.0638287290930748\n",
            "Train step - Step 3720, Loss 0.07076075673103333\n",
            "Train step - Step 3730, Loss 0.06363384425640106\n",
            "Train epoch - Accuracy: 0.8356834532374101 Loss: 0.06538867134413273 Corrects: 5808\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.0670810341835022\n",
            "Train step - Step 3750, Loss 0.062466394156217575\n",
            "Train step - Step 3760, Loss 0.06771600246429443\n",
            "Train step - Step 3770, Loss 0.06633460521697998\n",
            "Train step - Step 3780, Loss 0.05785433202981949\n",
            "Train step - Step 3790, Loss 0.05846219137310982\n",
            "Train epoch - Accuracy: 0.8364028776978417 Loss: 0.06521887555396814 Corrects: 5813\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.06871316581964493\n",
            "Train step - Step 3810, Loss 0.06317370384931564\n",
            "Train step - Step 3820, Loss 0.060373373329639435\n",
            "Train step - Step 3830, Loss 0.05292872339487076\n",
            "Train step - Step 3840, Loss 0.06337867677211761\n",
            "Train epoch - Accuracy: 0.8332374100719424 Loss: 0.06565725827388626 Corrects: 5791\n",
            "Training finished in 391.5239861011505 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60]\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238246690>\n",
            "Constructing exemplars of class 95\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [32186, 26713, 38023, 42809, 45990, 42270, 3547, 22104, 34010, 46326, 33476, 44573, 24463, 44400, 16062, 35680, 4692, 33093, 6109, 41274, 34789, 43747, 31116, 36160, 6485, 36315, 1046, 9398, 12520, 151, 8536, 25703, 6860, 18987, 41953, 7551, 22863, 8358, 7046, 46579, 8166, 24552, 12686, 29929, 6505, 3321, 9249, 32678, 20478, 26438, 9249, 19353, 48605, 21403, 36988, 261, 9422, 20410, 38023, 5657, 23873, 2028, 44254, 6501, 15488, 49037, 8148, 1246, 15850, 1304, 44254, 19214, 49109, 42728, 23225, 20104, 38072, 49683, 29397, 4466, 48605, 16946, 24974, 47545, 31610, 30193, 15850, 28993, 37094, 29176, 22863, 4692, 45775, 32001, 4281, 46459, 15696, 49059, 14016, 1568]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238250250>\n",
            "Constructing exemplars of class 59\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [23545, 45838, 44095, 39867, 20216, 19474, 37533, 21624, 49401, 36223, 982, 37719, 29305, 20481, 25142, 46102, 8931, 8676, 5672, 37490, 8113, 22436, 45468, 29352, 5848, 28258, 16001, 17943, 48988, 28526, 16288, 40888, 49789, 42330, 27824, 34455, 25774, 18865, 19474, 24261, 15079, 2834, 45659, 5848, 1927, 42060, 14413, 6571, 6283, 39867, 28724, 31607, 37228, 21624, 14671, 46599, 17943, 43511, 35106, 34163, 47226, 13105, 13249, 32614, 38064, 14358, 20491, 6100, 13873, 849, 14478, 27824, 26113, 27601, 48062, 3488, 16709, 45339, 16295, 37719, 15381, 7118, 14376, 47268, 16295, 41678, 31150, 28208, 34504, 5504, 32778, 31097, 5277, 33936, 38957, 16446, 12254, 38064, 8271, 44581]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224140d290>\n",
            "Constructing exemplars of class 58\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [37824, 12360, 8420, 32617, 37677, 9675, 8635, 35887, 28288, 1868, 33374, 17397, 16728, 21254, 49640, 4060, 15070, 7012, 33641, 40195, 46080, 25374, 17308, 40385, 2937, 38588, 21533, 15471, 23558, 4204, 3247, 21251, 46859, 6213, 17912, 806, 32988, 40195, 1276, 12305, 22875, 15921, 46859, 19550, 38641, 4843, 27657, 38394, 2436, 31306, 14966, 19340, 14563, 26996, 2646, 30320, 1364, 4898, 4470, 46080, 2646, 22402, 30648, 5971, 9540, 48857, 47002, 31122, 42827, 16657, 10885, 38296, 5611, 3916, 37709, 46535, 27465, 43421, 32867, 41401, 22831, 34107, 3450, 27231, 6423, 16795, 36494, 27665, 26743, 37653, 34611, 46406, 15142, 49640, 12219, 38663, 25074, 6171, 48857, 10685]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223821bb10>\n",
            "Constructing exemplars of class 34\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [17608, 23639, 10770, 7312, 9271, 8406, 40004, 17632, 8112, 28868, 24929, 17839, 35219, 43022, 28339, 36114, 20437, 43552, 8406, 18397, 27418, 38997, 5064, 17549, 8949, 4556, 19416, 3773, 42158, 49973, 1938, 41170, 38174, 22132, 8494, 40789, 19437, 21205, 11594, 1394, 31527, 42036, 31794, 49978, 18275, 3283, 8396, 32312, 3181, 3512, 1447, 17581, 16662, 18727, 23992, 12228, 14310, 49973, 45164, 42049, 42189, 36177, 43683, 16299, 21112, 40789, 26595, 8494, 19311, 31783, 22477, 13481, 28868, 20461, 1938, 12267, 35708, 4001, 13758, 3868, 120, 30789, 40779, 26432, 22089, 27238, 33555, 10911, 9271, 25992, 21662, 11804, 13758, 28439, 1938, 45960, 19440, 16944, 14459, 28672]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2231718b10>\n",
            "Constructing exemplars of class 81\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [7373, 33201, 17952, 19383, 26521, 41277, 21025, 29807, 15624, 41112, 37599, 8559, 9254, 33016, 37741, 4954, 32660, 9281, 24275, 12683, 20487, 14338, 41058, 31727, 22344, 12346, 39790, 20328, 24481, 34515, 24965, 6258, 22076, 13354, 29415, 22144, 22512, 10584, 12751, 15149, 1429, 4904, 36593, 36497, 16499, 186, 43371, 14660, 37917, 9275, 25550, 21118, 11575, 24640, 20967, 45181, 7533, 3802, 21829, 39569, 12246, 25283, 29807, 48255, 14621, 47085, 20081, 29471, 30365, 18666, 26745, 17913, 48259, 7208, 41058, 12588, 11885, 19430, 19146, 39819, 41333, 33718, 29751, 33428, 9348, 48259, 35097, 5539, 9696, 20657, 37252, 29408, 21025, 38668, 11675, 675, 15624, 23286, 20470, 12746]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a95a10>\n",
            "Constructing exemplars of class 49\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [40611, 44090, 45236, 26777, 14246, 23516, 45650, 17241, 18112, 45369, 17776, 47850, 33744, 13062, 12860, 27122, 34446, 17524, 36563, 14536, 17737, 29462, 31721, 46595, 31350, 47029, 27276, 38402, 30596, 34982, 34092, 14890, 2265, 32239, 14929, 23516, 2920, 515, 20833, 27289, 34303, 11517, 29567, 2372, 36073, 24605, 7209, 1140, 21923, 41320, 23625, 18996, 36643, 6747, 16363, 23913, 28384, 2219, 46973, 14929, 41836, 33565, 46931, 7785, 42740, 29777, 8498, 18189, 3638, 22247, 27888, 46767, 23625, 18112, 19048, 204, 23342, 6287, 34801, 5529, 25737, 29567, 3750, 12189, 2294, 49966, 27289, 38731, 1542, 35550, 42904, 11517, 5061, 8181, 31578, 22292, 467, 6287, 3638, 43738]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232f37510>\n",
            "Constructing exemplars of class 13\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [20536, 11395, 35183, 12091, 29418, 38523, 33916, 49198, 29916, 10130, 47791, 28335, 39012, 38685, 35365, 49326, 27555, 24097, 35201, 7422, 30495, 40220, 786, 24313, 18481, 12655, 38685, 43496, 47819, 35821, 5411, 37720, 31336, 19445, 40043, 40047, 49326, 16607, 7509, 34252, 32936, 28765, 41936, 28334, 240, 3803, 5108, 29916, 24621, 39918, 24202, 29766, 36769, 46129, 21373, 18285, 7509, 12379, 3798, 41944, 7397, 27002, 5243, 2471, 3300, 32018, 40139, 38685, 240, 39012, 19445, 33026, 18334, 32257, 13536, 10298, 46223, 12900, 39903, 34191, 41936, 33845, 33287, 19537, 15928, 24534, 21193, 10848, 40220, 45728, 10735, 3908, 5175, 16053, 46223, 33048, 41936, 13171, 20524, 40043]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22437a4090>\n",
            "Constructing exemplars of class 88\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [18125, 3372, 25616, 10043, 47259, 410, 39507, 32080, 31666, 19590, 25659, 21681, 1766, 5859, 20173, 37988, 23488, 42247, 5092, 31216, 18631, 34079, 48573, 3992, 42150, 48057, 8729, 30573, 19558, 18424, 10554, 20369, 30652, 10708, 31529, 41775, 10636, 22995, 23703, 49375, 27419, 22568, 36967, 23220, 37571, 48977, 17428, 27437, 48434, 18283, 27437, 42342, 26433, 3494, 25097, 45933, 28487, 34538, 37945, 44312, 19590, 31666, 10708, 39639, 11344, 2680, 49131, 1638, 26724, 31872, 17206, 11935, 8284, 34045, 7691, 14905, 29762, 38485, 45452, 18258, 39639, 49844, 38821, 15437, 39998, 337, 41869, 44936, 25092, 33890, 32562, 32702, 1215, 41108, 33302, 42342, 40569, 5859, 19775, 27300]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a95a10>\n",
            "Constructing exemplars of class 68\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [12280, 15573, 21958, 47878, 11225, 9030, 16427, 29487, 43742, 28450, 7778, 21198, 38968, 2450, 26732, 31044, 39862, 19692, 17868, 16168, 10601, 209, 44458, 17859, 32181, 46100, 45044, 18947, 31919, 31749, 37974, 31765, 7963, 30341, 20256, 7915, 26327, 39615, 28655, 19380, 45993, 33990, 47799, 31765, 32628, 33189, 9347, 24338, 3170, 37930, 12826, 6449, 4361, 36856, 7915, 45020, 20734, 28217, 5934, 21628, 28450, 4361, 22454, 37932, 17950, 42956, 45613, 39173, 47074, 7963, 45020, 7915, 37274, 19149, 8137, 31953, 44939, 25899, 27702, 17950, 46305, 4930, 36010, 24915, 32918, 10351, 28443, 12920, 17614, 13132, 41640, 32693, 15864, 23689, 3845, 22188, 44639, 23724, 932, 37253]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232feeb90>\n",
            "Constructing exemplars of class 60\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [34786, 28818, 15514, 3685, 3872, 38494, 2389, 34008, 39924, 7364, 34100, 41753, 29787, 32347, 49469, 37184, 11340, 42186, 38947, 11013, 13067, 20062, 321, 1335, 48517, 20729, 23440, 4172, 23213, 997, 7814, 6904, 29787, 29843, 29774, 3674, 23484, 27501, 31284, 39410, 35494, 25424, 18485, 25648, 23213, 19902, 44937, 14350, 27836, 46939, 35234, 25766, 27545, 23267, 34587, 39957, 20752, 24959, 12940, 35461, 33985, 39171, 24862, 27635, 36949, 38662, 30095, 18047, 32966, 16305, 23195, 25237, 38947, 9301, 33985, 8694, 34100, 17797, 18379, 30018, 10801, 8295, 17604, 18246, 2354, 16794, 34954, 28793, 4172, 45827, 27836, 20798, 39241, 21800, 4465, 25213, 17543, 5795, 34587, 3685]\n",
            "x train:  [-0.12761475 -0.15750642 -0.23773655 -0.08929659 -0.0321173  -0.2115917\n",
            " -0.08551868 -0.144941   -0.05664165 -0.0589585  -0.16049495 -0.46802476\n",
            " -0.10960289  0.2163591  -0.23706165 -0.3072646  -0.30117854 -0.23612618\n",
            " -0.2998108  -0.3507351 ]\n",
            "y_train:  [tensor([34]), tensor([78]), tensor([50]), tensor([50]), tensor([27]), tensor([13]), tensor([86]), tensor([58]), tensor([69]), tensor([49]), tensor([34]), tensor([27]), tensor([60]), tensor([82]), tensor([82]), tensor([49]), tensor([58]), tensor([97]), tensor([82]), tensor([57]), tensor([86]), tensor([69]), tensor([97]), tensor([95]), tensor([13]), tensor([30]), tensor([97]), tensor([78]), tensor([34]), tensor([25]), tensor([69]), tensor([58]), tensor([69]), tensor([78]), tensor([68]), tensor([86]), tensor([27]), tensor([25]), tensor([69]), tensor([30]), tensor([69]), tensor([59]), tensor([78]), tensor([69]), tensor([78]), tensor([30]), tensor([88]), tensor([95]), tensor([13]), tensor([25]), tensor([68]), tensor([58]), tensor([82]), tensor([57]), tensor([97]), tensor([13]), tensor([13]), tensor([68]), tensor([95]), tensor([27]), tensor([27]), tensor([60]), tensor([97]), tensor([95]), tensor([82]), tensor([69]), tensor([50]), tensor([59]), tensor([13]), tensor([57]), tensor([30]), tensor([97]), tensor([97]), tensor([49]), tensor([60]), tensor([81]), tensor([25]), tensor([97]), tensor([13]), tensor([95]), tensor([50]), tensor([25]), tensor([88]), tensor([69]), tensor([25]), tensor([50]), tensor([95]), tensor([50]), tensor([86]), tensor([25]), tensor([86]), tensor([82]), tensor([50]), tensor([58]), tensor([81]), tensor([97]), tensor([57]), tensor([82]), tensor([69]), tensor([27]), tensor([27]), tensor([69]), tensor([59]), tensor([34]), tensor([25]), tensor([88]), tensor([50]), tensor([49]), tensor([57]), tensor([50]), tensor([58]), tensor([27]), tensor([27]), tensor([58]), tensor([82]), tensor([82]), tensor([68]), tensor([34]), tensor([82]), tensor([59]), tensor([97]), tensor([57]), tensor([78]), tensor([68]), tensor([95]), tensor([25]), tensor([88]), tensor([59]), tensor([49]), tensor([97]), tensor([78]), tensor([27]), tensor([86]), tensor([86]), tensor([88]), tensor([78]), tensor([81]), tensor([34]), tensor([50]), tensor([57]), tensor([25]), tensor([13]), tensor([25]), tensor([78]), tensor([25]), tensor([86]), tensor([68]), tensor([49]), tensor([95]), tensor([27]), tensor([59]), tensor([27]), tensor([59]), tensor([88]), tensor([60]), tensor([30]), tensor([27]), tensor([97]), tensor([60]), tensor([13]), tensor([13]), tensor([78]), tensor([27]), tensor([69]), tensor([13]), tensor([68]), tensor([88]), tensor([50]), tensor([13]), tensor([82]), tensor([58]), tensor([81]), tensor([27]), tensor([60]), tensor([58]), tensor([82]), tensor([86]), tensor([30]), tensor([13]), tensor([69]), tensor([59]), tensor([30]), tensor([49]), tensor([69]), tensor([68]), tensor([78]), tensor([95]), tensor([13]), tensor([58]), tensor([78]), tensor([59]), tensor([49]), tensor([86]), tensor([88]), tensor([88]), tensor([86]), tensor([58]), tensor([49]), tensor([86]), tensor([25]), tensor([49]), tensor([57]), tensor([78]), tensor([25]), tensor([82]), tensor([49]), tensor([82]), tensor([88]), tensor([86]), tensor([50]), tensor([50]), tensor([25]), tensor([25]), tensor([82]), tensor([59]), tensor([97]), tensor([68]), tensor([25]), tensor([30]), tensor([30]), tensor([27]), tensor([30]), tensor([78]), tensor([82]), tensor([68]), tensor([95]), tensor([30]), tensor([50]), tensor([13]), tensor([68]), tensor([86]), tensor([95]), tensor([95]), tensor([49]), tensor([82]), tensor([57]), tensor([49]), tensor([58]), tensor([69]), tensor([95]), tensor([88]), tensor([81]), tensor([60]), tensor([27]), tensor([50]), tensor([86]), tensor([27]), tensor([69]), tensor([49]), tensor([82]), tensor([97]), tensor([69]), tensor([27]), tensor([88]), tensor([95]), tensor([30]), tensor([27]), tensor([88]), tensor([58]), tensor([49]), tensor([60]), tensor([50]), tensor([25]), tensor([86]), tensor([81]), tensor([25]), tensor([49]), tensor([13]), tensor([81]), tensor([78]), tensor([34]), tensor([59]), tensor([82]), tensor([13]), tensor([88]), tensor([50]), tensor([88]), tensor([25]), tensor([59]), tensor([69]), tensor([30]), tensor([68]), tensor([30]), tensor([69]), tensor([13]), tensor([59]), tensor([88]), tensor([60]), tensor([34]), tensor([81]), tensor([95]), tensor([78]), tensor([58]), tensor([60]), tensor([25]), tensor([86]), tensor([30]), tensor([30]), tensor([58]), tensor([81]), tensor([49]), tensor([58]), tensor([60]), tensor([58]), tensor([82]), tensor([68]), tensor([97]), tensor([25]), tensor([59]), tensor([78]), tensor([25]), tensor([58]), tensor([95]), tensor([81]), tensor([13]), tensor([59]), tensor([81]), tensor([69]), tensor([57]), tensor([50]), tensor([78]), tensor([88]), tensor([86]), tensor([60]), tensor([82]), tensor([58]), tensor([25]), tensor([50]), tensor([69]), tensor([95]), tensor([13]), tensor([30]), tensor([68]), tensor([97]), tensor([68]), tensor([50]), tensor([34]), tensor([30]), tensor([57]), tensor([97]), tensor([86]), tensor([86]), tensor([57]), tensor([57]), tensor([81]), tensor([68]), tensor([95]), tensor([68]), tensor([30]), tensor([86]), tensor([88]), tensor([13]), tensor([97]), tensor([50]), tensor([49]), tensor([49]), tensor([60]), tensor([68]), tensor([50]), tensor([25]), tensor([78]), tensor([78]), tensor([60]), tensor([49]), tensor([49]), tensor([50]), tensor([59]), tensor([59]), tensor([58]), tensor([86]), tensor([49]), tensor([88]), tensor([88]), tensor([97]), tensor([57]), tensor([82]), tensor([57]), tensor([13]), tensor([49]), tensor([49]), tensor([95]), tensor([27]), tensor([95]), tensor([97]), tensor([13]), tensor([59]), tensor([82]), tensor([58]), tensor([30]), tensor([30]), tensor([97]), tensor([60]), tensor([57]), tensor([60]), tensor([58]), tensor([13]), tensor([25]), tensor([34]), tensor([59]), tensor([86]), tensor([81]), tensor([69]), tensor([59]), tensor([49]), tensor([13]), tensor([49]), tensor([25]), tensor([34]), tensor([88]), tensor([13]), tensor([13]), tensor([30]), tensor([60]), tensor([50]), tensor([50]), tensor([57]), tensor([69]), tensor([86]), tensor([97]), tensor([58]), tensor([69]), tensor([68]), tensor([82]), tensor([60]), tensor([86]), tensor([60]), tensor([88]), tensor([88]), tensor([50]), tensor([50]), tensor([49]), tensor([49]), tensor([50]), tensor([81]), tensor([49]), tensor([68]), tensor([69]), tensor([50]), tensor([60]), tensor([58]), tensor([34]), tensor([69]), tensor([59]), tensor([88]), tensor([58]), tensor([97]), tensor([30]), tensor([88]), tensor([49]), tensor([27]), tensor([68]), tensor([82]), tensor([60]), tensor([13]), tensor([58]), tensor([57]), tensor([69]), tensor([34]), tensor([95]), tensor([30]), tensor([57]), tensor([57]), tensor([57]), tensor([34]), tensor([81]), tensor([50]), tensor([50]), tensor([60]), tensor([86]), tensor([81]), tensor([69]), tensor([78]), tensor([27]), tensor([58]), tensor([30]), tensor([95]), tensor([68]), tensor([82]), tensor([30]), tensor([58]), tensor([82]), tensor([81]), tensor([27]), tensor([88]), tensor([81]), tensor([50]), tensor([25]), tensor([13]), tensor([78]), tensor([50]), tensor([27]), tensor([81]), tensor([86]), tensor([58]), tensor([25]), tensor([34]), tensor([30]), tensor([34]), tensor([86]), tensor([59]), tensor([82]), tensor([81]), tensor([49]), tensor([58]), tensor([81]), tensor([27]), tensor([25]), tensor([50]), tensor([57]), tensor([57]), tensor([82]), tensor([25]), tensor([68]), tensor([59]), tensor([34]), tensor([78]), tensor([58]), tensor([69]), tensor([81]), tensor([86]), tensor([68]), tensor([88]), tensor([25]), tensor([97]), tensor([86]), tensor([34]), tensor([86]), tensor([34]), tensor([50]), tensor([68]), tensor([69]), tensor([25]), tensor([30]), tensor([58]), tensor([25]), tensor([50]), tensor([25]), tensor([34]), tensor([58]), tensor([50]), tensor([95]), tensor([69]), tensor([34]), tensor([86]), tensor([27]), tensor([86]), tensor([34]), tensor([30]), tensor([68]), tensor([78]), tensor([88]), tensor([25]), tensor([13]), tensor([27]), tensor([68]), tensor([57]), tensor([69]), tensor([57]), tensor([58]), tensor([34]), tensor([97]), tensor([81]), tensor([78]), tensor([58]), tensor([60]), tensor([50]), tensor([69]), tensor([27]), tensor([57]), tensor([57]), tensor([27]), tensor([88]), tensor([30]), tensor([88]), tensor([34]), tensor([34]), tensor([49]), tensor([97]), tensor([86]), tensor([78]), tensor([58]), tensor([59]), tensor([95]), tensor([58]), tensor([49]), tensor([59]), tensor([57]), tensor([49]), tensor([86]), tensor([50]), tensor([82]), tensor([86]), tensor([68]), tensor([78]), tensor([50]), tensor([81]), tensor([30]), tensor([69]), tensor([34]), tensor([82]), tensor([69]), tensor([27]), tensor([95]), tensor([13]), tensor([49]), tensor([78]), tensor([34]), tensor([27]), tensor([60]), tensor([25]), tensor([58]), tensor([30]), tensor([49]), tensor([97]), tensor([34]), tensor([60]), tensor([95]), tensor([27]), tensor([49]), tensor([13]), tensor([78]), tensor([86]), tensor([59]), tensor([82]), tensor([97]), tensor([95]), tensor([78]), tensor([81]), tensor([50]), tensor([69]), tensor([34]), tensor([68]), tensor([58]), tensor([60]), tensor([78]), tensor([68]), tensor([82]), tensor([34]), tensor([30]), tensor([68]), tensor([60]), tensor([34]), tensor([59]), tensor([50]), tensor([97]), tensor([27]), tensor([13]), tensor([25]), tensor([95]), tensor([30]), tensor([78]), tensor([68]), tensor([88]), tensor([82]), tensor([27]), tensor([30]), tensor([58]), tensor([68]), tensor([81]), tensor([50]), tensor([82]), tensor([34]), tensor([81]), tensor([25]), tensor([88]), tensor([68]), tensor([82]), tensor([95]), tensor([60]), tensor([58]), tensor([25]), tensor([69]), tensor([25]), tensor([49]), tensor([50]), tensor([95]), tensor([68]), tensor([82]), tensor([69]), tensor([13]), tensor([60]), tensor([57]), tensor([60]), tensor([30]), tensor([50]), tensor([34]), tensor([78]), tensor([30]), tensor([27]), tensor([59]), tensor([88]), tensor([97]), tensor([86]), tensor([34]), tensor([50]), tensor([13]), tensor([95]), tensor([59]), tensor([59]), tensor([30]), tensor([57]), tensor([69]), tensor([60]), tensor([30]), tensor([95]), tensor([49]), tensor([60]), tensor([13]), tensor([78]), tensor([30]), tensor([50]), tensor([97]), tensor([58]), tensor([13]), tensor([78]), tensor([68]), tensor([69]), tensor([68]), tensor([50]), tensor([13]), tensor([88]), tensor([57]), tensor([13]), tensor([13]), tensor([59]), tensor([68]), tensor([95]), tensor([50]), tensor([97]), tensor([57]), tensor([13]), tensor([82]), tensor([58]), tensor([86]), tensor([82]), tensor([88]), tensor([30]), tensor([60]), tensor([27]), tensor([50]), tensor([13]), tensor([57]), tensor([58]), tensor([78]), tensor([13]), tensor([13]), tensor([88]), tensor([88]), tensor([49]), tensor([78]), tensor([86]), tensor([13]), tensor([25]), tensor([34]), tensor([69]), tensor([57]), tensor([57]), tensor([49]), tensor([59]), tensor([13]), tensor([13]), tensor([13]), tensor([58]), tensor([49]), tensor([25]), tensor([50]), tensor([59]), tensor([57]), tensor([30]), tensor([50]), tensor([78]), tensor([95]), tensor([34]), tensor([27]), tensor([25]), tensor([81]), tensor([34]), tensor([78]), tensor([97]), tensor([81]), tensor([86]), tensor([59]), tensor([34]), tensor([88]), tensor([25]), tensor([34]), tensor([27]), tensor([69]), tensor([81]), tensor([97]), tensor([58]), tensor([82]), tensor([60]), tensor([68]), tensor([68]), tensor([13]), tensor([60]), tensor([30]), tensor([68]), tensor([30]), tensor([88]), tensor([58]), tensor([13]), tensor([68]), tensor([57]), tensor([30]), tensor([88]), tensor([50]), tensor([81]), tensor([78]), tensor([60]), tensor([57]), tensor([97]), tensor([81]), tensor([95]), tensor([25]), tensor([78]), tensor([86]), tensor([60]), tensor([30]), tensor([69]), tensor([82]), tensor([27]), tensor([34]), tensor([86]), tensor([30]), tensor([59]), tensor([25]), tensor([30]), tensor([81]), tensor([81]), tensor([69]), tensor([82]), tensor([34]), tensor([59]), tensor([97]), tensor([30]), tensor([27]), tensor([88]), tensor([69]), tensor([97]), tensor([88]), tensor([25]), tensor([58]), tensor([68]), tensor([30]), tensor([59]), tensor([81]), tensor([60]), tensor([95]), tensor([95]), tensor([25]), tensor([88]), tensor([25]), tensor([68]), tensor([68]), tensor([81]), tensor([50]), tensor([81]), tensor([59]), tensor([57]), tensor([60]), tensor([34]), tensor([95]), tensor([50]), tensor([57]), tensor([27]), tensor([50]), tensor([57]), tensor([68]), tensor([82]), tensor([78]), tensor([88]), tensor([34]), tensor([34]), tensor([27]), tensor([57]), tensor([82]), tensor([57]), tensor([86]), tensor([13]), tensor([25]), tensor([34]), tensor([59]), tensor([86]), tensor([59]), tensor([82]), tensor([49]), tensor([82]), tensor([57]), tensor([13]), tensor([86]), tensor([58]), tensor([78]), tensor([50]), tensor([49]), tensor([34]), tensor([82]), tensor([88]), tensor([78]), tensor([57]), tensor([82]), tensor([34]), tensor([97]), tensor([13]), tensor([68]), tensor([97]), tensor([86]), tensor([78]), tensor([82]), tensor([27]), tensor([69]), tensor([49]), tensor([95]), tensor([57]), tensor([50]), tensor([30]), tensor([81]), tensor([34]), tensor([13]), tensor([34]), tensor([95]), tensor([25]), tensor([78]), tensor([81]), tensor([69]), tensor([25]), tensor([59]), tensor([60]), tensor([50]), tensor([27]), tensor([68]), tensor([57]), tensor([97]), tensor([59]), tensor([95]), tensor([95]), tensor([30]), tensor([57]), tensor([57]), tensor([69]), tensor([97]), tensor([68]), tensor([69]), tensor([78]), tensor([27]), tensor([27]), tensor([97]), tensor([34]), tensor([25]), tensor([30]), tensor([97]), tensor([82]), tensor([57]), tensor([50]), tensor([25]), tensor([58]), tensor([81]), tensor([50]), tensor([50]), tensor([49]), tensor([57]), tensor([68]), tensor([95]), tensor([68]), tensor([82]), tensor([57]), tensor([13]), tensor([81]), tensor([81]), tensor([88]), tensor([69]), tensor([81]), tensor([13]), tensor([81]), tensor([60]), tensor([49]), tensor([81]), tensor([27]), tensor([57]), tensor([59]), tensor([95]), tensor([68]), tensor([57]), tensor([68]), tensor([30]), tensor([95]), tensor([50]), tensor([49]), tensor([34]), tensor([88]), tensor([25]), tensor([30]), tensor([13]), tensor([95]), tensor([86]), tensor([13]), tensor([34]), tensor([59]), tensor([13]), tensor([30]), tensor([13]), tensor([59]), tensor([57]), tensor([60]), tensor([13]), tensor([34]), tensor([95]), tensor([88]), tensor([59]), tensor([59]), tensor([95]), tensor([34]), tensor([81]), tensor([60]), tensor([81]), tensor([58]), tensor([86]), tensor([88]), tensor([27]), tensor([68]), tensor([97]), tensor([57]), tensor([78]), tensor([86]), tensor([50]), tensor([30]), tensor([88]), tensor([60]), tensor([50]), tensor([13]), tensor([13]), tensor([60]), tensor([95]), tensor([59]), tensor([25]), tensor([57]), tensor([88]), tensor([88]), tensor([25]), tensor([49]), tensor([82]), tensor([27]), tensor([58]), tensor([86]), tensor([97]), tensor([68]), tensor([81]), tensor([59]), tensor([27]), tensor([60]), tensor([50]), tensor([58]), tensor([78]), tensor([78]), tensor([68]), tensor([58]), tensor([88]), tensor([34]), tensor([27]), tensor([95]), tensor([34]), tensor([97]), tensor([69]), tensor([78]), tensor([97]), tensor([49]), tensor([57]), tensor([95]), tensor([97]), tensor([57]), tensor([50]), tensor([27]), tensor([30]), tensor([86]), tensor([81]), tensor([60]), tensor([97]), tensor([13]), tensor([81]), tensor([95]), tensor([49]), tensor([50]), tensor([49]), tensor([59]), tensor([68]), tensor([49]), tensor([30]), tensor([82]), tensor([82]), tensor([82]), tensor([50]), tensor([95]), tensor([86]), tensor([81]), tensor([68]), tensor([60]), tensor([58]), tensor([13]), tensor([50]), tensor([59]), tensor([49]), tensor([34]), tensor([25]), tensor([34]), tensor([81]), tensor([13]), tensor([49]), tensor([59]), tensor([60]), tensor([60]), tensor([25]), tensor([49]), tensor([81]), tensor([30]), tensor([59]), tensor([69]), tensor([69]), tensor([86]), tensor([59]), tensor([49]), tensor([95]), tensor([57]), tensor([50]), tensor([68]), tensor([50]), tensor([69]), tensor([60]), tensor([82]), tensor([97]), tensor([30]), tensor([58]), tensor([69]), tensor([88]), tensor([13]), tensor([34]), tensor([27]), tensor([86]), tensor([68]), tensor([95]), tensor([34]), tensor([95]), tensor([30]), tensor([58]), tensor([25]), tensor([95]), tensor([88]), tensor([49]), tensor([58]), tensor([25]), tensor([30]), tensor([88]), tensor([82]), tensor([13]), tensor([86]), tensor([58]), tensor([68]), tensor([57]), tensor([49]), tensor([78]), tensor([34]), tensor([59]), tensor([97]), tensor([86]), tensor([78]), tensor([88]), tensor([82]), tensor([34]), tensor([88]), tensor([81]), tensor([97]), tensor([59]), tensor([27]), tensor([58]), tensor([60]), tensor([68]), tensor([69]), tensor([60]), tensor([34]), tensor([68]), tensor([81]), tensor([27]), tensor([27]), tensor([49]), tensor([13]), tensor([58]), tensor([69]), tensor([57]), tensor([97]), tensor([30]), tensor([60]), tensor([34]), tensor([25]), tensor([60]), tensor([34]), tensor([49]), tensor([78]), tensor([30]), tensor([13]), tensor([34]), tensor([69]), tensor([34]), tensor([78]), tensor([50]), tensor([27]), tensor([50]), tensor([88]), tensor([58]), tensor([57]), tensor([49]), tensor([30]), tensor([27]), tensor([68]), tensor([88]), tensor([60]), tensor([58]), tensor([25]), tensor([97]), tensor([81]), tensor([57]), tensor([34]), tensor([69]), tensor([95]), tensor([30]), tensor([78]), tensor([60]), tensor([25]), tensor([88]), tensor([69]), tensor([82]), tensor([81]), tensor([34]), tensor([88]), tensor([82]), tensor([86]), tensor([25]), tensor([82]), tensor([49]), tensor([68]), tensor([97]), tensor([49]), tensor([81]), tensor([97]), tensor([95]), tensor([88]), tensor([34]), tensor([68]), tensor([13]), tensor([95]), tensor([68]), tensor([88]), tensor([58]), tensor([58]), tensor([58]), tensor([13]), tensor([97]), tensor([95]), tensor([95]), tensor([34]), tensor([27]), tensor([57]), tensor([97]), tensor([25]), tensor([97]), tensor([81]), tensor([57]), tensor([82]), tensor([78]), tensor([97]), tensor([82]), tensor([69]), tensor([81]), tensor([27]), tensor([27]), tensor([60]), tensor([97]), tensor([78]), tensor([68]), tensor([68]), tensor([97]), tensor([97]), tensor([49]), tensor([69]), tensor([95]), tensor([82]), tensor([57]), tensor([57]), tensor([86]), tensor([25]), tensor([34]), tensor([81]), tensor([57]), tensor([58]), tensor([88]), tensor([86]), tensor([60]), tensor([88]), tensor([78]), tensor([68]), tensor([97]), tensor([50]), tensor([57]), tensor([49]), tensor([68]), tensor([50]), tensor([68]), tensor([69]), tensor([60]), tensor([57]), tensor([49]), tensor([78]), tensor([95]), tensor([49]), tensor([59]), tensor([27]), tensor([49]), tensor([25]), tensor([82]), tensor([57]), tensor([88]), tensor([58]), tensor([78]), tensor([30]), tensor([59]), tensor([86]), tensor([60]), tensor([13]), tensor([57]), tensor([57]), tensor([95]), tensor([68]), tensor([25]), tensor([60]), tensor([78]), tensor([25]), tensor([60]), tensor([69]), tensor([49]), tensor([95]), tensor([34]), tensor([59]), tensor([78]), tensor([78]), tensor([34]), tensor([49]), tensor([86]), tensor([27]), tensor([49]), tensor([88]), tensor([86]), tensor([97]), tensor([78]), tensor([82]), tensor([88]), tensor([68]), tensor([95]), tensor([69]), tensor([95]), tensor([13]), tensor([82]), tensor([69]), tensor([97]), tensor([86]), tensor([68]), tensor([27]), tensor([86]), tensor([57]), tensor([68]), tensor([49]), tensor([78]), tensor([57]), tensor([60]), tensor([50]), tensor([82]), tensor([88]), tensor([86]), tensor([82]), tensor([88]), tensor([34]), tensor([59]), tensor([49]), tensor([81]), tensor([78]), tensor([25]), tensor([88]), tensor([34]), tensor([58]), tensor([34]), tensor([58]), tensor([49]), tensor([81]), tensor([68]), tensor([86]), tensor([82]), tensor([97]), tensor([97]), tensor([95]), tensor([69]), tensor([88]), tensor([57]), tensor([78]), tensor([78]), tensor([95]), tensor([27]), tensor([78]), tensor([88]), tensor([27]), tensor([95]), tensor([49]), tensor([81]), tensor([86]), tensor([59]), tensor([27]), tensor([50]), tensor([95]), tensor([34]), tensor([57]), tensor([97]), tensor([50]), tensor([86]), tensor([58]), tensor([30]), tensor([57]), tensor([82]), tensor([78]), tensor([30]), tensor([68]), tensor([81]), tensor([30]), tensor([57]), tensor([86]), tensor([81]), tensor([49]), tensor([88]), tensor([78]), tensor([30]), tensor([81]), tensor([25]), tensor([88]), tensor([13]), tensor([78]), tensor([57]), tensor([27]), tensor([58]), tensor([78]), tensor([27]), tensor([81]), tensor([81]), tensor([78]), tensor([86]), tensor([34]), tensor([50]), tensor([57]), tensor([97]), tensor([49]), tensor([95]), tensor([13]), tensor([88]), tensor([78]), tensor([69]), tensor([68]), tensor([82]), tensor([97]), tensor([81]), tensor([69]), tensor([60]), tensor([25]), tensor([69]), tensor([82]), tensor([13]), tensor([88]), tensor([81]), tensor([57]), tensor([60]), tensor([59]), tensor([82]), tensor([86]), tensor([78]), tensor([68]), tensor([95]), tensor([82]), tensor([97]), tensor([81]), tensor([58]), tensor([68]), tensor([95]), tensor([30]), tensor([60]), tensor([82]), tensor([59]), tensor([58]), tensor([59]), tensor([69]), tensor([30]), tensor([50]), tensor([27]), tensor([58]), tensor([82]), tensor([60]), tensor([30]), tensor([49]), tensor([86]), tensor([82]), tensor([86]), tensor([81]), tensor([13]), tensor([82]), tensor([78]), tensor([50]), tensor([88]), tensor([59]), tensor([58]), tensor([60]), tensor([69]), tensor([27]), tensor([88]), tensor([58]), tensor([34]), tensor([86]), tensor([97]), tensor([68]), tensor([59]), tensor([69]), tensor([95]), tensor([97]), tensor([59]), tensor([86]), tensor([27]), tensor([27]), tensor([69]), tensor([13]), tensor([60]), tensor([97]), tensor([58]), tensor([27]), tensor([81]), tensor([25]), tensor([97]), tensor([58]), tensor([59]), tensor([30]), tensor([50]), tensor([88]), tensor([59]), tensor([82]), tensor([50]), tensor([25]), tensor([49]), tensor([50]), tensor([34]), tensor([78]), tensor([86]), tensor([60]), tensor([25]), tensor([81]), tensor([13]), tensor([49]), tensor([27]), tensor([30]), tensor([60]), tensor([78]), tensor([95]), tensor([13]), tensor([95]), tensor([82]), tensor([88]), tensor([27]), tensor([69]), tensor([78]), tensor([58]), tensor([78]), tensor([30]), tensor([58]), tensor([50]), tensor([27]), tensor([59]), tensor([69]), tensor([95]), tensor([58]), tensor([57]), tensor([27]), tensor([86]), tensor([78]), tensor([82]), tensor([59]), tensor([78]), tensor([95]), tensor([60]), tensor([95]), tensor([86]), tensor([57]), tensor([30]), tensor([59]), tensor([82]), tensor([50]), tensor([27]), tensor([88]), tensor([82]), tensor([81]), tensor([78]), tensor([49]), tensor([27]), tensor([97]), tensor([95]), tensor([59]), tensor([69]), tensor([68]), tensor([13]), tensor([97]), tensor([27]), tensor([86]), tensor([81]), tensor([81]), tensor([34]), tensor([81]), tensor([27]), tensor([50]), tensor([50]), tensor([57]), tensor([68]), tensor([27]), tensor([27]), tensor([69]), tensor([86]), tensor([60]), tensor([25]), tensor([58]), tensor([57]), tensor([58]), tensor([60]), tensor([69]), tensor([81]), tensor([86]), tensor([59]), tensor([69]), tensor([88]), tensor([81]), tensor([68]), tensor([97]), tensor([59]), tensor([30]), tensor([60]), tensor([97]), tensor([34]), tensor([95]), tensor([30]), tensor([88]), tensor([13]), tensor([97]), tensor([95]), tensor([25]), tensor([97]), tensor([86]), tensor([34]), tensor([60]), tensor([30]), tensor([95]), tensor([25]), tensor([81]), tensor([49]), tensor([57]), tensor([59]), tensor([68]), tensor([88]), tensor([95]), tensor([49]), tensor([60]), tensor([69]), tensor([69]), tensor([86]), tensor([50]), tensor([25]), tensor([81]), tensor([95]), tensor([50]), tensor([25]), tensor([27]), tensor([58]), tensor([34]), tensor([68]), tensor([86]), tensor([49]), tensor([30]), tensor([95]), tensor([27]), tensor([81]), tensor([34]), tensor([58]), tensor([60]), tensor([69]), tensor([13]), tensor([82]), tensor([13]), tensor([57]), tensor([68]), tensor([49]), tensor([30]), tensor([25]), tensor([13]), tensor([58]), tensor([59]), tensor([69]), tensor([81]), tensor([82]), tensor([50]), tensor([81]), tensor([88]), tensor([27]), tensor([97]), tensor([13]), tensor([86]), tensor([58]), tensor([49]), tensor([34]), tensor([60]), tensor([68]), tensor([86]), tensor([58]), tensor([50]), tensor([69]), tensor([97]), tensor([97]), tensor([60]), tensor([88]), tensor([25]), tensor([97]), tensor([82]), tensor([25]), tensor([13]), tensor([27]), tensor([49]), tensor([95]), tensor([82]), tensor([59]), tensor([57]), tensor([57]), tensor([68]), tensor([59]), tensor([25]), tensor([58]), tensor([25]), tensor([78]), tensor([30]), tensor([58]), tensor([60]), tensor([95]), tensor([82]), tensor([57]), tensor([59]), tensor([34]), tensor([88]), tensor([59]), tensor([30]), tensor([27]), tensor([60]), tensor([34]), tensor([13]), tensor([25]), tensor([95]), tensor([49]), tensor([88]), tensor([95]), tensor([30]), tensor([49]), tensor([82]), tensor([81]), tensor([59]), tensor([86]), tensor([68]), tensor([60]), tensor([69]), tensor([13]), tensor([88]), tensor([27]), tensor([60]), tensor([30]), tensor([68]), tensor([88]), tensor([25]), tensor([86]), tensor([86]), tensor([95]), tensor([60]), tensor([88]), tensor([60]), tensor([58]), tensor([57]), tensor([30]), tensor([86]), tensor([34]), tensor([69]), tensor([59]), tensor([81]), tensor([69]), tensor([81]), tensor([13]), tensor([82]), tensor([95]), tensor([59]), tensor([34]), tensor([59]), tensor([57]), tensor([34]), tensor([81]), tensor([78]), tensor([97]), tensor([58]), tensor([59]), tensor([30]), tensor([69]), tensor([81]), tensor([60]), tensor([69]), tensor([57]), tensor([69]), tensor([68]), tensor([27]), tensor([27]), tensor([82]), tensor([68]), tensor([50]), tensor([88]), tensor([69]), tensor([49]), tensor([81]), tensor([30]), tensor([30]), tensor([81]), tensor([78]), tensor([59]), tensor([30]), tensor([50]), tensor([25]), tensor([97]), tensor([82]), tensor([68]), tensor([25]), tensor([58]), tensor([69]), tensor([59]), tensor([82]), tensor([69]), tensor([97]), tensor([34]), tensor([97]), tensor([25]), tensor([81]), tensor([88]), tensor([60]), tensor([13]), tensor([50]), tensor([25]), tensor([30]), tensor([78]), tensor([68]), tensor([59]), tensor([57]), tensor([25]), tensor([78]), tensor([34]), tensor([57]), tensor([78]), tensor([68]), tensor([57]), tensor([69]), tensor([25]), tensor([59]), tensor([49]), tensor([60]), tensor([86]), tensor([97]), tensor([60]), tensor([86]), tensor([69]), tensor([59]), tensor([78]), tensor([49]), tensor([60]), tensor([60]), tensor([57]), tensor([49]), tensor([27]), tensor([95]), tensor([49]), tensor([30]), tensor([50]), tensor([78]), tensor([60]), tensor([88]), tensor([13]), tensor([78]), tensor([49]), tensor([88]), tensor([97]), tensor([86]), tensor([58]), tensor([57]), tensor([81]), tensor([69]), tensor([34]), tensor([97]), tensor([27]), tensor([59]), tensor([88]), tensor([58]), tensor([59]), tensor([86]), tensor([60]), tensor([50]), tensor([69]), tensor([30]), tensor([34]), tensor([27]), tensor([25]), tensor([68]), tensor([27]), tensor([59]), tensor([82]), tensor([95]), tensor([81]), tensor([49]), tensor([82]), tensor([25]), tensor([97]), tensor([13]), tensor([49]), tensor([81]), tensor([59]), tensor([34]), tensor([30]), tensor([59]), tensor([78]), tensor([34]), tensor([58]), tensor([78]), tensor([30]), tensor([82]), tensor([58]), tensor([25]), tensor([78]), tensor([13]), tensor([97]), tensor([13]), tensor([95]), tensor([25]), tensor([50]), tensor([59]), tensor([60]), tensor([86]), tensor([81]), tensor([86]), tensor([78]), tensor([86]), tensor([60]), tensor([25]), tensor([82]), tensor([69]), tensor([95]), tensor([81]), tensor([50]), tensor([69]), tensor([13]), tensor([49]), tensor([59]), tensor([97]), tensor([78]), tensor([13]), tensor([97]), tensor([97]), tensor([27]), tensor([88]), tensor([30]), tensor([60]), tensor([58]), tensor([34]), tensor([13]), tensor([82]), tensor([95]), tensor([88]), tensor([86]), tensor([34]), tensor([60]), tensor([95]), tensor([68]), tensor([59]), tensor([27]), tensor([86]), tensor([59]), tensor([81]), tensor([60]), tensor([30]), tensor([68]), tensor([78]), tensor([27]), tensor([13]), tensor([97]), tensor([49]), tensor([58]), tensor([82]), tensor([97])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.78 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.78\n",
            "TEST ALL:  0.7615\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  3000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 95, 24, 30, 34, 36, 50, 58, 60, 68, 72, 78, 80, 82, 86, 88, 94, 3, 13, 25, 27, 35, 37, 45, 49, 57, 59, 69, 81, 10]\n",
            "TRAIN_SET CLASSES:  [35, 3, 94, 10, 45, 37, 80, 72, 36, 24]\n",
            "VALIDATION CLASSES:  [45, 37, 36, 35, 94, 24, 80, 10, 72, 3]\n",
            "GROUP:  3\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [35, 3, 94, 10, 45, 37, 80, 72, 36, 24]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  30\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.3655283749103546\n",
            "Train step - Step 10, Loss 0.1471903920173645\n",
            "Train step - Step 20, Loss 0.13680699467658997\n",
            "Train step - Step 30, Loss 0.13708963990211487\n",
            "Train step - Step 40, Loss 0.1262548714876175\n",
            "Train step - Step 50, Loss 0.13335397839546204\n",
            "Train epoch - Accuracy: 0.27784172661870504 Loss: 0.1551868379716393 Corrects: 1931\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12387491017580032\n",
            "Train step - Step 70, Loss 0.12180088460445404\n",
            "Train step - Step 80, Loss 0.11773530393838882\n",
            "Train step - Step 90, Loss 0.12467190623283386\n",
            "Train step - Step 100, Loss 0.12684659659862518\n",
            "Train epoch - Accuracy: 0.33913669064748203 Loss: 0.12049913643933029 Corrects: 2357\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11842267215251923\n",
            "Train step - Step 120, Loss 0.11390971392393112\n",
            "Train step - Step 130, Loss 0.11250793188810349\n",
            "Train step - Step 140, Loss 0.12058088183403015\n",
            "Train step - Step 150, Loss 0.1177511215209961\n",
            "Train step - Step 160, Loss 0.11764604598283768\n",
            "Train epoch - Accuracy: 0.3543884892086331 Loss: 0.1172574834686389 Corrects: 2463\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.11415193229913712\n",
            "Train step - Step 180, Loss 0.10946138203144073\n",
            "Train step - Step 190, Loss 0.11654669791460037\n",
            "Train step - Step 200, Loss 0.10938170552253723\n",
            "Train step - Step 210, Loss 0.10694452375173569\n",
            "Train epoch - Accuracy: 0.3912230215827338 Loss: 0.11464503289555474 Corrects: 2719\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.12029964476823807\n",
            "Train step - Step 230, Loss 0.10585962235927582\n",
            "Train step - Step 240, Loss 0.10988374054431915\n",
            "Train step - Step 250, Loss 0.1186482384800911\n",
            "Train step - Step 260, Loss 0.12558776140213013\n",
            "Train step - Step 270, Loss 0.10924842953681946\n",
            "Train epoch - Accuracy: 0.4116546762589928 Loss: 0.11270299019787809 Corrects: 2861\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.11761637032032013\n",
            "Train step - Step 290, Loss 0.10602931678295135\n",
            "Train step - Step 300, Loss 0.11248591542243958\n",
            "Train step - Step 310, Loss 0.11223272979259491\n",
            "Train step - Step 320, Loss 0.11122937500476837\n",
            "Train epoch - Accuracy: 0.4251798561151079 Loss: 0.11163229740780892 Corrects: 2955\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.1139887124300003\n",
            "Train step - Step 340, Loss 0.10825194418430328\n",
            "Train step - Step 350, Loss 0.11099588125944138\n",
            "Train step - Step 360, Loss 0.11344196647405624\n",
            "Train step - Step 370, Loss 0.1148267388343811\n",
            "Train step - Step 380, Loss 0.10766288638114929\n",
            "Train epoch - Accuracy: 0.4389928057553957 Loss: 0.11020940079320249 Corrects: 3051\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.1028960794210434\n",
            "Train step - Step 400, Loss 0.11122769117355347\n",
            "Train step - Step 410, Loss 0.11673148721456528\n",
            "Train step - Step 420, Loss 0.11290652304887772\n",
            "Train step - Step 430, Loss 0.10281186550855637\n",
            "Train epoch - Accuracy: 0.45366906474820146 Loss: 0.10975612811476207 Corrects: 3153\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.111140176653862\n",
            "Train step - Step 450, Loss 0.10554233938455582\n",
            "Train step - Step 460, Loss 0.10790922492742538\n",
            "Train step - Step 470, Loss 0.11385266482830048\n",
            "Train step - Step 480, Loss 0.10535689443349838\n",
            "Train step - Step 490, Loss 0.10356055945158005\n",
            "Train epoch - Accuracy: 0.4660431654676259 Loss: 0.1083947665249701 Corrects: 3239\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10899171233177185\n",
            "Train step - Step 510, Loss 0.1006239503622055\n",
            "Train step - Step 520, Loss 0.11163724213838577\n",
            "Train step - Step 530, Loss 0.11440769582986832\n",
            "Train step - Step 540, Loss 0.11593274027109146\n",
            "Train epoch - Accuracy: 0.4753956834532374 Loss: 0.10845811480026452 Corrects: 3304\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.09824761003255844\n",
            "Train step - Step 560, Loss 0.11270561069250107\n",
            "Train step - Step 570, Loss 0.10148702561855316\n",
            "Train step - Step 580, Loss 0.10986015945672989\n",
            "Train step - Step 590, Loss 0.10251349955797195\n",
            "Train step - Step 600, Loss 0.09963274002075195\n",
            "Train epoch - Accuracy: 0.4856115107913669 Loss: 0.10724863022780247 Corrects: 3375\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.11300929635763168\n",
            "Train step - Step 620, Loss 0.10432419180870056\n",
            "Train step - Step 630, Loss 0.10660799592733383\n",
            "Train step - Step 640, Loss 0.10557826608419418\n",
            "Train step - Step 650, Loss 0.10019223392009735\n",
            "Train epoch - Accuracy: 0.49151079136690645 Loss: 0.10647151007283506 Corrects: 3416\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10154450684785843\n",
            "Train step - Step 670, Loss 0.11291321367025375\n",
            "Train step - Step 680, Loss 0.10507655888795853\n",
            "Train step - Step 690, Loss 0.10679423809051514\n",
            "Train step - Step 700, Loss 0.09995181113481522\n",
            "Train step - Step 710, Loss 0.11621768027544022\n",
            "Train epoch - Accuracy: 0.5004316546762589 Loss: 0.10659612375412056 Corrects: 3478\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.1129692941904068\n",
            "Train step - Step 730, Loss 0.10840572416782379\n",
            "Train step - Step 740, Loss 0.10696452856063843\n",
            "Train step - Step 750, Loss 0.1134609580039978\n",
            "Train step - Step 760, Loss 0.09605546295642853\n",
            "Train epoch - Accuracy: 0.5148201438848921 Loss: 0.10538749699755538 Corrects: 3578\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10500804334878922\n",
            "Train step - Step 780, Loss 0.10041309148073196\n",
            "Train step - Step 790, Loss 0.10621672123670578\n",
            "Train step - Step 800, Loss 0.09452123194932938\n",
            "Train step - Step 810, Loss 0.1046031191945076\n",
            "Train step - Step 820, Loss 0.10263539850711823\n",
            "Train epoch - Accuracy: 0.5135251798561151 Loss: 0.10535967927184894 Corrects: 3569\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.10115336626768112\n",
            "Train step - Step 840, Loss 0.10726671665906906\n",
            "Train step - Step 850, Loss 0.10240891575813293\n",
            "Train step - Step 860, Loss 0.10599426180124283\n",
            "Train step - Step 870, Loss 0.10086945444345474\n",
            "Train epoch - Accuracy: 0.5178417266187051 Loss: 0.10457348813470319 Corrects: 3599\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.10069550573825836\n",
            "Train step - Step 890, Loss 0.1000175029039383\n",
            "Train step - Step 900, Loss 0.11421291530132294\n",
            "Train step - Step 910, Loss 0.10426384955644608\n",
            "Train step - Step 920, Loss 0.11191707104444504\n",
            "Train step - Step 930, Loss 0.09919759631156921\n",
            "Train epoch - Accuracy: 0.523453237410072 Loss: 0.10382477703497564 Corrects: 3638\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.09736958891153336\n",
            "Train step - Step 950, Loss 0.10539144277572632\n",
            "Train step - Step 960, Loss 0.11228587478399277\n",
            "Train step - Step 970, Loss 0.10139790177345276\n",
            "Train step - Step 980, Loss 0.09946922957897186\n",
            "Train epoch - Accuracy: 0.5372661870503597 Loss: 0.10308113813614674 Corrects: 3734\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.11096440255641937\n",
            "Train step - Step 1000, Loss 0.10394163429737091\n",
            "Train step - Step 1010, Loss 0.09588644653558731\n",
            "Train step - Step 1020, Loss 0.10948914289474487\n",
            "Train step - Step 1030, Loss 0.09932424128055573\n",
            "Train step - Step 1040, Loss 0.09465715289115906\n",
            "Train epoch - Accuracy: 0.541726618705036 Loss: 0.1027594394070639 Corrects: 3765\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.09919654577970505\n",
            "Train step - Step 1060, Loss 0.10855916887521744\n",
            "Train step - Step 1070, Loss 0.09877190738916397\n",
            "Train step - Step 1080, Loss 0.10317105054855347\n",
            "Train step - Step 1090, Loss 0.10234267264604568\n",
            "Train epoch - Accuracy: 0.5415827338129496 Loss: 0.10174281511804183 Corrects: 3764\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10130622237920761\n",
            "Train step - Step 1110, Loss 0.09804131835699081\n",
            "Train step - Step 1120, Loss 0.09498488157987595\n",
            "Train step - Step 1130, Loss 0.09302910417318344\n",
            "Train step - Step 1140, Loss 0.10222765058279037\n",
            "Train step - Step 1150, Loss 0.10424795001745224\n",
            "Train epoch - Accuracy: 0.5483453237410072 Loss: 0.10219252356093564 Corrects: 3811\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.1056530624628067\n",
            "Train step - Step 1170, Loss 0.11063896119594574\n",
            "Train step - Step 1180, Loss 0.09998775273561478\n",
            "Train step - Step 1190, Loss 0.10549645870923996\n",
            "Train step - Step 1200, Loss 0.09957142919301987\n",
            "Train epoch - Accuracy: 0.5509352517985612 Loss: 0.10219472217474053 Corrects: 3829\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.09297382831573486\n",
            "Train step - Step 1220, Loss 0.10460666567087173\n",
            "Train step - Step 1230, Loss 0.09586283564567566\n",
            "Train step - Step 1240, Loss 0.1017710492014885\n",
            "Train step - Step 1250, Loss 0.10229744017124176\n",
            "Train step - Step 1260, Loss 0.10842575132846832\n",
            "Train epoch - Accuracy: 0.5627338129496403 Loss: 0.10110514364439807 Corrects: 3911\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.09625657647848129\n",
            "Train step - Step 1280, Loss 0.10300273448228836\n",
            "Train step - Step 1290, Loss 0.10354281216859818\n",
            "Train step - Step 1300, Loss 0.096091628074646\n",
            "Train step - Step 1310, Loss 0.10330094397068024\n",
            "Train epoch - Accuracy: 0.5690647482014388 Loss: 0.10057311272235225 Corrects: 3955\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.09420546889305115\n",
            "Train step - Step 1330, Loss 0.09460929781198502\n",
            "Train step - Step 1340, Loss 0.11530350148677826\n",
            "Train step - Step 1350, Loss 0.09786470234394073\n",
            "Train step - Step 1360, Loss 0.09552475810050964\n",
            "Train step - Step 1370, Loss 0.09297427535057068\n",
            "Train epoch - Accuracy: 0.5748201438848921 Loss: 0.10006986922926182 Corrects: 3995\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.09906842559576035\n",
            "Train step - Step 1390, Loss 0.10348088294267654\n",
            "Train step - Step 1400, Loss 0.10386592149734497\n",
            "Train step - Step 1410, Loss 0.1063530445098877\n",
            "Train step - Step 1420, Loss 0.10246821492910385\n",
            "Train epoch - Accuracy: 0.5743884892086331 Loss: 0.10046292179136825 Corrects: 3992\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.09808224439620972\n",
            "Train step - Step 1440, Loss 0.103947214782238\n",
            "Train step - Step 1450, Loss 0.1048632487654686\n",
            "Train step - Step 1460, Loss 0.10153444856405258\n",
            "Train step - Step 1470, Loss 0.09233013540506363\n",
            "Train step - Step 1480, Loss 0.09243472665548325\n",
            "Train epoch - Accuracy: 0.5831654676258993 Loss: 0.09981255408885667 Corrects: 4053\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.0955040380358696\n",
            "Train step - Step 1500, Loss 0.09586256742477417\n",
            "Train step - Step 1510, Loss 0.10250281542539597\n",
            "Train step - Step 1520, Loss 0.10302609205245972\n",
            "Train step - Step 1530, Loss 0.10499188303947449\n",
            "Train epoch - Accuracy: 0.5860431654676259 Loss: 0.09941293252672223 Corrects: 4073\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.094258151948452\n",
            "Train step - Step 1550, Loss 0.09078412503004074\n",
            "Train step - Step 1560, Loss 0.10212531685829163\n",
            "Train step - Step 1570, Loss 0.09887834638357162\n",
            "Train step - Step 1580, Loss 0.10115248709917068\n",
            "Train step - Step 1590, Loss 0.10857584327459335\n",
            "Train epoch - Accuracy: 0.5928057553956835 Loss: 0.0998282300611194 Corrects: 4120\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.09855499863624573\n",
            "Train step - Step 1610, Loss 0.1116127222776413\n",
            "Train step - Step 1620, Loss 0.10459968447685242\n",
            "Train step - Step 1630, Loss 0.09329255670309067\n",
            "Train step - Step 1640, Loss 0.09954743087291718\n",
            "Train epoch - Accuracy: 0.6030215827338129 Loss: 0.09812683506406468 Corrects: 4191\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.09304775297641754\n",
            "Train step - Step 1660, Loss 0.09510088711977005\n",
            "Train step - Step 1670, Loss 0.10076016187667847\n",
            "Train step - Step 1680, Loss 0.09753479063510895\n",
            "Train step - Step 1690, Loss 0.0993725061416626\n",
            "Train step - Step 1700, Loss 0.09557796269655228\n",
            "Train epoch - Accuracy: 0.5992805755395684 Loss: 0.09778589696335278 Corrects: 4165\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.0976453423500061\n",
            "Train step - Step 1720, Loss 0.09921053797006607\n",
            "Train step - Step 1730, Loss 0.08726401627063751\n",
            "Train step - Step 1740, Loss 0.09796535968780518\n",
            "Train step - Step 1750, Loss 0.09303759783506393\n",
            "Train epoch - Accuracy: 0.6050359712230216 Loss: 0.09779588385237206 Corrects: 4205\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.09382766485214233\n",
            "Train step - Step 1770, Loss 0.09058260172605515\n",
            "Train step - Step 1780, Loss 0.10241126269102097\n",
            "Train step - Step 1790, Loss 0.0931248888373375\n",
            "Train step - Step 1800, Loss 0.10469073057174683\n",
            "Train step - Step 1810, Loss 0.09437908232212067\n",
            "Train epoch - Accuracy: 0.6027338129496402 Loss: 0.09687381631178822 Corrects: 4189\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.10298492014408112\n",
            "Train step - Step 1830, Loss 0.09861698746681213\n",
            "Train step - Step 1840, Loss 0.09226816147565842\n",
            "Train step - Step 1850, Loss 0.10008730739355087\n",
            "Train step - Step 1860, Loss 0.09097132831811905\n",
            "Train epoch - Accuracy: 0.6184172661870504 Loss: 0.09586998703454039 Corrects: 4298\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.09973586350679398\n",
            "Train step - Step 1880, Loss 0.09028739482164383\n",
            "Train step - Step 1890, Loss 0.09504267573356628\n",
            "Train step - Step 1900, Loss 0.09402509033679962\n",
            "Train step - Step 1910, Loss 0.10027351975440979\n",
            "Train step - Step 1920, Loss 0.09207746386528015\n",
            "Train epoch - Accuracy: 0.6224460431654676 Loss: 0.09673701241290826 Corrects: 4326\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.09366109222173691\n",
            "Train step - Step 1940, Loss 0.09102505445480347\n",
            "Train step - Step 1950, Loss 0.10456205904483795\n",
            "Train step - Step 1960, Loss 0.0913504809141159\n",
            "Train step - Step 1970, Loss 0.0922856405377388\n",
            "Train epoch - Accuracy: 0.6271942446043165 Loss: 0.0964507012508756 Corrects: 4359\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.09633725881576538\n",
            "Train step - Step 1990, Loss 0.08794169127941132\n",
            "Train step - Step 2000, Loss 0.09108951687812805\n",
            "Train step - Step 2010, Loss 0.09136001020669937\n",
            "Train step - Step 2020, Loss 0.09422671794891357\n",
            "Train step - Step 2030, Loss 0.09768716990947723\n",
            "Train epoch - Accuracy: 0.6264748201438849 Loss: 0.09580322246328532 Corrects: 4354\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.09953667223453522\n",
            "Train step - Step 2050, Loss 0.09318437427282333\n",
            "Train step - Step 2060, Loss 0.09688735008239746\n",
            "Train step - Step 2070, Loss 0.08771035075187683\n",
            "Train step - Step 2080, Loss 0.10136935114860535\n",
            "Train epoch - Accuracy: 0.6333812949640287 Loss: 0.0954260997077544 Corrects: 4402\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.09112586826086044\n",
            "Train step - Step 2100, Loss 0.09487347304821014\n",
            "Train step - Step 2110, Loss 0.09460693597793579\n",
            "Train step - Step 2120, Loss 0.09404806047677994\n",
            "Train step - Step 2130, Loss 0.0954691469669342\n",
            "Train step - Step 2140, Loss 0.09689813107252121\n",
            "Train epoch - Accuracy: 0.6379856115107914 Loss: 0.09515246644937735 Corrects: 4434\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.09152717888355255\n",
            "Train step - Step 2160, Loss 0.09556018561124802\n",
            "Train step - Step 2170, Loss 0.09677540510892868\n",
            "Train step - Step 2180, Loss 0.09830033779144287\n",
            "Train step - Step 2190, Loss 0.0910569578409195\n",
            "Train epoch - Accuracy: 0.638705035971223 Loss: 0.09496494691363341 Corrects: 4439\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.11007659137248993\n",
            "Train step - Step 2210, Loss 0.09412585943937302\n",
            "Train step - Step 2220, Loss 0.10149621218442917\n",
            "Train step - Step 2230, Loss 0.08651544898748398\n",
            "Train step - Step 2240, Loss 0.09518826007843018\n",
            "Train step - Step 2250, Loss 0.0926475003361702\n",
            "Train epoch - Accuracy: 0.6378417266187051 Loss: 0.09488975748955775 Corrects: 4433\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.09957603365182877\n",
            "Train step - Step 2270, Loss 0.09896735101938248\n",
            "Train step - Step 2280, Loss 0.1034431979060173\n",
            "Train step - Step 2290, Loss 0.10102290660142899\n",
            "Train step - Step 2300, Loss 0.09412673860788345\n",
            "Train epoch - Accuracy: 0.6517985611510791 Loss: 0.09480754027263724 Corrects: 4530\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.09089864045381546\n",
            "Train step - Step 2320, Loss 0.0920552909374237\n",
            "Train step - Step 2330, Loss 0.08583377301692963\n",
            "Train step - Step 2340, Loss 0.09616861492395401\n",
            "Train step - Step 2350, Loss 0.0870760977268219\n",
            "Train step - Step 2360, Loss 0.10272204130887985\n",
            "Train epoch - Accuracy: 0.6545323741007194 Loss: 0.09378804924033529 Corrects: 4549\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.10021774470806122\n",
            "Train step - Step 2380, Loss 0.09053858369588852\n",
            "Train step - Step 2390, Loss 0.08985821157693863\n",
            "Train step - Step 2400, Loss 0.09593766182661057\n",
            "Train step - Step 2410, Loss 0.09891527146100998\n",
            "Train epoch - Accuracy: 0.6499280575539569 Loss: 0.09403922844704964 Corrects: 4517\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.0936519056558609\n",
            "Train step - Step 2430, Loss 0.09517037868499756\n",
            "Train step - Step 2440, Loss 0.08937764167785645\n",
            "Train step - Step 2450, Loss 0.09105692058801651\n",
            "Train step - Step 2460, Loss 0.10124481469392776\n",
            "Train step - Step 2470, Loss 0.09525445848703384\n",
            "Train epoch - Accuracy: 0.6641726618705036 Loss: 0.09367797396809077 Corrects: 4616\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.09882916510105133\n",
            "Train step - Step 2490, Loss 0.0838240534067154\n",
            "Train step - Step 2500, Loss 0.09555936604738235\n",
            "Train step - Step 2510, Loss 0.09115926921367645\n",
            "Train step - Step 2520, Loss 0.09516802430152893\n",
            "Train epoch - Accuracy: 0.6635971223021583 Loss: 0.09255266832576381 Corrects: 4612\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.09424183517694473\n",
            "Train step - Step 2540, Loss 0.08983970433473587\n",
            "Train step - Step 2550, Loss 0.10016480088233948\n",
            "Train step - Step 2560, Loss 0.10226918011903763\n",
            "Train step - Step 2570, Loss 0.0934678465127945\n",
            "Train step - Step 2580, Loss 0.09142544120550156\n",
            "Train epoch - Accuracy: 0.6585611510791367 Loss: 0.0925947022095001 Corrects: 4577\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.0950917899608612\n",
            "Train step - Step 2600, Loss 0.09257205575704575\n",
            "Train step - Step 2610, Loss 0.0933300033211708\n",
            "Train step - Step 2620, Loss 0.0928674265742302\n",
            "Train step - Step 2630, Loss 0.09252287447452545\n",
            "Train epoch - Accuracy: 0.663021582733813 Loss: 0.09248385411586693 Corrects: 4608\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.09395809471607208\n",
            "Train step - Step 2650, Loss 0.09360671043395996\n",
            "Train step - Step 2660, Loss 0.09106128662824631\n",
            "Train step - Step 2670, Loss 0.08956111967563629\n",
            "Train step - Step 2680, Loss 0.09626634418964386\n",
            "Train step - Step 2690, Loss 0.09340263158082962\n",
            "Train epoch - Accuracy: 0.6733812949640288 Loss: 0.09249979703546428 Corrects: 4680\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.08575862646102905\n",
            "Train step - Step 2710, Loss 0.08900928497314453\n",
            "Train step - Step 2720, Loss 0.08676702529191971\n",
            "Train step - Step 2730, Loss 0.09200768172740936\n",
            "Train step - Step 2740, Loss 0.09473780542612076\n",
            "Train epoch - Accuracy: 0.6893525179856115 Loss: 0.08918260546468144 Corrects: 4791\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.08488817512989044\n",
            "Train step - Step 2760, Loss 0.08888044208288193\n",
            "Train step - Step 2770, Loss 0.08875874429941177\n",
            "Train step - Step 2780, Loss 0.08743438869714737\n",
            "Train step - Step 2790, Loss 0.08491537719964981\n",
            "Train step - Step 2800, Loss 0.09103624522686005\n",
            "Train epoch - Accuracy: 0.6897841726618705 Loss: 0.08829260634646999 Corrects: 4794\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.09502662718296051\n",
            "Train step - Step 2820, Loss 0.08533711731433868\n",
            "Train step - Step 2830, Loss 0.08565380424261093\n",
            "Train step - Step 2840, Loss 0.08459056913852692\n",
            "Train step - Step 2850, Loss 0.09001649916172028\n",
            "Train epoch - Accuracy: 0.6955395683453237 Loss: 0.08811420190677369 Corrects: 4834\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.08176196366548538\n",
            "Train step - Step 2870, Loss 0.08539918810129166\n",
            "Train step - Step 2880, Loss 0.08468573540449142\n",
            "Train step - Step 2890, Loss 0.08469324558973312\n",
            "Train step - Step 2900, Loss 0.0980789065361023\n",
            "Train step - Step 2910, Loss 0.08363844454288483\n",
            "Train epoch - Accuracy: 0.6959712230215828 Loss: 0.08787912537511304 Corrects: 4837\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.09174506366252899\n",
            "Train step - Step 2930, Loss 0.09530851989984512\n",
            "Train step - Step 2940, Loss 0.0849711075425148\n",
            "Train step - Step 2950, Loss 0.0904371365904808\n",
            "Train step - Step 2960, Loss 0.09332605451345444\n",
            "Train epoch - Accuracy: 0.6968345323741008 Loss: 0.08788327982743009 Corrects: 4843\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.08981061726808548\n",
            "Train step - Step 2980, Loss 0.0884333923459053\n",
            "Train step - Step 2990, Loss 0.09038124233484268\n",
            "Train step - Step 3000, Loss 0.08734169602394104\n",
            "Train step - Step 3010, Loss 0.08466026186943054\n",
            "Train step - Step 3020, Loss 0.08701649308204651\n",
            "Train epoch - Accuracy: 0.698705035971223 Loss: 0.08762596082129924 Corrects: 4856\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.09550351649522781\n",
            "Train step - Step 3040, Loss 0.09735383093357086\n",
            "Train step - Step 3050, Loss 0.08832011371850967\n",
            "Train step - Step 3060, Loss 0.09377045184373856\n",
            "Train step - Step 3070, Loss 0.08814537525177002\n",
            "Train epoch - Accuracy: 0.6956834532374101 Loss: 0.08775687511018712 Corrects: 4835\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.08800650388002396\n",
            "Train step - Step 3090, Loss 0.08652743697166443\n",
            "Train step - Step 3100, Loss 0.08019568771123886\n",
            "Train step - Step 3110, Loss 0.08407812565565109\n",
            "Train step - Step 3120, Loss 0.08106923848390579\n",
            "Train step - Step 3130, Loss 0.08312807232141495\n",
            "Train epoch - Accuracy: 0.6972661870503597 Loss: 0.0870231265258446 Corrects: 4846\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.09392258524894714\n",
            "Train step - Step 3150, Loss 0.0873727947473526\n",
            "Train step - Step 3160, Loss 0.08179916441440582\n",
            "Train step - Step 3170, Loss 0.08278728276491165\n",
            "Train step - Step 3180, Loss 0.08383549004793167\n",
            "Train epoch - Accuracy: 0.6966906474820144 Loss: 0.08745538196760974 Corrects: 4842\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.09326320141553879\n",
            "Train step - Step 3200, Loss 0.0893259271979332\n",
            "Train step - Step 3210, Loss 0.08353132754564285\n",
            "Train step - Step 3220, Loss 0.0878303125500679\n",
            "Train step - Step 3230, Loss 0.08217602968215942\n",
            "Train step - Step 3240, Loss 0.09065430611371994\n",
            "Train epoch - Accuracy: 0.700863309352518 Loss: 0.08699049244038493 Corrects: 4871\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.08708683401346207\n",
            "Train step - Step 3260, Loss 0.08945474773645401\n",
            "Train step - Step 3270, Loss 0.08780074119567871\n",
            "Train step - Step 3280, Loss 0.08549186587333679\n",
            "Train step - Step 3290, Loss 0.09311715513467789\n",
            "Train epoch - Accuracy: 0.6975539568345324 Loss: 0.08664893234805238 Corrects: 4848\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.08992630988359451\n",
            "Train step - Step 3310, Loss 0.08583366870880127\n",
            "Train step - Step 3320, Loss 0.09877697378396988\n",
            "Train step - Step 3330, Loss 0.09101920574903488\n",
            "Train step - Step 3340, Loss 0.07805202901363373\n",
            "Train step - Step 3350, Loss 0.08680865168571472\n",
            "Train epoch - Accuracy: 0.7076258992805755 Loss: 0.0866389168573798 Corrects: 4918\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.07882194966077805\n",
            "Train step - Step 3370, Loss 0.0871996134519577\n",
            "Train step - Step 3380, Loss 0.09110616892576218\n",
            "Train step - Step 3390, Loss 0.08478055149316788\n",
            "Train step - Step 3400, Loss 0.08752996474504471\n",
            "Train epoch - Accuracy: 0.6949640287769784 Loss: 0.08705895822254016 Corrects: 4830\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.09262833744287491\n",
            "Train step - Step 3420, Loss 0.088290274143219\n",
            "Train step - Step 3430, Loss 0.08387775719165802\n",
            "Train step - Step 3440, Loss 0.08439023047685623\n",
            "Train step - Step 3450, Loss 0.09179317951202393\n",
            "Train step - Step 3460, Loss 0.08415535092353821\n",
            "Train epoch - Accuracy: 0.7037410071942446 Loss: 0.08676867697950748 Corrects: 4891\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.08715655654668808\n",
            "Train step - Step 3480, Loss 0.09122674912214279\n",
            "Train step - Step 3490, Loss 0.08417436480522156\n",
            "Train step - Step 3500, Loss 0.08957863599061966\n",
            "Train step - Step 3510, Loss 0.08199720829725266\n",
            "Train epoch - Accuracy: 0.7005755395683453 Loss: 0.08650843660822875 Corrects: 4869\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.08872242271900177\n",
            "Train step - Step 3530, Loss 0.09293386340141296\n",
            "Train step - Step 3540, Loss 0.08072957396507263\n",
            "Train step - Step 3550, Loss 0.08275590091943741\n",
            "Train step - Step 3560, Loss 0.08734768629074097\n",
            "Train step - Step 3570, Loss 0.08845309168100357\n",
            "Train epoch - Accuracy: 0.7058992805755395 Loss: 0.0859767212627603 Corrects: 4906\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.08808489143848419\n",
            "Train step - Step 3590, Loss 0.08836689591407776\n",
            "Train step - Step 3600, Loss 0.08136056363582611\n",
            "Train step - Step 3610, Loss 0.08639241009950638\n",
            "Train step - Step 3620, Loss 0.08243419975042343\n",
            "Train epoch - Accuracy: 0.7050359712230215 Loss: 0.08593380964702839 Corrects: 4900\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.08930964767932892\n",
            "Train step - Step 3640, Loss 0.08721937984228134\n",
            "Train step - Step 3650, Loss 0.09154876321554184\n",
            "Train step - Step 3660, Loss 0.09662441164255142\n",
            "Train step - Step 3670, Loss 0.08625861257314682\n",
            "Train step - Step 3680, Loss 0.08180764317512512\n",
            "Train epoch - Accuracy: 0.7070503597122302 Loss: 0.08617740971150158 Corrects: 4914\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.08417168259620667\n",
            "Train step - Step 3700, Loss 0.09297706931829453\n",
            "Train step - Step 3710, Loss 0.08687571436166763\n",
            "Train step - Step 3720, Loss 0.08168043941259384\n",
            "Train step - Step 3730, Loss 0.08433074504137039\n",
            "Train epoch - Accuracy: 0.7125179856115108 Loss: 0.08591447647955777 Corrects: 4952\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.08867671340703964\n",
            "Train step - Step 3750, Loss 0.08356068283319473\n",
            "Train step - Step 3760, Loss 0.0884842723608017\n",
            "Train step - Step 3770, Loss 0.07670504599809647\n",
            "Train step - Step 3780, Loss 0.0785355344414711\n",
            "Train step - Step 3790, Loss 0.09497208148241043\n",
            "Train epoch - Accuracy: 0.7063309352517986 Loss: 0.08616313581629623 Corrects: 4909\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.08938184380531311\n",
            "Train step - Step 3810, Loss 0.08601180464029312\n",
            "Train step - Step 3820, Loss 0.09116224199533463\n",
            "Train step - Step 3830, Loss 0.08707931637763977\n",
            "Train step - Step 3840, Loss 0.08212748169898987\n",
            "Train epoch - Accuracy: 0.7096402877697842 Loss: 0.08627890648601724 Corrects: 4932\n",
            "Training finished in 391.86886191368103 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24]\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238246450>\n",
            "Constructing exemplars of class 35\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [33483, 29607, 7990, 39415, 3351, 49967, 37302, 10115, 45119, 33176, 31045, 23578, 34763, 49876, 15283, 49687, 17219, 40091, 39716, 20375, 1757, 35224, 35721, 33372, 5196, 40344, 3012, 17870, 3973, 19766, 1109, 28429, 46064, 7388, 3456, 21591, 48836, 26190, 47645, 35495, 25400, 33661, 27490, 3137, 10552, 10445, 40091, 29072, 45720, 23218, 3351, 49876, 1266, 28737, 23218, 47650, 33831, 43572, 11937, 25565, 43095, 36741, 40110, 35495, 16599, 7158]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e45250>\n",
            "Constructing exemplars of class 3\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [1174, 27383, 5360, 37028, 18509, 21545, 27153, 4410, 34113, 30884, 27542, 21545, 12671, 21904, 35835, 40302, 26514, 44290, 10513, 5173, 1637, 48182, 28978, 13146, 16490, 26708, 18216, 612, 2085, 49954, 29385, 35315, 13735, 40708, 35799, 41083, 12671, 46182, 6868, 35435, 16011, 12034, 20805, 2459, 20654, 8365, 16147, 18562, 45058, 46182, 19469, 21545, 21232, 19050, 28981, 23120, 27931, 17546, 23456, 13911, 31510, 47026, 46726, 49376, 47454, 45475]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e45a10>\n",
            "Constructing exemplars of class 94\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [37068, 2309, 18304, 36318, 20021, 5905, 1305, 43459, 37175, 49542, 30002, 2298, 22407, 15797, 44145, 4278, 45593, 11974, 16046, 21717, 41750, 39422, 38694, 7307, 27701, 3559, 4278, 29636, 21693, 11391, 40927, 44310, 9278, 3701, 15538, 25415, 42625, 45955, 9223, 31562, 2608, 17962, 24818, 1466, 13250, 41683, 8487, 46875, 10808, 31910, 3363, 44720, 14488, 43612, 11974, 8164, 9714, 9056, 925, 2246, 18675, 7189, 39857, 44997, 4061, 30951]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224548f250>\n",
            "Constructing exemplars of class 10\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [29329, 18383, 23295, 12442, 23498, 19042, 2600, 36955, 42544, 18346, 47187, 19864, 26126, 19470, 39793, 41961, 33623, 33805, 11218, 21167, 12703, 28393, 39934, 40883, 24971, 43313, 23675, 47995, 22611, 38293, 13554, 28944, 41600, 28098, 14588, 42860, 5193, 17835, 5218, 6203, 42493, 7596, 9250, 12703, 8612, 24971, 12347, 11858, 3682, 23324, 307, 45738, 9250, 45122, 8458, 48217, 2078, 33749, 39956, 18361, 47052, 4797, 22107, 30042, 40997, 36762]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e9b910>\n",
            "Constructing exemplars of class 45\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [34770, 49984, 32346, 26914, 39094, 7197, 4174, 23395, 39566, 47639, 32627, 14226, 42601, 48033, 19487, 46928, 40478, 21605, 28850, 10949, 25282, 6508, 22072, 36852, 33646, 16982, 22680, 41, 17675, 3098, 26244, 39674, 37743, 23244, 49570, 34572, 33006, 31279, 21039, 22291, 23832, 24699, 15485, 24069, 24407, 14112, 28149, 28415, 37039, 41696, 48780, 35740, 26212, 13776, 12914, 21433, 35423, 3073, 48362, 17893, 13041, 31056, 45863, 16152, 28638, 35505]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224548f250>\n",
            "Constructing exemplars of class 37\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [14781, 48555, 25575, 10839, 40493, 42135, 30344, 1461, 42883, 13941, 21386, 1657, 6396, 43721, 9279, 14746, 20162, 34643, 29920, 868, 41533, 40346, 40493, 2948, 12229, 13164, 35125, 23237, 49767, 26645, 42430, 13164, 27854, 27789, 17778, 22155, 38632, 5952, 4050, 39283, 780, 11152, 11145, 28419, 868, 1915, 9279, 12498, 33630, 12524, 6208, 49775, 693, 12157, 17409, 5635, 47950, 20260, 2035, 23012, 1556, 41199, 39760, 39021, 36043, 20176]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223293af10>\n",
            "Constructing exemplars of class 80\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [40393, 31171, 37506, 48667, 16315, 43167, 6819, 40095, 12081, 42652, 27436, 48899, 33998, 21576, 17949, 47797, 46027, 19901, 43609, 30148, 10210, 28719, 1157, 42206, 20818, 46829, 48095, 48667, 30177, 38368, 192, 30524, 41806, 13998, 30933, 36652, 8076, 4360, 648, 6706, 45814, 19341, 38932, 7032, 26730, 46522, 39933, 28539, 20442, 19088, 22449, 34060, 31244, 15339, 28987, 32587, 41469, 8094, 29481, 6819, 44518, 11791, 16315, 35914, 1194, 43204]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224548f250>\n",
            "Constructing exemplars of class 72\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [46472, 24223, 15650, 17951, 12744, 20337, 35222, 39035, 40775, 32996, 15862, 42792, 48680, 175, 27794, 25002, 34443, 25901, 10182, 10763, 26457, 7615, 41207, 35718, 15977, 28531, 366, 2849, 37914, 31270, 33599, 48535, 43569, 22639, 8530, 21925, 31199, 13753, 49399, 26363, 16065, 2629, 38476, 46052, 43200, 6808, 47020, 39047, 3899, 34381, 26457, 9011, 23232, 32816, 48053, 41758, 7850, 25029, 15163, 28194, 15650, 7661, 22880, 23650, 42453, 33412]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223824ad10>\n",
            "Constructing exemplars of class 36\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [49703, 34628, 23384, 40990, 26779, 14586, 16245, 8743, 24229, 9143, 6985, 35484, 43855, 35377, 32061, 74, 14331, 42505, 21836, 34405, 36638, 14988, 44878, 1953, 13336, 46806, 24857, 24152, 29941, 1886, 29101, 18611, 23256, 37555, 48720, 2505, 26003, 26288, 21513, 14818, 45277, 15161, 37870, 17194, 6555, 1878, 43770, 20508, 41854, 15368, 42239, 33871, 12823, 40566, 12969, 4758, 31117, 46692, 49511, 9678, 18648, 12063, 33223, 21085, 9965, 29226]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223190a290>\n",
            "Constructing exemplars of class 24\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [47115, 5501, 35673, 29672, 41770, 37031, 37108, 34527, 21804, 11015, 7175, 40015, 30448, 36752, 21892, 9738, 21978, 31264, 20750, 3715, 42167, 46749, 8516, 28113, 26151, 24610, 2366, 24647, 38058, 1496, 45952, 23737, 44519, 23061, 49481, 23997, 17584, 4279, 42106, 23802, 43138, 44782, 44835, 44700, 39052, 34279, 8083, 11672, 49477, 40015, 3717, 37173, 16008, 26741, 1524, 1863, 44117, 23199, 38564, 1718, 8440, 37040, 3261, 17834, 11511, 9119]\n",
            "x train:  [-0.13962898 -0.15788312  0.10308097 -0.13827321 -0.14207406 -0.18010783\n",
            " -0.19979425 -0.13976938 -0.09247549 -0.09836888 -0.31739867 -0.23496416\n",
            " -0.06538626 -0.183022   -0.12628408 -0.16400474  0.06208349 -0.19973294\n",
            " -0.31905657 -0.21433294 -0.12914461 -0.14619474 -0.22592269 -0.09664242\n",
            " -0.12398364 -0.10073257 -0.1983899  -0.21789515 -0.257017   -0.30104747]\n",
            "y_train:  [tensor([13]), tensor([81]), tensor([80]), tensor([30]), tensor([97]), tensor([94]), tensor([81]), tensor([59]), tensor([49]), tensor([68]), tensor([81]), tensor([81]), tensor([30]), tensor([88]), tensor([35]), tensor([49]), tensor([36]), tensor([88]), tensor([13]), tensor([10]), tensor([24]), tensor([50]), tensor([59]), tensor([95]), tensor([69]), tensor([49]), tensor([57]), tensor([13]), tensor([50]), tensor([72]), tensor([81]), tensor([34]), tensor([95]), tensor([80]), tensor([24]), tensor([3]), tensor([30]), tensor([35]), tensor([13]), tensor([49]), tensor([69]), tensor([25]), tensor([81]), tensor([49]), tensor([36]), tensor([30]), tensor([94]), tensor([81]), tensor([60]), tensor([81]), tensor([88]), tensor([59]), tensor([27]), tensor([94]), tensor([95]), tensor([69]), tensor([82]), tensor([50]), tensor([68]), tensor([72]), tensor([10]), tensor([27]), tensor([81]), tensor([50]), tensor([27]), tensor([45]), tensor([45]), tensor([30]), tensor([50]), tensor([34]), tensor([45]), tensor([50]), tensor([59]), tensor([97]), tensor([35]), tensor([80]), tensor([82]), tensor([36]), tensor([49]), tensor([50]), tensor([24]), tensor([57]), tensor([57]), tensor([34]), tensor([25]), tensor([49]), tensor([34]), tensor([35]), tensor([10]), tensor([72]), tensor([35]), tensor([82]), tensor([45]), tensor([36]), tensor([50]), tensor([59]), tensor([69]), tensor([13]), tensor([72]), tensor([78]), tensor([36]), tensor([59]), tensor([78]), tensor([80]), tensor([86]), tensor([25]), tensor([24]), tensor([59]), tensor([57]), tensor([45]), tensor([78]), tensor([36]), tensor([25]), tensor([49]), tensor([68]), tensor([10]), tensor([72]), tensor([68]), tensor([25]), tensor([88]), tensor([36]), tensor([10]), tensor([97]), tensor([68]), tensor([35]), tensor([13]), tensor([49]), tensor([30]), tensor([81]), tensor([81]), tensor([68]), tensor([45]), tensor([34]), tensor([49]), tensor([35]), tensor([35]), tensor([78]), tensor([35]), tensor([68]), tensor([68]), tensor([82]), tensor([69]), tensor([69]), tensor([68]), tensor([25]), tensor([72]), tensor([58]), tensor([80]), tensor([3]), tensor([13]), tensor([36]), tensor([13]), tensor([86]), tensor([82]), tensor([34]), tensor([69]), tensor([37]), tensor([27]), tensor([30]), tensor([68]), tensor([45]), tensor([45]), tensor([25]), tensor([3]), tensor([10]), tensor([59]), tensor([94]), tensor([80]), tensor([45]), tensor([94]), tensor([86]), tensor([37]), tensor([3]), tensor([97]), tensor([69]), tensor([97]), tensor([81]), tensor([13]), tensor([24]), tensor([94]), tensor([94]), tensor([27]), tensor([60]), tensor([80]), tensor([3]), tensor([69]), tensor([81]), tensor([27]), tensor([97]), tensor([45]), tensor([82]), tensor([50]), tensor([88]), tensor([60]), tensor([78]), tensor([95]), tensor([59]), tensor([45]), tensor([95]), tensor([97]), tensor([10]), tensor([60]), tensor([57]), tensor([82]), tensor([72]), tensor([24]), tensor([27]), tensor([36]), tensor([59]), tensor([59]), tensor([80]), tensor([36]), tensor([94]), tensor([30]), tensor([95]), tensor([57]), tensor([81]), tensor([49]), tensor([49]), tensor([13]), tensor([24]), tensor([3]), tensor([13]), tensor([94]), tensor([13]), tensor([30]), tensor([86]), tensor([68]), tensor([88]), tensor([82]), tensor([57]), tensor([60]), tensor([88]), tensor([57]), tensor([36]), tensor([86]), tensor([95]), tensor([97]), tensor([57]), tensor([36]), tensor([81]), tensor([81]), tensor([72]), tensor([68]), tensor([3]), tensor([36]), tensor([34]), tensor([57]), tensor([81]), tensor([3]), tensor([27]), tensor([95]), tensor([69]), tensor([30]), tensor([95]), tensor([72]), tensor([80]), tensor([25]), tensor([60]), tensor([58]), tensor([58]), tensor([94]), tensor([58]), tensor([57]), tensor([59]), tensor([60]), tensor([97]), tensor([59]), tensor([35]), tensor([59]), tensor([36]), tensor([58]), tensor([45]), tensor([13]), tensor([27]), tensor([81]), tensor([27]), tensor([60]), tensor([69]), tensor([25]), tensor([57]), tensor([97]), tensor([58]), tensor([82]), tensor([82]), tensor([35]), tensor([37]), tensor([82]), tensor([45]), tensor([80]), tensor([36]), tensor([57]), tensor([24]), tensor([24]), tensor([95]), tensor([81]), tensor([25]), tensor([88]), tensor([80]), tensor([57]), tensor([36]), tensor([45]), tensor([34]), tensor([58]), tensor([37]), tensor([94]), tensor([36]), tensor([59]), tensor([82]), tensor([58]), tensor([10]), tensor([35]), tensor([27]), tensor([27]), tensor([94]), tensor([24]), tensor([50]), tensor([78]), tensor([97]), tensor([57]), tensor([49]), tensor([95]), tensor([50]), tensor([88]), tensor([68]), tensor([30]), tensor([94]), tensor([3]), tensor([34]), tensor([95]), tensor([13]), tensor([24]), tensor([3]), tensor([82]), tensor([60]), tensor([10]), tensor([3]), tensor([37]), tensor([10]), tensor([82]), tensor([88]), tensor([88]), tensor([35]), tensor([35]), tensor([72]), tensor([24]), tensor([34]), tensor([81]), tensor([49]), tensor([25]), tensor([25]), tensor([34]), tensor([94]), tensor([30]), tensor([81]), tensor([34]), tensor([34]), tensor([25]), tensor([10]), tensor([49]), tensor([45]), tensor([88]), tensor([94]), tensor([81]), tensor([27]), tensor([88]), tensor([36]), tensor([69]), tensor([36]), tensor([50]), tensor([35]), tensor([49]), tensor([69]), tensor([3]), tensor([81]), tensor([36]), tensor([24]), tensor([82]), tensor([30]), tensor([36]), tensor([82]), tensor([58]), tensor([81]), tensor([24]), tensor([24]), tensor([81]), tensor([59]), tensor([13]), tensor([35]), tensor([3]), tensor([78]), tensor([30]), tensor([36]), tensor([94]), tensor([59]), tensor([94]), tensor([59]), tensor([57]), tensor([3]), tensor([30]), tensor([68]), tensor([13]), tensor([25]), tensor([10]), tensor([95]), tensor([69]), tensor([86]), tensor([50]), tensor([45]), tensor([60]), tensor([72]), tensor([97]), tensor([82]), tensor([72]), tensor([25]), tensor([30]), tensor([34]), tensor([68]), tensor([94]), tensor([94]), tensor([68]), tensor([68]), tensor([69]), tensor([50]), tensor([72]), tensor([45]), tensor([3]), tensor([3]), tensor([34]), tensor([80]), tensor([24]), tensor([59]), tensor([25]), tensor([58]), tensor([3]), tensor([45]), tensor([86]), tensor([10]), tensor([37]), tensor([37]), tensor([88]), tensor([95]), tensor([58]), tensor([57]), tensor([82]), tensor([95]), tensor([94]), tensor([3]), tensor([94]), tensor([80]), tensor([59]), tensor([94]), tensor([80]), tensor([25]), tensor([30]), tensor([25]), tensor([88]), tensor([30]), tensor([50]), tensor([45]), tensor([88]), tensor([34]), tensor([59]), tensor([78]), tensor([30]), tensor([94]), tensor([36]), tensor([80]), tensor([49]), tensor([80]), tensor([58]), tensor([25]), tensor([50]), tensor([86]), tensor([49]), tensor([35]), tensor([13]), tensor([72]), tensor([68]), tensor([82]), tensor([49]), tensor([81]), tensor([69]), tensor([78]), tensor([95]), tensor([81]), tensor([59]), tensor([95]), tensor([69]), tensor([24]), tensor([97]), tensor([36]), tensor([10]), tensor([37]), tensor([94]), tensor([97]), tensor([69]), tensor([57]), tensor([72]), tensor([3]), tensor([59]), tensor([60]), tensor([94]), tensor([97]), tensor([97]), tensor([3]), tensor([3]), tensor([34]), tensor([45]), tensor([94]), tensor([80]), tensor([69]), tensor([94]), tensor([97]), tensor([72]), tensor([57]), tensor([81]), tensor([24]), tensor([78]), tensor([13]), tensor([60]), tensor([50]), tensor([58]), tensor([30]), tensor([81]), tensor([88]), tensor([60]), tensor([10]), tensor([81]), tensor([45]), tensor([34]), tensor([82]), tensor([34]), tensor([24]), tensor([27]), tensor([58]), tensor([25]), tensor([45]), tensor([24]), tensor([88]), tensor([78]), tensor([97]), tensor([25]), tensor([35]), tensor([10]), tensor([35]), tensor([81]), tensor([69]), tensor([36]), tensor([57]), tensor([45]), tensor([95]), tensor([36]), tensor([24]), tensor([86]), tensor([58]), tensor([82]), tensor([35]), tensor([72]), tensor([49]), tensor([81]), tensor([27]), tensor([57]), tensor([34]), tensor([69]), tensor([59]), tensor([49]), tensor([36]), tensor([34]), tensor([72]), tensor([97]), tensor([37]), tensor([59]), tensor([69]), tensor([36]), tensor([35]), tensor([49]), tensor([59]), tensor([24]), tensor([69]), tensor([60]), tensor([27]), tensor([35]), tensor([50]), tensor([50]), tensor([80]), tensor([25]), tensor([94]), tensor([35]), tensor([95]), tensor([49]), tensor([69]), tensor([94]), tensor([59]), tensor([24]), tensor([78]), tensor([10]), tensor([86]), tensor([60]), tensor([69]), tensor([60]), tensor([34]), tensor([97]), tensor([30]), tensor([95]), tensor([35]), tensor([58]), tensor([80]), tensor([72]), tensor([30]), tensor([68]), tensor([35]), tensor([13]), tensor([59]), tensor([45]), tensor([24]), tensor([88]), tensor([24]), tensor([78]), tensor([80]), tensor([59]), tensor([88]), tensor([78]), tensor([68]), tensor([88]), tensor([45]), tensor([78]), tensor([35]), tensor([80]), tensor([34]), tensor([58]), tensor([69]), tensor([69]), tensor([68]), tensor([36]), tensor([81]), tensor([60]), tensor([86]), tensor([36]), tensor([58]), tensor([35]), tensor([35]), tensor([50]), tensor([86]), tensor([50]), tensor([78]), tensor([78]), tensor([72]), tensor([94]), tensor([72]), tensor([13]), tensor([88]), tensor([30]), tensor([36]), tensor([80]), tensor([95]), tensor([37]), tensor([88]), tensor([86]), tensor([86]), tensor([72]), tensor([37]), tensor([72]), tensor([35]), tensor([88]), tensor([27]), tensor([25]), tensor([81]), tensor([50]), tensor([86]), tensor([49]), tensor([81]), tensor([94]), tensor([34]), tensor([88]), tensor([45]), tensor([30]), tensor([97]), tensor([13]), tensor([59]), tensor([49]), tensor([45]), tensor([13]), tensor([72]), tensor([60]), tensor([50]), tensor([86]), tensor([97]), tensor([69]), tensor([36]), tensor([36]), tensor([37]), tensor([49]), tensor([10]), tensor([27]), tensor([3]), tensor([3]), tensor([13]), tensor([97]), tensor([45]), tensor([60]), tensor([13]), tensor([25]), tensor([88]), tensor([50]), tensor([86]), tensor([50]), tensor([27]), tensor([80]), tensor([58]), tensor([88]), tensor([72]), tensor([35]), tensor([68]), tensor([57]), tensor([45]), tensor([72]), tensor([49]), tensor([37]), tensor([37]), tensor([60]), tensor([37]), tensor([86]), tensor([69]), tensor([35]), tensor([57]), tensor([88]), tensor([3]), tensor([69]), tensor([81]), tensor([13]), tensor([58]), tensor([27]), tensor([36]), tensor([72]), tensor([45]), tensor([80]), tensor([50]), tensor([25]), tensor([13]), tensor([97]), tensor([37]), tensor([59]), tensor([10]), tensor([30]), tensor([36]), tensor([58]), tensor([58]), tensor([3]), tensor([37]), tensor([37]), tensor([25]), tensor([50]), tensor([34]), tensor([24]), tensor([88]), tensor([37]), tensor([10]), tensor([10]), tensor([88]), tensor([50]), tensor([27]), tensor([49]), tensor([59]), tensor([30]), tensor([80]), tensor([3]), tensor([30]), tensor([37]), tensor([27]), tensor([82]), tensor([78]), tensor([82]), tensor([30]), tensor([72]), tensor([27]), tensor([45]), tensor([10]), tensor([59]), tensor([10]), tensor([57]), tensor([30]), tensor([35]), tensor([81]), tensor([37]), tensor([88]), tensor([49]), tensor([35]), tensor([10]), tensor([50]), tensor([68]), tensor([88]), tensor([97]), tensor([37]), tensor([3]), tensor([27]), tensor([34]), tensor([37]), tensor([94]), tensor([24]), tensor([37]), tensor([49]), tensor([3]), tensor([59]), tensor([58]), tensor([13]), tensor([59]), tensor([95]), tensor([60]), tensor([57]), tensor([36]), tensor([37]), tensor([69]), tensor([13]), tensor([59]), tensor([95]), tensor([95]), tensor([25]), tensor([68]), tensor([37]), tensor([30]), tensor([59]), tensor([68]), tensor([30]), tensor([3]), tensor([35]), tensor([80]), tensor([49]), tensor([25]), tensor([27]), tensor([88]), tensor([50]), tensor([60]), tensor([94]), tensor([13]), tensor([81]), tensor([37]), tensor([80]), tensor([10]), tensor([24]), tensor([78]), tensor([86]), tensor([30]), tensor([35]), tensor([94]), tensor([13]), tensor([95]), tensor([72]), tensor([24]), tensor([94]), tensor([82]), tensor([30]), tensor([50]), tensor([69]), tensor([81]), tensor([49]), tensor([88]), tensor([80]), tensor([60]), tensor([59]), tensor([88]), tensor([25]), tensor([57]), tensor([25]), tensor([35]), tensor([78]), tensor([88]), tensor([80]), tensor([68]), tensor([60]), tensor([34]), tensor([37]), tensor([57]), tensor([86]), tensor([86]), tensor([50]), tensor([35]), tensor([37]), tensor([35]), tensor([27]), tensor([30]), tensor([78]), tensor([49]), tensor([69]), tensor([59]), tensor([88]), tensor([58]), tensor([3]), tensor([34]), tensor([80]), tensor([95]), tensor([60]), tensor([72]), tensor([68]), tensor([45]), tensor([60]), tensor([80]), tensor([49]), tensor([34]), tensor([27]), tensor([94]), tensor([68]), tensor([78]), tensor([27]), tensor([25]), tensor([86]), tensor([94]), tensor([45]), tensor([37]), tensor([49]), tensor([97]), tensor([24]), tensor([68]), tensor([78]), tensor([60]), tensor([80]), tensor([78]), tensor([35]), tensor([81]), tensor([72]), tensor([58]), tensor([3]), tensor([34]), tensor([3]), tensor([97]), tensor([97]), tensor([95]), tensor([86]), tensor([24]), tensor([80]), tensor([59]), tensor([94]), tensor([30]), tensor([82]), tensor([58]), tensor([80]), tensor([86]), tensor([57]), tensor([78]), tensor([35]), tensor([25]), tensor([10]), tensor([78]), tensor([82]), tensor([94]), tensor([95]), tensor([97]), tensor([86]), tensor([97]), tensor([59]), tensor([50]), tensor([78]), tensor([58]), tensor([57]), tensor([80]), tensor([13]), tensor([24]), tensor([59]), tensor([59]), tensor([72]), tensor([97]), tensor([34]), tensor([95]), tensor([80]), tensor([13]), tensor([78]), tensor([45]), tensor([60]), tensor([60]), tensor([10]), tensor([60]), tensor([78]), tensor([60]), tensor([80]), tensor([80]), tensor([72]), tensor([10]), tensor([25]), tensor([60]), tensor([24]), tensor([82]), tensor([50]), tensor([80]), tensor([86]), tensor([60]), tensor([57]), tensor([49]), tensor([78]), tensor([49]), tensor([69]), tensor([45]), tensor([50]), tensor([69]), tensor([68]), tensor([35]), tensor([10]), tensor([69]), tensor([30]), tensor([72]), tensor([95]), tensor([86]), tensor([69]), tensor([45]), tensor([60]), tensor([86]), tensor([35]), tensor([35]), tensor([86]), tensor([57]), tensor([24]), tensor([34]), tensor([95]), tensor([10]), tensor([88]), tensor([69]), tensor([95]), tensor([49]), tensor([30]), tensor([27]), tensor([25]), tensor([27]), tensor([27]), tensor([82]), tensor([78]), tensor([50]), tensor([95]), tensor([36]), tensor([35]), tensor([58]), tensor([81]), tensor([13]), tensor([45]), tensor([81]), tensor([97]), tensor([94]), tensor([68]), tensor([68]), tensor([81]), tensor([95]), tensor([60]), tensor([37]), tensor([3]), tensor([13]), tensor([50]), tensor([25]), tensor([94]), tensor([80]), tensor([35]), tensor([82]), tensor([30]), tensor([36]), tensor([27]), tensor([37]), tensor([13]), tensor([94]), tensor([59]), tensor([37]), tensor([34]), tensor([13]), tensor([78]), tensor([72]), tensor([57]), tensor([78]), tensor([78]), tensor([36]), tensor([25]), tensor([10]), tensor([34]), tensor([10]), tensor([97]), tensor([95]), tensor([37]), tensor([86]), tensor([10]), tensor([30]), tensor([82]), tensor([95]), tensor([24]), tensor([60]), tensor([24]), tensor([27]), tensor([30]), tensor([24]), tensor([80]), tensor([24]), tensor([27]), tensor([3]), tensor([36]), tensor([88]), tensor([68]), tensor([37]), tensor([60]), tensor([36]), tensor([37]), tensor([68]), tensor([82]), tensor([68]), tensor([82]), tensor([97]), tensor([88]), tensor([69]), tensor([10]), tensor([72]), tensor([45]), tensor([82]), tensor([94]), tensor([45]), tensor([37]), tensor([69]), tensor([86]), tensor([25]), tensor([34]), tensor([88]), tensor([35]), tensor([95]), tensor([10]), tensor([94]), tensor([10]), tensor([25]), tensor([50]), tensor([69]), tensor([78]), tensor([88]), tensor([57]), tensor([80]), tensor([59]), tensor([45]), tensor([45]), tensor([94]), tensor([30]), tensor([68]), tensor([25]), tensor([27]), tensor([30]), tensor([97]), tensor([35]), tensor([82]), tensor([82]), tensor([10]), tensor([10]), tensor([24]), tensor([35]), tensor([88]), tensor([13]), tensor([95]), tensor([95]), tensor([50]), tensor([49]), tensor([37]), tensor([37]), tensor([95]), tensor([86]), tensor([88]), tensor([34]), tensor([78]), tensor([45]), tensor([27]), tensor([36]), tensor([50]), tensor([49]), tensor([97]), tensor([86]), tensor([50]), tensor([37]), tensor([36]), tensor([78]), tensor([10]), tensor([80]), tensor([34]), tensor([37]), tensor([86]), tensor([86]), tensor([82]), tensor([27]), tensor([60]), tensor([30]), tensor([30]), tensor([58]), tensor([49]), tensor([97]), tensor([45]), tensor([49]), tensor([34]), tensor([72]), tensor([3]), tensor([80]), tensor([94]), tensor([24]), tensor([69]), tensor([10]), tensor([24]), tensor([37]), tensor([86]), tensor([59]), tensor([81]), tensor([58]), tensor([35]), tensor([37]), tensor([25]), tensor([57]), tensor([35]), tensor([25]), tensor([50]), tensor([80]), tensor([95]), tensor([81]), tensor([78]), tensor([60]), tensor([95]), tensor([58]), tensor([60]), tensor([81]), tensor([3]), tensor([59]), tensor([58]), tensor([58]), tensor([95]), tensor([24]), tensor([60]), tensor([68]), tensor([27]), tensor([59]), tensor([68]), tensor([58]), tensor([3]), tensor([36]), tensor([88]), tensor([69]), tensor([45]), tensor([45]), tensor([27]), tensor([37]), tensor([34]), tensor([94]), tensor([24]), tensor([49]), tensor([72]), tensor([82]), tensor([60]), tensor([69]), tensor([57]), tensor([49]), tensor([13]), tensor([36]), tensor([45]), tensor([58]), tensor([69]), tensor([57]), tensor([72]), tensor([30]), tensor([49]), tensor([10]), tensor([35]), tensor([13]), tensor([95]), tensor([60]), tensor([13]), tensor([35]), tensor([45]), tensor([50]), tensor([45]), tensor([25]), tensor([86]), tensor([58]), tensor([80]), tensor([50]), tensor([13]), tensor([86]), tensor([69]), tensor([82]), tensor([81]), tensor([57]), tensor([60]), tensor([50]), tensor([45]), tensor([78]), tensor([72]), tensor([3]), tensor([81]), tensor([3]), tensor([78]), tensor([60]), tensor([24]), tensor([57]), tensor([57]), tensor([81]), tensor([3]), tensor([35]), tensor([10]), tensor([81]), tensor([88]), tensor([10]), tensor([68]), tensor([3]), tensor([27]), tensor([58]), tensor([27]), tensor([24]), tensor([50]), tensor([97]), tensor([27]), tensor([69]), tensor([34]), tensor([30]), tensor([58]), tensor([69]), tensor([60]), tensor([80]), tensor([13]), tensor([72]), tensor([60]), tensor([82]), tensor([10]), tensor([27]), tensor([97]), tensor([24]), tensor([36]), tensor([13]), tensor([34]), tensor([13]), tensor([81]), tensor([57]), tensor([60]), tensor([49]), tensor([10]), tensor([78]), tensor([81]), tensor([10]), tensor([60]), tensor([50]), tensor([3]), tensor([45]), tensor([10]), tensor([59]), tensor([81]), tensor([36]), tensor([24]), tensor([95]), tensor([25]), tensor([69]), tensor([97]), tensor([78]), tensor([78]), tensor([3]), tensor([27]), tensor([57]), tensor([24]), tensor([95]), tensor([60]), tensor([57]), tensor([69]), tensor([69]), tensor([57]), tensor([36]), tensor([13]), tensor([81]), tensor([94]), tensor([10]), tensor([57]), tensor([3]), tensor([57]), tensor([49]), tensor([30]), tensor([59]), tensor([69]), tensor([34]), tensor([57]), tensor([34]), tensor([68]), tensor([57]), tensor([81]), tensor([69]), tensor([78]), tensor([82]), tensor([25]), tensor([78]), tensor([97]), tensor([82]), tensor([49]), tensor([81]), tensor([45]), tensor([49]), tensor([37]), tensor([35]), tensor([86]), tensor([88]), tensor([80]), tensor([57]), tensor([82]), tensor([13]), tensor([25]), tensor([34]), tensor([30]), tensor([69]), tensor([97]), tensor([3]), tensor([36]), tensor([86]), tensor([30]), tensor([37]), tensor([94]), tensor([80]), tensor([37]), tensor([57]), tensor([58]), tensor([80]), tensor([3]), tensor([86]), tensor([95]), tensor([35]), tensor([60]), tensor([58]), tensor([3]), tensor([13]), tensor([13]), tensor([57]), tensor([68]), tensor([58]), tensor([34]), tensor([82]), tensor([68]), tensor([57]), tensor([45]), tensor([81]), tensor([86]), tensor([69]), tensor([97]), tensor([58]), tensor([97]), tensor([36]), tensor([27]), tensor([34]), tensor([80]), tensor([24]), tensor([45]), tensor([24]), tensor([68]), tensor([97]), tensor([72]), tensor([69]), tensor([97]), tensor([13]), tensor([82]), tensor([82]), tensor([27]), tensor([50]), tensor([27]), tensor([72]), tensor([68]), tensor([86]), tensor([95]), tensor([94]), tensor([86]), tensor([34]), tensor([78]), tensor([10]), tensor([13]), tensor([58]), tensor([88]), tensor([78]), tensor([97]), tensor([50]), tensor([81]), tensor([58]), tensor([78]), tensor([27]), tensor([30]), tensor([95]), tensor([49]), tensor([37]), tensor([80]), tensor([59]), tensor([24]), tensor([58]), tensor([69]), tensor([36]), tensor([86]), tensor([68]), tensor([88]), tensor([88]), tensor([25]), tensor([37]), tensor([88]), tensor([24]), tensor([60]), tensor([88]), tensor([86]), tensor([35]), tensor([88]), tensor([37]), tensor([97]), tensor([36]), tensor([30]), tensor([10]), tensor([86]), tensor([25]), tensor([37]), tensor([58]), tensor([68]), tensor([60]), tensor([3]), tensor([24]), tensor([82]), tensor([86]), tensor([27]), tensor([58]), tensor([50]), tensor([36]), tensor([3]), tensor([50]), tensor([34]), tensor([59]), tensor([30]), tensor([80]), tensor([58]), tensor([27]), tensor([3]), tensor([95]), tensor([37]), tensor([58]), tensor([37]), tensor([58]), tensor([68]), tensor([95]), tensor([49]), tensor([37]), tensor([25]), tensor([59]), tensor([49]), tensor([27]), tensor([3]), tensor([86]), tensor([58]), tensor([82]), tensor([97]), tensor([30]), tensor([68]), tensor([68]), tensor([27]), tensor([97]), tensor([95]), tensor([82]), tensor([60]), tensor([86]), tensor([36]), tensor([95]), tensor([78]), tensor([97]), tensor([3]), tensor([59]), tensor([27]), tensor([34]), tensor([35]), tensor([80]), tensor([49]), tensor([34]), tensor([72]), tensor([34]), tensor([88]), tensor([78]), tensor([69]), tensor([88]), tensor([24]), tensor([34]), tensor([88]), tensor([81]), tensor([13]), tensor([82]), tensor([58]), tensor([36]), tensor([95]), tensor([50]), tensor([27]), tensor([57]), tensor([86]), tensor([94]), tensor([80]), tensor([86]), tensor([60]), tensor([34]), tensor([13]), tensor([72]), tensor([78]), tensor([57]), tensor([82]), tensor([95]), tensor([57]), tensor([35]), tensor([36]), tensor([69]), tensor([72]), tensor([45]), tensor([25]), tensor([78]), tensor([58]), tensor([30]), tensor([10]), tensor([13]), tensor([57]), tensor([24]), tensor([82]), tensor([60]), tensor([72]), tensor([68]), tensor([58]), tensor([3]), tensor([24]), tensor([25]), tensor([37]), tensor([88]), tensor([24]), tensor([86]), tensor([95]), tensor([60]), tensor([58]), tensor([60]), tensor([30]), tensor([95]), tensor([36]), tensor([86]), tensor([94]), tensor([59]), tensor([27]), tensor([94]), tensor([97]), tensor([69]), tensor([13]), tensor([36]), tensor([82]), tensor([94]), tensor([10]), tensor([82]), tensor([10]), tensor([50]), tensor([50]), tensor([49]), tensor([24]), tensor([94]), tensor([82]), tensor([58]), tensor([25]), tensor([68]), tensor([78]), tensor([3]), tensor([68]), tensor([49]), tensor([78]), tensor([80]), tensor([72]), tensor([24]), tensor([95]), tensor([30]), tensor([60]), tensor([58]), tensor([97]), tensor([34]), tensor([60]), tensor([13]), tensor([72]), tensor([25]), tensor([25]), tensor([88]), tensor([13]), tensor([25]), tensor([24]), tensor([36]), tensor([59]), tensor([13]), tensor([97]), tensor([97]), tensor([50]), tensor([27]), tensor([82]), tensor([25]), tensor([24]), tensor([86]), tensor([57]), tensor([57]), tensor([59]), tensor([45]), tensor([86]), tensor([78]), tensor([57]), tensor([68]), tensor([69]), tensor([97]), tensor([10]), tensor([97]), tensor([78]), tensor([81]), tensor([10]), tensor([34]), tensor([68]), tensor([10]), tensor([10]), tensor([58]), tensor([34]), tensor([58]), tensor([49]), tensor([69]), tensor([60]), tensor([24]), tensor([82]), tensor([50]), tensor([30]), tensor([72]), tensor([78]), tensor([45]), tensor([37]), tensor([81]), tensor([88]), tensor([69]), tensor([86]), tensor([49]), tensor([97]), tensor([45]), tensor([27]), tensor([59]), tensor([10]), tensor([80]), tensor([72]), tensor([80]), tensor([50]), tensor([72]), tensor([97]), tensor([86]), tensor([37]), tensor([68]), tensor([49]), tensor([37]), tensor([86]), tensor([25]), tensor([82]), tensor([81]), tensor([86]), tensor([69]), tensor([24]), tensor([69]), tensor([30]), tensor([49]), tensor([3]), tensor([78]), tensor([68]), tensor([24]), tensor([94]), tensor([59]), tensor([27]), tensor([50]), tensor([27]), tensor([57]), tensor([45]), tensor([10]), tensor([80]), tensor([94]), tensor([88]), tensor([72]), tensor([13]), tensor([88]), tensor([30]), tensor([59]), tensor([45]), tensor([57]), tensor([72]), tensor([36]), tensor([45]), tensor([30]), tensor([30]), tensor([34]), tensor([34]), tensor([82]), tensor([3]), tensor([3]), tensor([27]), tensor([86]), tensor([72]), tensor([3]), tensor([72]), tensor([57]), tensor([80]), tensor([27]), tensor([45]), tensor([88]), tensor([86]), tensor([24]), tensor([95]), tensor([60]), tensor([50]), tensor([72]), tensor([13]), tensor([94]), tensor([94]), tensor([72]), tensor([27]), tensor([68]), tensor([50]), tensor([25]), tensor([34]), tensor([50]), tensor([50]), tensor([27]), tensor([59]), tensor([3]), tensor([10]), tensor([13]), tensor([80]), tensor([49]), tensor([58]), tensor([72]), tensor([81]), tensor([82]), tensor([10]), tensor([88]), tensor([59]), tensor([59]), tensor([50]), tensor([97]), tensor([58]), tensor([78]), tensor([13]), tensor([82]), tensor([10]), tensor([68]), tensor([25]), tensor([35]), tensor([50]), tensor([50]), tensor([97]), tensor([49]), tensor([13]), tensor([37]), tensor([57]), tensor([30]), tensor([35]), tensor([35]), tensor([78]), tensor([25]), tensor([86]), tensor([3]), tensor([30]), tensor([37]), tensor([37]), tensor([94]), tensor([68]), tensor([94]), tensor([50]), tensor([49]), tensor([86]), tensor([88]), tensor([25]), tensor([25]), tensor([57]), tensor([60]), tensor([88]), tensor([97]), tensor([78]), tensor([36]), tensor([80]), tensor([25]), tensor([94]), tensor([58]), tensor([60]), tensor([72]), tensor([35]), tensor([49]), tensor([95]), tensor([10]), tensor([25]), tensor([25]), tensor([97]), tensor([37]), tensor([30]), tensor([81]), tensor([35]), tensor([68]), tensor([95]), tensor([34]), tensor([58]), tensor([13]), tensor([80]), tensor([25]), tensor([49]), tensor([82]), tensor([94]), tensor([34]), tensor([88]), tensor([95]), tensor([80]), tensor([30]), tensor([35]), tensor([68]), tensor([13]), tensor([34]), tensor([59]), tensor([35]), tensor([24]), tensor([30]), tensor([82]), tensor([82]), tensor([35]), tensor([97]), tensor([37]), tensor([78]), tensor([72]), tensor([58]), tensor([36]), tensor([45]), tensor([97]), tensor([10]), tensor([34]), tensor([49]), tensor([27]), tensor([68]), tensor([82]), tensor([3]), tensor([36]), tensor([78]), tensor([72]), tensor([49]), tensor([58]), tensor([72]), tensor([94]), tensor([68]), tensor([13]), tensor([95]), tensor([3]), tensor([95]), tensor([58]), tensor([72]), tensor([72]), tensor([81]), tensor([36]), tensor([59]), tensor([57]), tensor([78]), tensor([81]), tensor([78]), tensor([57]), tensor([60]), tensor([34]), tensor([94]), tensor([37]), tensor([3]), tensor([3]), tensor([82]), tensor([80]), tensor([27]), tensor([68]), tensor([68]), tensor([13]), tensor([57]), tensor([97]), tensor([78]), tensor([45]), tensor([86]), tensor([95]), tensor([34]), tensor([69]), tensor([45]), tensor([10]), tensor([37]), tensor([68]), tensor([45]), tensor([82]), tensor([10]), tensor([34]), tensor([3]), tensor([95]), tensor([27]), tensor([94]), tensor([3]), tensor([25]), tensor([72]), tensor([45]), tensor([80]), tensor([60]), tensor([86]), tensor([36]), tensor([37]), tensor([36]), tensor([59])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.66 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.629\n",
            "TEST ALL:  0.6686666666666666\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  4000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 95, 92, 88, 86, 82, 80, 78, 72, 68, 60, 58, 52, 50, 36, 34, 30, 24, 12, 94, 96, 98, 45, 81, 77, 69, 59, 57, 53, 49, 37, 3, 35, 33, 31, 27, 25, 13, 11, 10]\n",
            "TRAIN_SET CLASSES:  [31, 11, 98, 77, 53, 33, 96, 92, 52, 12]\n",
            "VALIDATION CLASSES:  [53, 52, 98, 33, 96, 31, 92, 77, 12, 11]\n",
            "GROUP:  4\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [31, 11, 98, 77, 53, 33, 96, 92, 52, 12]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "num classes till now:  40\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.3073346018791199\n",
            "Train step - Step 10, Loss 0.1480991691350937\n",
            "Train step - Step 20, Loss 0.13023869693279266\n",
            "Train step - Step 30, Loss 0.12014985084533691\n",
            "Train step - Step 40, Loss 0.11557386070489883\n",
            "Train step - Step 50, Loss 0.11761804670095444\n",
            "Train epoch - Accuracy: 0.2253968253968254 Loss: 0.14208179342454302 Corrects: 1562\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.11956510692834854\n",
            "Train step - Step 70, Loss 0.11104216426610947\n",
            "Train step - Step 80, Loss 0.11280784755945206\n",
            "Train step - Step 90, Loss 0.11579904705286026\n",
            "Train step - Step 100, Loss 0.11548564583063126\n",
            "Train epoch - Accuracy: 0.25266955266955266 Loss: 0.11274286488347927 Corrects: 1751\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.10096504539251328\n",
            "Train step - Step 120, Loss 0.12171955406665802\n",
            "Train step - Step 130, Loss 0.1140647754073143\n",
            "Train step - Step 140, Loss 0.10412990301847458\n",
            "Train step - Step 150, Loss 0.11500344425439835\n",
            "Train step - Step 160, Loss 0.10406651347875595\n",
            "Train epoch - Accuracy: 0.26810966810966813 Loss: 0.11041771090047872 Corrects: 1858\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.10949307680130005\n",
            "Train step - Step 180, Loss 0.11347679048776627\n",
            "Train step - Step 190, Loss 0.10351679474115372\n",
            "Train step - Step 200, Loss 0.10616955906152725\n",
            "Train step - Step 210, Loss 0.11427489668130875\n",
            "Train epoch - Accuracy: 0.2867243867243867 Loss: 0.10821211563173311 Corrects: 1987\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.10581977665424347\n",
            "Train step - Step 230, Loss 0.10697080194950104\n",
            "Train step - Step 240, Loss 0.11016473919153214\n",
            "Train step - Step 250, Loss 0.1080569252371788\n",
            "Train step - Step 260, Loss 0.10822806507349014\n",
            "Train step - Step 270, Loss 0.10467226803302765\n",
            "Train epoch - Accuracy: 0.30447330447330445 Loss: 0.10708875214683718 Corrects: 2110\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.10313472896814346\n",
            "Train step - Step 290, Loss 0.10525494068861008\n",
            "Train step - Step 300, Loss 0.10753007233142853\n",
            "Train step - Step 310, Loss 0.09284785389900208\n",
            "Train step - Step 320, Loss 0.10813412815332413\n",
            "Train epoch - Accuracy: 0.31154401154401157 Loss: 0.10655235765128253 Corrects: 2159\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11063102632761002\n",
            "Train step - Step 340, Loss 0.1032189279794693\n",
            "Train step - Step 350, Loss 0.10238444060087204\n",
            "Train step - Step 360, Loss 0.09718789160251617\n",
            "Train step - Step 370, Loss 0.10745420306921005\n",
            "Train step - Step 380, Loss 0.1108352541923523\n",
            "Train epoch - Accuracy: 0.33015873015873015 Loss: 0.10541329168181054 Corrects: 2288\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.11108193546533585\n",
            "Train step - Step 400, Loss 0.1079028844833374\n",
            "Train step - Step 410, Loss 0.10221685469150543\n",
            "Train step - Step 420, Loss 0.10259101539850235\n",
            "Train step - Step 430, Loss 0.09454160928726196\n",
            "Train epoch - Accuracy: 0.3329004329004329 Loss: 0.10546921327774658 Corrects: 2307\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.1103404089808464\n",
            "Train step - Step 450, Loss 0.10151410102844238\n",
            "Train step - Step 460, Loss 0.10482936352491379\n",
            "Train step - Step 470, Loss 0.10243437439203262\n",
            "Train step - Step 480, Loss 0.1061353012919426\n",
            "Train step - Step 490, Loss 0.09856237471103668\n",
            "Train epoch - Accuracy: 0.33910533910533913 Loss: 0.10420990554242252 Corrects: 2350\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10924811661243439\n",
            "Train step - Step 510, Loss 0.10205985605716705\n",
            "Train step - Step 520, Loss 0.10572909563779831\n",
            "Train step - Step 530, Loss 0.10204048454761505\n",
            "Train step - Step 540, Loss 0.10180198401212692\n",
            "Train epoch - Accuracy: 0.3464646464646465 Loss: 0.1046870245540469 Corrects: 2401\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.10944072157144547\n",
            "Train step - Step 560, Loss 0.11173780262470245\n",
            "Train step - Step 570, Loss 0.09572450071573257\n",
            "Train step - Step 580, Loss 0.10566937178373337\n",
            "Train step - Step 590, Loss 0.11440491676330566\n",
            "Train step - Step 600, Loss 0.10373914241790771\n",
            "Train epoch - Accuracy: 0.3577200577200577 Loss: 0.1042647104328673 Corrects: 2479\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.10635427385568619\n",
            "Train step - Step 620, Loss 0.09808206558227539\n",
            "Train step - Step 630, Loss 0.10784973949193954\n",
            "Train step - Step 640, Loss 0.10120978206396103\n",
            "Train step - Step 650, Loss 0.10698049515485764\n",
            "Train epoch - Accuracy: 0.3660894660894661 Loss: 0.103577994457399 Corrects: 2537\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10872111469507217\n",
            "Train step - Step 670, Loss 0.09544453769922256\n",
            "Train step - Step 680, Loss 0.1096452847123146\n",
            "Train step - Step 690, Loss 0.09726341813802719\n",
            "Train step - Step 700, Loss 0.09812896698713303\n",
            "Train step - Step 710, Loss 0.10519436746835709\n",
            "Train epoch - Accuracy: 0.37676767676767675 Loss: 0.10337531611129835 Corrects: 2611\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10167764872312546\n",
            "Train step - Step 730, Loss 0.1072809249162674\n",
            "Train step - Step 740, Loss 0.11063728481531143\n",
            "Train step - Step 750, Loss 0.09839712828397751\n",
            "Train step - Step 760, Loss 0.09868873655796051\n",
            "Train epoch - Accuracy: 0.38225108225108223 Loss: 0.1026655432164755 Corrects: 2649\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10115231573581696\n",
            "Train step - Step 780, Loss 0.10021913051605225\n",
            "Train step - Step 790, Loss 0.09926768392324448\n",
            "Train step - Step 800, Loss 0.10027740150690079\n",
            "Train step - Step 810, Loss 0.10009435564279556\n",
            "Train step - Step 820, Loss 0.10086234658956528\n",
            "Train epoch - Accuracy: 0.38441558441558443 Loss: 0.10209871838662188 Corrects: 2664\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.09181171655654907\n",
            "Train step - Step 840, Loss 0.10229893028736115\n",
            "Train step - Step 850, Loss 0.10149097442626953\n",
            "Train step - Step 860, Loss 0.0968412309885025\n",
            "Train step - Step 870, Loss 0.10262322425842285\n",
            "Train epoch - Accuracy: 0.3930735930735931 Loss: 0.10216522926806028 Corrects: 2724\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.09923968464136124\n",
            "Train step - Step 890, Loss 0.0982104241847992\n",
            "Train step - Step 900, Loss 0.10101731866598129\n",
            "Train step - Step 910, Loss 0.09948103874921799\n",
            "Train step - Step 920, Loss 0.1038048192858696\n",
            "Train step - Step 930, Loss 0.09447764605283737\n",
            "Train epoch - Accuracy: 0.4002886002886003 Loss: 0.10227622474082793 Corrects: 2774\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.10148904472589493\n",
            "Train step - Step 950, Loss 0.09863685816526413\n",
            "Train step - Step 960, Loss 0.10014193505048752\n",
            "Train step - Step 970, Loss 0.09791777282953262\n",
            "Train step - Step 980, Loss 0.10515346378087997\n",
            "Train epoch - Accuracy: 0.39855699855699855 Loss: 0.10174654174977739 Corrects: 2762\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.0977393090724945\n",
            "Train step - Step 1000, Loss 0.09715157747268677\n",
            "Train step - Step 1010, Loss 0.09563546627759933\n",
            "Train step - Step 1020, Loss 0.10423469543457031\n",
            "Train step - Step 1030, Loss 0.10386121273040771\n",
            "Train step - Step 1040, Loss 0.09685119241476059\n",
            "Train epoch - Accuracy: 0.40894660894660895 Loss: 0.10105522846504723 Corrects: 2834\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.09621112048625946\n",
            "Train step - Step 1060, Loss 0.10888013988733292\n",
            "Train step - Step 1070, Loss 0.099806047976017\n",
            "Train step - Step 1080, Loss 0.1054057851433754\n",
            "Train step - Step 1090, Loss 0.10287012159824371\n",
            "Train epoch - Accuracy: 0.41284271284271284 Loss: 0.10114753580454625 Corrects: 2861\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.09727699309587479\n",
            "Train step - Step 1110, Loss 0.09593977779150009\n",
            "Train step - Step 1120, Loss 0.09867918491363525\n",
            "Train step - Step 1130, Loss 0.09354998171329498\n",
            "Train step - Step 1140, Loss 0.09836740046739578\n",
            "Train step - Step 1150, Loss 0.10430946201086044\n",
            "Train epoch - Accuracy: 0.4288600288600289 Loss: 0.1009411845114324 Corrects: 2972\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10636980831623077\n",
            "Train step - Step 1170, Loss 0.10506885498762131\n",
            "Train step - Step 1180, Loss 0.09726681560277939\n",
            "Train step - Step 1190, Loss 0.10805258899927139\n",
            "Train step - Step 1200, Loss 0.09933515638113022\n",
            "Train epoch - Accuracy: 0.42655122655122657 Loss: 0.10050496612534379 Corrects: 2956\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.09644343703985214\n",
            "Train step - Step 1220, Loss 0.10693464428186417\n",
            "Train step - Step 1230, Loss 0.09511859714984894\n",
            "Train step - Step 1240, Loss 0.10007991641759872\n",
            "Train step - Step 1250, Loss 0.10014473646879196\n",
            "Train step - Step 1260, Loss 0.1005738154053688\n",
            "Train epoch - Accuracy: 0.43102453102453103 Loss: 0.1002027116167597 Corrects: 2987\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.1025499701499939\n",
            "Train step - Step 1280, Loss 0.10265964269638062\n",
            "Train step - Step 1290, Loss 0.09789279848337173\n",
            "Train step - Step 1300, Loss 0.09562303125858307\n",
            "Train step - Step 1310, Loss 0.09099739044904709\n",
            "Train epoch - Accuracy: 0.43232323232323233 Loss: 0.09960526055716849 Corrects: 2996\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.09537853300571442\n",
            "Train step - Step 1330, Loss 0.09826133400201797\n",
            "Train step - Step 1340, Loss 0.10436160862445831\n",
            "Train step - Step 1350, Loss 0.09823613613843918\n",
            "Train step - Step 1360, Loss 0.09901461750268936\n",
            "Train step - Step 1370, Loss 0.10087913274765015\n",
            "Train epoch - Accuracy: 0.4406926406926407 Loss: 0.09989547381153355 Corrects: 3054\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.09808685630559921\n",
            "Train step - Step 1390, Loss 0.09831305593252182\n",
            "Train step - Step 1400, Loss 0.09382259845733643\n",
            "Train step - Step 1410, Loss 0.09444605559110641\n",
            "Train step - Step 1420, Loss 0.10030974447727203\n",
            "Train epoch - Accuracy: 0.44372294372294374 Loss: 0.09953289036282902 Corrects: 3075\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.10691952705383301\n",
            "Train step - Step 1440, Loss 0.10131946951150894\n",
            "Train step - Step 1450, Loss 0.10133900493383408\n",
            "Train step - Step 1460, Loss 0.10075552761554718\n",
            "Train step - Step 1470, Loss 0.09960083663463593\n",
            "Train step - Step 1480, Loss 0.10413713753223419\n",
            "Train epoch - Accuracy: 0.44516594516594515 Loss: 0.10080906451823564 Corrects: 3085\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.09905374050140381\n",
            "Train step - Step 1500, Loss 0.10154397785663605\n",
            "Train step - Step 1510, Loss 0.09310275316238403\n",
            "Train step - Step 1520, Loss 0.09694653004407883\n",
            "Train step - Step 1530, Loss 0.11009764671325684\n",
            "Train epoch - Accuracy: 0.4531024531024531 Loss: 0.09917264888855974 Corrects: 3140\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.10101651400327682\n",
            "Train step - Step 1550, Loss 0.09937717765569687\n",
            "Train step - Step 1560, Loss 0.1013263687491417\n",
            "Train step - Step 1570, Loss 0.095622219145298\n",
            "Train step - Step 1580, Loss 0.09873232245445251\n",
            "Train step - Step 1590, Loss 0.09185225516557693\n",
            "Train epoch - Accuracy: 0.45656565656565656 Loss: 0.09846686403687727 Corrects: 3164\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.11031820625066757\n",
            "Train step - Step 1610, Loss 0.09628962725400925\n",
            "Train step - Step 1620, Loss 0.09936636686325073\n",
            "Train step - Step 1630, Loss 0.09174177050590515\n",
            "Train step - Step 1640, Loss 0.1013450175523758\n",
            "Train epoch - Accuracy: 0.45916305916305916 Loss: 0.09943630806295387 Corrects: 3182\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.09426383674144745\n",
            "Train step - Step 1660, Loss 0.10330326855182648\n",
            "Train step - Step 1670, Loss 0.09845548868179321\n",
            "Train step - Step 1680, Loss 0.09715045988559723\n",
            "Train step - Step 1690, Loss 0.1036992073059082\n",
            "Train step - Step 1700, Loss 0.09432192891836166\n",
            "Train epoch - Accuracy: 0.4665223665223665 Loss: 0.09832121624919071 Corrects: 3233\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.0961555615067482\n",
            "Train step - Step 1720, Loss 0.09692354500293732\n",
            "Train step - Step 1730, Loss 0.09616636484861374\n",
            "Train step - Step 1740, Loss 0.09876399487257004\n",
            "Train step - Step 1750, Loss 0.09453659504652023\n",
            "Train epoch - Accuracy: 0.4668109668109668 Loss: 0.09819515299960477 Corrects: 3235\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.09560181200504303\n",
            "Train step - Step 1770, Loss 0.09465458244085312\n",
            "Train step - Step 1780, Loss 0.09735988080501556\n",
            "Train step - Step 1790, Loss 0.0949755385518074\n",
            "Train step - Step 1800, Loss 0.10063965618610382\n",
            "Train step - Step 1810, Loss 0.09421682357788086\n",
            "Train epoch - Accuracy: 0.4746031746031746 Loss: 0.0977420178627727 Corrects: 3289\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.09949290007352829\n",
            "Train step - Step 1830, Loss 0.09804785251617432\n",
            "Train step - Step 1840, Loss 0.09120408445596695\n",
            "Train step - Step 1850, Loss 0.10210692882537842\n",
            "Train step - Step 1860, Loss 0.10320629924535751\n",
            "Train epoch - Accuracy: 0.4743145743145743 Loss: 0.09823688956738207 Corrects: 3287\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.09535720944404602\n",
            "Train step - Step 1880, Loss 0.09315773099660873\n",
            "Train step - Step 1890, Loss 0.09699176996946335\n",
            "Train step - Step 1900, Loss 0.10058505833148956\n",
            "Train step - Step 1910, Loss 0.09439336508512497\n",
            "Train step - Step 1920, Loss 0.09726909548044205\n",
            "Train epoch - Accuracy: 0.47503607503607503 Loss: 0.09761840320295757 Corrects: 3292\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.09700318425893784\n",
            "Train step - Step 1940, Loss 0.10350699722766876\n",
            "Train step - Step 1950, Loss 0.1042802557349205\n",
            "Train step - Step 1960, Loss 0.1007702499628067\n",
            "Train step - Step 1970, Loss 0.0960497185587883\n",
            "Train epoch - Accuracy: 0.4832611832611833 Loss: 0.0975247573134359 Corrects: 3349\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.0995960459113121\n",
            "Train step - Step 1990, Loss 0.09890419989824295\n",
            "Train step - Step 2000, Loss 0.09924816340208054\n",
            "Train step - Step 2010, Loss 0.10442771762609482\n",
            "Train step - Step 2020, Loss 0.09527073800563812\n",
            "Train step - Step 2030, Loss 0.1119525209069252\n",
            "Train epoch - Accuracy: 0.48787878787878786 Loss: 0.09782196283340454 Corrects: 3381\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.09880878776311874\n",
            "Train step - Step 2050, Loss 0.0917138084769249\n",
            "Train step - Step 2060, Loss 0.0970720574259758\n",
            "Train step - Step 2070, Loss 0.09437098354101181\n",
            "Train step - Step 2080, Loss 0.09208861738443375\n",
            "Train epoch - Accuracy: 0.4871572871572872 Loss: 0.09768979359971841 Corrects: 3376\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.09233410656452179\n",
            "Train step - Step 2100, Loss 0.10121779888868332\n",
            "Train step - Step 2110, Loss 0.10158912092447281\n",
            "Train step - Step 2120, Loss 0.0979093536734581\n",
            "Train step - Step 2130, Loss 0.10225389152765274\n",
            "Train step - Step 2140, Loss 0.09455644339323044\n",
            "Train epoch - Accuracy: 0.49855699855699853 Loss: 0.09748673806931893 Corrects: 3455\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.09596476703882217\n",
            "Train step - Step 2160, Loss 0.09842758625745773\n",
            "Train step - Step 2170, Loss 0.0992061197757721\n",
            "Train step - Step 2180, Loss 0.09637937694787979\n",
            "Train step - Step 2190, Loss 0.09887118637561798\n",
            "Train epoch - Accuracy: 0.5053391053391053 Loss: 0.0971109595585179 Corrects: 3502\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.09126700460910797\n",
            "Train step - Step 2210, Loss 0.09742414951324463\n",
            "Train step - Step 2220, Loss 0.10693866014480591\n",
            "Train step - Step 2230, Loss 0.09405491501092911\n",
            "Train step - Step 2240, Loss 0.0967523455619812\n",
            "Train step - Step 2250, Loss 0.09396063536405563\n",
            "Train epoch - Accuracy: 0.49393939393939396 Loss: 0.09674559909027892 Corrects: 3423\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.09733986109495163\n",
            "Train step - Step 2270, Loss 0.0905914306640625\n",
            "Train step - Step 2280, Loss 0.09753473848104477\n",
            "Train step - Step 2290, Loss 0.09721497446298599\n",
            "Train step - Step 2300, Loss 0.09175533056259155\n",
            "Train epoch - Accuracy: 0.5092352092352093 Loss: 0.09713010327812568 Corrects: 3529\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.09656532853841782\n",
            "Train step - Step 2320, Loss 0.09329584985971451\n",
            "Train step - Step 2330, Loss 0.10366035997867584\n",
            "Train step - Step 2340, Loss 0.0958186611533165\n",
            "Train step - Step 2350, Loss 0.09515539556741714\n",
            "Train step - Step 2360, Loss 0.09603734314441681\n",
            "Train epoch - Accuracy: 0.5050505050505051 Loss: 0.09685444028088541 Corrects: 3500\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.09467434883117676\n",
            "Train step - Step 2380, Loss 0.09631792455911636\n",
            "Train step - Step 2390, Loss 0.09425117075443268\n",
            "Train step - Step 2400, Loss 0.09982263296842575\n",
            "Train step - Step 2410, Loss 0.09597712755203247\n",
            "Train epoch - Accuracy: 0.5124098124098124 Loss: 0.09623493250529315 Corrects: 3551\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.09113726764917374\n",
            "Train step - Step 2430, Loss 0.09242258965969086\n",
            "Train step - Step 2440, Loss 0.09291740506887436\n",
            "Train step - Step 2450, Loss 0.09430890530347824\n",
            "Train step - Step 2460, Loss 0.08821886032819748\n",
            "Train step - Step 2470, Loss 0.0990137830376625\n",
            "Train epoch - Accuracy: 0.5131313131313131 Loss: 0.0961864707746891 Corrects: 3556\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.09657149761915207\n",
            "Train step - Step 2490, Loss 0.09433891624212265\n",
            "Train step - Step 2500, Loss 0.09282097220420837\n",
            "Train step - Step 2510, Loss 0.0940684899687767\n",
            "Train step - Step 2520, Loss 0.09670952707529068\n",
            "Train epoch - Accuracy: 0.515007215007215 Loss: 0.09611561370618416 Corrects: 3569\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.0988488644361496\n",
            "Train step - Step 2540, Loss 0.09696682542562485\n",
            "Train step - Step 2550, Loss 0.09623509645462036\n",
            "Train step - Step 2560, Loss 0.09986110776662827\n",
            "Train step - Step 2570, Loss 0.10041172802448273\n",
            "Train step - Step 2580, Loss 0.102719746530056\n",
            "Train epoch - Accuracy: 0.5253968253968254 Loss: 0.0957125021932273 Corrects: 3641\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.09741155803203583\n",
            "Train step - Step 2600, Loss 0.09216010570526123\n",
            "Train step - Step 2610, Loss 0.09637414664030075\n",
            "Train step - Step 2620, Loss 0.09358341246843338\n",
            "Train step - Step 2630, Loss 0.10094467550516129\n",
            "Train epoch - Accuracy: 0.5321789321789322 Loss: 0.09551383651051171 Corrects: 3688\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.09432730823755264\n",
            "Train step - Step 2650, Loss 0.08999278396368027\n",
            "Train step - Step 2660, Loss 0.0986882895231247\n",
            "Train step - Step 2670, Loss 0.09716159105300903\n",
            "Train step - Step 2680, Loss 0.100896455347538\n",
            "Train step - Step 2690, Loss 0.09381072968244553\n",
            "Train epoch - Accuracy: 0.5357864357864358 Loss: 0.09532553843003502 Corrects: 3713\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.09680300951004028\n",
            "Train step - Step 2710, Loss 0.0990324541926384\n",
            "Train step - Step 2720, Loss 0.09866403043270111\n",
            "Train step - Step 2730, Loss 0.08957703411579132\n",
            "Train step - Step 2740, Loss 0.09261628985404968\n",
            "Train epoch - Accuracy: 0.5341991341991342 Loss: 0.09354934946111099 Corrects: 3702\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.09428530931472778\n",
            "Train step - Step 2760, Loss 0.09089644253253937\n",
            "Train step - Step 2770, Loss 0.08993344753980637\n",
            "Train step - Step 2780, Loss 0.08447644114494324\n",
            "Train step - Step 2790, Loss 0.09393709897994995\n",
            "Train step - Step 2800, Loss 0.0995442271232605\n",
            "Train epoch - Accuracy: 0.5366522366522366 Loss: 0.09331502225061859 Corrects: 3719\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.09498847275972366\n",
            "Train step - Step 2820, Loss 0.08946675062179565\n",
            "Train step - Step 2830, Loss 0.0881929025053978\n",
            "Train step - Step 2840, Loss 0.09293469786643982\n",
            "Train step - Step 2850, Loss 0.09044908732175827\n",
            "Train epoch - Accuracy: 0.5522366522366522 Loss: 0.09286967732689598 Corrects: 3827\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.08868025243282318\n",
            "Train step - Step 2870, Loss 0.09488091617822647\n",
            "Train step - Step 2880, Loss 0.0995616614818573\n",
            "Train step - Step 2890, Loss 0.09401190280914307\n",
            "Train step - Step 2900, Loss 0.09237315505743027\n",
            "Train step - Step 2910, Loss 0.09339506924152374\n",
            "Train epoch - Accuracy: 0.5415584415584416 Loss: 0.09321577552851144 Corrects: 3753\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.08922895044088364\n",
            "Train step - Step 2930, Loss 0.09169792383909225\n",
            "Train step - Step 2940, Loss 0.08825687319040298\n",
            "Train step - Step 2950, Loss 0.09228914231061935\n",
            "Train step - Step 2960, Loss 0.08830014616250992\n",
            "Train epoch - Accuracy: 0.5526695526695526 Loss: 0.09322876889167238 Corrects: 3830\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.09444893896579742\n",
            "Train step - Step 2980, Loss 0.09270517528057098\n",
            "Train step - Step 2990, Loss 0.09125449508428574\n",
            "Train step - Step 3000, Loss 0.09402626752853394\n",
            "Train step - Step 3010, Loss 0.09743861854076385\n",
            "Train step - Step 3020, Loss 0.08824288100004196\n",
            "Train epoch - Accuracy: 0.543001443001443 Loss: 0.09293593450585141 Corrects: 3763\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.08723610639572144\n",
            "Train step - Step 3040, Loss 0.09257420152425766\n",
            "Train step - Step 3050, Loss 0.08960682898759842\n",
            "Train step - Step 3060, Loss 0.09354548901319504\n",
            "Train step - Step 3070, Loss 0.09095194190740585\n",
            "Train epoch - Accuracy: 0.5431457431457432 Loss: 0.09211607575201541 Corrects: 3764\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.08957380056381226\n",
            "Train step - Step 3090, Loss 0.09344011545181274\n",
            "Train step - Step 3100, Loss 0.08739560097455978\n",
            "Train step - Step 3110, Loss 0.10008648782968521\n",
            "Train step - Step 3120, Loss 0.09030144661664963\n",
            "Train step - Step 3130, Loss 0.09074580669403076\n",
            "Train epoch - Accuracy: 0.5556998556998557 Loss: 0.09267354704987951 Corrects: 3851\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.08734912425279617\n",
            "Train step - Step 3150, Loss 0.0885373055934906\n",
            "Train step - Step 3160, Loss 0.09001864492893219\n",
            "Train step - Step 3170, Loss 0.09557676315307617\n",
            "Train step - Step 3180, Loss 0.09071671962738037\n",
            "Train epoch - Accuracy: 0.5418470418470418 Loss: 0.0925060353388346 Corrects: 3755\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.09464231878519058\n",
            "Train step - Step 3200, Loss 0.09572774171829224\n",
            "Train step - Step 3210, Loss 0.0915985032916069\n",
            "Train step - Step 3220, Loss 0.09168683737516403\n",
            "Train step - Step 3230, Loss 0.09162979573011398\n",
            "Train step - Step 3240, Loss 0.09659158438444138\n",
            "Train epoch - Accuracy: 0.5507936507936508 Loss: 0.09281958529099414 Corrects: 3817\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.08960920572280884\n",
            "Train step - Step 3260, Loss 0.08637744188308716\n",
            "Train step - Step 3270, Loss 0.09524865448474884\n",
            "Train step - Step 3280, Loss 0.09869902580976486\n",
            "Train step - Step 3290, Loss 0.09311310946941376\n",
            "Train epoch - Accuracy: 0.5542568542568542 Loss: 0.0922262026784568 Corrects: 3841\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.08904316276311874\n",
            "Train step - Step 3310, Loss 0.09020593017339706\n",
            "Train step - Step 3320, Loss 0.09071778506040573\n",
            "Train step - Step 3330, Loss 0.09596502780914307\n",
            "Train step - Step 3340, Loss 0.08906444162130356\n",
            "Train step - Step 3350, Loss 0.08740934729576111\n",
            "Train epoch - Accuracy: 0.5471861471861472 Loss: 0.0926636230549943 Corrects: 3792\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.09533378481864929\n",
            "Train step - Step 3370, Loss 0.09181796759366989\n",
            "Train step - Step 3380, Loss 0.09714767336845398\n",
            "Train step - Step 3390, Loss 0.08787932991981506\n",
            "Train step - Step 3400, Loss 0.09818834066390991\n",
            "Train epoch - Accuracy: 0.5477633477633478 Loss: 0.09254343082851489 Corrects: 3796\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.08641214668750763\n",
            "Train step - Step 3420, Loss 0.09167183190584183\n",
            "Train step - Step 3430, Loss 0.09157409518957138\n",
            "Train step - Step 3440, Loss 0.08931421488523483\n",
            "Train step - Step 3450, Loss 0.09531664103269577\n",
            "Train step - Step 3460, Loss 0.09456660598516464\n",
            "Train epoch - Accuracy: 0.5595959595959596 Loss: 0.09278860530811987 Corrects: 3878\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.09325321763753891\n",
            "Train step - Step 3480, Loss 0.09583693742752075\n",
            "Train step - Step 3490, Loss 0.09236392378807068\n",
            "Train step - Step 3500, Loss 0.08953793346881866\n",
            "Train step - Step 3510, Loss 0.09123405069112778\n",
            "Train epoch - Accuracy: 0.5525252525252525 Loss: 0.0920817723973489 Corrects: 3829\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.08786027878522873\n",
            "Train step - Step 3530, Loss 0.08874095976352692\n",
            "Train step - Step 3540, Loss 0.10168089717626572\n",
            "Train step - Step 3550, Loss 0.0891566053032875\n",
            "Train step - Step 3560, Loss 0.09224238246679306\n",
            "Train step - Step 3570, Loss 0.09911925345659256\n",
            "Train epoch - Accuracy: 0.5506493506493506 Loss: 0.09190083130398526 Corrects: 3816\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.08482461422681808\n",
            "Train step - Step 3590, Loss 0.09803100675344467\n",
            "Train step - Step 3600, Loss 0.09786628931760788\n",
            "Train step - Step 3610, Loss 0.0825764462351799\n",
            "Train step - Step 3620, Loss 0.08680054545402527\n",
            "Train epoch - Accuracy: 0.554978354978355 Loss: 0.09191817584921959 Corrects: 3846\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.08175988495349884\n",
            "Train step - Step 3640, Loss 0.09481433779001236\n",
            "Train step - Step 3650, Loss 0.09139343351125717\n",
            "Train step - Step 3660, Loss 0.10146579891443253\n",
            "Train step - Step 3670, Loss 0.09209644794464111\n",
            "Train step - Step 3680, Loss 0.09385232627391815\n",
            "Train epoch - Accuracy: 0.551948051948052 Loss: 0.09188630887096234 Corrects: 3825\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.09047018736600876\n",
            "Train step - Step 3700, Loss 0.09408275783061981\n",
            "Train step - Step 3710, Loss 0.08903385698795319\n",
            "Train step - Step 3720, Loss 0.09319835156202316\n",
            "Train step - Step 3730, Loss 0.09326326847076416\n",
            "Train epoch - Accuracy: 0.5581529581529582 Loss: 0.09187070429540128 Corrects: 3868\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.08610884100198746\n",
            "Train step - Step 3750, Loss 0.0929359719157219\n",
            "Train step - Step 3760, Loss 0.08689181506633759\n",
            "Train step - Step 3770, Loss 0.0958409458398819\n",
            "Train step - Step 3780, Loss 0.08985867351293564\n",
            "Train step - Step 3790, Loss 0.08801876753568649\n",
            "Train epoch - Accuracy: 0.5608946608946609 Loss: 0.09176041794803752 Corrects: 3887\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.09162988513708115\n",
            "Train step - Step 3810, Loss 0.09269370138645172\n",
            "Train step - Step 3820, Loss 0.09265007078647614\n",
            "Train step - Step 3830, Loss 0.0964227095246315\n",
            "Train step - Step 3840, Loss 0.0839262381196022\n",
            "Train epoch - Accuracy: 0.5565656565656566 Loss: 0.0917451428214537 Corrects: 3857\n",
            "Training finished in 394.04053378105164 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12]\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a91110>\n",
            "Constructing exemplars of class 31\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [27608, 43659, 45735, 18319, 10264, 39658, 14187, 42240, 20503, 35417, 9751, 39763, 40510, 577, 12848, 43388, 32942, 12992, 44187, 48403, 35975, 20730, 14539, 13151, 9753, 13459, 28480, 42087, 15529, 20777, 40691, 27670, 40252, 48626, 8073, 17216, 4710, 41840, 33089, 23690, 29212, 29539, 41495, 24566, 15771, 21101, 13961, 35620, 7567, 16115]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22442a9510>\n",
            "Constructing exemplars of class 11\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [4897, 49105, 20085, 15865, 7458, 38189, 367, 26462, 16452, 33575, 36056, 20769, 3, 9023, 25957, 3227, 1368, 48891, 7693, 28452, 35464, 30176, 21793, 42551, 16581, 5619, 36180, 2423, 41156, 36488, 12481, 27340, 26880, 10855, 7885, 24853, 15539, 29728, 40971, 33108, 48891, 2419, 39356, 13508, 23647, 11909, 39843, 13999, 44073, 45868]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224468fe90>\n",
            "Constructing exemplars of class 98\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [42690, 5299, 42097, 9619, 49448, 34236, 32663, 36700, 19014, 14675, 657, 45929, 32663, 7033, 39330, 12289, 38887, 31312, 11092, 44549, 40002, 13020, 9881, 7263, 35778, 22129, 30599, 33973, 34740, 47852, 18208, 41402, 48986, 5738, 48221, 11547, 2106, 22507, 3011, 40002, 41829, 26782, 29909, 39449, 18876, 32017, 24587, 32462, 41970, 15118]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22afb73a50>\n",
            "Constructing exemplars of class 77\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [13547, 43871, 37689, 9098, 35045, 5255, 8467, 8840, 41679, 27822, 29258, 37355, 31643, 992, 5699, 40491, 32844, 13791, 32869, 42729, 6332, 27473, 20773, 47167, 21701, 10396, 6656, 46407, 23121, 38440, 22896, 45253, 21941, 17033, 10835, 6500, 22465, 8461, 20893, 13263, 539, 21383, 47203, 18773, 48564, 30310, 46407, 22131, 28847, 36550]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2243f0aa50>\n",
            "Constructing exemplars of class 53\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [44630, 43029, 18438, 48992, 15773, 22135, 3825, 27872, 19337, 24716, 38892, 30518, 2708, 29794, 44055, 4447, 35266, 30053, 9417, 21665, 12652, 32479, 14688, 20633, 49992, 13245, 10821, 41619, 15483, 30519, 44868, 46646, 43519, 5933, 11352, 28831, 8198, 11550, 2379, 18106, 26054, 28185, 40277, 48159, 19243, 31218, 37017, 37739, 31087, 30519]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2243757550>\n",
            "Constructing exemplars of class 33\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [32055, 11467, 154, 37027, 19528, 22526, 47986, 43132, 28066, 38607, 32074, 47490, 49772, 1332, 26253, 15370, 7284, 36341, 18663, 22509, 6182, 5145, 25604, 45691, 7894, 19844, 834, 19528, 27107, 44156, 49835, 42756, 24006, 25604, 20376, 16691, 2284, 29588, 819, 8831, 44220, 21227, 36538, 47897, 11467, 49993, 30831, 4513, 5407, 33149]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a5af50>\n",
            "Constructing exemplars of class 96\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [15588, 16698, 21651, 36334, 30255, 15523, 25669, 45594, 30847, 25675, 21935, 31046, 45779, 13450, 30204, 20076, 3575, 25886, 8274, 47334, 9244, 22752, 47766, 16426, 20531, 35316, 45607, 10026, 10348, 19209, 17725, 41293, 38091, 34022, 5568, 27953, 12665, 14192, 35853, 5097, 26739, 49507, 29970, 39740, 37342, 46882, 999, 20758, 43076, 39951]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238218350>\n",
            "Constructing exemplars of class 92\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [11549, 29991, 42542, 39393, 31054, 30317, 48481, 47900, 48370, 8027, 34778, 17050, 9671, 2243, 41693, 18710, 23096, 8964, 42443, 12128, 4808, 40868, 3404, 36876, 6660, 27112, 3418, 27252, 27136, 8964, 26679, 29877, 16912, 15497, 31696, 8774, 34162, 24796, 25121, 14277, 1056, 38742, 34554, 14714, 25644, 5334, 2678, 24476, 47350, 38651]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232ee5210>\n",
            "Constructing exemplars of class 52\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [48718, 43144, 29331, 36704, 27974, 4661, 34015, 37200, 41662, 9971, 11295, 41163, 14629, 35930, 258, 10450, 33921, 37091, 4103, 41003, 3282, 39369, 45117, 13699, 47204, 6939, 7842, 4077, 20324, 41159, 15762, 20869, 26984, 37179, 36475, 48284, 2308, 34461, 39113, 13662, 41642, 29785, 3770, 25972, 34574, 38464, 9344, 44637, 47087, 36835]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2243757550>\n",
            "Constructing exemplars of class 12\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [4133, 22039, 16898, 1805, 1877, 5796, 30424, 38424, 21150, 46788, 29908, 40992, 10034, 43012, 13408, 13818, 1016, 40877, 42152, 38015, 6835, 11891, 11482, 16677, 34032, 18814, 18568, 12178, 24292, 47504, 12497, 19275, 36386, 49421, 2269, 11590, 1230, 8749, 30222, 37906, 41827, 6003, 49977, 26192, 27596, 4115, 39732, 7825, 24144, 20126]\n",
            "x train:  [-0.02059093 -0.06732447 -0.10034069 -0.09277993 -0.01150913 -0.11669035\n",
            " -0.12046364 -0.05698308 -0.07635212 -0.15259278 -0.19799119  0.04024148\n",
            " -0.239633   -0.21351177 -0.16089296 -0.15952861 -0.16811673 -0.1649122\n",
            " -0.03320993 -0.13229288 -0.16527365 -0.18694079 -0.2587738  -0.18745014\n",
            " -0.10967166 -0.08390739 -0.14917962 -0.07950906 -0.1550511  -0.20178099\n",
            " -0.25627798 -0.2381275  -0.17097144 -0.19202118 -0.19456273 -0.14712217\n",
            " -0.09356982 -0.27231842  0.06785555 -0.18505909]\n",
            "y_train:  [tensor([52]), tensor([35]), tensor([50]), tensor([97]), tensor([59]), tensor([3]), tensor([13]), tensor([53]), tensor([45]), tensor([77]), tensor([82]), tensor([95]), tensor([49]), tensor([10]), tensor([69]), tensor([34]), tensor([36]), tensor([24]), tensor([88]), tensor([10]), tensor([53]), tensor([95]), tensor([78]), tensor([92]), tensor([35]), tensor([94]), tensor([37]), tensor([31]), tensor([52]), tensor([33]), tensor([80]), tensor([33]), tensor([34]), tensor([34]), tensor([80]), tensor([24]), tensor([33]), tensor([58]), tensor([30]), tensor([57]), tensor([25]), tensor([82]), tensor([25]), tensor([37]), tensor([52]), tensor([3]), tensor([97]), tensor([12]), tensor([33]), tensor([13]), tensor([24]), tensor([78]), tensor([30]), tensor([59]), tensor([86]), tensor([52]), tensor([11]), tensor([13]), tensor([95]), tensor([49]), tensor([31]), tensor([80]), tensor([34]), tensor([52]), tensor([37]), tensor([24]), tensor([36]), tensor([25]), tensor([80]), tensor([36]), tensor([24]), tensor([33]), tensor([58]), tensor([81]), tensor([98]), tensor([72]), tensor([25]), tensor([13]), tensor([97]), tensor([94]), tensor([98]), tensor([52]), tensor([92]), tensor([94]), tensor([33]), tensor([60]), tensor([52]), tensor([33]), tensor([36]), tensor([31]), tensor([92]), tensor([72]), tensor([50]), tensor([30]), tensor([37]), tensor([36]), tensor([11]), tensor([95]), tensor([53]), tensor([37]), tensor([35]), tensor([13]), tensor([80]), tensor([72]), tensor([33]), tensor([57]), tensor([98]), tensor([36]), tensor([35]), tensor([97]), tensor([94]), tensor([52]), tensor([25]), tensor([34]), tensor([52]), tensor([24]), tensor([59]), tensor([82]), tensor([27]), tensor([97]), tensor([52]), tensor([30]), tensor([36]), tensor([59]), tensor([35]), tensor([57]), tensor([35]), tensor([58]), tensor([13]), tensor([33]), tensor([24]), tensor([57]), tensor([78]), tensor([92]), tensor([50]), tensor([98]), tensor([35]), tensor([97]), tensor([92]), tensor([33]), tensor([27]), tensor([78]), tensor([78]), tensor([78]), tensor([88]), tensor([49]), tensor([59]), tensor([13]), tensor([59]), tensor([72]), tensor([12]), tensor([82]), tensor([98]), tensor([24]), tensor([35]), tensor([78]), tensor([53]), tensor([88]), tensor([88]), tensor([35]), tensor([34]), tensor([77]), tensor([86]), tensor([3]), tensor([78]), tensor([59]), tensor([37]), tensor([24]), tensor([31]), tensor([86]), tensor([95]), tensor([69]), tensor([69]), tensor([37]), tensor([98]), tensor([45]), tensor([13]), tensor([98]), tensor([81]), tensor([52]), tensor([69]), tensor([24]), tensor([57]), tensor([77]), tensor([45]), tensor([60]), tensor([10]), tensor([52]), tensor([77]), tensor([35]), tensor([94]), tensor([36]), tensor([94]), tensor([53]), tensor([27]), tensor([77]), tensor([50]), tensor([37]), tensor([69]), tensor([31]), tensor([94]), tensor([37]), tensor([88]), tensor([77]), tensor([53]), tensor([50]), tensor([92]), tensor([50]), tensor([34]), tensor([96]), tensor([34]), tensor([52]), tensor([25]), tensor([58]), tensor([12]), tensor([30]), tensor([13]), tensor([81]), tensor([60]), tensor([53]), tensor([58]), tensor([25]), tensor([27]), tensor([59]), tensor([24]), tensor([98]), tensor([50]), tensor([33]), tensor([72]), tensor([35]), tensor([94]), tensor([94]), tensor([86]), tensor([96]), tensor([13]), tensor([36]), tensor([95]), tensor([72]), tensor([57]), tensor([34]), tensor([3]), tensor([36]), tensor([33]), tensor([35]), tensor([88]), tensor([10]), tensor([60]), tensor([97]), tensor([52]), tensor([57]), tensor([69]), tensor([33]), tensor([50]), tensor([37]), tensor([50]), tensor([24]), tensor([12]), tensor([95]), tensor([88]), tensor([31]), tensor([11]), tensor([95]), tensor([45]), tensor([25]), tensor([50]), tensor([80]), tensor([3]), tensor([27]), tensor([72]), tensor([94]), tensor([86]), tensor([50]), tensor([72]), tensor([3]), tensor([96]), tensor([25]), tensor([94]), tensor([27]), tensor([11]), tensor([34]), tensor([57]), tensor([57]), tensor([50]), tensor([45]), tensor([25]), tensor([36]), tensor([34]), tensor([88]), tensor([11]), tensor([3]), tensor([33]), tensor([60]), tensor([78]), tensor([30]), tensor([27]), tensor([10]), tensor([77]), tensor([30]), tensor([30]), tensor([31]), tensor([12]), tensor([11]), tensor([11]), tensor([58]), tensor([92]), tensor([35]), tensor([92]), tensor([49]), tensor([49]), tensor([12]), tensor([95]), tensor([13]), tensor([34]), tensor([60]), tensor([33]), tensor([77]), tensor([88]), tensor([69]), tensor([72]), tensor([34]), tensor([45]), tensor([81]), tensor([58]), tensor([58]), tensor([25]), tensor([92]), tensor([60]), tensor([58]), tensor([82]), tensor([27]), tensor([98]), tensor([37]), tensor([50]), tensor([80]), tensor([82]), tensor([25]), tensor([25]), tensor([68]), tensor([69]), tensor([96]), tensor([57]), tensor([78]), tensor([25]), tensor([33]), tensor([82]), tensor([34]), tensor([3]), tensor([49]), tensor([49]), tensor([78]), tensor([98]), tensor([95]), tensor([86]), tensor([31]), tensor([57]), tensor([45]), tensor([82]), tensor([11]), tensor([97]), tensor([58]), tensor([57]), tensor([50]), tensor([25]), tensor([88]), tensor([31]), tensor([53]), tensor([36]), tensor([94]), tensor([86]), tensor([27]), tensor([69]), tensor([77]), tensor([10]), tensor([31]), tensor([94]), tensor([86]), tensor([68]), tensor([3]), tensor([31]), tensor([12]), tensor([3]), tensor([53]), tensor([24]), tensor([92]), tensor([35]), tensor([95]), tensor([82]), tensor([24]), tensor([27]), tensor([59]), tensor([69]), tensor([35]), tensor([60]), tensor([98]), tensor([77]), tensor([50]), tensor([81]), tensor([98]), tensor([52]), tensor([77]), tensor([58]), tensor([96]), tensor([72]), tensor([11]), tensor([97]), tensor([59]), tensor([50]), tensor([10]), tensor([96]), tensor([25]), tensor([10]), tensor([58]), tensor([80]), tensor([25]), tensor([98]), tensor([58]), tensor([57]), tensor([37]), tensor([12]), tensor([57]), tensor([12]), tensor([77]), tensor([96]), tensor([80]), tensor([81]), tensor([30]), tensor([33]), tensor([96]), tensor([82]), tensor([82]), tensor([35]), tensor([49]), tensor([68]), tensor([78]), tensor([49]), tensor([36]), tensor([59]), tensor([37]), tensor([59]), tensor([49]), tensor([72]), tensor([58]), tensor([27]), tensor([33]), tensor([24]), tensor([88]), tensor([69]), tensor([60]), tensor([45]), tensor([80]), tensor([35]), tensor([72]), tensor([10]), tensor([35]), tensor([95]), tensor([59]), tensor([94]), tensor([96]), tensor([52]), tensor([49]), tensor([31]), tensor([27]), tensor([95]), tensor([97]), tensor([98]), tensor([31]), tensor([68]), tensor([11]), tensor([81]), tensor([30]), tensor([53]), tensor([11]), tensor([96]), tensor([10]), tensor([34]), tensor([35]), tensor([68]), tensor([33]), tensor([49]), tensor([27]), tensor([53]), tensor([27]), tensor([98]), tensor([94]), tensor([98]), tensor([52]), tensor([3]), tensor([24]), tensor([45]), tensor([72]), tensor([31]), tensor([95]), tensor([52]), tensor([69]), tensor([53]), tensor([60]), tensor([30]), tensor([58]), tensor([59]), tensor([59]), tensor([80]), tensor([96]), tensor([34]), tensor([33]), tensor([88]), tensor([78]), tensor([52]), tensor([80]), tensor([13]), tensor([13]), tensor([81]), tensor([36]), tensor([92]), tensor([33]), tensor([34]), tensor([94]), tensor([24]), tensor([88]), tensor([68]), tensor([36]), tensor([80]), tensor([60]), tensor([13]), tensor([52]), tensor([88]), tensor([69]), tensor([94]), tensor([34]), tensor([82]), tensor([69]), tensor([53]), tensor([35]), tensor([37]), tensor([35]), tensor([72]), tensor([95]), tensor([27]), tensor([81]), tensor([30]), tensor([94]), tensor([53]), tensor([94]), tensor([77]), tensor([50]), tensor([25]), tensor([69]), tensor([31]), tensor([95]), tensor([95]), tensor([97]), tensor([53]), tensor([10]), tensor([94]), tensor([49]), tensor([34]), tensor([37]), tensor([60]), tensor([92]), tensor([98]), tensor([86]), tensor([30]), tensor([37]), tensor([12]), tensor([97]), tensor([27]), tensor([78]), tensor([30]), tensor([50]), tensor([13]), tensor([80]), tensor([96]), tensor([27]), tensor([96]), tensor([35]), tensor([52]), tensor([34]), tensor([96]), tensor([97]), tensor([3]), tensor([10]), tensor([58]), tensor([13]), tensor([53]), tensor([78]), tensor([81]), tensor([68]), tensor([12]), tensor([10]), tensor([33]), tensor([98]), tensor([53]), tensor([53]), tensor([10]), tensor([92]), tensor([88]), tensor([11]), tensor([50]), tensor([3]), tensor([33]), tensor([3]), tensor([45]), tensor([10]), tensor([10]), tensor([82]), tensor([37]), tensor([30]), tensor([30]), tensor([68]), tensor([68]), tensor([58]), tensor([78]), tensor([50]), tensor([13]), tensor([53]), tensor([59]), tensor([59]), tensor([98]), tensor([57]), tensor([68]), tensor([10]), tensor([31]), tensor([52]), tensor([10]), tensor([53]), tensor([3]), tensor([34]), tensor([92]), tensor([68]), tensor([68]), tensor([97]), tensor([72]), tensor([3]), tensor([97]), tensor([72]), tensor([59]), tensor([80]), tensor([86]), tensor([31]), tensor([59]), tensor([53]), tensor([78]), tensor([59]), tensor([30]), tensor([3]), tensor([36]), tensor([57]), tensor([77]), tensor([52]), tensor([34]), tensor([57]), tensor([25]), tensor([59]), tensor([36]), tensor([3]), tensor([34]), tensor([86]), tensor([13]), tensor([36]), tensor([57]), tensor([31]), tensor([31]), tensor([30]), tensor([52]), tensor([59]), tensor([98]), tensor([12]), tensor([98]), tensor([3]), tensor([80]), tensor([77]), tensor([37]), tensor([96]), tensor([69]), tensor([27]), tensor([58]), tensor([95]), tensor([3]), tensor([94]), tensor([52]), tensor([25]), tensor([88]), tensor([11]), tensor([11]), tensor([96]), tensor([37]), tensor([12]), tensor([11]), tensor([33]), tensor([30]), tensor([72]), tensor([92]), tensor([92]), tensor([50]), tensor([78]), tensor([13]), tensor([60]), tensor([57]), tensor([13]), tensor([59]), tensor([98]), tensor([80]), tensor([80]), tensor([94]), tensor([78]), tensor([97]), tensor([59]), tensor([34]), tensor([31]), tensor([80]), tensor([35]), tensor([45]), tensor([45]), tensor([57]), tensor([80]), tensor([72]), tensor([96]), tensor([52]), tensor([60]), tensor([92]), tensor([34]), tensor([80]), tensor([58]), tensor([82]), tensor([77]), tensor([68]), tensor([37]), tensor([69]), tensor([12]), tensor([57]), tensor([69]), tensor([81]), tensor([36]), tensor([49]), tensor([68]), tensor([77]), tensor([68]), tensor([10]), tensor([37]), tensor([30]), tensor([77]), tensor([82]), tensor([69]), tensor([12]), tensor([36]), tensor([3]), tensor([88]), tensor([3]), tensor([77]), tensor([86]), tensor([68]), tensor([11]), tensor([88]), tensor([52]), tensor([35]), tensor([96]), tensor([78]), tensor([80]), tensor([98]), tensor([45]), tensor([36]), tensor([58]), tensor([80]), tensor([12]), tensor([33]), tensor([88]), tensor([77]), tensor([81]), tensor([31]), tensor([45]), tensor([59]), tensor([88]), tensor([49]), tensor([96]), tensor([82]), tensor([45]), tensor([30]), tensor([27]), tensor([34]), tensor([88]), tensor([31]), tensor([68]), tensor([88]), tensor([59]), tensor([33]), tensor([58]), tensor([36]), tensor([31]), tensor([31]), tensor([96]), tensor([92]), tensor([58]), tensor([82]), tensor([24]), tensor([13]), tensor([81]), tensor([94]), tensor([10]), tensor([98]), tensor([96]), tensor([59]), tensor([31]), tensor([45]), tensor([11]), tensor([95]), tensor([60]), tensor([81]), tensor([31]), tensor([52]), tensor([33]), tensor([96]), tensor([36]), tensor([95]), tensor([45]), tensor([24]), tensor([88]), tensor([27]), tensor([60]), tensor([24]), tensor([24]), tensor([25]), tensor([36]), tensor([36]), tensor([78]), tensor([92]), tensor([98]), tensor([94]), tensor([80]), tensor([92]), tensor([92]), tensor([59]), tensor([24]), tensor([52]), tensor([49]), tensor([69]), tensor([24]), tensor([10]), tensor([97]), tensor([82]), tensor([97]), tensor([96]), tensor([25]), tensor([34]), tensor([34]), tensor([68]), tensor([80]), tensor([97]), tensor([95]), tensor([82]), tensor([77]), tensor([92]), tensor([45]), tensor([24]), tensor([37]), tensor([50]), tensor([49]), tensor([57]), tensor([3]), tensor([80]), tensor([98]), tensor([77]), tensor([96]), tensor([53]), tensor([45]), tensor([88]), tensor([57]), tensor([60]), tensor([53]), tensor([50]), tensor([35]), tensor([68]), tensor([81]), tensor([92]), tensor([96]), tensor([68]), tensor([35]), tensor([81]), tensor([69]), tensor([45]), tensor([33]), tensor([96]), tensor([96]), tensor([12]), tensor([50]), tensor([10]), tensor([12]), tensor([92]), tensor([57]), tensor([57]), tensor([92]), tensor([78]), tensor([86]), tensor([27]), tensor([45]), tensor([81]), tensor([24]), tensor([94]), tensor([77]), tensor([13]), tensor([94]), tensor([69]), tensor([13]), tensor([80]), tensor([60]), tensor([96]), tensor([50]), tensor([10]), tensor([57]), tensor([50]), tensor([3]), tensor([49]), tensor([82]), tensor([82]), tensor([60]), tensor([60]), tensor([52]), tensor([31]), tensor([57]), tensor([59]), tensor([96]), tensor([68]), tensor([25]), tensor([97]), tensor([58]), tensor([10]), tensor([30]), tensor([69]), tensor([36]), tensor([60]), tensor([97]), tensor([12]), tensor([72]), tensor([34]), tensor([86]), tensor([13]), tensor([81]), tensor([78]), tensor([72]), tensor([30]), tensor([95]), tensor([53]), tensor([25]), tensor([12]), tensor([3]), tensor([49]), tensor([77]), tensor([96]), tensor([31]), tensor([81]), tensor([24]), tensor([78]), tensor([80]), tensor([95]), tensor([97]), tensor([95]), tensor([13]), tensor([94]), tensor([34]), tensor([31]), tensor([69]), tensor([77]), tensor([97]), tensor([53]), tensor([11]), tensor([50]), tensor([11]), tensor([78]), tensor([82]), tensor([92]), tensor([52]), tensor([97]), tensor([25]), tensor([12]), tensor([97]), tensor([98]), tensor([11]), tensor([3]), tensor([97]), tensor([88]), tensor([35]), tensor([37]), tensor([78]), tensor([95]), tensor([92]), tensor([37]), tensor([37]), tensor([31]), tensor([92]), tensor([98]), tensor([77]), tensor([24]), tensor([3]), tensor([12]), tensor([49]), tensor([31]), tensor([68]), tensor([37]), tensor([80]), tensor([33]), tensor([3]), tensor([49]), tensor([50]), tensor([92]), tensor([27]), tensor([69]), tensor([81]), tensor([24]), tensor([88]), tensor([27]), tensor([49]), tensor([45]), tensor([49]), tensor([98]), tensor([81]), tensor([13]), tensor([78]), tensor([25]), tensor([97]), tensor([97]), tensor([12]), tensor([69]), tensor([96]), tensor([3]), tensor([33]), tensor([95]), tensor([57]), tensor([81]), tensor([97]), tensor([86]), tensor([24]), tensor([60]), tensor([13]), tensor([77]), tensor([59]), tensor([59]), tensor([13]), tensor([10]), tensor([25]), tensor([57]), tensor([36]), tensor([82]), tensor([86]), tensor([86]), tensor([53]), tensor([68]), tensor([77]), tensor([58]), tensor([25]), tensor([80]), tensor([95]), tensor([82]), tensor([81]), tensor([96]), tensor([35]), tensor([27]), tensor([52]), tensor([53]), tensor([13]), tensor([53]), tensor([10]), tensor([27]), tensor([77]), tensor([95]), tensor([45]), tensor([58]), tensor([68]), tensor([11]), tensor([3]), tensor([53]), tensor([33]), tensor([72]), tensor([92]), tensor([49]), tensor([35]), tensor([50]), tensor([60]), tensor([95]), tensor([69]), tensor([45]), tensor([35]), tensor([88]), tensor([10]), tensor([82]), tensor([50]), tensor([94]), tensor([59]), tensor([94]), tensor([60]), tensor([81]), tensor([78]), tensor([53]), tensor([31]), tensor([27]), tensor([27]), tensor([69]), tensor([78]), tensor([92]), tensor([60]), tensor([3]), tensor([78]), tensor([81]), tensor([13]), tensor([86]), tensor([34]), tensor([49]), tensor([25]), tensor([12]), tensor([12]), tensor([33]), tensor([81]), tensor([33]), tensor([53]), tensor([57]), tensor([86]), tensor([12]), tensor([88]), tensor([11]), tensor([69]), tensor([86]), tensor([80]), tensor([50]), tensor([81]), tensor([30]), tensor([78]), tensor([82]), tensor([35]), tensor([98]), tensor([11]), tensor([27]), tensor([77]), tensor([53]), tensor([34]), tensor([96]), tensor([92]), tensor([69]), tensor([30]), tensor([60]), tensor([92]), tensor([49]), tensor([88]), tensor([37]), tensor([97]), tensor([27]), tensor([45]), tensor([53]), tensor([88]), tensor([81]), tensor([11]), tensor([27]), tensor([59]), tensor([11]), tensor([98]), tensor([81]), tensor([11]), tensor([45]), tensor([58]), tensor([86]), tensor([82]), tensor([11]), tensor([77]), tensor([30]), tensor([25]), tensor([72]), tensor([36]), tensor([81]), tensor([88]), tensor([68]), tensor([69]), tensor([24]), tensor([11]), tensor([68]), tensor([77]), tensor([45]), tensor([50]), tensor([92]), tensor([35]), tensor([11]), tensor([31]), tensor([36]), tensor([27]), tensor([50]), tensor([13]), tensor([60]), tensor([98]), tensor([37]), tensor([45]), tensor([95]), tensor([72]), tensor([33]), tensor([60]), tensor([49]), tensor([69]), tensor([49]), tensor([31]), tensor([37]), tensor([60]), tensor([78]), tensor([30]), tensor([33]), tensor([96]), tensor([95]), tensor([37]), tensor([34]), tensor([82]), tensor([45]), tensor([95]), tensor([3]), tensor([53]), tensor([68]), tensor([68]), tensor([94]), tensor([3]), tensor([88]), tensor([24]), tensor([24]), tensor([24]), tensor([57]), tensor([92]), tensor([25]), tensor([95]), tensor([72]), tensor([34]), tensor([80]), tensor([82]), tensor([24]), tensor([68]), tensor([45]), tensor([78]), tensor([57]), tensor([81]), tensor([98]), tensor([11]), tensor([81]), tensor([92]), tensor([30]), tensor([10]), tensor([58]), tensor([13]), tensor([52]), tensor([77]), tensor([3]), tensor([72]), tensor([30]), tensor([31]), tensor([35]), tensor([52]), tensor([45]), tensor([97]), tensor([86]), tensor([60]), tensor([31]), tensor([98]), tensor([52]), tensor([77]), tensor([80]), tensor([53]), tensor([3]), tensor([97]), tensor([86]), tensor([97]), tensor([58]), tensor([31]), tensor([68]), tensor([11]), tensor([77]), tensor([13]), tensor([36]), tensor([96]), tensor([80]), tensor([31]), tensor([36]), tensor([96]), tensor([58]), tensor([33]), tensor([58]), tensor([30]), tensor([92]), tensor([53]), tensor([35]), tensor([58]), tensor([12]), tensor([97]), tensor([59]), tensor([36]), tensor([97]), tensor([97]), tensor([96]), tensor([72]), tensor([50]), tensor([82]), tensor([53]), tensor([37]), tensor([13]), tensor([52]), tensor([27]), tensor([95]), tensor([82]), tensor([97]), tensor([69]), tensor([49]), tensor([34]), tensor([30]), tensor([95]), tensor([94]), tensor([24]), tensor([57]), tensor([58]), tensor([68]), tensor([72]), tensor([45]), tensor([25]), tensor([69]), tensor([37]), tensor([77]), tensor([97]), tensor([31]), tensor([37]), tensor([94]), tensor([24]), tensor([36]), tensor([60]), tensor([36]), tensor([82]), tensor([37]), tensor([98]), tensor([49]), tensor([27]), tensor([80]), tensor([86]), tensor([86]), tensor([50]), tensor([81]), tensor([86]), tensor([72]), tensor([60]), tensor([30]), tensor([12]), tensor([77]), tensor([86]), tensor([88]), tensor([59]), tensor([77]), tensor([98]), tensor([82]), tensor([88]), tensor([77]), tensor([37]), tensor([96]), tensor([72]), tensor([82]), tensor([59]), tensor([10]), tensor([37]), tensor([35]), tensor([12]), tensor([59]), tensor([10]), tensor([49]), tensor([92]), tensor([49]), tensor([49]), tensor([35]), tensor([27]), tensor([92]), tensor([80]), tensor([37]), tensor([97]), tensor([94]), tensor([81]), tensor([12]), tensor([58]), tensor([82]), tensor([86]), tensor([33]), tensor([27]), tensor([10]), tensor([81]), tensor([92]), tensor([11]), tensor([77]), tensor([3]), tensor([98]), tensor([53]), tensor([13]), tensor([3]), tensor([60]), tensor([98]), tensor([57]), tensor([80]), tensor([98]), tensor([78]), tensor([88]), tensor([78]), tensor([68]), tensor([3]), tensor([95]), tensor([97]), tensor([82]), tensor([72]), tensor([86]), tensor([88]), tensor([78]), tensor([53]), tensor([24]), tensor([72]), tensor([72]), tensor([35]), tensor([95]), tensor([94]), tensor([81]), tensor([36]), tensor([88]), tensor([60]), tensor([53]), tensor([35]), tensor([80]), tensor([69]), tensor([25]), tensor([34]), tensor([82]), tensor([45]), tensor([78]), tensor([3]), tensor([13]), tensor([12]), tensor([86]), tensor([12]), tensor([50]), tensor([95]), tensor([97]), tensor([45]), tensor([86]), tensor([53]), tensor([81]), tensor([80]), tensor([77]), tensor([10]), tensor([50]), tensor([94]), tensor([34]), tensor([52]), tensor([13]), tensor([94]), tensor([81]), tensor([86]), tensor([49]), tensor([24]), tensor([12]), tensor([10]), tensor([10]), tensor([12]), tensor([33]), tensor([98]), tensor([24]), tensor([36]), tensor([80]), tensor([69]), tensor([34]), tensor([96]), tensor([30]), tensor([36]), tensor([88]), tensor([33]), tensor([34]), tensor([11]), tensor([25]), tensor([35]), tensor([72]), tensor([58]), tensor([82]), tensor([95]), tensor([60]), tensor([80]), tensor([58]), tensor([72]), tensor([52]), tensor([92]), tensor([30]), tensor([30]), tensor([86]), tensor([10]), tensor([82]), tensor([12]), tensor([27]), tensor([49]), tensor([13]), tensor([49]), tensor([10]), tensor([86]), tensor([57]), tensor([95]), tensor([50]), tensor([37]), tensor([31]), tensor([37]), tensor([52]), tensor([34]), tensor([27]), tensor([3]), tensor([96]), tensor([59]), tensor([25]), tensor([58]), tensor([72]), tensor([3]), tensor([45]), tensor([50]), tensor([45]), tensor([11]), tensor([59]), tensor([60]), tensor([50]), tensor([50]), tensor([33]), tensor([27]), tensor([53]), tensor([97]), tensor([88]), tensor([37]), tensor([81]), tensor([53]), tensor([36]), tensor([10]), tensor([57]), tensor([12]), tensor([33]), tensor([52]), tensor([27]), tensor([97]), tensor([81]), tensor([68]), tensor([36]), tensor([95]), tensor([82]), tensor([77]), tensor([82]), tensor([24]), tensor([11]), tensor([24]), tensor([96]), tensor([25]), tensor([60]), tensor([27]), tensor([95]), tensor([69]), tensor([35]), tensor([94]), tensor([77]), tensor([24]), tensor([24]), tensor([10]), tensor([58]), tensor([30]), tensor([88]), tensor([80]), tensor([31]), tensor([94]), tensor([30]), tensor([24]), tensor([60]), tensor([49]), tensor([78]), tensor([72]), tensor([94]), tensor([98]), tensor([13]), tensor([68]), tensor([97]), tensor([13]), tensor([30]), tensor([98]), tensor([82]), tensor([53]), tensor([12]), tensor([27]), tensor([52]), tensor([50]), tensor([3]), tensor([57]), tensor([30]), tensor([31]), tensor([81]), tensor([57]), tensor([94]), tensor([57]), tensor([59]), tensor([88]), tensor([31]), tensor([49]), tensor([27]), tensor([68]), tensor([60]), tensor([50]), tensor([88]), tensor([92]), tensor([24]), tensor([36]), tensor([10]), tensor([57]), tensor([53]), tensor([80]), tensor([68]), tensor([25]), tensor([13]), tensor([11]), tensor([80]), tensor([25]), tensor([96]), tensor([96]), tensor([69]), tensor([69]), tensor([52]), tensor([59]), tensor([13]), tensor([94]), tensor([35]), tensor([82]), tensor([78]), tensor([78]), tensor([12]), tensor([60]), tensor([78]), tensor([49]), tensor([59]), tensor([3]), tensor([88]), tensor([95]), tensor([60]), tensor([49]), tensor([13]), tensor([30]), tensor([60]), tensor([27]), tensor([35]), tensor([31]), tensor([30]), tensor([30]), tensor([88]), tensor([37]), tensor([27]), tensor([58]), tensor([30]), tensor([69]), tensor([10]), tensor([82]), tensor([58]), tensor([34]), tensor([36]), tensor([37]), tensor([49]), tensor([36]), tensor([11]), tensor([11]), tensor([37]), tensor([98]), tensor([49]), tensor([25]), tensor([36]), tensor([57]), tensor([80]), tensor([86]), tensor([45]), tensor([35]), tensor([34]), tensor([37]), tensor([78]), tensor([25]), tensor([31]), tensor([58]), tensor([88]), tensor([12]), tensor([81]), tensor([78]), tensor([53]), tensor([59]), tensor([13]), tensor([10]), tensor([57]), tensor([72]), tensor([31]), tensor([86]), tensor([49]), tensor([60]), tensor([49]), tensor([27]), tensor([45]), tensor([3]), tensor([45]), tensor([69]), tensor([81]), tensor([37]), tensor([69]), tensor([96]), tensor([97]), tensor([49]), tensor([58]), tensor([58]), tensor([25]), tensor([68]), tensor([45]), tensor([72]), tensor([50]), tensor([11]), tensor([35]), tensor([58]), tensor([45]), tensor([49]), tensor([58]), tensor([50]), tensor([86]), tensor([36]), tensor([58]), tensor([94]), tensor([13]), tensor([94]), tensor([24]), tensor([33]), tensor([24]), tensor([36]), tensor([13]), tensor([52]), tensor([72]), tensor([94]), tensor([86]), tensor([59]), tensor([69]), tensor([30]), tensor([13]), tensor([86]), tensor([57]), tensor([25]), tensor([72]), tensor([94]), tensor([98]), tensor([57]), tensor([57]), tensor([78]), tensor([45]), tensor([69]), tensor([31]), tensor([50]), tensor([31]), tensor([78]), tensor([81]), tensor([92]), tensor([57]), tensor([72]), tensor([36]), tensor([24]), tensor([86]), tensor([86]), tensor([82]), tensor([95]), tensor([77]), tensor([92]), tensor([3]), tensor([36]), tensor([45]), tensor([25]), tensor([30]), tensor([45]), tensor([10]), tensor([52]), tensor([10]), tensor([34]), tensor([25]), tensor([88]), tensor([25]), tensor([3]), tensor([78]), tensor([69]), tensor([3]), tensor([59]), tensor([92]), tensor([11]), tensor([77]), tensor([80]), tensor([68]), tensor([86]), tensor([86]), tensor([68]), tensor([30]), tensor([78]), tensor([68]), tensor([12]), tensor([88]), tensor([96]), tensor([11]), tensor([81]), tensor([12]), tensor([78]), tensor([69]), tensor([49]), tensor([58]), tensor([3]), tensor([33]), tensor([68]), tensor([82]), tensor([59]), tensor([86]), tensor([68]), tensor([53]), tensor([94]), tensor([10]), tensor([95]), tensor([82]), tensor([31]), tensor([12]), tensor([25]), tensor([34]), tensor([72]), tensor([33]), tensor([10]), tensor([88]), tensor([95]), tensor([88]), tensor([68]), tensor([57]), tensor([12]), tensor([11]), tensor([34]), tensor([12]), tensor([69]), tensor([50]), tensor([11]), tensor([11]), tensor([72]), tensor([98]), tensor([25]), tensor([37]), tensor([80]), tensor([36]), tensor([69]), tensor([33]), tensor([53]), tensor([45]), tensor([33]), tensor([60]), tensor([72]), tensor([10]), tensor([12]), tensor([24]), tensor([92]), tensor([58]), tensor([60]), tensor([49]), tensor([34]), tensor([35]), tensor([24]), tensor([52]), tensor([3]), tensor([34]), tensor([95]), tensor([95]), tensor([25]), tensor([37]), tensor([52]), tensor([50]), tensor([30]), tensor([11]), tensor([12]), tensor([95]), tensor([94]), tensor([82]), tensor([97]), tensor([97]), tensor([25]), tensor([86]), tensor([10]), tensor([35]), tensor([11]), tensor([60]), tensor([81]), tensor([96]), tensor([45]), tensor([33]), tensor([72]), tensor([92]), tensor([72]), tensor([37]), tensor([11]), tensor([45]), tensor([30]), tensor([86]), tensor([45]), tensor([60]), tensor([59]), tensor([60]), tensor([53]), tensor([11]), tensor([59]), tensor([77]), tensor([13]), tensor([11]), tensor([86]), tensor([27]), tensor([81]), tensor([97]), tensor([36]), tensor([68]), tensor([49]), tensor([3]), tensor([30]), tensor([52]), tensor([27]), tensor([68]), tensor([95]), tensor([72]), tensor([92]), tensor([49]), tensor([35]), tensor([31]), tensor([97]), tensor([12]), tensor([68]), tensor([25]), tensor([98]), tensor([96]), tensor([10]), tensor([37]), tensor([60]), tensor([58]), tensor([77]), tensor([33]), tensor([77]), tensor([33]), tensor([50]), tensor([11]), tensor([77]), tensor([69]), tensor([34]), tensor([57]), tensor([94]), tensor([68]), tensor([80]), tensor([30]), tensor([60]), tensor([78]), tensor([97]), tensor([72]), tensor([81]), tensor([96]), tensor([82]), tensor([12]), tensor([58]), tensor([94]), tensor([96]), tensor([13]), tensor([37]), tensor([52]), tensor([86]), tensor([35]), tensor([27]), tensor([92]), tensor([81]), tensor([88]), tensor([98]), tensor([27]), tensor([45]), tensor([94]), tensor([12]), tensor([78]), tensor([68]), tensor([96]), tensor([80]), tensor([52]), tensor([80]), tensor([53]), tensor([10]), tensor([68]), tensor([57]), tensor([10]), tensor([34]), tensor([57]), tensor([35]), tensor([36]), tensor([58]), tensor([82]), tensor([77]), tensor([72]), tensor([50]), tensor([98]), tensor([86]), tensor([72]), tensor([59]), tensor([59]), tensor([68]), tensor([33]), tensor([45]), tensor([35]), tensor([34]), tensor([98]), tensor([13]), tensor([98]), tensor([52]), tensor([92]), tensor([86]), tensor([81]), tensor([12]), tensor([69]), tensor([27])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.62 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.621\n",
            "TEST ALL:  0.60725\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  5000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 92, 57, 53, 49, 45, 37, 33, 25, 21, 13, 96, 88, 91, 80, 72, 68, 60, 52, 36, 32, 24, 16, 12, 65, 69, 77, 81, 67, 59, 35, 31, 27, 11, 7, 3, 98, 94, 86, 82, 78, 62, 58, 50, 46, 34, 30, 10, 97, 0]\n",
            "TRAIN_SET CLASSES:  [91, 67, 7, 62, 46, 65, 21, 32, 16, 0]\n",
            "VALIDATION CLASSES:  [62, 46, 32, 91, 21, 16, 7, 67, 65, 0]\n",
            "GROUP:  5\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [91, 67, 7, 62, 46, 65, 21, 32, 16, 0]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  50\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.2546118497848511\n",
            "Train step - Step 10, Loss 0.13615931570529938\n",
            "Train step - Step 20, Loss 0.13159416615962982\n",
            "Train step - Step 30, Loss 0.13171856105327606\n",
            "Train step - Step 40, Loss 0.12001270055770874\n",
            "Train step - Step 50, Loss 0.11502552032470703\n",
            "Train epoch - Accuracy: 0.19870503597122302 Loss: 0.1392400737632093 Corrects: 1381\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12060771882534027\n",
            "Train step - Step 70, Loss 0.11152762919664383\n",
            "Train step - Step 80, Loss 0.115134596824646\n",
            "Train step - Step 90, Loss 0.12204354256391525\n",
            "Train step - Step 100, Loss 0.11738988012075424\n",
            "Train epoch - Accuracy: 0.22172661870503596 Loss: 0.11667206875283083 Corrects: 1541\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11725146323442459\n",
            "Train step - Step 120, Loss 0.11701810359954834\n",
            "Train step - Step 130, Loss 0.10807836055755615\n",
            "Train step - Step 140, Loss 0.11482053250074387\n",
            "Train step - Step 150, Loss 0.10766826570034027\n",
            "Train step - Step 160, Loss 0.11434187740087509\n",
            "Train epoch - Accuracy: 0.24388489208633093 Loss: 0.11396793855608796 Corrects: 1695\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.1137232780456543\n",
            "Train step - Step 180, Loss 0.1141018271446228\n",
            "Train step - Step 190, Loss 0.11184515058994293\n",
            "Train step - Step 200, Loss 0.11051492393016815\n",
            "Train step - Step 210, Loss 0.11439350992441177\n",
            "Train epoch - Accuracy: 0.2589928057553957 Loss: 0.11304383812619628 Corrects: 1800\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11776764690876007\n",
            "Train step - Step 230, Loss 0.1168118268251419\n",
            "Train step - Step 240, Loss 0.11128567159175873\n",
            "Train step - Step 250, Loss 0.10771572589874268\n",
            "Train step - Step 260, Loss 0.11209642142057419\n",
            "Train step - Step 270, Loss 0.1140456572175026\n",
            "Train epoch - Accuracy: 0.2745323741007194 Loss: 0.11177246631478234 Corrects: 1908\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.11270158737897873\n",
            "Train step - Step 290, Loss 0.10956428200006485\n",
            "Train step - Step 300, Loss 0.10885967314243317\n",
            "Train step - Step 310, Loss 0.11358761787414551\n",
            "Train step - Step 320, Loss 0.11304683238267899\n",
            "Train epoch - Accuracy: 0.2887769784172662 Loss: 0.11135379072144735 Corrects: 2007\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11051391065120697\n",
            "Train step - Step 340, Loss 0.11133944243192673\n",
            "Train step - Step 350, Loss 0.11296160519123077\n",
            "Train step - Step 360, Loss 0.10772325098514557\n",
            "Train step - Step 370, Loss 0.10942546278238297\n",
            "Train step - Step 380, Loss 0.10419360548257828\n",
            "Train epoch - Accuracy: 0.2961151079136691 Loss: 0.11075633823013992 Corrects: 2058\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.1064787432551384\n",
            "Train step - Step 400, Loss 0.11323632299900055\n",
            "Train step - Step 410, Loss 0.11644540727138519\n",
            "Train step - Step 420, Loss 0.11006111651659012\n",
            "Train step - Step 430, Loss 0.10916438698768616\n",
            "Train epoch - Accuracy: 0.3097841726618705 Loss: 0.11011229398653662 Corrects: 2153\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.11058427393436432\n",
            "Train step - Step 450, Loss 0.11143773794174194\n",
            "Train step - Step 460, Loss 0.11020062118768692\n",
            "Train step - Step 470, Loss 0.10667828470468521\n",
            "Train step - Step 480, Loss 0.10353843122720718\n",
            "Train step - Step 490, Loss 0.1102447658777237\n",
            "Train epoch - Accuracy: 0.3130935251798561 Loss: 0.10971390797508707 Corrects: 2176\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10714899003505707\n",
            "Train step - Step 510, Loss 0.11575951427221298\n",
            "Train step - Step 520, Loss 0.10613996535539627\n",
            "Train step - Step 530, Loss 0.11239919066429138\n",
            "Train step - Step 540, Loss 0.11286856234073639\n",
            "Train epoch - Accuracy: 0.3194244604316547 Loss: 0.10918653756165676 Corrects: 2220\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.11135975271463394\n",
            "Train step - Step 560, Loss 0.10916551202535629\n",
            "Train step - Step 570, Loss 0.11007167398929596\n",
            "Train step - Step 580, Loss 0.11253722757101059\n",
            "Train step - Step 590, Loss 0.12087110430002213\n",
            "Train step - Step 600, Loss 0.10854323953390121\n",
            "Train epoch - Accuracy: 0.3368345323741007 Loss: 0.10904771880065794 Corrects: 2341\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.1055738627910614\n",
            "Train step - Step 620, Loss 0.10858270525932312\n",
            "Train step - Step 630, Loss 0.1036655530333519\n",
            "Train step - Step 640, Loss 0.10593398660421371\n",
            "Train step - Step 650, Loss 0.10501333326101303\n",
            "Train epoch - Accuracy: 0.3358273381294964 Loss: 0.10850969069938865 Corrects: 2334\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10211625695228577\n",
            "Train step - Step 670, Loss 0.10245218127965927\n",
            "Train step - Step 680, Loss 0.11370555311441422\n",
            "Train step - Step 690, Loss 0.10928510129451752\n",
            "Train step - Step 700, Loss 0.10459409654140472\n",
            "Train step - Step 710, Loss 0.10210712254047394\n",
            "Train epoch - Accuracy: 0.34043165467625897 Loss: 0.10825281084012642 Corrects: 2366\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10500278323888779\n",
            "Train step - Step 730, Loss 0.10729211568832397\n",
            "Train step - Step 740, Loss 0.10107851028442383\n",
            "Train step - Step 750, Loss 0.10491810739040375\n",
            "Train step - Step 760, Loss 0.10246700793504715\n",
            "Train epoch - Accuracy: 0.35827338129496406 Loss: 0.10836393890192182 Corrects: 2490\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10118608176708221\n",
            "Train step - Step 780, Loss 0.10424835234880447\n",
            "Train step - Step 790, Loss 0.10775035619735718\n",
            "Train step - Step 800, Loss 0.10482561588287354\n",
            "Train step - Step 810, Loss 0.1059117317199707\n",
            "Train step - Step 820, Loss 0.10804218053817749\n",
            "Train epoch - Accuracy: 0.358705035971223 Loss: 0.10800690013513291 Corrects: 2493\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.10545768588781357\n",
            "Train step - Step 840, Loss 0.10348966717720032\n",
            "Train step - Step 850, Loss 0.11306391656398773\n",
            "Train step - Step 860, Loss 0.10653258860111237\n",
            "Train step - Step 870, Loss 0.1034652590751648\n",
            "Train epoch - Accuracy: 0.36935251798561153 Loss: 0.10779346287250519 Corrects: 2567\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.1126464307308197\n",
            "Train step - Step 890, Loss 0.10932713001966476\n",
            "Train step - Step 900, Loss 0.09923704713582993\n",
            "Train step - Step 910, Loss 0.11369603872299194\n",
            "Train step - Step 920, Loss 0.10522542893886566\n",
            "Train step - Step 930, Loss 0.10675191879272461\n",
            "Train epoch - Accuracy: 0.377410071942446 Loss: 0.10717171892630968 Corrects: 2623\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.10794875770807266\n",
            "Train step - Step 950, Loss 0.11031055450439453\n",
            "Train step - Step 960, Loss 0.11418807506561279\n",
            "Train step - Step 970, Loss 0.10578931868076324\n",
            "Train step - Step 980, Loss 0.10592444241046906\n",
            "Train epoch - Accuracy: 0.38316546762589926 Loss: 0.10713583235260395 Corrects: 2663\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.10865499079227448\n",
            "Train step - Step 1000, Loss 0.11040443927049637\n",
            "Train step - Step 1010, Loss 0.109550341963768\n",
            "Train step - Step 1020, Loss 0.10857883095741272\n",
            "Train step - Step 1030, Loss 0.10475587844848633\n",
            "Train step - Step 1040, Loss 0.1125776618719101\n",
            "Train epoch - Accuracy: 0.3915107913669065 Loss: 0.10689446020040581 Corrects: 2721\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.10833892226219177\n",
            "Train step - Step 1060, Loss 0.09979503601789474\n",
            "Train step - Step 1070, Loss 0.10877429693937302\n",
            "Train step - Step 1080, Loss 0.10998522490262985\n",
            "Train step - Step 1090, Loss 0.1000124141573906\n",
            "Train epoch - Accuracy: 0.38633093525179857 Loss: 0.10715703597600512 Corrects: 2685\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10655400156974792\n",
            "Train step - Step 1110, Loss 0.10764048993587494\n",
            "Train step - Step 1120, Loss 0.10941404104232788\n",
            "Train step - Step 1130, Loss 0.10695327818393707\n",
            "Train step - Step 1140, Loss 0.10476546734571457\n",
            "Train step - Step 1150, Loss 0.10755869001150131\n",
            "Train epoch - Accuracy: 0.39467625899280573 Loss: 0.10641350402248849 Corrects: 2743\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10640475153923035\n",
            "Train step - Step 1170, Loss 0.101453997194767\n",
            "Train step - Step 1180, Loss 0.10257808119058609\n",
            "Train step - Step 1190, Loss 0.11068938672542572\n",
            "Train step - Step 1200, Loss 0.10248542577028275\n",
            "Train epoch - Accuracy: 0.40431654676258993 Loss: 0.10661084259371106 Corrects: 2810\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.10491970181465149\n",
            "Train step - Step 1220, Loss 0.10281399637460709\n",
            "Train step - Step 1230, Loss 0.10225324332714081\n",
            "Train step - Step 1240, Loss 0.10905403643846512\n",
            "Train step - Step 1250, Loss 0.11167445778846741\n",
            "Train step - Step 1260, Loss 0.11058954894542694\n",
            "Train epoch - Accuracy: 0.4063309352517986 Loss: 0.10641458415084605 Corrects: 2824\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.103298120200634\n",
            "Train step - Step 1280, Loss 0.10936851054430008\n",
            "Train step - Step 1290, Loss 0.10650234669446945\n",
            "Train step - Step 1300, Loss 0.10302133113145828\n",
            "Train step - Step 1310, Loss 0.10471926629543304\n",
            "Train epoch - Accuracy: 0.4172661870503597 Loss: 0.10611397308196953 Corrects: 2900\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.10818652808666229\n",
            "Train step - Step 1330, Loss 0.10485441237688065\n",
            "Train step - Step 1340, Loss 0.1093021109700203\n",
            "Train step - Step 1350, Loss 0.09931037575006485\n",
            "Train step - Step 1360, Loss 0.10640624165534973\n",
            "Train step - Step 1370, Loss 0.10166700184345245\n",
            "Train epoch - Accuracy: 0.418705035971223 Loss: 0.1058514670438046 Corrects: 2910\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.10649902373552322\n",
            "Train step - Step 1390, Loss 0.10157186537981033\n",
            "Train step - Step 1400, Loss 0.11196543276309967\n",
            "Train step - Step 1410, Loss 0.1108059287071228\n",
            "Train step - Step 1420, Loss 0.10084458440542221\n",
            "Train epoch - Accuracy: 0.4244604316546763 Loss: 0.10580738421609934 Corrects: 2950\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.10161390155553818\n",
            "Train step - Step 1440, Loss 0.10321464389562607\n",
            "Train step - Step 1450, Loss 0.11089616268873215\n",
            "Train step - Step 1460, Loss 0.11461817473173141\n",
            "Train step - Step 1470, Loss 0.10184233635663986\n",
            "Train step - Step 1480, Loss 0.10194941610097885\n",
            "Train epoch - Accuracy: 0.42848920863309353 Loss: 0.10606413916074972 Corrects: 2978\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.10566145926713943\n",
            "Train step - Step 1500, Loss 0.10832327604293823\n",
            "Train step - Step 1510, Loss 0.10777545720338821\n",
            "Train step - Step 1520, Loss 0.10839366912841797\n",
            "Train step - Step 1530, Loss 0.1034741923213005\n",
            "Train epoch - Accuracy: 0.43381294964028777 Loss: 0.10552062838412017 Corrects: 3015\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.101997509598732\n",
            "Train step - Step 1550, Loss 0.1044112965464592\n",
            "Train step - Step 1560, Loss 0.10410559177398682\n",
            "Train step - Step 1570, Loss 0.10611503571271896\n",
            "Train step - Step 1580, Loss 0.10514593869447708\n",
            "Train step - Step 1590, Loss 0.10667693614959717\n",
            "Train epoch - Accuracy: 0.43755395683453235 Loss: 0.1048650146366881 Corrects: 3041\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.1002109944820404\n",
            "Train step - Step 1610, Loss 0.10246577113866806\n",
            "Train step - Step 1620, Loss 0.10254170000553131\n",
            "Train step - Step 1630, Loss 0.09840015321969986\n",
            "Train step - Step 1640, Loss 0.10047109425067902\n",
            "Train epoch - Accuracy: 0.44244604316546765 Loss: 0.10556999857691553 Corrects: 3075\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.10515611618757248\n",
            "Train step - Step 1660, Loss 0.10142489522695541\n",
            "Train step - Step 1670, Loss 0.1102081686258316\n",
            "Train step - Step 1680, Loss 0.10670974850654602\n",
            "Train step - Step 1690, Loss 0.10395299643278122\n",
            "Train step - Step 1700, Loss 0.10129691660404205\n",
            "Train epoch - Accuracy: 0.4507913669064748 Loss: 0.10553267603512291 Corrects: 3133\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.10562264919281006\n",
            "Train step - Step 1720, Loss 0.10295797139406204\n",
            "Train step - Step 1730, Loss 0.10865586996078491\n",
            "Train step - Step 1740, Loss 0.1075352281332016\n",
            "Train step - Step 1750, Loss 0.10591544955968857\n",
            "Train epoch - Accuracy: 0.45568345323741005 Loss: 0.10503640666711245 Corrects: 3167\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.10224958509206772\n",
            "Train step - Step 1770, Loss 0.09611335396766663\n",
            "Train step - Step 1780, Loss 0.10559318959712982\n",
            "Train step - Step 1790, Loss 0.10965368151664734\n",
            "Train step - Step 1800, Loss 0.10362490266561508\n",
            "Train step - Step 1810, Loss 0.10322888940572739\n",
            "Train epoch - Accuracy: 0.4494964028776978 Loss: 0.10477736646322895 Corrects: 3124\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.10302804410457611\n",
            "Train step - Step 1830, Loss 0.10912961512804031\n",
            "Train step - Step 1840, Loss 0.10308963805437088\n",
            "Train step - Step 1850, Loss 0.10445097833871841\n",
            "Train step - Step 1860, Loss 0.11082127690315247\n",
            "Train epoch - Accuracy: 0.45784172661870504 Loss: 0.10477797544474224 Corrects: 3182\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.09918319433927536\n",
            "Train step - Step 1880, Loss 0.11390265822410583\n",
            "Train step - Step 1890, Loss 0.09980857372283936\n",
            "Train step - Step 1900, Loss 0.1085871085524559\n",
            "Train step - Step 1910, Loss 0.10162214189767838\n",
            "Train step - Step 1920, Loss 0.10534927248954773\n",
            "Train epoch - Accuracy: 0.4631654676258993 Loss: 0.10483105499967396 Corrects: 3219\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.10277356207370758\n",
            "Train step - Step 1940, Loss 0.10395488142967224\n",
            "Train step - Step 1950, Loss 0.1040952280163765\n",
            "Train step - Step 1960, Loss 0.1050909087061882\n",
            "Train step - Step 1970, Loss 0.10423200577497482\n",
            "Train epoch - Accuracy: 0.4651798561151079 Loss: 0.10418781641790335 Corrects: 3233\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.10378845036029816\n",
            "Train step - Step 1990, Loss 0.10618409514427185\n",
            "Train step - Step 2000, Loss 0.10576808452606201\n",
            "Train step - Step 2010, Loss 0.10929874330759048\n",
            "Train step - Step 2020, Loss 0.1087755635380745\n",
            "Train step - Step 2030, Loss 0.10377475619316101\n",
            "Train epoch - Accuracy: 0.4646043165467626 Loss: 0.1045873856973305 Corrects: 3229\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.10768083482980728\n",
            "Train step - Step 2050, Loss 0.10192542523145676\n",
            "Train step - Step 2060, Loss 0.09550834447145462\n",
            "Train step - Step 2070, Loss 0.10426560044288635\n",
            "Train step - Step 2080, Loss 0.10458014905452728\n",
            "Train epoch - Accuracy: 0.4660431654676259 Loss: 0.10414780566375033 Corrects: 3239\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.10719403624534607\n",
            "Train step - Step 2100, Loss 0.10480926930904388\n",
            "Train step - Step 2110, Loss 0.10266458243131638\n",
            "Train step - Step 2120, Loss 0.10639040917158127\n",
            "Train step - Step 2130, Loss 0.10165707021951675\n",
            "Train step - Step 2140, Loss 0.10727342963218689\n",
            "Train epoch - Accuracy: 0.47467625899280574 Loss: 0.10396805868731986 Corrects: 3299\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.10078908503055573\n",
            "Train step - Step 2160, Loss 0.09855945408344269\n",
            "Train step - Step 2170, Loss 0.1015959307551384\n",
            "Train step - Step 2180, Loss 0.10977998375892639\n",
            "Train step - Step 2190, Loss 0.10901035368442535\n",
            "Train epoch - Accuracy: 0.47654676258992806 Loss: 0.10382922932827215 Corrects: 3312\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.1024831160902977\n",
            "Train step - Step 2210, Loss 0.1036418154835701\n",
            "Train step - Step 2220, Loss 0.10587634891271591\n",
            "Train step - Step 2230, Loss 0.10515251010656357\n",
            "Train step - Step 2240, Loss 0.1084832176566124\n",
            "Train step - Step 2250, Loss 0.11545612663030624\n",
            "Train epoch - Accuracy: 0.47597122302158273 Loss: 0.1037931456775974 Corrects: 3308\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.1032240092754364\n",
            "Train step - Step 2270, Loss 0.10755809396505356\n",
            "Train step - Step 2280, Loss 0.10702770203351974\n",
            "Train step - Step 2290, Loss 0.10188472270965576\n",
            "Train step - Step 2300, Loss 0.1075555682182312\n",
            "Train epoch - Accuracy: 0.481294964028777 Loss: 0.10403227676590569 Corrects: 3345\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.10388948023319244\n",
            "Train step - Step 2320, Loss 0.10300634801387787\n",
            "Train step - Step 2330, Loss 0.10251758247613907\n",
            "Train step - Step 2340, Loss 0.10904312133789062\n",
            "Train step - Step 2350, Loss 0.10441549867391586\n",
            "Train step - Step 2360, Loss 0.10689807683229446\n",
            "Train epoch - Accuracy: 0.4962589928057554 Loss: 0.1035076899386996 Corrects: 3449\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.1020892858505249\n",
            "Train step - Step 2380, Loss 0.10298406332731247\n",
            "Train step - Step 2390, Loss 0.10735685378313065\n",
            "Train step - Step 2400, Loss 0.10416682809591293\n",
            "Train step - Step 2410, Loss 0.10944070667028427\n",
            "Train epoch - Accuracy: 0.49884892086330934 Loss: 0.10351226571866934 Corrects: 3467\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.09904327988624573\n",
            "Train step - Step 2430, Loss 0.09960681945085526\n",
            "Train step - Step 2440, Loss 0.1028023287653923\n",
            "Train step - Step 2450, Loss 0.10326962918043137\n",
            "Train step - Step 2460, Loss 0.10091658681631088\n",
            "Train step - Step 2470, Loss 0.10841823369264603\n",
            "Train epoch - Accuracy: 0.501726618705036 Loss: 0.10299293209537327 Corrects: 3487\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.10230182111263275\n",
            "Train step - Step 2490, Loss 0.10544493794441223\n",
            "Train step - Step 2500, Loss 0.10156445950269699\n",
            "Train step - Step 2510, Loss 0.10586908459663391\n",
            "Train step - Step 2520, Loss 0.10547954589128494\n",
            "Train epoch - Accuracy: 0.5004316546762589 Loss: 0.10382327295250172 Corrects: 3478\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.09789558500051498\n",
            "Train step - Step 2540, Loss 0.10398639738559723\n",
            "Train step - Step 2550, Loss 0.10854138433933258\n",
            "Train step - Step 2560, Loss 0.10236348956823349\n",
            "Train step - Step 2570, Loss 0.10322756320238113\n",
            "Train step - Step 2580, Loss 0.10409682989120483\n",
            "Train epoch - Accuracy: 0.5069064748201438 Loss: 0.10326409822102073 Corrects: 3523\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.10640695691108704\n",
            "Train step - Step 2600, Loss 0.10679042339324951\n",
            "Train step - Step 2610, Loss 0.10472314804792404\n",
            "Train step - Step 2620, Loss 0.10155800729990005\n",
            "Train step - Step 2630, Loss 0.10125239193439484\n",
            "Train epoch - Accuracy: 0.5021582733812949 Loss: 0.10303364585415065 Corrects: 3490\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.09982692450284958\n",
            "Train step - Step 2650, Loss 0.09921858459711075\n",
            "Train step - Step 2660, Loss 0.10201072692871094\n",
            "Train step - Step 2670, Loss 0.09983152151107788\n",
            "Train step - Step 2680, Loss 0.10175212472677231\n",
            "Train step - Step 2690, Loss 0.0987972691655159\n",
            "Train epoch - Accuracy: 0.5077697841726618 Loss: 0.10269035564266521 Corrects: 3529\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.10062777996063232\n",
            "Train step - Step 2710, Loss 0.10232770442962646\n",
            "Train step - Step 2720, Loss 0.10614404082298279\n",
            "Train step - Step 2730, Loss 0.09616024047136307\n",
            "Train step - Step 2740, Loss 0.09963319450616837\n",
            "Train epoch - Accuracy: 0.523453237410072 Loss: 0.10179586612492157 Corrects: 3638\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.0997505635023117\n",
            "Train step - Step 2760, Loss 0.09913536161184311\n",
            "Train step - Step 2770, Loss 0.10301759839057922\n",
            "Train step - Step 2780, Loss 0.10804865509271622\n",
            "Train step - Step 2790, Loss 0.10104254633188248\n",
            "Train step - Step 2800, Loss 0.10103104263544083\n",
            "Train epoch - Accuracy: 0.5176978417266187 Loss: 0.10134497406028158 Corrects: 3598\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.0993650034070015\n",
            "Train step - Step 2820, Loss 0.09902278333902359\n",
            "Train step - Step 2830, Loss 0.10210242867469788\n",
            "Train step - Step 2840, Loss 0.10025409609079361\n",
            "Train step - Step 2850, Loss 0.10328355431556702\n",
            "Train epoch - Accuracy: 0.522589928057554 Loss: 0.10143507482336579 Corrects: 3632\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.10110879689455032\n",
            "Train step - Step 2870, Loss 0.09742680937051773\n",
            "Train step - Step 2880, Loss 0.09819327294826508\n",
            "Train step - Step 2890, Loss 0.09990246593952179\n",
            "Train step - Step 2900, Loss 0.10404502600431442\n",
            "Train step - Step 2910, Loss 0.09730266034603119\n",
            "Train epoch - Accuracy: 0.5250359712230216 Loss: 0.10116979160325991 Corrects: 3649\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.10198220610618591\n",
            "Train step - Step 2930, Loss 0.10607339441776276\n",
            "Train step - Step 2940, Loss 0.10222390294075012\n",
            "Train step - Step 2950, Loss 0.09296309947967529\n",
            "Train step - Step 2960, Loss 0.10416406393051147\n",
            "Train epoch - Accuracy: 0.5244604316546763 Loss: 0.10133230351715637 Corrects: 3645\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.10411947220563889\n",
            "Train step - Step 2980, Loss 0.10043708235025406\n",
            "Train step - Step 2990, Loss 0.10648401826620102\n",
            "Train step - Step 3000, Loss 0.10726609826087952\n",
            "Train step - Step 3010, Loss 0.10724038630723953\n",
            "Train step - Step 3020, Loss 0.10338069498538971\n",
            "Train epoch - Accuracy: 0.516546762589928 Loss: 0.10112978101205483 Corrects: 3590\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.10373078286647797\n",
            "Train step - Step 3040, Loss 0.10121960192918777\n",
            "Train step - Step 3050, Loss 0.09790748357772827\n",
            "Train step - Step 3060, Loss 0.10050838440656662\n",
            "Train step - Step 3070, Loss 0.0955556258559227\n",
            "Train epoch - Accuracy: 0.520863309352518 Loss: 0.10106191885128296 Corrects: 3620\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.10107851773500443\n",
            "Train step - Step 3090, Loss 0.10109523683786392\n",
            "Train step - Step 3100, Loss 0.09777210652828217\n",
            "Train step - Step 3110, Loss 0.10382112115621567\n",
            "Train step - Step 3120, Loss 0.10031460225582123\n",
            "Train step - Step 3130, Loss 0.09954152256250381\n",
            "Train epoch - Accuracy: 0.52 Loss: 0.1010393875344194 Corrects: 3614\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.10669122636318207\n",
            "Train step - Step 3150, Loss 0.10326750576496124\n",
            "Train step - Step 3160, Loss 0.09739354997873306\n",
            "Train step - Step 3170, Loss 0.10371484607458115\n",
            "Train step - Step 3180, Loss 0.10201778262853622\n",
            "Train epoch - Accuracy: 0.5284892086330936 Loss: 0.10078935200147492 Corrects: 3673\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.09711984544992447\n",
            "Train step - Step 3200, Loss 0.09898725152015686\n",
            "Train step - Step 3210, Loss 0.09897750616073608\n",
            "Train step - Step 3220, Loss 0.09991554915904999\n",
            "Train step - Step 3230, Loss 0.0995786264538765\n",
            "Train step - Step 3240, Loss 0.11074994504451752\n",
            "Train epoch - Accuracy: 0.521294964028777 Loss: 0.10125011191093665 Corrects: 3623\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.09829214215278625\n",
            "Train step - Step 3260, Loss 0.10142118483781815\n",
            "Train step - Step 3270, Loss 0.10470382124185562\n",
            "Train step - Step 3280, Loss 0.09964612126350403\n",
            "Train step - Step 3290, Loss 0.09896053373813629\n",
            "Train epoch - Accuracy: 0.5264748201438849 Loss: 0.10100822784489007 Corrects: 3659\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.10095050930976868\n",
            "Train step - Step 3310, Loss 0.10121738165616989\n",
            "Train step - Step 3320, Loss 0.09830178320407867\n",
            "Train step - Step 3330, Loss 0.10109609365463257\n",
            "Train step - Step 3340, Loss 0.10447227209806442\n",
            "Train step - Step 3350, Loss 0.09692078083753586\n",
            "Train epoch - Accuracy: 0.5290647482014389 Loss: 0.10052659414869418 Corrects: 3677\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.10227256268262863\n",
            "Train step - Step 3370, Loss 0.09711068868637085\n",
            "Train step - Step 3380, Loss 0.10573772341012955\n",
            "Train step - Step 3390, Loss 0.10487230122089386\n",
            "Train step - Step 3400, Loss 0.10265979170799255\n",
            "Train epoch - Accuracy: 0.5302158273381294 Loss: 0.10100606354020482 Corrects: 3685\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.09685061872005463\n",
            "Train step - Step 3420, Loss 0.09906322509050369\n",
            "Train step - Step 3430, Loss 0.09275573492050171\n",
            "Train step - Step 3440, Loss 0.1042419970035553\n",
            "Train step - Step 3450, Loss 0.09637989103794098\n",
            "Train step - Step 3460, Loss 0.1005079597234726\n",
            "Train epoch - Accuracy: 0.5296402877697842 Loss: 0.1010613944075948 Corrects: 3681\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.10029992461204529\n",
            "Train step - Step 3480, Loss 0.10308559983968735\n",
            "Train step - Step 3490, Loss 0.0998394787311554\n",
            "Train step - Step 3500, Loss 0.10230888426303864\n",
            "Train step - Step 3510, Loss 0.10098915547132492\n",
            "Train epoch - Accuracy: 0.5292086330935252 Loss: 0.10081120709721134 Corrects: 3678\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.09623835980892181\n",
            "Train step - Step 3530, Loss 0.11062892526388168\n",
            "Train step - Step 3540, Loss 0.10612928122282028\n",
            "Train step - Step 3550, Loss 0.09784313291311264\n",
            "Train step - Step 3560, Loss 0.0991465151309967\n",
            "Train step - Step 3570, Loss 0.09905070811510086\n",
            "Train epoch - Accuracy: 0.5228776978417267 Loss: 0.10078423799370691 Corrects: 3634\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.10124944150447845\n",
            "Train step - Step 3590, Loss 0.10045324265956879\n",
            "Train step - Step 3600, Loss 0.09300102293491364\n",
            "Train step - Step 3610, Loss 0.09924951195716858\n",
            "Train step - Step 3620, Loss 0.1065649762749672\n",
            "Train epoch - Accuracy: 0.5253237410071943 Loss: 0.10077958438036252 Corrects: 3651\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.10180073231458664\n",
            "Train step - Step 3640, Loss 0.10470789670944214\n",
            "Train step - Step 3650, Loss 0.10322132706642151\n",
            "Train step - Step 3660, Loss 0.09671569615602493\n",
            "Train step - Step 3670, Loss 0.09546029567718506\n",
            "Train step - Step 3680, Loss 0.10450244694948196\n",
            "Train epoch - Accuracy: 0.5336690647482014 Loss: 0.10058183018252147 Corrects: 3709\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.103182353079319\n",
            "Train step - Step 3700, Loss 0.10103137791156769\n",
            "Train step - Step 3710, Loss 0.09099584072828293\n",
            "Train step - Step 3720, Loss 0.10314563661813736\n",
            "Train step - Step 3730, Loss 0.09421086311340332\n",
            "Train epoch - Accuracy: 0.5329496402877698 Loss: 0.1002563196978123 Corrects: 3704\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.09868410229682922\n",
            "Train step - Step 3750, Loss 0.10192018002271652\n",
            "Train step - Step 3760, Loss 0.10092095285654068\n",
            "Train step - Step 3770, Loss 0.09711553156375885\n",
            "Train step - Step 3780, Loss 0.09578556567430496\n",
            "Train step - Step 3790, Loss 0.09848064184188843\n",
            "Train epoch - Accuracy: 0.5276258992805756 Loss: 0.10027281107876798 Corrects: 3667\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.09868211299180984\n",
            "Train step - Step 3810, Loss 0.10438191890716553\n",
            "Train step - Step 3820, Loss 0.09703671932220459\n",
            "Train step - Step 3830, Loss 0.10381843149662018\n",
            "Train step - Step 3840, Loss 0.09997216612100601\n",
            "Train epoch - Accuracy: 0.5289208633093525 Loss: 0.10069348608203929 Corrects: 3676\n",
            "Training finished in 394.65611386299133 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0]\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232ee8c50>\n",
            "Constructing exemplars of class 91\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [34077, 28521, 34389, 3413, 11375, 4065, 36364, 41314, 41298, 35722, 17138, 28172, 30632, 28690, 25757, 35303, 21625, 13439, 47896, 43430, 5265, 17063, 32154, 21340, 3454, 16314, 20049, 44419, 15376, 31831, 16415, 21511, 43460, 32007, 40124, 39392, 34389, 7827, 313, 42716]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238ac7a10>\n",
            "Constructing exemplars of class 67\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [9791, 26411, 5543, 45668, 1247, 46644, 23572, 20614, 30991, 29453, 15324, 32687, 33761, 13575, 10630, 21582, 31732, 21143, 9174, 1101, 26910, 27157, 10946, 34006, 2742, 28209, 27360, 2785, 38326, 2860, 11131, 14249, 29453, 1627, 25966, 1973, 8692, 3818, 28403, 45668]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232eeff50>\n",
            "Constructing exemplars of class 7\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [38690, 2149, 49996, 4884, 606, 44890, 31359, 21660, 38438, 37815, 17493, 13162, 37623, 31764, 40343, 36420, 49939, 22695, 19233, 5819, 42560, 42172, 2149, 31840, 2356, 28351, 40131, 24631, 8306, 37815, 41605, 26064, 45534, 28106, 17791, 37071, 34209, 47101, 35159, 16035]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223822c110>\n",
            "Constructing exemplars of class 62\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [34356, 35609, 4784, 43418, 18170, 32291, 40692, 17468, 21547, 26092, 10731, 6830, 9341, 24885, 42933, 10007, 14909, 41433, 6339, 33591, 29105, 19422, 41603, 18391, 46545, 40891, 21068, 34449, 7935, 47702, 19674, 30713, 3787, 31405, 18696, 20625, 47953, 897, 25417, 719]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22414294d0>\n",
            "Constructing exemplars of class 46\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [41147, 16261, 9803, 12436, 10021, 16394, 48211, 18113, 885, 22659, 44318, 4198, 17212, 19846, 44738, 30397, 18770, 46919, 12019, 27714, 14132, 27411, 11627, 18191, 45758, 28231, 22668, 45493, 28028, 41311, 22439, 27366, 26040, 25003, 21666, 5784, 22305, 44533, 14095, 11990]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238035550>\n",
            "Constructing exemplars of class 65\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [28261, 3807, 11632, 3834, 48464, 14214, 19026, 47299, 23766, 38799, 21975, 22118, 48476, 6411, 44002, 38115, 31038, 12479, 43785, 28543, 17603, 10704, 42332, 44363, 38308, 16505, 17978, 30989, 6255, 48692, 23565, 41727, 28544, 33043, 21059, 11441, 48174, 41321, 34105, 17480]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238048710>\n",
            "Constructing exemplars of class 21\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [32570, 34798, 27534, 20006, 35040, 45390, 25561, 12784, 15874, 215, 14614, 6650, 1333, 13623, 14101, 7092, 48260, 46014, 44316, 12998, 15874, 28967, 5849, 40466, 7358, 33008, 23144, 44201, 11172, 1794, 14404, 7893, 21431, 8572, 17038, 14451, 46251, 15320, 25670, 11031]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2244662210>\n",
            "Constructing exemplars of class 32\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [1732, 33467, 32146, 6698, 47635, 26772, 27951, 21365, 28352, 28321, 36853, 29001, 21249, 40113, 22600, 33857, 7444, 11831, 9834, 8787, 11332, 33705, 17352, 44672, 30068, 48866, 6698, 16745, 20744, 33040, 35436, 38820, 33281, 25811, 33140, 18676, 30294, 15504, 21788, 20356]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232963310>\n",
            "Constructing exemplars of class 16\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [1792, 40901, 34886, 34812, 21303, 39592, 4620, 6972, 25959, 23067, 45357, 44376, 9075, 18377, 30494, 22380, 25541, 14019, 1300, 38638, 20553, 35320, 22464, 23899, 1743, 12643, 9261, 10611, 4687, 15221, 21588, 26400, 3552, 21274, 38681, 20555, 38191, 14425, 12643, 18225]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232edc350>\n",
            "Constructing exemplars of class 0\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [44150, 15990, 30172, 6438, 22165, 47445, 4055, 44972, 24717, 48469, 2126, 28361, 43179, 17882, 38908, 45466, 37056, 31600, 22324, 34042, 27759, 3594, 17727, 35665, 45717, 21725, 41774, 4398, 18064, 42847, 915, 20673, 16540, 42285, 44327, 32714, 47979, 43179, 17390, 27576]\n",
            "x train:  [-0.11418867 -0.06397434 -0.09129415 -0.11471892 -0.1541793   0.0197516\n",
            " -0.02684177 -0.07511663 -0.13134332 -0.03152029 -0.03477676 -0.12528445\n",
            " -0.11254081 -0.09080856 -0.12249456 -0.14362288 -0.00942367 -0.14356144\n",
            " -0.15762042 -0.26915073 -0.05780768 -0.09320907 -0.12336543 -0.10031076\n",
            " -0.15795988 -0.11999255 -0.11741342 -0.06357858 -0.17300823 -0.2699407\n",
            " -0.1191648  -0.04370972 -0.05420838 -0.20760371 -0.27911434 -0.15708354\n",
            " -0.21568969 -0.13978493 -0.21691616 -0.04688708 -0.10786799 -0.1834206\n",
            " -0.28298703 -0.16760553 -0.0432343  -0.09225056 -0.11114978 -0.04877943\n",
            " -0.04576128 -0.25135705]\n",
            "y_train:  [tensor([30]), tensor([37]), tensor([36]), tensor([37]), tensor([69]), tensor([59]), tensor([34]), tensor([35]), tensor([91]), tensor([96]), tensor([34]), tensor([80]), tensor([94]), tensor([53]), tensor([36]), tensor([35]), tensor([81]), tensor([72]), tensor([59]), tensor([11]), tensor([37]), tensor([0]), tensor([57]), tensor([45]), tensor([86]), tensor([12]), tensor([52]), tensor([82]), tensor([33]), tensor([60]), tensor([77]), tensor([24]), tensor([24]), tensor([58]), tensor([12]), tensor([3]), tensor([16]), tensor([81]), tensor([33]), tensor([36]), tensor([88]), tensor([12]), tensor([35]), tensor([31]), tensor([35]), tensor([33]), tensor([45]), tensor([31]), tensor([16]), tensor([12]), tensor([52]), tensor([50]), tensor([24]), tensor([13]), tensor([65]), tensor([0]), tensor([27]), tensor([46]), tensor([67]), tensor([97]), tensor([57]), tensor([68]), tensor([21]), tensor([37]), tensor([97]), tensor([82]), tensor([57]), tensor([11]), tensor([12]), tensor([21]), tensor([57]), tensor([65]), tensor([81]), tensor([86]), tensor([82]), tensor([62]), tensor([62]), tensor([77]), tensor([37]), tensor([98]), tensor([72]), tensor([72]), tensor([60]), tensor([7]), tensor([30]), tensor([95]), tensor([32]), tensor([13]), tensor([78]), tensor([33]), tensor([13]), tensor([57]), tensor([72]), tensor([25]), tensor([60]), tensor([25]), tensor([97]), tensor([82]), tensor([58]), tensor([16]), tensor([60]), tensor([88]), tensor([92]), tensor([88]), tensor([80]), tensor([60]), tensor([88]), tensor([21]), tensor([46]), tensor([0]), tensor([49]), tensor([12]), tensor([30]), tensor([13]), tensor([7]), tensor([49]), tensor([34]), tensor([0]), tensor([45]), tensor([34]), tensor([11]), tensor([97]), tensor([13]), tensor([25]), tensor([7]), tensor([27]), tensor([11]), tensor([62]), tensor([67]), tensor([52]), tensor([96]), tensor([96]), tensor([31]), tensor([36]), tensor([60]), tensor([91]), tensor([27]), tensor([59]), tensor([35]), tensor([80]), tensor([59]), tensor([58]), tensor([67]), tensor([16]), tensor([27]), tensor([24]), tensor([58]), tensor([32]), tensor([50]), tensor([24]), tensor([32]), tensor([24]), tensor([91]), tensor([11]), tensor([45]), tensor([11]), tensor([69]), tensor([96]), tensor([97]), tensor([65]), tensor([96]), tensor([30]), tensor([88]), tensor([72]), tensor([45]), tensor([25]), tensor([72]), tensor([49]), tensor([49]), tensor([97]), tensor([32]), tensor([30]), tensor([95]), tensor([3]), tensor([59]), tensor([31]), tensor([45]), tensor([3]), tensor([94]), tensor([97]), tensor([59]), tensor([78]), tensor([16]), tensor([24]), tensor([97]), tensor([30]), tensor([98]), tensor([81]), tensor([58]), tensor([68]), tensor([52]), tensor([24]), tensor([16]), tensor([58]), tensor([78]), tensor([36]), tensor([58]), tensor([21]), tensor([33]), tensor([92]), tensor([36]), tensor([46]), tensor([77]), tensor([31]), tensor([45]), tensor([49]), tensor([7]), tensor([80]), tensor([67]), tensor([27]), tensor([95]), tensor([92]), tensor([86]), tensor([52]), tensor([92]), tensor([7]), tensor([60]), tensor([98]), tensor([95]), tensor([59]), tensor([34]), tensor([36]), tensor([10]), tensor([82]), tensor([13]), tensor([25]), tensor([65]), tensor([37]), tensor([21]), tensor([91]), tensor([98]), tensor([80]), tensor([36]), tensor([32]), tensor([98]), tensor([69]), tensor([46]), tensor([13]), tensor([98]), tensor([67]), tensor([52]), tensor([27]), tensor([58]), tensor([52]), tensor([32]), tensor([96]), tensor([91]), tensor([65]), tensor([10]), tensor([77]), tensor([53]), tensor([91]), tensor([27]), tensor([30]), tensor([24]), tensor([16]), tensor([96]), tensor([31]), tensor([7]), tensor([95]), tensor([88]), tensor([95]), tensor([96]), tensor([86]), tensor([72]), tensor([68]), tensor([52]), tensor([68]), tensor([52]), tensor([72]), tensor([0]), tensor([69]), tensor([25]), tensor([16]), tensor([31]), tensor([32]), tensor([60]), tensor([7]), tensor([59]), tensor([33]), tensor([94]), tensor([32]), tensor([12]), tensor([96]), tensor([60]), tensor([62]), tensor([10]), tensor([94]), tensor([36]), tensor([34]), tensor([72]), tensor([65]), tensor([13]), tensor([86]), tensor([81]), tensor([35]), tensor([82]), tensor([25]), tensor([60]), tensor([67]), tensor([82]), tensor([53]), tensor([0]), tensor([37]), tensor([58]), tensor([30]), tensor([36]), tensor([91]), tensor([31]), tensor([77]), tensor([0]), tensor([12]), tensor([72]), tensor([35]), tensor([34]), tensor([32]), tensor([96]), tensor([37]), tensor([13]), tensor([46]), tensor([34]), tensor([78]), tensor([27]), tensor([36]), tensor([11]), tensor([25]), tensor([68]), tensor([80]), tensor([80]), tensor([65]), tensor([11]), tensor([57]), tensor([53]), tensor([94]), tensor([58]), tensor([82]), tensor([68]), tensor([7]), tensor([24]), tensor([46]), tensor([27]), tensor([10]), tensor([92]), tensor([31]), tensor([95]), tensor([95]), tensor([36]), tensor([53]), tensor([25]), tensor([68]), tensor([24]), tensor([59]), tensor([62]), tensor([12]), tensor([34]), tensor([52]), tensor([12]), tensor([34]), tensor([67]), tensor([49]), tensor([3]), tensor([98]), tensor([12]), tensor([59]), tensor([24]), tensor([12]), tensor([57]), tensor([35]), tensor([30]), tensor([24]), tensor([68]), tensor([10]), tensor([12]), tensor([7]), tensor([50]), tensor([69]), tensor([59]), tensor([94]), tensor([30]), tensor([92]), tensor([33]), tensor([36]), tensor([21]), tensor([46]), tensor([58]), tensor([72]), tensor([95]), tensor([53]), tensor([62]), tensor([77]), tensor([31]), tensor([86]), tensor([81]), tensor([72]), tensor([95]), tensor([88]), tensor([21]), tensor([13]), tensor([80]), tensor([45]), tensor([68]), tensor([59]), tensor([31]), tensor([32]), tensor([52]), tensor([68]), tensor([31]), tensor([62]), tensor([10]), tensor([94]), tensor([7]), tensor([91]), tensor([52]), tensor([77]), tensor([35]), tensor([50]), tensor([11]), tensor([95]), tensor([78]), tensor([45]), tensor([25]), tensor([57]), tensor([68]), tensor([72]), tensor([45]), tensor([72]), tensor([0]), tensor([13]), tensor([3]), tensor([27]), tensor([52]), tensor([27]), tensor([58]), tensor([0]), tensor([25]), tensor([60]), tensor([94]), tensor([3]), tensor([37]), tensor([65]), tensor([53]), tensor([32]), tensor([81]), tensor([59]), tensor([72]), tensor([69]), tensor([16]), tensor([34]), tensor([3]), tensor([46]), tensor([67]), tensor([77]), tensor([27]), tensor([32]), tensor([49]), tensor([98]), tensor([86]), tensor([53]), tensor([24]), tensor([98]), tensor([33]), tensor([98]), tensor([60]), tensor([30]), tensor([96]), tensor([86]), tensor([60]), tensor([3]), tensor([58]), tensor([69]), tensor([95]), tensor([30]), tensor([31]), tensor([88]), tensor([53]), tensor([24]), tensor([91]), tensor([81]), tensor([53]), tensor([82]), tensor([69]), tensor([35]), tensor([82]), tensor([82]), tensor([68]), tensor([31]), tensor([45]), tensor([91]), tensor([13]), tensor([10]), tensor([59]), tensor([81]), tensor([80]), tensor([31]), tensor([21]), tensor([67]), tensor([34]), tensor([82]), tensor([50]), tensor([49]), tensor([31]), tensor([86]), tensor([37]), tensor([57]), tensor([33]), tensor([68]), tensor([86]), tensor([24]), tensor([77]), tensor([65]), tensor([36]), tensor([37]), tensor([58]), tensor([37]), tensor([92]), tensor([96]), tensor([25]), tensor([96]), tensor([77]), tensor([13]), tensor([11]), tensor([30]), tensor([88]), tensor([46]), tensor([98]), tensor([92]), tensor([10]), tensor([77]), tensor([36]), tensor([46]), tensor([33]), tensor([88]), tensor([52]), tensor([34]), tensor([27]), tensor([80]), tensor([27]), tensor([91]), tensor([98]), tensor([95]), tensor([67]), tensor([45]), tensor([80]), tensor([13]), tensor([57]), tensor([33]), tensor([97]), tensor([94]), tensor([77]), tensor([94]), tensor([31]), tensor([25]), tensor([13]), tensor([65]), tensor([33]), tensor([50]), tensor([97]), tensor([33]), tensor([81]), tensor([77]), tensor([81]), tensor([35]), tensor([92]), tensor([80]), tensor([32]), tensor([30]), tensor([80]), tensor([32]), tensor([13]), tensor([59]), tensor([57]), tensor([31]), tensor([21]), tensor([24]), tensor([80]), tensor([32]), tensor([97]), tensor([65]), tensor([33]), tensor([25]), tensor([12]), tensor([77]), tensor([92]), tensor([30]), tensor([78]), tensor([57]), tensor([11]), tensor([46]), tensor([49]), tensor([52]), tensor([46]), tensor([11]), tensor([58]), tensor([92]), tensor([35]), tensor([49]), tensor([27]), tensor([37]), tensor([68]), tensor([33]), tensor([16]), tensor([46]), tensor([57]), tensor([81]), tensor([30]), tensor([10]), tensor([81]), tensor([24]), tensor([3]), tensor([30]), tensor([12]), tensor([50]), tensor([65]), tensor([98]), tensor([3]), tensor([98]), tensor([58]), tensor([96]), tensor([10]), tensor([92]), tensor([35]), tensor([57]), tensor([65]), tensor([50]), tensor([86]), tensor([92]), tensor([13]), tensor([27]), tensor([34]), tensor([67]), tensor([98]), tensor([10]), tensor([25]), tensor([77]), tensor([30]), tensor([88]), tensor([35]), tensor([72]), tensor([72]), tensor([12]), tensor([97]), tensor([65]), tensor([65]), tensor([92]), tensor([12]), tensor([62]), tensor([58]), tensor([31]), tensor([95]), tensor([94]), tensor([36]), tensor([11]), tensor([65]), tensor([37]), tensor([94]), tensor([65]), tensor([59]), tensor([16]), tensor([7]), tensor([88]), tensor([96]), tensor([86]), tensor([31]), tensor([0]), tensor([35]), tensor([24]), tensor([82]), tensor([81]), tensor([46]), tensor([53]), tensor([94]), tensor([34]), tensor([12]), tensor([3]), tensor([32]), tensor([72]), tensor([0]), tensor([12]), tensor([86]), tensor([46]), tensor([10]), tensor([67]), tensor([10]), tensor([88]), tensor([13]), tensor([78]), tensor([27]), tensor([62]), tensor([62]), tensor([12]), tensor([24]), tensor([68]), tensor([60]), tensor([65]), tensor([60]), tensor([33]), tensor([95]), tensor([72]), tensor([16]), tensor([53]), tensor([46]), tensor([69]), tensor([77]), tensor([27]), tensor([52]), tensor([91]), tensor([25]), tensor([67]), tensor([45]), tensor([53]), tensor([33]), tensor([95]), tensor([34]), tensor([21]), tensor([33]), tensor([10]), tensor([91]), tensor([11]), tensor([81]), tensor([57]), tensor([91]), tensor([46]), tensor([46]), tensor([12]), tensor([49]), tensor([0]), tensor([69]), tensor([86]), tensor([78]), tensor([11]), tensor([92]), tensor([0]), tensor([53]), tensor([45]), tensor([49]), tensor([62]), tensor([16]), tensor([60]), tensor([80]), tensor([11]), tensor([11]), tensor([62]), tensor([91]), tensor([67]), tensor([65]), tensor([10]), tensor([94]), tensor([69]), tensor([36]), tensor([3]), tensor([33]), tensor([94]), tensor([86]), tensor([37]), tensor([86]), tensor([57]), tensor([69]), tensor([77]), tensor([11]), tensor([46]), tensor([53]), tensor([77]), tensor([31]), tensor([45]), tensor([36]), tensor([80]), tensor([37]), tensor([7]), tensor([12]), tensor([65]), tensor([96]), tensor([32]), tensor([50]), tensor([50]), tensor([81]), tensor([59]), tensor([27]), tensor([45]), tensor([82]), tensor([24]), tensor([27]), tensor([57]), tensor([60]), tensor([58]), tensor([10]), tensor([92]), tensor([25]), tensor([86]), tensor([60]), tensor([67]), tensor([24]), tensor([35]), tensor([21]), tensor([24]), tensor([67]), tensor([68]), tensor([62]), tensor([30]), tensor([86]), tensor([3]), tensor([65]), tensor([60]), tensor([78]), tensor([94]), tensor([32]), tensor([59]), tensor([78]), tensor([69]), tensor([67]), tensor([25]), tensor([94]), tensor([62]), tensor([37]), tensor([32]), tensor([60]), tensor([92]), tensor([13]), tensor([57]), tensor([16]), tensor([31]), tensor([37]), tensor([50]), tensor([24]), tensor([97]), tensor([0]), tensor([58]), tensor([36]), tensor([86]), tensor([30]), tensor([67]), tensor([45]), tensor([49]), tensor([21]), tensor([59]), tensor([68]), tensor([25]), tensor([3]), tensor([65]), tensor([78]), tensor([80]), tensor([37]), tensor([65]), tensor([67]), tensor([91]), tensor([21]), tensor([25]), tensor([36]), tensor([97]), tensor([46]), tensor([0]), tensor([57]), tensor([3]), tensor([37]), tensor([58]), tensor([21]), tensor([0]), tensor([21]), tensor([13]), tensor([24]), tensor([27]), tensor([92]), tensor([86]), tensor([67]), tensor([0]), tensor([0]), tensor([62]), tensor([77]), tensor([34]), tensor([3]), tensor([95]), tensor([24]), tensor([33]), tensor([52]), tensor([58]), tensor([59]), tensor([34]), tensor([80]), tensor([68]), tensor([81]), tensor([68]), tensor([62]), tensor([11]), tensor([46]), tensor([37]), tensor([65]), tensor([7]), tensor([98]), tensor([94]), tensor([35]), tensor([88]), tensor([96]), tensor([16]), tensor([13]), tensor([80]), tensor([16]), tensor([21]), tensor([96]), tensor([37]), tensor([69]), tensor([67]), tensor([16]), tensor([25]), tensor([36]), tensor([30]), tensor([49]), tensor([12]), tensor([0]), tensor([32]), tensor([34]), tensor([86]), tensor([59]), tensor([21]), tensor([94]), tensor([94]), tensor([92]), tensor([21]), tensor([50]), tensor([31]), tensor([52]), tensor([62]), tensor([58]), tensor([34]), tensor([68]), tensor([78]), tensor([69]), tensor([78]), tensor([34]), tensor([98]), tensor([72]), tensor([16]), tensor([80]), tensor([37]), tensor([24]), tensor([52]), tensor([35]), tensor([10]), tensor([96]), tensor([16]), tensor([24]), tensor([35]), tensor([7]), tensor([0]), tensor([45]), tensor([97]), tensor([65]), tensor([77]), tensor([53]), tensor([11]), tensor([34]), tensor([3]), tensor([78]), tensor([60]), tensor([91]), tensor([95]), tensor([45]), tensor([78]), tensor([95]), tensor([78]), tensor([31]), tensor([11]), tensor([57]), tensor([13]), tensor([59]), tensor([16]), tensor([37]), tensor([86]), tensor([31]), tensor([58]), tensor([59]), tensor([31]), tensor([10]), tensor([45]), tensor([46]), tensor([67]), tensor([72]), tensor([32]), tensor([21]), tensor([91]), tensor([50]), tensor([57]), tensor([80]), tensor([32]), tensor([45]), tensor([36]), tensor([21]), tensor([49]), tensor([91]), tensor([86]), tensor([49]), tensor([88]), tensor([69]), tensor([65]), tensor([92]), tensor([97]), tensor([72]), tensor([88]), tensor([35]), tensor([81]), tensor([27]), tensor([3]), tensor([46]), tensor([10]), tensor([68]), tensor([88]), tensor([27]), tensor([49]), tensor([86]), tensor([24]), tensor([78]), tensor([86]), tensor([58]), tensor([69]), tensor([27]), tensor([96]), tensor([77]), tensor([50]), tensor([80]), tensor([80]), tensor([46]), tensor([81]), tensor([25]), tensor([52]), tensor([88]), tensor([37]), tensor([30]), tensor([80]), tensor([49]), tensor([46]), tensor([95]), tensor([7]), tensor([92]), tensor([50]), tensor([16]), tensor([88]), tensor([68]), tensor([60]), tensor([49]), tensor([24]), tensor([67]), tensor([68]), tensor([78]), tensor([82]), tensor([82]), tensor([32]), tensor([72]), tensor([97]), tensor([78]), tensor([72]), tensor([53]), tensor([60]), tensor([13]), tensor([60]), tensor([30]), tensor([31]), tensor([78]), tensor([52]), tensor([31]), tensor([92]), tensor([25]), tensor([27]), tensor([32]), tensor([88]), tensor([82]), tensor([98]), tensor([65]), tensor([49]), tensor([60]), tensor([34]), tensor([94]), tensor([50]), tensor([98]), tensor([30]), tensor([50]), tensor([81]), tensor([45]), tensor([3]), tensor([31]), tensor([34]), tensor([98]), tensor([88]), tensor([36]), tensor([94]), tensor([32]), tensor([91]), tensor([91]), tensor([3]), tensor([95]), tensor([31]), tensor([82]), tensor([32]), tensor([7]), tensor([78]), tensor([7]), tensor([97]), tensor([65]), tensor([62]), tensor([69]), tensor([3]), tensor([77]), tensor([7]), tensor([21]), tensor([65]), tensor([7]), tensor([91]), tensor([7]), tensor([50]), tensor([77]), tensor([77]), tensor([25]), tensor([60]), tensor([25]), tensor([12]), tensor([81]), tensor([72]), tensor([68]), tensor([35]), tensor([98]), tensor([91]), tensor([60]), tensor([7]), tensor([78]), tensor([98]), tensor([3]), tensor([65]), tensor([95]), tensor([35]), tensor([12]), tensor([86]), tensor([24]), tensor([88]), tensor([7]), tensor([67]), tensor([34]), tensor([27]), tensor([67]), tensor([62]), tensor([98]), tensor([11]), tensor([46]), tensor([12]), tensor([53]), tensor([92]), tensor([81]), tensor([95]), tensor([12]), tensor([12]), tensor([27]), tensor([33]), tensor([49]), tensor([30]), tensor([3]), tensor([72]), tensor([50]), tensor([35]), tensor([27]), tensor([97]), tensor([52]), tensor([96]), tensor([92]), tensor([59]), tensor([12]), tensor([80]), tensor([67]), tensor([78]), tensor([77]), tensor([32]), tensor([65]), tensor([68]), tensor([53]), tensor([92]), tensor([0]), tensor([58]), tensor([68]), tensor([50]), tensor([91]), tensor([62]), tensor([96]), tensor([3]), tensor([12]), tensor([86]), tensor([13]), tensor([52]), tensor([60]), tensor([35]), tensor([3]), tensor([95]), tensor([82]), tensor([10]), tensor([49]), tensor([58]), tensor([91]), tensor([62]), tensor([82]), tensor([13]), tensor([36]), tensor([94]), tensor([7]), tensor([3]), tensor([72]), tensor([36]), tensor([25]), tensor([97]), tensor([25]), tensor([53]), tensor([91]), tensor([35]), tensor([95]), tensor([52]), tensor([27]), tensor([7]), tensor([98]), tensor([96]), tensor([11]), tensor([34]), tensor([46]), tensor([72]), tensor([59]), tensor([49]), tensor([98]), tensor([35]), tensor([91]), tensor([21]), tensor([80]), tensor([86]), tensor([97]), tensor([13]), tensor([13]), tensor([49]), tensor([88]), tensor([91]), tensor([3]), tensor([35]), tensor([50]), tensor([16]), tensor([81]), tensor([45]), tensor([86]), tensor([30]), tensor([52]), tensor([10]), tensor([72]), tensor([59]), tensor([96]), tensor([50]), tensor([7]), tensor([21]), tensor([97]), tensor([80]), tensor([52]), tensor([91]), tensor([46]), tensor([30]), tensor([82]), tensor([7]), tensor([34]), tensor([91]), tensor([35]), tensor([46]), tensor([34]), tensor([97]), tensor([12]), tensor([21]), tensor([10]), tensor([12]), tensor([60]), tensor([13]), tensor([31]), tensor([53]), tensor([98]), tensor([62]), tensor([96]), tensor([97]), tensor([21]), tensor([36]), tensor([95]), tensor([32]), tensor([27]), tensor([37]), tensor([10]), tensor([65]), tensor([81]), tensor([94]), tensor([45]), tensor([69]), tensor([69]), tensor([59]), tensor([82]), tensor([7]), tensor([80]), tensor([72]), tensor([80]), tensor([24]), tensor([97]), tensor([50]), tensor([12]), tensor([30]), tensor([68]), tensor([94]), tensor([80]), tensor([97]), tensor([31]), tensor([11]), tensor([69]), tensor([25]), tensor([10]), tensor([92]), tensor([69]), tensor([78]), tensor([82]), tensor([57]), tensor([32]), tensor([32]), tensor([3]), tensor([58]), tensor([72]), tensor([96]), tensor([60]), tensor([96]), tensor([33]), tensor([27]), tensor([58]), tensor([97]), tensor([46]), tensor([21]), tensor([46]), tensor([96]), tensor([58]), tensor([60]), tensor([13]), tensor([81]), tensor([58]), tensor([86]), tensor([67]), tensor([82]), tensor([50]), tensor([0]), tensor([21]), tensor([33]), tensor([67]), tensor([0]), tensor([16]), tensor([62]), tensor([96]), tensor([32]), tensor([97]), tensor([72]), tensor([46]), tensor([50]), tensor([27]), tensor([78]), tensor([36]), tensor([52]), tensor([86]), tensor([52]), tensor([37]), tensor([57]), tensor([49]), tensor([45]), tensor([69]), tensor([45]), tensor([88]), tensor([13]), tensor([11]), tensor([81]), tensor([37]), tensor([31]), tensor([60]), tensor([80]), tensor([57]), tensor([98]), tensor([88]), tensor([98]), tensor([92]), tensor([62]), tensor([81]), tensor([94]), tensor([16]), tensor([45]), tensor([45]), tensor([65]), tensor([50]), tensor([52]), tensor([78]), tensor([31]), tensor([69]), tensor([49]), tensor([58]), tensor([57]), tensor([13]), tensor([37]), tensor([68]), tensor([7]), tensor([80]), tensor([52]), tensor([58]), tensor([97]), tensor([46]), tensor([53]), tensor([77]), tensor([46]), tensor([11]), tensor([57]), tensor([57]), tensor([80]), tensor([81]), tensor([50]), tensor([77]), tensor([49]), tensor([37]), tensor([60]), tensor([57]), tensor([10]), tensor([95]), tensor([62]), tensor([52]), tensor([24]), tensor([72]), tensor([88]), tensor([50]), tensor([35]), tensor([69]), tensor([7]), tensor([37]), tensor([95]), tensor([53]), tensor([25]), tensor([35]), tensor([25]), tensor([52]), tensor([34]), tensor([57]), tensor([86]), tensor([13]), tensor([33]), tensor([81]), tensor([82]), tensor([10]), tensor([33]), tensor([60]), tensor([53]), tensor([81]), tensor([91]), tensor([16]), tensor([95]), tensor([60]), tensor([10]), tensor([11]), tensor([25]), tensor([58]), tensor([95]), tensor([7]), tensor([53]), tensor([80]), tensor([3]), tensor([11]), tensor([10]), tensor([11]), tensor([53]), tensor([58]), tensor([21]), tensor([0]), tensor([98]), tensor([92]), tensor([16]), tensor([69]), tensor([94]), tensor([3]), tensor([67]), tensor([92]), tensor([77]), tensor([94]), tensor([59]), tensor([11]), tensor([37]), tensor([33]), tensor([95]), tensor([21]), tensor([57]), tensor([32]), tensor([21]), tensor([80]), tensor([0]), tensor([25]), tensor([96]), tensor([11]), tensor([82]), tensor([68]), tensor([62]), tensor([7]), tensor([77]), tensor([33]), tensor([86]), tensor([36]), tensor([49]), tensor([45]), tensor([27]), tensor([68]), tensor([92]), tensor([0]), tensor([21]), tensor([77]), tensor([0]), tensor([36]), tensor([33]), tensor([3]), tensor([53]), tensor([13]), tensor([96]), tensor([3]), tensor([16]), tensor([80]), tensor([67]), tensor([49]), tensor([88]), tensor([12]), tensor([92]), tensor([27]), tensor([50]), tensor([59]), tensor([53]), tensor([95]), tensor([30]), tensor([35]), tensor([96]), tensor([78]), tensor([97]), tensor([81]), tensor([24]), tensor([37]), tensor([94]), tensor([45]), tensor([62]), tensor([88]), tensor([46]), tensor([0]), tensor([11]), tensor([91]), tensor([11]), tensor([94]), tensor([62]), tensor([10]), tensor([94]), tensor([10]), tensor([97]), tensor([36]), tensor([81]), tensor([80]), tensor([72]), tensor([7]), tensor([45]), tensor([46]), tensor([82]), tensor([69]), tensor([21]), tensor([57]), tensor([31]), tensor([96]), tensor([36]), tensor([11]), tensor([33]), tensor([27]), tensor([88]), tensor([13]), tensor([37]), tensor([69]), tensor([35]), tensor([25]), tensor([16]), tensor([21]), tensor([69]), tensor([78]), tensor([78]), tensor([88]), tensor([94]), tensor([13]), tensor([82]), tensor([86]), tensor([68]), tensor([80]), tensor([78]), tensor([49]), tensor([10]), tensor([77]), tensor([24]), tensor([34]), tensor([16]), tensor([68]), tensor([57]), tensor([7]), tensor([7]), tensor([88]), tensor([98]), tensor([34]), tensor([91]), tensor([91]), tensor([31]), tensor([58]), tensor([67]), tensor([59]), tensor([59]), tensor([95]), tensor([31]), tensor([82]), tensor([58]), tensor([62]), tensor([81]), tensor([27]), tensor([69]), tensor([77]), tensor([25]), tensor([65]), tensor([53]), tensor([77]), tensor([37]), tensor([77]), tensor([34]), tensor([50]), tensor([0]), tensor([97]), tensor([81]), tensor([98]), tensor([21]), tensor([10]), tensor([92]), tensor([69]), tensor([69]), tensor([82]), tensor([97]), tensor([16]), tensor([81]), tensor([62]), tensor([7]), tensor([65]), tensor([82]), tensor([52]), tensor([30]), tensor([52]), tensor([32]), tensor([32]), tensor([53]), tensor([50]), tensor([78]), tensor([12]), tensor([45]), tensor([92]), tensor([0]), tensor([52]), tensor([86]), tensor([21]), tensor([7]), tensor([62]), tensor([69]), tensor([94]), tensor([10]), tensor([88]), tensor([10]), tensor([58]), tensor([97]), tensor([78]), tensor([35]), tensor([67]), tensor([25]), tensor([3]), tensor([59]), tensor([67]), tensor([16]), tensor([50]), tensor([98]), tensor([0]), tensor([92]), tensor([0]), tensor([78]), tensor([32]), tensor([16]), tensor([45]), tensor([33]), tensor([49]), tensor([50]), tensor([34]), tensor([35]), tensor([72]), tensor([50]), tensor([0]), tensor([21]), tensor([59]), tensor([98]), tensor([33]), tensor([69]), tensor([46]), tensor([3]), tensor([62]), tensor([91]), tensor([98]), tensor([72]), tensor([81]), tensor([97]), tensor([24]), tensor([82]), tensor([10]), tensor([72]), tensor([10]), tensor([3]), tensor([30]), tensor([88]), tensor([82]), tensor([62]), tensor([49]), tensor([10]), tensor([30]), tensor([57]), tensor([94]), tensor([95]), tensor([68]), tensor([94]), tensor([32]), tensor([0]), tensor([36]), tensor([53]), tensor([53]), tensor([82]), tensor([59]), tensor([27]), tensor([69]), tensor([78]), tensor([98]), tensor([98]), tensor([49]), tensor([97]), tensor([30]), tensor([11]), tensor([62]), tensor([88]), tensor([32]), tensor([7]), tensor([60]), tensor([50]), tensor([82]), tensor([65]), tensor([13]), tensor([33]), tensor([53]), tensor([94]), tensor([78]), tensor([60]), tensor([68]), tensor([45]), tensor([30]), tensor([3]), tensor([11]), tensor([30]), tensor([92]), tensor([92]), tensor([12]), tensor([21]), tensor([37]), tensor([82]), tensor([81]), tensor([33]), tensor([46]), tensor([11]), tensor([97]), tensor([91]), tensor([11]), tensor([86]), tensor([82]), tensor([52]), tensor([96]), tensor([37]), tensor([53]), tensor([59]), tensor([16]), tensor([77]), tensor([50]), tensor([7]), tensor([31]), tensor([91]), tensor([31]), tensor([59]), tensor([97]), tensor([34]), tensor([77]), tensor([24]), tensor([96]), tensor([95]), tensor([68]), tensor([80]), tensor([82]), tensor([72]), tensor([65]), tensor([62]), tensor([67]), tensor([97]), tensor([57]), tensor([3]), tensor([88]), tensor([88]), tensor([36]), tensor([50]), tensor([53]), tensor([96]), tensor([65]), tensor([58]), tensor([30]), tensor([37]), tensor([92]), tensor([3]), tensor([86]), tensor([30]), tensor([32]), tensor([94]), tensor([21]), tensor([67]), tensor([30]), tensor([45]), tensor([95]), tensor([16]), tensor([68]), tensor([0]), tensor([33]), tensor([91]), tensor([30]), tensor([65]), tensor([78]), tensor([11]), tensor([78]), tensor([77]), tensor([59]), tensor([72]), tensor([50]), tensor([3]), tensor([45]), tensor([67]), tensor([49]), tensor([77]), tensor([13]), tensor([53]), tensor([16]), tensor([25]), tensor([92]), tensor([98]), tensor([95]), tensor([33]), tensor([16]), tensor([33]), tensor([49]), tensor([13]), tensor([88]), tensor([34]), tensor([24]), tensor([36]), tensor([25]), tensor([96]), tensor([32]), tensor([46]), tensor([72]), tensor([7]), tensor([0]), tensor([59]), tensor([52]), tensor([62]), tensor([97]), tensor([36]), tensor([78]), tensor([57]), tensor([52]), tensor([35]), tensor([96]), tensor([10]), tensor([12]), tensor([68]), tensor([46]), tensor([52]), tensor([82]), tensor([0]), tensor([69]), tensor([82]), tensor([92]), tensor([10]), tensor([45]), tensor([69]), tensor([67]), tensor([12]), tensor([37]), tensor([34]), tensor([95]), tensor([31]), tensor([88]), tensor([68]), tensor([95]), tensor([68]), tensor([16]), tensor([88]), tensor([35]), tensor([98]), tensor([25]), tensor([12]), tensor([32]), tensor([53]), tensor([58]), tensor([45]), tensor([52]), tensor([69]), tensor([60]), tensor([34]), tensor([25]), tensor([67]), tensor([86]), tensor([96]), tensor([57]), tensor([24]), tensor([35]), tensor([50]), tensor([57]), tensor([25]), tensor([0]), tensor([57]), tensor([67]), tensor([94]), tensor([81]), tensor([34]), tensor([53]), tensor([13]), tensor([45]), tensor([88]), tensor([21]), tensor([65]), tensor([77]), tensor([80]), tensor([27]), tensor([0]), tensor([10]), tensor([53]), tensor([36]), tensor([13]), tensor([11]), tensor([92]), tensor([57]), tensor([49]), tensor([94]), tensor([21]), tensor([45]), tensor([27]), tensor([81]), tensor([50]), tensor([91]), tensor([57]), tensor([62]), tensor([94]), tensor([16]), tensor([49]), tensor([24]), tensor([86]), tensor([49]), tensor([35]), tensor([98]), tensor([98]), tensor([86]), tensor([62]), tensor([77]), tensor([53]), tensor([58]), tensor([33]), tensor([34]), tensor([35]), tensor([33]), tensor([68]), tensor([60]), tensor([95]), tensor([36]), tensor([33]), tensor([69]), tensor([49]), tensor([78]), tensor([49]), tensor([37]), tensor([36]), tensor([27]), tensor([12]), tensor([16]), tensor([81]), tensor([96]), tensor([91]), tensor([35]), tensor([34]), tensor([33]), tensor([0]), tensor([36]), tensor([7]), tensor([30]), tensor([36]), tensor([78]), tensor([62]), tensor([67]), tensor([69]), tensor([62]), tensor([59]), tensor([59]), tensor([3]), tensor([60]), tensor([16])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.54 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.539\n",
            "TEST ALL:  0.5356\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  6000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 91, 69, 65, 57, 53, 49, 45, 41, 37, 33, 25, 21, 13, 96, 92, 88, 80, 72, 68, 60, 56, 52, 48, 36, 32, 24, 16, 12, 73, 77, 81, 94, 79, 71, 67, 63, 59, 35, 31, 27, 11, 7, 3, 98, 86, 97, 82, 78, 62, 58, 50, 46, 34, 30, 26, 14, 10, 2, 0]\n",
            "TRAIN_SET CLASSES:  [79, 71, 63, 26, 14, 2, 73, 41, 56, 48]\n",
            "VALIDATION CLASSES:  [63, 56, 48, 41, 26, 79, 14, 73, 71, 2]\n",
            "GROUP:  6\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [79, 71, 63, 26, 14, 2, 73, 41, 56, 48]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  60\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.2442663460969925\n",
            "Train step - Step 10, Loss 0.14584596455097198\n",
            "Train step - Step 20, Loss 0.14033186435699463\n",
            "Train step - Step 30, Loss 0.13283905386924744\n",
            "Train step - Step 40, Loss 0.1239665150642395\n",
            "Train step - Step 50, Loss 0.1202162504196167\n",
            "Train epoch - Accuracy: 0.17496402877697842 Loss: 0.14057387321758613 Corrects: 1216\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12514041364192963\n",
            "Train step - Step 70, Loss 0.12050531059503555\n",
            "Train step - Step 80, Loss 0.11861281096935272\n",
            "Train step - Step 90, Loss 0.11431744694709778\n",
            "Train step - Step 100, Loss 0.11040681600570679\n",
            "Train epoch - Accuracy: 0.2018705035971223 Loss: 0.11873220914344994 Corrects: 1403\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11832695454359055\n",
            "Train step - Step 120, Loss 0.11562176793813705\n",
            "Train step - Step 130, Loss 0.11715500056743622\n",
            "Train step - Step 140, Loss 0.11941944807767868\n",
            "Train step - Step 150, Loss 0.11917033791542053\n",
            "Train step - Step 160, Loss 0.1093059778213501\n",
            "Train epoch - Accuracy: 0.22848920863309352 Loss: 0.1161804647497136 Corrects: 1588\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.10862230509519577\n",
            "Train step - Step 180, Loss 0.11505290120840073\n",
            "Train step - Step 190, Loss 0.11433843523263931\n",
            "Train step - Step 200, Loss 0.11374836415052414\n",
            "Train step - Step 210, Loss 0.11280535906553268\n",
            "Train epoch - Accuracy: 0.24532374100719426 Loss: 0.11486285984944954 Corrects: 1705\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.1120816320180893\n",
            "Train step - Step 230, Loss 0.11645577102899551\n",
            "Train step - Step 240, Loss 0.11717190593481064\n",
            "Train step - Step 250, Loss 0.10720402747392654\n",
            "Train step - Step 260, Loss 0.10903088003396988\n",
            "Train step - Step 270, Loss 0.11226630955934525\n",
            "Train epoch - Accuracy: 0.2715107913669065 Loss: 0.11386414785393709 Corrects: 1887\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.11289524286985397\n",
            "Train step - Step 290, Loss 0.11178285628557205\n",
            "Train step - Step 300, Loss 0.12390243262052536\n",
            "Train step - Step 310, Loss 0.11132893711328506\n",
            "Train step - Step 320, Loss 0.10947875678539276\n",
            "Train epoch - Accuracy: 0.2840287769784173 Loss: 0.11294395544760519 Corrects: 1974\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.10851984471082687\n",
            "Train step - Step 340, Loss 0.11461995542049408\n",
            "Train step - Step 350, Loss 0.11471790075302124\n",
            "Train step - Step 360, Loss 0.11315198242664337\n",
            "Train step - Step 370, Loss 0.11139710992574692\n",
            "Train step - Step 380, Loss 0.11600710451602936\n",
            "Train epoch - Accuracy: 0.2997122302158273 Loss: 0.11242575993855222 Corrects: 2083\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.11447665840387344\n",
            "Train step - Step 400, Loss 0.10988873243331909\n",
            "Train step - Step 410, Loss 0.11441420763731003\n",
            "Train step - Step 420, Loss 0.108719602227211\n",
            "Train step - Step 430, Loss 0.1079876497387886\n",
            "Train epoch - Accuracy: 0.3185611510791367 Loss: 0.11175928070819635 Corrects: 2214\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10862608999013901\n",
            "Train step - Step 450, Loss 0.10980255156755447\n",
            "Train step - Step 460, Loss 0.10728000104427338\n",
            "Train step - Step 470, Loss 0.11303501576185226\n",
            "Train step - Step 480, Loss 0.11312711238861084\n",
            "Train step - Step 490, Loss 0.11707466095685959\n",
            "Train epoch - Accuracy: 0.33050359712230215 Loss: 0.11170547083127413 Corrects: 2297\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.11319267004728317\n",
            "Train step - Step 510, Loss 0.11730580776929855\n",
            "Train step - Step 520, Loss 0.11476722359657288\n",
            "Train step - Step 530, Loss 0.11145532876253128\n",
            "Train step - Step 540, Loss 0.11096905916929245\n",
            "Train epoch - Accuracy: 0.34014388489208636 Loss: 0.11099676878117828 Corrects: 2364\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.11501114070415497\n",
            "Train step - Step 560, Loss 0.11128076165914536\n",
            "Train step - Step 570, Loss 0.10295035690069199\n",
            "Train step - Step 580, Loss 0.11827278882265091\n",
            "Train step - Step 590, Loss 0.10658615827560425\n",
            "Train step - Step 600, Loss 0.11134421825408936\n",
            "Train epoch - Accuracy: 0.3493525179856115 Loss: 0.11101375310112246 Corrects: 2428\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.11258593946695328\n",
            "Train step - Step 620, Loss 0.11102600395679474\n",
            "Train step - Step 630, Loss 0.1056322455406189\n",
            "Train step - Step 640, Loss 0.10792068392038345\n",
            "Train step - Step 650, Loss 0.10730436444282532\n",
            "Train epoch - Accuracy: 0.36071942446043165 Loss: 0.11020796057774866 Corrects: 2507\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.11066558212041855\n",
            "Train step - Step 670, Loss 0.11288904398679733\n",
            "Train step - Step 680, Loss 0.10426728427410126\n",
            "Train step - Step 690, Loss 0.10744098573923111\n",
            "Train step - Step 700, Loss 0.10602428019046783\n",
            "Train step - Step 710, Loss 0.10712360590696335\n",
            "Train epoch - Accuracy: 0.3702158273381295 Loss: 0.110066921421521 Corrects: 2573\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10937181115150452\n",
            "Train step - Step 730, Loss 0.10786282271146774\n",
            "Train step - Step 740, Loss 0.10914939641952515\n",
            "Train step - Step 750, Loss 0.10800999402999878\n",
            "Train step - Step 760, Loss 0.11427739262580872\n",
            "Train epoch - Accuracy: 0.38 Loss: 0.10964772231930452 Corrects: 2641\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.11638180166482925\n",
            "Train step - Step 780, Loss 0.10777569562196732\n",
            "Train step - Step 790, Loss 0.11205922067165375\n",
            "Train step - Step 800, Loss 0.10944947600364685\n",
            "Train step - Step 810, Loss 0.10887950658798218\n",
            "Train step - Step 820, Loss 0.11191801726818085\n",
            "Train epoch - Accuracy: 0.381726618705036 Loss: 0.10984239157369668 Corrects: 2653\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.10552877932786942\n",
            "Train step - Step 840, Loss 0.11035443097352982\n",
            "Train step - Step 850, Loss 0.11135648936033249\n",
            "Train step - Step 860, Loss 0.11085517704486847\n",
            "Train step - Step 870, Loss 0.106376051902771\n",
            "Train epoch - Accuracy: 0.39611510791366905 Loss: 0.10980070103415482 Corrects: 2753\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.11063484102487564\n",
            "Train step - Step 890, Loss 0.10849412530660629\n",
            "Train step - Step 900, Loss 0.10343017429113388\n",
            "Train step - Step 910, Loss 0.11363963037729263\n",
            "Train step - Step 920, Loss 0.10777392983436584\n",
            "Train step - Step 930, Loss 0.10521790385246277\n",
            "Train epoch - Accuracy: 0.4015827338129496 Loss: 0.10909019065417831 Corrects: 2791\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.10410507768392563\n",
            "Train step - Step 950, Loss 0.1135193258523941\n",
            "Train step - Step 960, Loss 0.11086706072092056\n",
            "Train step - Step 970, Loss 0.1022353246808052\n",
            "Train step - Step 980, Loss 0.11069587618112564\n",
            "Train epoch - Accuracy: 0.4084892086330935 Loss: 0.109110687636643 Corrects: 2839\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.10590897500514984\n",
            "Train step - Step 1000, Loss 0.10960717499256134\n",
            "Train step - Step 1010, Loss 0.11054231971502304\n",
            "Train step - Step 1020, Loss 0.10458037257194519\n",
            "Train step - Step 1030, Loss 0.1078043207526207\n",
            "Train step - Step 1040, Loss 0.1055213063955307\n",
            "Train epoch - Accuracy: 0.4148201438848921 Loss: 0.10899393879681182 Corrects: 2883\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.10847651958465576\n",
            "Train step - Step 1060, Loss 0.10324785113334656\n",
            "Train step - Step 1070, Loss 0.11076927930116653\n",
            "Train step - Step 1080, Loss 0.1110614761710167\n",
            "Train step - Step 1090, Loss 0.10932018607854843\n",
            "Train epoch - Accuracy: 0.41640287769784173 Loss: 0.10916411236893359 Corrects: 2894\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10988724231719971\n",
            "Train step - Step 1110, Loss 0.10882183909416199\n",
            "Train step - Step 1120, Loss 0.10858450829982758\n",
            "Train step - Step 1130, Loss 0.10944074392318726\n",
            "Train step - Step 1140, Loss 0.10224276036024094\n",
            "Train step - Step 1150, Loss 0.11240540444850922\n",
            "Train epoch - Accuracy: 0.42805755395683454 Loss: 0.10879161368171088 Corrects: 2975\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10751443356275558\n",
            "Train step - Step 1170, Loss 0.10611221194267273\n",
            "Train step - Step 1180, Loss 0.107391357421875\n",
            "Train step - Step 1190, Loss 0.1083715483546257\n",
            "Train step - Step 1200, Loss 0.1022985652089119\n",
            "Train epoch - Accuracy: 0.42762589928057554 Loss: 0.10849828185366212 Corrects: 2972\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.10936697572469711\n",
            "Train step - Step 1220, Loss 0.10384456068277359\n",
            "Train step - Step 1230, Loss 0.1111157238483429\n",
            "Train step - Step 1240, Loss 0.11170855909585953\n",
            "Train step - Step 1250, Loss 0.10750347375869751\n",
            "Train step - Step 1260, Loss 0.10777806490659714\n",
            "Train epoch - Accuracy: 0.43841726618705035 Loss: 0.10846541840824292 Corrects: 3047\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.10915259271860123\n",
            "Train step - Step 1280, Loss 0.10558263212442398\n",
            "Train step - Step 1290, Loss 0.1049615889787674\n",
            "Train step - Step 1300, Loss 0.10334455221891403\n",
            "Train step - Step 1310, Loss 0.11393915116786957\n",
            "Train epoch - Accuracy: 0.44071942446043166 Loss: 0.10813819582299362 Corrects: 3063\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.10948723554611206\n",
            "Train step - Step 1330, Loss 0.10940002650022507\n",
            "Train step - Step 1340, Loss 0.10240117460489273\n",
            "Train step - Step 1350, Loss 0.10407703369855881\n",
            "Train step - Step 1360, Loss 0.10451992601156235\n",
            "Train step - Step 1370, Loss 0.10926217585802078\n",
            "Train epoch - Accuracy: 0.4474820143884892 Loss: 0.10784845602598121 Corrects: 3110\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.10692956298589706\n",
            "Train step - Step 1390, Loss 0.10991809517145157\n",
            "Train step - Step 1400, Loss 0.1044841855764389\n",
            "Train step - Step 1410, Loss 0.10726053267717361\n",
            "Train step - Step 1420, Loss 0.1067446917295456\n",
            "Train epoch - Accuracy: 0.4469064748201439 Loss: 0.10786004447465321 Corrects: 3106\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.10831305384635925\n",
            "Train step - Step 1440, Loss 0.10950174182653427\n",
            "Train step - Step 1450, Loss 0.1047799289226532\n",
            "Train step - Step 1460, Loss 0.10619644075632095\n",
            "Train step - Step 1470, Loss 0.10457178205251694\n",
            "Train step - Step 1480, Loss 0.10805624723434448\n",
            "Train epoch - Accuracy: 0.4486330935251799 Loss: 0.10789981774288973 Corrects: 3118\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.10641077905893326\n",
            "Train step - Step 1500, Loss 0.1042727679014206\n",
            "Train step - Step 1510, Loss 0.10809000581502914\n",
            "Train step - Step 1520, Loss 0.10471265017986298\n",
            "Train step - Step 1530, Loss 0.11293952167034149\n",
            "Train epoch - Accuracy: 0.4520863309352518 Loss: 0.10827980230394885 Corrects: 3142\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.10468224436044693\n",
            "Train step - Step 1550, Loss 0.10379858314990997\n",
            "Train step - Step 1560, Loss 0.10291475802659988\n",
            "Train step - Step 1570, Loss 0.10899843275547028\n",
            "Train step - Step 1580, Loss 0.10869476199150085\n",
            "Train step - Step 1590, Loss 0.11127480119466782\n",
            "Train epoch - Accuracy: 0.46345323741007194 Loss: 0.1076992332635166 Corrects: 3221\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.10699468106031418\n",
            "Train step - Step 1610, Loss 0.11144952476024628\n",
            "Train step - Step 1620, Loss 0.10988409072160721\n",
            "Train step - Step 1630, Loss 0.10252410918474197\n",
            "Train step - Step 1640, Loss 0.11075979471206665\n",
            "Train epoch - Accuracy: 0.4680575539568345 Loss: 0.10719969454000322 Corrects: 3253\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.10623514652252197\n",
            "Train step - Step 1660, Loss 0.10833185166120529\n",
            "Train step - Step 1670, Loss 0.10042989253997803\n",
            "Train step - Step 1680, Loss 0.10430184751749039\n",
            "Train step - Step 1690, Loss 0.10711782425642014\n",
            "Train step - Step 1700, Loss 0.10884369164705276\n",
            "Train epoch - Accuracy: 0.46489208633093526 Loss: 0.10747646863511998 Corrects: 3231\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.10769577324390411\n",
            "Train step - Step 1720, Loss 0.10585575550794601\n",
            "Train step - Step 1730, Loss 0.10965635627508163\n",
            "Train step - Step 1740, Loss 0.10399632900953293\n",
            "Train step - Step 1750, Loss 0.11032389849424362\n",
            "Train epoch - Accuracy: 0.46489208633093526 Loss: 0.107204982016584 Corrects: 3231\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.10365395992994308\n",
            "Train step - Step 1770, Loss 0.10599410533905029\n",
            "Train step - Step 1780, Loss 0.10530189424753189\n",
            "Train step - Step 1790, Loss 0.1059897318482399\n",
            "Train step - Step 1800, Loss 0.11096874624490738\n",
            "Train step - Step 1810, Loss 0.10916835069656372\n",
            "Train epoch - Accuracy: 0.4785611510791367 Loss: 0.107425279593725 Corrects: 3326\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.10638442635536194\n",
            "Train step - Step 1830, Loss 0.1139182448387146\n",
            "Train step - Step 1840, Loss 0.10573162138462067\n",
            "Train step - Step 1850, Loss 0.10492657870054245\n",
            "Train step - Step 1860, Loss 0.10962823778390884\n",
            "Train epoch - Accuracy: 0.48115107913669064 Loss: 0.10707856850872795 Corrects: 3344\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.10702270269393921\n",
            "Train step - Step 1880, Loss 0.10332884639501572\n",
            "Train step - Step 1890, Loss 0.10882677882909775\n",
            "Train step - Step 1900, Loss 0.10891064256429672\n",
            "Train step - Step 1910, Loss 0.10617412626743317\n",
            "Train step - Step 1920, Loss 0.10459358990192413\n",
            "Train epoch - Accuracy: 0.4823021582733813 Loss: 0.10680996696726024 Corrects: 3352\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.09944601356983185\n",
            "Train step - Step 1940, Loss 0.11007899045944214\n",
            "Train step - Step 1950, Loss 0.10211805254220963\n",
            "Train step - Step 1960, Loss 0.10990987718105316\n",
            "Train step - Step 1970, Loss 0.10205283015966415\n",
            "Train epoch - Accuracy: 0.48848920863309353 Loss: 0.10632445169224156 Corrects: 3395\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.11198601871728897\n",
            "Train step - Step 1990, Loss 0.10509084910154343\n",
            "Train step - Step 2000, Loss 0.1079331636428833\n",
            "Train step - Step 2010, Loss 0.1025017648935318\n",
            "Train step - Step 2020, Loss 0.10418720543384552\n",
            "Train step - Step 2030, Loss 0.10260757803916931\n",
            "Train epoch - Accuracy: 0.49237410071942445 Loss: 0.1065158180624461 Corrects: 3422\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.10869959741830826\n",
            "Train step - Step 2050, Loss 0.10322649031877518\n",
            "Train step - Step 2060, Loss 0.10668233036994934\n",
            "Train step - Step 2070, Loss 0.10223214328289032\n",
            "Train step - Step 2080, Loss 0.10680133104324341\n",
            "Train epoch - Accuracy: 0.4956834532374101 Loss: 0.1065323098572038 Corrects: 3445\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.10482995212078094\n",
            "Train step - Step 2100, Loss 0.09853992611169815\n",
            "Train step - Step 2110, Loss 0.10484384745359421\n",
            "Train step - Step 2120, Loss 0.10426056385040283\n",
            "Train step - Step 2130, Loss 0.1032155230641365\n",
            "Train step - Step 2140, Loss 0.11053260415792465\n",
            "Train epoch - Accuracy: 0.501726618705036 Loss: 0.1065673346635249 Corrects: 3487\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.10909273475408554\n",
            "Train step - Step 2160, Loss 0.10038762539625168\n",
            "Train step - Step 2170, Loss 0.10319071263074875\n",
            "Train step - Step 2180, Loss 0.10952822864055634\n",
            "Train step - Step 2190, Loss 0.10421600937843323\n",
            "Train epoch - Accuracy: 0.49712230215827335 Loss: 0.1060817199580961 Corrects: 3455\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.10251683741807938\n",
            "Train step - Step 2210, Loss 0.10549671202898026\n",
            "Train step - Step 2220, Loss 0.10306166857481003\n",
            "Train step - Step 2230, Loss 0.10512357205152512\n",
            "Train step - Step 2240, Loss 0.10781046003103256\n",
            "Train step - Step 2250, Loss 0.10522529482841492\n",
            "Train epoch - Accuracy: 0.5074820143884892 Loss: 0.10642759599059606 Corrects: 3527\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.10458233207464218\n",
            "Train step - Step 2270, Loss 0.10588622093200684\n",
            "Train step - Step 2280, Loss 0.10347797721624374\n",
            "Train step - Step 2290, Loss 0.10429985076189041\n",
            "Train step - Step 2300, Loss 0.1059543788433075\n",
            "Train epoch - Accuracy: 0.5044604316546762 Loss: 0.10611717989976457 Corrects: 3506\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.1110401526093483\n",
            "Train step - Step 2320, Loss 0.1175244078040123\n",
            "Train step - Step 2330, Loss 0.10994697362184525\n",
            "Train step - Step 2340, Loss 0.10375949740409851\n",
            "Train step - Step 2350, Loss 0.11188794672489166\n",
            "Train step - Step 2360, Loss 0.1090795248746872\n",
            "Train epoch - Accuracy: 0.5100719424460431 Loss: 0.10610116195978878 Corrects: 3545\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.1100773960351944\n",
            "Train step - Step 2380, Loss 0.10836124420166016\n",
            "Train step - Step 2390, Loss 0.10175775736570358\n",
            "Train step - Step 2400, Loss 0.10418502986431122\n",
            "Train step - Step 2410, Loss 0.0995899960398674\n",
            "Train epoch - Accuracy: 0.5109352517985611 Loss: 0.10594084232402362 Corrects: 3551\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.1026524007320404\n",
            "Train step - Step 2430, Loss 0.10780471563339233\n",
            "Train step - Step 2440, Loss 0.10661326348781586\n",
            "Train step - Step 2450, Loss 0.10612910985946655\n",
            "Train step - Step 2460, Loss 0.10469069331884384\n",
            "Train step - Step 2470, Loss 0.1019456759095192\n",
            "Train epoch - Accuracy: 0.518273381294964 Loss: 0.10593980796688753 Corrects: 3602\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.10253409296274185\n",
            "Train step - Step 2490, Loss 0.10301919281482697\n",
            "Train step - Step 2500, Loss 0.10767341405153275\n",
            "Train step - Step 2510, Loss 0.10273522138595581\n",
            "Train step - Step 2520, Loss 0.10482942312955856\n",
            "Train epoch - Accuracy: 0.5179856115107914 Loss: 0.105993977105446 Corrects: 3600\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.10983860492706299\n",
            "Train step - Step 2540, Loss 0.10833217948675156\n",
            "Train step - Step 2550, Loss 0.10909033566713333\n",
            "Train step - Step 2560, Loss 0.11145397275686264\n",
            "Train step - Step 2570, Loss 0.10487928986549377\n",
            "Train step - Step 2580, Loss 0.10566049069166183\n",
            "Train epoch - Accuracy: 0.5202877697841727 Loss: 0.10564585519565953 Corrects: 3616\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.10907354950904846\n",
            "Train step - Step 2600, Loss 0.1056540235877037\n",
            "Train step - Step 2610, Loss 0.1073165163397789\n",
            "Train step - Step 2620, Loss 0.11185795068740845\n",
            "Train step - Step 2630, Loss 0.10634885728359222\n",
            "Train epoch - Accuracy: 0.5237410071942447 Loss: 0.10567927594021928 Corrects: 3640\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.11010042577981949\n",
            "Train step - Step 2650, Loss 0.10321551561355591\n",
            "Train step - Step 2660, Loss 0.10750718414783478\n",
            "Train step - Step 2670, Loss 0.10645417124032974\n",
            "Train step - Step 2680, Loss 0.10634351521730423\n",
            "Train step - Step 2690, Loss 0.10164505988359451\n",
            "Train epoch - Accuracy: 0.5224460431654676 Loss: 0.10577943728553306 Corrects: 3631\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.10289950668811798\n",
            "Train step - Step 2710, Loss 0.1066782996058464\n",
            "Train step - Step 2720, Loss 0.1032189279794693\n",
            "Train step - Step 2730, Loss 0.102934829890728\n",
            "Train step - Step 2740, Loss 0.10535632818937302\n",
            "Train epoch - Accuracy: 0.5384172661870503 Loss: 0.10453306268445021 Corrects: 3742\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.09703701734542847\n",
            "Train step - Step 2760, Loss 0.10150238871574402\n",
            "Train step - Step 2770, Loss 0.09918868541717529\n",
            "Train step - Step 2780, Loss 0.10595586895942688\n",
            "Train step - Step 2790, Loss 0.1032341718673706\n",
            "Train step - Step 2800, Loss 0.10154412686824799\n",
            "Train epoch - Accuracy: 0.5424460431654676 Loss: 0.10425035757555379 Corrects: 3770\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.10567530244588852\n",
            "Train step - Step 2820, Loss 0.10129272937774658\n",
            "Train step - Step 2830, Loss 0.10302989184856415\n",
            "Train step - Step 2840, Loss 0.10230647772550583\n",
            "Train step - Step 2850, Loss 0.10765980929136276\n",
            "Train epoch - Accuracy: 0.5394244604316547 Loss: 0.10445779107671847 Corrects: 3749\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.09994404762983322\n",
            "Train step - Step 2870, Loss 0.10503305494785309\n",
            "Train step - Step 2880, Loss 0.10360874235630035\n",
            "Train step - Step 2890, Loss 0.1114845871925354\n",
            "Train step - Step 2900, Loss 0.10729218274354935\n",
            "Train step - Step 2910, Loss 0.10716747492551804\n",
            "Train epoch - Accuracy: 0.5361151079136691 Loss: 0.10437026945163878 Corrects: 3726\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.10507721453905106\n",
            "Train step - Step 2930, Loss 0.100873202085495\n",
            "Train step - Step 2940, Loss 0.10304510593414307\n",
            "Train step - Step 2950, Loss 0.10270886868238449\n",
            "Train step - Step 2960, Loss 0.1004813015460968\n",
            "Train epoch - Accuracy: 0.5351079136690647 Loss: 0.10403672170081585 Corrects: 3719\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.10419385880231857\n",
            "Train step - Step 2980, Loss 0.10447675734758377\n",
            "Train step - Step 2990, Loss 0.10842657834291458\n",
            "Train step - Step 3000, Loss 0.1082792654633522\n",
            "Train step - Step 3010, Loss 0.09951188415288925\n",
            "Train step - Step 3020, Loss 0.10397721081972122\n",
            "Train epoch - Accuracy: 0.5428776978417266 Loss: 0.10415329255860487 Corrects: 3773\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.09942469745874405\n",
            "Train step - Step 3040, Loss 0.10457368195056915\n",
            "Train step - Step 3050, Loss 0.10910530388355255\n",
            "Train step - Step 3060, Loss 0.10535210371017456\n",
            "Train step - Step 3070, Loss 0.10681068897247314\n",
            "Train epoch - Accuracy: 0.5361151079136691 Loss: 0.10449453427422818 Corrects: 3726\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.10506831854581833\n",
            "Train step - Step 3090, Loss 0.10500160604715347\n",
            "Train step - Step 3100, Loss 0.10736943036317825\n",
            "Train step - Step 3110, Loss 0.10472376644611359\n",
            "Train step - Step 3120, Loss 0.10191050916910172\n",
            "Train step - Step 3130, Loss 0.09908831119537354\n",
            "Train epoch - Accuracy: 0.5443165467625899 Loss: 0.10439054412378682 Corrects: 3783\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.10016041249036789\n",
            "Train step - Step 3150, Loss 0.10834866017103195\n",
            "Train step - Step 3160, Loss 0.101277656853199\n",
            "Train step - Step 3170, Loss 0.10087389498949051\n",
            "Train step - Step 3180, Loss 0.10065266489982605\n",
            "Train epoch - Accuracy: 0.5365467625899281 Loss: 0.10424083875023203 Corrects: 3729\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.10461262613534927\n",
            "Train step - Step 3200, Loss 0.1012769564986229\n",
            "Train step - Step 3210, Loss 0.10552255809307098\n",
            "Train step - Step 3220, Loss 0.10599397867918015\n",
            "Train step - Step 3230, Loss 0.1055767610669136\n",
            "Train step - Step 3240, Loss 0.10167320817708969\n",
            "Train epoch - Accuracy: 0.539568345323741 Loss: 0.10405543103492518 Corrects: 3750\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.10442639142274857\n",
            "Train step - Step 3260, Loss 0.1076677069067955\n",
            "Train step - Step 3270, Loss 0.1032116487622261\n",
            "Train step - Step 3280, Loss 0.1076711043715477\n",
            "Train step - Step 3290, Loss 0.10469046235084534\n",
            "Train epoch - Accuracy: 0.537410071942446 Loss: 0.10442194054667041 Corrects: 3735\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.10465382784605026\n",
            "Train step - Step 3310, Loss 0.10854324698448181\n",
            "Train step - Step 3320, Loss 0.10439377278089523\n",
            "Train step - Step 3330, Loss 0.11102534830570221\n",
            "Train step - Step 3340, Loss 0.10497969388961792\n",
            "Train step - Step 3350, Loss 0.10459490120410919\n",
            "Train epoch - Accuracy: 0.5362589928057554 Loss: 0.10454979485101837 Corrects: 3727\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.09972986578941345\n",
            "Train step - Step 3370, Loss 0.106480173766613\n",
            "Train step - Step 3380, Loss 0.10329771041870117\n",
            "Train step - Step 3390, Loss 0.10327710956335068\n",
            "Train step - Step 3400, Loss 0.10726167261600494\n",
            "Train epoch - Accuracy: 0.5407194244604316 Loss: 0.10429192688182104 Corrects: 3758\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.10106861591339111\n",
            "Train step - Step 3420, Loss 0.10791169106960297\n",
            "Train step - Step 3430, Loss 0.1025119498372078\n",
            "Train step - Step 3440, Loss 0.1017877385020256\n",
            "Train step - Step 3450, Loss 0.1052020937204361\n",
            "Train step - Step 3460, Loss 0.10427732020616531\n",
            "Train epoch - Accuracy: 0.5410071942446043 Loss: 0.10426930253239844 Corrects: 3760\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.1023046150803566\n",
            "Train step - Step 3480, Loss 0.09996762126684189\n",
            "Train step - Step 3490, Loss 0.10374172776937485\n",
            "Train step - Step 3500, Loss 0.10735546052455902\n",
            "Train step - Step 3510, Loss 0.10694684088230133\n",
            "Train epoch - Accuracy: 0.5458992805755396 Loss: 0.10404540001059608 Corrects: 3794\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.10264456272125244\n",
            "Train step - Step 3530, Loss 0.10061927139759064\n",
            "Train step - Step 3540, Loss 0.10020523518323898\n",
            "Train step - Step 3550, Loss 0.10114647448062897\n",
            "Train step - Step 3560, Loss 0.10111887007951736\n",
            "Train step - Step 3570, Loss 0.10169950127601624\n",
            "Train epoch - Accuracy: 0.5420143884892087 Loss: 0.10415723025155582 Corrects: 3767\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.10668715089559555\n",
            "Train step - Step 3590, Loss 0.10134349763393402\n",
            "Train step - Step 3600, Loss 0.10726932436227798\n",
            "Train step - Step 3610, Loss 0.1044187843799591\n",
            "Train step - Step 3620, Loss 0.09563486278057098\n",
            "Train epoch - Accuracy: 0.537841726618705 Loss: 0.10382699017902072 Corrects: 3738\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.09867283701896667\n",
            "Train step - Step 3640, Loss 0.10767408460378647\n",
            "Train step - Step 3650, Loss 0.10714616626501083\n",
            "Train step - Step 3660, Loss 0.09764159470796585\n",
            "Train step - Step 3670, Loss 0.10620222985744476\n",
            "Train step - Step 3680, Loss 0.10028119385242462\n",
            "Train epoch - Accuracy: 0.5389928057553957 Loss: 0.10393921988259117 Corrects: 3746\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.1080852746963501\n",
            "Train step - Step 3700, Loss 0.0963902398943901\n",
            "Train step - Step 3710, Loss 0.10441242158412933\n",
            "Train step - Step 3720, Loss 0.10664631426334381\n",
            "Train step - Step 3730, Loss 0.10232873260974884\n",
            "Train epoch - Accuracy: 0.5418705035971223 Loss: 0.103852283073415 Corrects: 3766\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.10500041395425797\n",
            "Train step - Step 3750, Loss 0.10481728613376617\n",
            "Train step - Step 3760, Loss 0.09624437987804413\n",
            "Train step - Step 3770, Loss 0.10794465243816376\n",
            "Train step - Step 3780, Loss 0.10786116868257523\n",
            "Train step - Step 3790, Loss 0.09883418679237366\n",
            "Train epoch - Accuracy: 0.5548201438848921 Loss: 0.10389299639266172 Corrects: 3856\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.10708127915859222\n",
            "Train step - Step 3810, Loss 0.10218454152345657\n",
            "Train step - Step 3820, Loss 0.09390666335821152\n",
            "Train step - Step 3830, Loss 0.1038324385881424\n",
            "Train step - Step 3840, Loss 0.10126802325248718\n",
            "Train epoch - Accuracy: 0.543884892086331 Loss: 0.10390674102006199 Corrects: 3780\n",
            "Training finished in 396.42685079574585 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0, 79, 71, 63, 26, 14, 2, 73, 41, 56, 48]\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238abb890>\n",
            "Constructing exemplars of class 79\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [23707, 12596, 44563, 32829, 4576, 10213, 4225, 19875, 47658, 21411, 14467, 44576, 42255, 14468, 2957, 34523, 6720, 43134, 4341, 640, 4827, 3813, 23184, 34274, 37348, 3269, 24547, 29058, 15651, 22421, 23272, 46969, 29585]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22318d2690>\n",
            "Constructing exemplars of class 71\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [22815, 30157, 1490, 28537, 18474, 11552, 47861, 20117, 3266, 44903, 8651, 20100, 27048, 16023, 42745, 20102, 16822, 49339, 23126, 18420, 47656, 28664, 1069, 19500, 32559, 49339, 45036, 5858, 23016, 37450, 16822, 19406, 9672]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2231568210>\n",
            "Constructing exemplars of class 63\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [41334, 13280, 8717, 4373, 23379, 44665, 10339, 40687, 44657, 45083, 15217, 2089, 46909, 24316, 19815, 40112, 45813, 35155, 9626, 5685, 3027, 9276, 43265, 26448, 21204, 21787, 29802, 24112, 13430, 24060, 39337, 32414, 31700]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224468b510>\n",
            "Constructing exemplars of class 26\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [46538, 29921, 39061, 11743, 31990, 14243, 34956, 47993, 38835, 9998, 7716, 8820, 31550, 42987, 842, 31508, 14547, 33088, 8917, 44882, 29572, 3328, 33567, 5200, 26159, 26285, 36635, 9616, 40415, 10702, 13348, 1530, 5769]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e8f490>\n",
            "Constructing exemplars of class 14\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [621, 17296, 7478, 22266, 48283, 37639, 46164, 2183, 3395, 13340, 12672, 44069, 1935, 33449, 7357, 28863, 16853, 14120, 45748, 22593, 44242, 36486, 48774, 42961, 16988, 27878, 41080, 14920, 30588, 7674, 19122, 37220, 22273]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e9b490>\n",
            "Constructing exemplars of class 2\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [1106, 15946, 31638, 34814, 3878, 18029, 11054, 19819, 37214, 30134, 37960, 2925, 23185, 1511, 25496, 33194, 1895, 46991, 382, 22417, 11903, 13956, 23678, 2352, 37758, 42302, 44202, 31151, 13017, 16964, 41550, 7066, 2051]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224468b510>\n",
            "Constructing exemplars of class 73\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [22908, 1063, 11323, 17274, 20665, 6651, 47075, 31895, 1423, 7541, 21222, 48102, 2117, 25797, 11532, 28792, 18133, 24665, 45890, 31433, 48102, 18418, 45541, 44211, 8563, 4571, 20054, 7036, 17803, 23605, 15253, 14219, 12658]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238219550>\n",
            "Constructing exemplars of class 41\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [14791, 22570, 41474, 37036, 45574, 36671, 43099, 24615, 17358, 26081, 48727, 45619, 34706, 19253, 26367, 2050, 40716, 42178, 41641, 26608, 6846, 1307, 21843, 33958, 27020, 7713, 998, 40619, 11246, 42081, 1307, 13530, 15782]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223824dc50>\n",
            "Constructing exemplars of class 56\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [12244, 38779, 12972, 26410, 4314, 38889, 15000, 24599, 41268, 29639, 31995, 30262, 42345, 13025, 16420, 21570, 14423, 29524, 4493, 39599, 5433, 27676, 37910, 47360, 9176, 41657, 23447, 39721, 28791, 20038, 15456, 15636, 17822]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2244394210>\n",
            "Constructing exemplars of class 48\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [4654, 45191, 25645, 19743, 20177, 7719, 46288, 41294, 36150, 49492, 195, 11625, 23587, 36957, 9328, 46996, 24182, 2525, 16574, 22581, 6687, 47302, 41800, 33870, 23744, 14306, 48760, 13009, 251, 195, 21067, 152, 4654]\n",
            "x train:  [ 0.03226107 -0.14881769 -0.12394965 -0.08490223 -0.04953309 -0.13368255\n",
            " -0.05037773 -0.12670901 -0.15285337 -0.04527627 -0.19681294  0.03760378\n",
            " -0.08676144 -0.16244866 -0.07754248 -0.14214084 -0.15155344 -0.0814168\n",
            " -0.11174899 -0.09487329 -0.12625481 -0.08111097 -0.16640784 -0.1475813\n",
            " -0.08610184 -0.07939965 -0.11572324 -0.07093013 -0.11739716 -0.17098913\n",
            " -0.11272877 -0.10268337 -0.1794504  -0.13986552 -0.2316634  -0.02480628\n",
            " -0.00627354 -0.17499778 -0.03010319 -0.12563132 -0.161612   -0.09257635\n",
            " -0.12037809 -0.15672836 -0.14632572 -0.10671978 -0.13955654 -0.09691554\n",
            " -0.18812704 -0.16803642 -0.12820397 -0.10843623 -0.03351827 -0.14154449\n",
            " -0.1833487  -0.18828349 -0.14845298 -0.15053593 -0.08673934 -0.21225375]\n",
            "y_train:  [tensor([33]), tensor([92]), tensor([16]), tensor([88]), tensor([88]), tensor([57]), tensor([27]), tensor([78]), tensor([62]), tensor([58]), tensor([91]), tensor([56]), tensor([46]), tensor([95]), tensor([86]), tensor([63]), tensor([10]), tensor([60]), tensor([98]), tensor([37]), tensor([30]), tensor([88]), tensor([3]), tensor([24]), tensor([60]), tensor([81]), tensor([80]), tensor([67]), tensor([73]), tensor([53]), tensor([57]), tensor([58]), tensor([52]), tensor([60]), tensor([91]), tensor([7]), tensor([67]), tensor([12]), tensor([88]), tensor([65]), tensor([7]), tensor([92]), tensor([63]), tensor([58]), tensor([65]), tensor([31]), tensor([57]), tensor([48]), tensor([98]), tensor([33]), tensor([96]), tensor([31]), tensor([14]), tensor([48]), tensor([98]), tensor([62]), tensor([50]), tensor([59]), tensor([36]), tensor([41]), tensor([50]), tensor([53]), tensor([92]), tensor([96]), tensor([21]), tensor([35]), tensor([10]), tensor([49]), tensor([77]), tensor([79]), tensor([16]), tensor([49]), tensor([68]), tensor([94]), tensor([30]), tensor([67]), tensor([26]), tensor([67]), tensor([33]), tensor([34]), tensor([62]), tensor([21]), tensor([35]), tensor([33]), tensor([31]), tensor([81]), tensor([97]), tensor([73]), tensor([36]), tensor([71]), tensor([80]), tensor([46]), tensor([56]), tensor([26]), tensor([26]), tensor([27]), tensor([68]), tensor([68]), tensor([46]), tensor([48]), tensor([62]), tensor([41]), tensor([21]), tensor([71]), tensor([10]), tensor([71]), tensor([34]), tensor([81]), tensor([11]), tensor([32]), tensor([46]), tensor([60]), tensor([11]), tensor([94]), tensor([11]), tensor([72]), tensor([82]), tensor([88]), tensor([48]), tensor([65]), tensor([68]), tensor([34]), tensor([58]), tensor([58]), tensor([24]), tensor([50]), tensor([97]), tensor([41]), tensor([31]), tensor([60]), tensor([78]), tensor([36]), tensor([26]), tensor([14]), tensor([60]), tensor([14]), tensor([11]), tensor([86]), tensor([80]), tensor([16]), tensor([30]), tensor([86]), tensor([67]), tensor([14]), tensor([16]), tensor([25]), tensor([77]), tensor([56]), tensor([80]), tensor([72]), tensor([16]), tensor([71]), tensor([2]), tensor([73]), tensor([37]), tensor([46]), tensor([73]), tensor([58]), tensor([79]), tensor([37]), tensor([82]), tensor([77]), tensor([73]), tensor([34]), tensor([69]), tensor([77]), tensor([36]), tensor([91]), tensor([16]), tensor([48]), tensor([88]), tensor([98]), tensor([78]), tensor([80]), tensor([33]), tensor([98]), tensor([65]), tensor([80]), tensor([31]), tensor([37]), tensor([49]), tensor([97]), tensor([59]), tensor([80]), tensor([50]), tensor([14]), tensor([41]), tensor([50]), tensor([60]), tensor([45]), tensor([36]), tensor([7]), tensor([11]), tensor([45]), tensor([53]), tensor([58]), tensor([91]), tensor([95]), tensor([36]), tensor([24]), tensor([50]), tensor([88]), tensor([60]), tensor([12]), tensor([73]), tensor([81]), tensor([58]), tensor([67]), tensor([63]), tensor([91]), tensor([81]), tensor([96]), tensor([73]), tensor([3]), tensor([33]), tensor([95]), tensor([96]), tensor([36]), tensor([13]), tensor([57]), tensor([36]), tensor([68]), tensor([72]), tensor([53]), tensor([35]), tensor([60]), tensor([26]), tensor([92]), tensor([16]), tensor([96]), tensor([82]), tensor([36]), tensor([11]), tensor([57]), tensor([26]), tensor([73]), tensor([92]), tensor([56]), tensor([45]), tensor([56]), tensor([41]), tensor([65]), tensor([41]), tensor([96]), tensor([26]), tensor([35]), tensor([41]), tensor([60]), tensor([80]), tensor([80]), tensor([0]), tensor([32]), tensor([41]), tensor([32]), tensor([77]), tensor([86]), tensor([62]), tensor([63]), tensor([41]), tensor([35]), tensor([88]), tensor([69]), tensor([94]), tensor([86]), tensor([34]), tensor([62]), tensor([14]), tensor([7]), tensor([96]), tensor([96]), tensor([96]), tensor([27]), tensor([53]), tensor([2]), tensor([2]), tensor([65]), tensor([91]), tensor([14]), tensor([32]), tensor([34]), tensor([31]), tensor([7]), tensor([10]), tensor([71]), tensor([58]), tensor([52]), tensor([12]), tensor([80]), tensor([14]), tensor([56]), tensor([86]), tensor([62]), tensor([16]), tensor([81]), tensor([33]), tensor([33]), tensor([11]), tensor([50]), tensor([73]), tensor([2]), tensor([25]), tensor([14]), tensor([0]), tensor([59]), tensor([79]), tensor([33]), tensor([37]), tensor([48]), tensor([81]), tensor([13]), tensor([14]), tensor([12]), tensor([79]), tensor([53]), tensor([88]), tensor([36]), tensor([71]), tensor([82]), tensor([92]), tensor([0]), tensor([57]), tensor([67]), tensor([53]), tensor([35]), tensor([52]), tensor([63]), tensor([60]), tensor([13]), tensor([98]), tensor([30]), tensor([97]), tensor([41]), tensor([31]), tensor([52]), tensor([91]), tensor([63]), tensor([62]), tensor([49]), tensor([62]), tensor([73]), tensor([53]), tensor([27]), tensor([77]), tensor([12]), tensor([27]), tensor([41]), tensor([7]), tensor([36]), tensor([45]), tensor([36]), tensor([45]), tensor([97]), tensor([49]), tensor([36]), tensor([78]), tensor([35]), tensor([49]), tensor([77]), tensor([68]), tensor([14]), tensor([78]), tensor([30]), tensor([80]), tensor([69]), tensor([82]), tensor([50]), tensor([97]), tensor([30]), tensor([95]), tensor([86]), tensor([69]), tensor([36]), tensor([21]), tensor([13]), tensor([53]), tensor([13]), tensor([2]), tensor([16]), tensor([25]), tensor([48]), tensor([78]), tensor([65]), tensor([92]), tensor([97]), tensor([86]), tensor([65]), tensor([21]), tensor([72]), tensor([88]), tensor([94]), tensor([94]), tensor([25]), tensor([91]), tensor([96]), tensor([82]), tensor([73]), tensor([12]), tensor([95]), tensor([58]), tensor([94]), tensor([50]), tensor([50]), tensor([16]), tensor([63]), tensor([96]), tensor([78]), tensor([69]), tensor([26]), tensor([3]), tensor([11]), tensor([7]), tensor([0]), tensor([71]), tensor([78]), tensor([56]), tensor([34]), tensor([59]), tensor([48]), tensor([45]), tensor([30]), tensor([24]), tensor([79]), tensor([86]), tensor([21]), tensor([72]), tensor([30]), tensor([26]), tensor([52]), tensor([7]), tensor([26]), tensor([72]), tensor([62]), tensor([59]), tensor([62]), tensor([45]), tensor([72]), tensor([2]), tensor([81]), tensor([13]), tensor([94]), tensor([3]), tensor([73]), tensor([86]), tensor([37]), tensor([95]), tensor([25]), tensor([45]), tensor([14]), tensor([41]), tensor([80]), tensor([56]), tensor([16]), tensor([2]), tensor([46]), tensor([77]), tensor([35]), tensor([13]), tensor([58]), tensor([69]), tensor([80]), tensor([52]), tensor([7]), tensor([58]), tensor([32]), tensor([77]), tensor([49]), tensor([48]), tensor([34]), tensor([59]), tensor([21]), tensor([41]), tensor([37]), tensor([68]), tensor([50]), tensor([81]), tensor([48]), tensor([82]), tensor([86]), tensor([68]), tensor([24]), tensor([77]), tensor([63]), tensor([78]), tensor([30]), tensor([25]), tensor([7]), tensor([88]), tensor([91]), tensor([24]), tensor([10]), tensor([48]), tensor([33]), tensor([21]), tensor([27]), tensor([10]), tensor([50]), tensor([78]), tensor([96]), tensor([56]), tensor([3]), tensor([3]), tensor([91]), tensor([97]), tensor([96]), tensor([67]), tensor([79]), tensor([27]), tensor([73]), tensor([72]), tensor([80]), tensor([3]), tensor([91]), tensor([72]), tensor([96]), tensor([95]), tensor([27]), tensor([94]), tensor([10]), tensor([72]), tensor([53]), tensor([34]), tensor([98]), tensor([65]), tensor([34]), tensor([57]), tensor([65]), tensor([7]), tensor([32]), tensor([98]), tensor([12]), tensor([7]), tensor([59]), tensor([27]), tensor([45]), tensor([35]), tensor([11]), tensor([92]), tensor([86]), tensor([25]), tensor([63]), tensor([30]), tensor([73]), tensor([48]), tensor([32]), tensor([46]), tensor([56]), tensor([81]), tensor([79]), tensor([13]), tensor([95]), tensor([68]), tensor([49]), tensor([94]), tensor([65]), tensor([68]), tensor([95]), tensor([3]), tensor([45]), tensor([16]), tensor([50]), tensor([80]), tensor([60]), tensor([94]), tensor([58]), tensor([67]), tensor([77]), tensor([35]), tensor([79]), tensor([25]), tensor([12]), tensor([49]), tensor([16]), tensor([71]), tensor([45]), tensor([30]), tensor([60]), tensor([34]), tensor([62]), tensor([45]), tensor([45]), tensor([80]), tensor([35]), tensor([94]), tensor([27]), tensor([27]), tensor([81]), tensor([79]), tensor([77]), tensor([45]), tensor([21]), tensor([41]), tensor([73]), tensor([33]), tensor([63]), tensor([71]), tensor([41]), tensor([25]), tensor([3]), tensor([77]), tensor([59]), tensor([98]), tensor([14]), tensor([96]), tensor([71]), tensor([71]), tensor([80]), tensor([7]), tensor([35]), tensor([25]), tensor([58]), tensor([67]), tensor([78]), tensor([31]), tensor([7]), tensor([97]), tensor([24]), tensor([86]), tensor([12]), tensor([57]), tensor([96]), tensor([37]), tensor([73]), tensor([2]), tensor([95]), tensor([3]), tensor([24]), tensor([77]), tensor([24]), tensor([46]), tensor([21]), tensor([16]), tensor([11]), tensor([59]), tensor([46]), tensor([73]), tensor([67]), tensor([82]), tensor([11]), tensor([41]), tensor([86]), tensor([35]), tensor([3]), tensor([95]), tensor([52]), tensor([63]), tensor([77]), tensor([10]), tensor([48]), tensor([31]), tensor([72]), tensor([13]), tensor([2]), tensor([91]), tensor([71]), tensor([77]), tensor([78]), tensor([52]), tensor([11]), tensor([73]), tensor([30]), tensor([62]), tensor([34]), tensor([13]), tensor([0]), tensor([35]), tensor([26]), tensor([71]), tensor([82]), tensor([95]), tensor([37]), tensor([79]), tensor([11]), tensor([41]), tensor([31]), tensor([3]), tensor([48]), tensor([59]), tensor([98]), tensor([2]), tensor([13]), tensor([69]), tensor([16]), tensor([65]), tensor([97]), tensor([58]), tensor([32]), tensor([71]), tensor([92]), tensor([10]), tensor([10]), tensor([21]), tensor([53]), tensor([59]), tensor([58]), tensor([81]), tensor([78]), tensor([7]), tensor([98]), tensor([69]), tensor([32]), tensor([27]), tensor([30]), tensor([73]), tensor([69]), tensor([30]), tensor([96]), tensor([92]), tensor([21]), tensor([35]), tensor([13]), tensor([91]), tensor([73]), tensor([80]), tensor([50]), tensor([25]), tensor([49]), tensor([7]), tensor([12]), tensor([50]), tensor([37]), tensor([2]), tensor([53]), tensor([96]), tensor([37]), tensor([52]), tensor([21]), tensor([11]), tensor([49]), tensor([37]), tensor([11]), tensor([98]), tensor([77]), tensor([32]), tensor([33]), tensor([72]), tensor([62]), tensor([96]), tensor([2]), tensor([46]), tensor([68]), tensor([63]), tensor([59]), tensor([21]), tensor([10]), tensor([12]), tensor([46]), tensor([12]), tensor([97]), tensor([34]), tensor([14]), tensor([81]), tensor([82]), tensor([73]), tensor([68]), tensor([53]), tensor([92]), tensor([79]), tensor([72]), tensor([50]), tensor([67]), tensor([11]), tensor([0]), tensor([7]), tensor([26]), tensor([45]), tensor([82]), tensor([25]), tensor([48]), tensor([10]), tensor([81]), tensor([59]), tensor([46]), tensor([13]), tensor([63]), tensor([33]), tensor([13]), tensor([31]), tensor([24]), tensor([56]), tensor([86]), tensor([97]), tensor([81]), tensor([24]), tensor([50]), tensor([63]), tensor([49]), tensor([91]), tensor([46]), tensor([46]), tensor([79]), tensor([36]), tensor([62]), tensor([80]), tensor([94]), tensor([65]), tensor([67]), tensor([26]), tensor([86]), tensor([79]), tensor([25]), tensor([56]), tensor([37]), tensor([52]), tensor([10]), tensor([82]), tensor([80]), tensor([33]), tensor([14]), tensor([96]), tensor([72]), tensor([79]), tensor([21]), tensor([0]), tensor([72]), tensor([36]), tensor([24]), tensor([79]), tensor([2]), tensor([48]), tensor([14]), tensor([81]), tensor([21]), tensor([35]), tensor([92]), tensor([3]), tensor([96]), tensor([58]), tensor([57]), tensor([72]), tensor([67]), tensor([53]), tensor([26]), tensor([52]), tensor([79]), tensor([27]), tensor([30]), tensor([32]), tensor([16]), tensor([98]), tensor([52]), tensor([94]), tensor([52]), tensor([60]), tensor([2]), tensor([45]), tensor([81]), tensor([3]), tensor([3]), tensor([60]), tensor([57]), tensor([95]), tensor([16]), tensor([32]), tensor([31]), tensor([82]), tensor([59]), tensor([80]), tensor([25]), tensor([81]), tensor([59]), tensor([94]), tensor([88]), tensor([31]), tensor([73]), tensor([25]), tensor([26]), tensor([37]), tensor([35]), tensor([95]), tensor([53]), tensor([58]), tensor([11]), tensor([3]), tensor([69]), tensor([67]), tensor([57]), tensor([95]), tensor([10]), tensor([24]), tensor([49]), tensor([71]), tensor([63]), tensor([86]), tensor([33]), tensor([25]), tensor([71]), tensor([49]), tensor([57]), tensor([7]), tensor([82]), tensor([97]), tensor([67]), tensor([72]), tensor([50]), tensor([95]), tensor([31]), tensor([27]), tensor([81]), tensor([71]), tensor([30]), tensor([46]), tensor([97]), tensor([31]), tensor([68]), tensor([24]), tensor([49]), tensor([63]), tensor([92]), tensor([69]), tensor([81]), tensor([33]), tensor([24]), tensor([25]), tensor([53]), tensor([35]), tensor([10]), tensor([79]), tensor([82]), tensor([7]), tensor([2]), tensor([50]), tensor([11]), tensor([65]), tensor([69]), tensor([53]), tensor([86]), tensor([52]), tensor([0]), tensor([98]), tensor([78]), tensor([14]), tensor([11]), tensor([95]), tensor([96]), tensor([92]), tensor([27]), tensor([12]), tensor([88]), tensor([82]), tensor([35]), tensor([37]), tensor([48]), tensor([26]), tensor([36]), tensor([12]), tensor([78]), tensor([57]), tensor([57]), tensor([25]), tensor([62]), tensor([24]), tensor([62]), tensor([92]), tensor([71]), tensor([49]), tensor([32]), tensor([58]), tensor([67]), tensor([49]), tensor([67]), tensor([86]), tensor([24]), tensor([78]), tensor([88]), tensor([27]), tensor([26]), tensor([32]), tensor([95]), tensor([48]), tensor([69]), tensor([24]), tensor([33]), tensor([68]), tensor([68]), tensor([63]), tensor([0]), tensor([37]), tensor([81]), tensor([26]), tensor([13]), tensor([78]), tensor([98]), tensor([69]), tensor([79]), tensor([27]), tensor([36]), tensor([37]), tensor([57]), tensor([30]), tensor([95]), tensor([45]), tensor([77]), tensor([71]), tensor([13]), tensor([46]), tensor([27]), tensor([91]), tensor([67]), tensor([31]), tensor([37]), tensor([10]), tensor([46]), tensor([7]), tensor([56]), tensor([21]), tensor([14]), tensor([37]), tensor([35]), tensor([50]), tensor([48]), tensor([91]), tensor([94]), tensor([94]), tensor([3]), tensor([16]), tensor([37]), tensor([94]), tensor([45]), tensor([57]), tensor([68]), tensor([81]), tensor([2]), tensor([33]), tensor([57]), tensor([63]), tensor([52]), tensor([14]), tensor([26]), tensor([92]), tensor([78]), tensor([41]), tensor([53]), tensor([10]), tensor([58]), tensor([78]), tensor([96]), tensor([79]), tensor([0]), tensor([92]), tensor([69]), tensor([2]), tensor([34]), tensor([34]), tensor([2]), tensor([65]), tensor([86]), tensor([30]), tensor([7]), tensor([62]), tensor([10]), tensor([91]), tensor([31]), tensor([68]), tensor([14]), tensor([94]), tensor([0]), tensor([72]), tensor([27]), tensor([71]), tensor([95]), tensor([62]), tensor([71]), tensor([12]), tensor([32]), tensor([68]), tensor([56]), tensor([30]), tensor([34]), tensor([57]), tensor([3]), tensor([53]), tensor([31]), tensor([25]), tensor([27]), tensor([26]), tensor([3]), tensor([77]), tensor([86]), tensor([41]), tensor([59]), tensor([56]), tensor([56]), tensor([14]), tensor([52]), tensor([71]), tensor([24]), tensor([95]), tensor([11]), tensor([97]), tensor([12]), tensor([45]), tensor([45]), tensor([34]), tensor([88]), tensor([34]), tensor([41]), tensor([33]), tensor([78]), tensor([0]), tensor([96]), tensor([63]), tensor([12]), tensor([95]), tensor([48]), tensor([88]), tensor([57]), tensor([59]), tensor([32]), tensor([56]), tensor([24]), tensor([88]), tensor([12]), tensor([49]), tensor([58]), tensor([37]), tensor([32]), tensor([98]), tensor([81]), tensor([53]), tensor([16]), tensor([68]), tensor([45]), tensor([25]), tensor([56]), tensor([13]), tensor([91]), tensor([35]), tensor([32]), tensor([69]), tensor([48]), tensor([95]), tensor([12]), tensor([95]), tensor([94]), tensor([56]), tensor([11]), tensor([2]), tensor([0]), tensor([65]), tensor([77]), tensor([0]), tensor([65]), tensor([13]), tensor([97]), tensor([7]), tensor([59]), tensor([7]), tensor([31]), tensor([7]), tensor([56]), tensor([14]), tensor([69]), tensor([0]), tensor([79]), tensor([36]), tensor([65]), tensor([21]), tensor([97]), tensor([56]), tensor([21]), tensor([35]), tensor([65]), tensor([53]), tensor([7]), tensor([57]), tensor([2]), tensor([41]), tensor([60]), tensor([45]), tensor([37]), tensor([25]), tensor([91]), tensor([53]), tensor([65]), tensor([37]), tensor([73]), tensor([14]), tensor([53]), tensor([91]), tensor([21]), tensor([36]), tensor([62]), tensor([60]), tensor([35]), tensor([34]), tensor([46]), tensor([67]), tensor([58]), tensor([13]), tensor([21]), tensor([71]), tensor([24]), tensor([46]), tensor([92]), tensor([98]), tensor([12]), tensor([0]), tensor([16]), tensor([97]), tensor([32]), tensor([68]), tensor([31]), tensor([3]), tensor([71]), tensor([36]), tensor([57]), tensor([62]), tensor([81]), tensor([21]), tensor([59]), tensor([95]), tensor([95]), tensor([27]), tensor([77]), tensor([33]), tensor([27]), tensor([30]), tensor([62]), tensor([31]), tensor([3]), tensor([52]), tensor([16]), tensor([96]), tensor([62]), tensor([80]), tensor([88]), tensor([72]), tensor([94]), tensor([52]), tensor([35]), tensor([67]), tensor([21]), tensor([32]), tensor([56]), tensor([98]), tensor([26]), tensor([11]), tensor([14]), tensor([34]), tensor([53]), tensor([45]), tensor([86]), tensor([88]), tensor([31]), tensor([72]), tensor([11]), tensor([60]), tensor([71]), tensor([2]), tensor([57]), tensor([21]), tensor([16]), tensor([34]), tensor([71]), tensor([49]), tensor([96]), tensor([16]), tensor([58]), tensor([81]), tensor([13]), tensor([32]), tensor([82]), tensor([24]), tensor([34]), tensor([2]), tensor([37]), tensor([36]), tensor([25]), tensor([94]), tensor([88]), tensor([25]), tensor([41]), tensor([25]), tensor([86]), tensor([59]), tensor([24]), tensor([97]), tensor([12]), tensor([56]), tensor([78]), tensor([56]), tensor([88]), tensor([98]), tensor([59]), tensor([12]), tensor([0]), tensor([60]), tensor([62]), tensor([67]), tensor([77]), tensor([98]), tensor([52]), tensor([82]), tensor([94]), tensor([80]), tensor([91]), tensor([41]), tensor([30]), tensor([48]), tensor([92]), tensor([96]), tensor([35]), tensor([92]), tensor([26]), tensor([33]), tensor([60]), tensor([62]), tensor([14]), tensor([13]), tensor([36]), tensor([80]), tensor([52]), tensor([59]), tensor([32]), tensor([12]), tensor([0]), tensor([73]), tensor([77]), tensor([56]), tensor([96]), tensor([59]), tensor([48]), tensor([49]), tensor([82]), tensor([11]), tensor([21]), tensor([68]), tensor([31]), tensor([32]), tensor([7]), tensor([78]), tensor([10]), tensor([94]), tensor([48]), tensor([80]), tensor([88]), tensor([69]), tensor([80]), tensor([32]), tensor([68]), tensor([50]), tensor([49]), tensor([52]), tensor([96]), tensor([46]), tensor([65]), tensor([37]), tensor([45]), tensor([72]), tensor([95]), tensor([0]), tensor([31]), tensor([65]), tensor([92]), tensor([98]), tensor([48]), tensor([62]), tensor([50]), tensor([26]), tensor([59]), tensor([33]), tensor([27]), tensor([82]), tensor([79]), tensor([12]), tensor([26]), tensor([98]), tensor([69]), tensor([72]), tensor([12]), tensor([62]), tensor([36]), tensor([48]), tensor([32]), tensor([82]), tensor([2]), tensor([49]), tensor([63]), tensor([0]), tensor([71]), tensor([41]), tensor([32]), tensor([78]), tensor([12]), tensor([91]), tensor([95]), tensor([78]), tensor([13]), tensor([63]), tensor([34]), tensor([30]), tensor([81]), tensor([0]), tensor([24]), tensor([97]), tensor([52]), tensor([12]), tensor([63]), tensor([98]), tensor([59]), tensor([50]), tensor([30]), tensor([77]), tensor([11]), tensor([77]), tensor([49]), tensor([71]), tensor([79]), tensor([73]), tensor([65]), tensor([16]), tensor([45]), tensor([88]), tensor([58]), tensor([68]), tensor([25]), tensor([25]), tensor([82]), tensor([65]), tensor([78]), tensor([67]), tensor([82]), tensor([36]), tensor([34]), tensor([82]), tensor([67]), tensor([30]), tensor([49]), tensor([30]), tensor([80]), tensor([10]), tensor([11]), tensor([2]), tensor([53]), tensor([36]), tensor([91]), tensor([10]), tensor([50]), tensor([46]), tensor([50]), tensor([45]), tensor([63]), tensor([27]), tensor([88]), tensor([81]), tensor([24]), tensor([97]), tensor([48]), tensor([11]), tensor([81]), tensor([12]), tensor([65]), tensor([68]), tensor([24]), tensor([46]), tensor([13]), tensor([30]), tensor([65]), tensor([92]), tensor([30]), tensor([41]), tensor([73]), tensor([26]), tensor([86]), tensor([46]), tensor([48]), tensor([10]), tensor([56]), tensor([35]), tensor([0]), tensor([59]), tensor([31]), tensor([46]), tensor([65]), tensor([52]), tensor([62]), tensor([27]), tensor([72]), tensor([10]), tensor([34]), tensor([7]), tensor([52]), tensor([81]), tensor([33]), tensor([36]), tensor([16]), tensor([34]), tensor([77]), tensor([16]), tensor([7]), tensor([79]), tensor([50]), tensor([67]), tensor([98]), tensor([77]), tensor([48]), tensor([11]), tensor([16]), tensor([60]), tensor([16]), tensor([31]), tensor([35]), tensor([78]), tensor([63]), tensor([68]), tensor([36]), tensor([37]), tensor([69]), tensor([60]), tensor([58]), tensor([45]), tensor([78]), tensor([77]), tensor([41]), tensor([11]), tensor([95]), tensor([88]), tensor([68]), tensor([58]), tensor([80]), tensor([3]), tensor([52]), tensor([67]), tensor([79]), tensor([98]), tensor([95]), tensor([56]), tensor([57]), tensor([50]), tensor([56]), tensor([3]), tensor([24]), tensor([79]), tensor([59]), tensor([68]), tensor([25]), tensor([41]), tensor([3]), tensor([27]), tensor([46]), tensor([57]), tensor([98]), tensor([0]), tensor([60]), tensor([81]), tensor([46]), tensor([98]), tensor([92]), tensor([92]), tensor([92]), tensor([60]), tensor([73]), tensor([78]), tensor([24]), tensor([62]), tensor([86]), tensor([24]), tensor([79]), tensor([3]), tensor([63]), tensor([33]), tensor([95]), tensor([10]), tensor([2]), tensor([69]), tensor([69]), tensor([92]), tensor([50]), tensor([33]), tensor([30]), tensor([65]), tensor([50]), tensor([68]), tensor([79]), tensor([49]), tensor([31]), tensor([98]), tensor([36]), tensor([98]), tensor([33]), tensor([10]), tensor([46]), tensor([77]), tensor([16]), tensor([30]), tensor([94]), tensor([31]), tensor([58]), tensor([60]), tensor([88]), tensor([35]), tensor([3]), tensor([57]), tensor([31]), tensor([14]), tensor([21]), tensor([48]), tensor([53]), tensor([2]), tensor([41]), tensor([13]), tensor([79]), tensor([91]), tensor([2]), tensor([26]), tensor([16]), tensor([33]), tensor([0]), tensor([16]), tensor([45]), tensor([92]), tensor([58]), tensor([52]), tensor([36]), tensor([80]), tensor([33]), tensor([11]), tensor([2]), tensor([88]), tensor([10]), tensor([73]), tensor([91]), tensor([72]), tensor([97]), tensor([0]), tensor([58]), tensor([16]), tensor([7]), tensor([35]), tensor([30]), tensor([52]), tensor([10]), tensor([0]), tensor([56]), tensor([69]), tensor([35]), tensor([59]), tensor([78]), tensor([30]), tensor([86]), tensor([94]), tensor([71]), tensor([86]), tensor([88]), tensor([3]), tensor([91]), tensor([13]), tensor([24]), tensor([31]), tensor([63]), tensor([27]), tensor([81]), tensor([46]), tensor([69]), tensor([53]), tensor([25]), tensor([97]), tensor([97]), tensor([10]), tensor([88]), tensor([79]), tensor([63]), tensor([63]), tensor([49]), tensor([50]), tensor([92]), tensor([0]), tensor([24]), tensor([49]), tensor([77]), tensor([2]), tensor([56]), tensor([49]), tensor([2]), tensor([62]), tensor([97]), tensor([25]), tensor([31]), tensor([14]), tensor([63]), tensor([73]), tensor([46]), tensor([58]), tensor([69]), tensor([91]), tensor([21]), tensor([41]), tensor([34]), tensor([30]), tensor([72]), tensor([25]), tensor([60]), tensor([67]), tensor([48]), tensor([33]), tensor([37]), tensor([25]), tensor([63]), tensor([36]), tensor([79]), tensor([68]), tensor([67]), tensor([41]), tensor([57]), tensor([41]), tensor([65]), tensor([45]), tensor([86]), tensor([14]), tensor([94]), tensor([78]), tensor([86]), tensor([10]), tensor([7]), tensor([59]), tensor([91]), tensor([71]), tensor([24]), tensor([45]), tensor([45]), tensor([52]), tensor([46]), tensor([21]), tensor([82]), tensor([82]), tensor([26]), tensor([49]), tensor([78]), tensor([92]), tensor([13]), tensor([21]), tensor([3]), tensor([97]), tensor([7]), tensor([80]), tensor([65]), tensor([60]), tensor([34]), tensor([26]), tensor([94]), tensor([3]), tensor([32]), tensor([37]), tensor([92]), tensor([11]), tensor([67]), tensor([49]), tensor([68]), tensor([80]), tensor([30]), tensor([72]), tensor([32]), tensor([13]), tensor([78]), tensor([34]), tensor([0]), tensor([60]), tensor([2]), tensor([79]), tensor([79]), tensor([65]), tensor([32]), tensor([72]), tensor([98]), tensor([62]), tensor([26]), tensor([97]), tensor([58]), tensor([27]), tensor([65]), tensor([0]), tensor([94]), tensor([52]), tensor([57]), tensor([34]), tensor([92]), tensor([60]), tensor([2]), tensor([46]), tensor([86]), tensor([33]), tensor([80]), tensor([67]), tensor([57]), tensor([10]), tensor([97]), tensor([7]), tensor([0]), tensor([52]), tensor([97]), tensor([37]), tensor([34]), tensor([35]), tensor([48]), tensor([86]), tensor([60]), tensor([94]), tensor([88]), tensor([37]), tensor([98]), tensor([33]), tensor([58]), tensor([45]), tensor([63]), tensor([68]), tensor([2]), tensor([82]), tensor([14]), tensor([53]), tensor([73]), tensor([16]), tensor([77]), tensor([91]), tensor([53]), tensor([96]), tensor([53]), tensor([13]), tensor([69]), tensor([82]), tensor([88]), tensor([73]), tensor([27]), tensor([71]), tensor([41]), tensor([13]), tensor([91]), tensor([46]), tensor([60]), tensor([21]), tensor([82]), tensor([12]), tensor([65]), tensor([88]), tensor([79]), tensor([72]), tensor([88]), tensor([57]), tensor([59]), tensor([69]), tensor([37]), tensor([7]), tensor([97]), tensor([95]), tensor([10]), tensor([57]), tensor([3]), tensor([60]), tensor([10]), tensor([82]), tensor([53]), tensor([13]), tensor([49]), tensor([56]), tensor([72]), tensor([13]), tensor([49]), tensor([35]), tensor([50]), tensor([27]), tensor([10]), tensor([35]), tensor([72]), tensor([96]), tensor([26]), tensor([59]), tensor([14]), tensor([50]), tensor([77]), tensor([59]), tensor([32]), tensor([91]), tensor([60]), tensor([50]), tensor([12]), tensor([57]), tensor([14]), tensor([3]), tensor([27]), tensor([37]), tensor([26]), tensor([97]), tensor([21]), tensor([94]), tensor([69]), tensor([33]), tensor([11]), tensor([26]), tensor([67]), tensor([72]), tensor([3]), tensor([69]), tensor([11]), tensor([25]), tensor([98]), tensor([62]), tensor([31]), tensor([63]), tensor([63]), tensor([33]), tensor([97]), tensor([80]), tensor([53]), tensor([46]), tensor([41]), tensor([95]), tensor([86]), tensor([71]), tensor([45]), tensor([52]), tensor([67]), tensor([68]), tensor([69]), tensor([26]), tensor([96]), tensor([13]), tensor([69]), tensor([21]), tensor([57]), tensor([81]), tensor([36]), tensor([24]), tensor([12]), tensor([82]), tensor([0]), tensor([14]), tensor([94]), tensor([86]), tensor([32]), tensor([32]), tensor([37]), tensor([77]), tensor([71]), tensor([68]), tensor([94]), tensor([36]), tensor([25]), tensor([12]), tensor([53]), tensor([98]), tensor([32]), tensor([52]), tensor([81]), tensor([69]), tensor([27]), tensor([62]), tensor([82]), tensor([13]), tensor([67]), tensor([3]), tensor([27]), tensor([96]), tensor([12]), tensor([52]), tensor([0]), tensor([94]), tensor([92]), tensor([31]), tensor([82]), tensor([58]), tensor([91]), tensor([97]), tensor([14]), tensor([91]), tensor([56]), tensor([73]), tensor([78]), tensor([72]), tensor([34]), tensor([59]), tensor([57]), tensor([69]), tensor([49]), tensor([48]), tensor([92]), tensor([0]), tensor([72]), tensor([73]), tensor([56]), tensor([21]), tensor([34])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.56 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.518\n",
            "TEST ALL:  0.499\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  7000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [99, 41, 13, 17, 21, 25, 33, 37, 45, 1, 49, 53, 57, 65, 69, 73, 5, 96, 95, 48, 12, 16, 24, 28, 32, 36, 52, 92, 56, 60, 68, 72, 80, 88, 77, 81, 93, 31, 98, 3, 7, 11, 23, 27, 35, 97, 59, 63, 67, 71, 79, 91, 94, 86, 82, 78, 66, 62, 58, 50, 46, 34, 30, 26, 22, 14, 10, 6, 2, 0]\n",
            "TRAIN_SET CLASSES:  [99, 23, 66, 22, 6, 93, 17, 5, 1, 28]\n",
            "VALIDATION CLASSES:  [99, 93, 28, 23, 22, 17, 6, 5, 66, 1]\n",
            "GROUP:  7\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [99, 23, 66, 22, 6, 93, 17, 5, 1, 28]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "num classes till now:  70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.23905381560325623\n",
            "Train step - Step 10, Loss 0.13669203221797943\n",
            "Train step - Step 20, Loss 0.12566988170146942\n",
            "Train step - Step 30, Loss 0.13623523712158203\n",
            "Train step - Step 40, Loss 0.12064658850431442\n",
            "Train step - Step 50, Loss 0.11870060116052628\n",
            "Train epoch - Accuracy: 0.15858585858585858 Loss: 0.13675020844131322 Corrects: 1099\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.11816417425870895\n",
            "Train step - Step 70, Loss 0.11707297712564468\n",
            "Train step - Step 80, Loss 0.12234858423471451\n",
            "Train step - Step 90, Loss 0.11572576314210892\n",
            "Train step - Step 100, Loss 0.11402927339076996\n",
            "Train epoch - Accuracy: 0.18441558441558442 Loss: 0.11625656518886272 Corrects: 1278\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11672504991292953\n",
            "Train step - Step 120, Loss 0.11395057290792465\n",
            "Train step - Step 130, Loss 0.11555652320384979\n",
            "Train step - Step 140, Loss 0.114253930747509\n",
            "Train step - Step 150, Loss 0.12108352035284042\n",
            "Train step - Step 160, Loss 0.11574304103851318\n",
            "Train epoch - Accuracy: 0.2049062049062049 Loss: 0.11414019317079932 Corrects: 1420\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.11349592357873917\n",
            "Train step - Step 180, Loss 0.11320056766271591\n",
            "Train step - Step 190, Loss 0.11165288835763931\n",
            "Train step - Step 200, Loss 0.11014541983604431\n",
            "Train step - Step 210, Loss 0.11781302839517593\n",
            "Train epoch - Accuracy: 0.2274170274170274 Loss: 0.11310364037623137 Corrects: 1576\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.10927753150463104\n",
            "Train step - Step 230, Loss 0.11749820411205292\n",
            "Train step - Step 240, Loss 0.11148890107870102\n",
            "Train step - Step 250, Loss 0.11362306773662567\n",
            "Train step - Step 260, Loss 0.1094914972782135\n",
            "Train step - Step 270, Loss 0.11733068525791168\n",
            "Train epoch - Accuracy: 0.24718614718614718 Loss: 0.11228026891339565 Corrects: 1713\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.10722170025110245\n",
            "Train step - Step 290, Loss 0.10917782038450241\n",
            "Train step - Step 300, Loss 0.1116352528333664\n",
            "Train step - Step 310, Loss 0.10738524794578552\n",
            "Train step - Step 320, Loss 0.10868899524211884\n",
            "Train epoch - Accuracy: 0.26392496392496395 Loss: 0.11129388471186419 Corrects: 1829\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11180974543094635\n",
            "Train step - Step 340, Loss 0.11537542194128036\n",
            "Train step - Step 350, Loss 0.10705467313528061\n",
            "Train step - Step 360, Loss 0.11228669434785843\n",
            "Train step - Step 370, Loss 0.1055174171924591\n",
            "Train step - Step 380, Loss 0.10036098212003708\n",
            "Train epoch - Accuracy: 0.2842712842712843 Loss: 0.11102423524056679 Corrects: 1970\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.11440838128328323\n",
            "Train step - Step 400, Loss 0.11289878934621811\n",
            "Train step - Step 410, Loss 0.10916239768266678\n",
            "Train step - Step 420, Loss 0.1053123027086258\n",
            "Train step - Step 430, Loss 0.10351001471281052\n",
            "Train epoch - Accuracy: 0.29668109668109666 Loss: 0.11054615692099795 Corrects: 2056\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.11375734955072403\n",
            "Train step - Step 450, Loss 0.11584082990884781\n",
            "Train step - Step 460, Loss 0.11376668512821198\n",
            "Train step - Step 470, Loss 0.1157798245549202\n",
            "Train step - Step 480, Loss 0.11458247154951096\n",
            "Train step - Step 490, Loss 0.11065834015607834\n",
            "Train epoch - Accuracy: 0.3088023088023088 Loss: 0.11040312588687927 Corrects: 2140\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10528449714183807\n",
            "Train step - Step 510, Loss 0.10925297439098358\n",
            "Train step - Step 520, Loss 0.11175741255283356\n",
            "Train step - Step 530, Loss 0.11301793903112411\n",
            "Train step - Step 540, Loss 0.10940908640623093\n",
            "Train epoch - Accuracy: 0.31933621933621936 Loss: 0.11004795192124008 Corrects: 2213\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.10854210704565048\n",
            "Train step - Step 560, Loss 0.1072874367237091\n",
            "Train step - Step 570, Loss 0.11487957835197449\n",
            "Train step - Step 580, Loss 0.10507407784461975\n",
            "Train step - Step 590, Loss 0.1107751652598381\n",
            "Train step - Step 600, Loss 0.1129264310002327\n",
            "Train epoch - Accuracy: 0.33246753246753247 Loss: 0.1101049629382757 Corrects: 2304\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.10854898393154144\n",
            "Train step - Step 620, Loss 0.11167825758457184\n",
            "Train step - Step 630, Loss 0.11356958001852036\n",
            "Train step - Step 640, Loss 0.11010760068893433\n",
            "Train step - Step 650, Loss 0.1095995232462883\n",
            "Train epoch - Accuracy: 0.3466089466089466 Loss: 0.10959221203347821 Corrects: 2402\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10523968189954758\n",
            "Train step - Step 670, Loss 0.11429677903652191\n",
            "Train step - Step 680, Loss 0.11170544475317001\n",
            "Train step - Step 690, Loss 0.10945519059896469\n",
            "Train step - Step 700, Loss 0.10543422400951385\n",
            "Train step - Step 710, Loss 0.11072827130556107\n",
            "Train epoch - Accuracy: 0.3588744588744589 Loss: 0.10941567022305031 Corrects: 2487\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10631461441516876\n",
            "Train step - Step 730, Loss 0.10935797542333603\n",
            "Train step - Step 740, Loss 0.10734395682811737\n",
            "Train step - Step 750, Loss 0.11270212382078171\n",
            "Train step - Step 760, Loss 0.10707554221153259\n",
            "Train epoch - Accuracy: 0.3660894660894661 Loss: 0.10952497205520949 Corrects: 2537\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10748269408941269\n",
            "Train step - Step 780, Loss 0.11105639487504959\n",
            "Train step - Step 790, Loss 0.11082019656896591\n",
            "Train step - Step 800, Loss 0.11208800971508026\n",
            "Train step - Step 810, Loss 0.11183154582977295\n",
            "Train step - Step 820, Loss 0.10423801094293594\n",
            "Train epoch - Accuracy: 0.3746031746031746 Loss: 0.10929501628204857 Corrects: 2596\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.11642370373010635\n",
            "Train step - Step 840, Loss 0.11202012002468109\n",
            "Train step - Step 850, Loss 0.1043146550655365\n",
            "Train step - Step 860, Loss 0.10331854224205017\n",
            "Train step - Step 870, Loss 0.11638268083333969\n",
            "Train epoch - Accuracy: 0.3810966810966811 Loss: 0.10888306968354904 Corrects: 2641\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.10221360623836517\n",
            "Train step - Step 890, Loss 0.10938599705696106\n",
            "Train step - Step 900, Loss 0.10376663506031036\n",
            "Train step - Step 910, Loss 0.10772034525871277\n",
            "Train step - Step 920, Loss 0.10598361492156982\n",
            "Train step - Step 930, Loss 0.10724897682666779\n",
            "Train epoch - Accuracy: 0.38152958152958155 Loss: 0.10893794480082276 Corrects: 2644\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.11437917500734329\n",
            "Train step - Step 950, Loss 0.11307124048471451\n",
            "Train step - Step 960, Loss 0.10632944107055664\n",
            "Train step - Step 970, Loss 0.10836666822433472\n",
            "Train step - Step 980, Loss 0.10526572912931442\n",
            "Train epoch - Accuracy: 0.3878787878787879 Loss: 0.10838951432558709 Corrects: 2688\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.10790877044200897\n",
            "Train step - Step 1000, Loss 0.10719441622495651\n",
            "Train step - Step 1010, Loss 0.1111677959561348\n",
            "Train step - Step 1020, Loss 0.11243896931409836\n",
            "Train step - Step 1030, Loss 0.10539543628692627\n",
            "Train step - Step 1040, Loss 0.10814011842012405\n",
            "Train epoch - Accuracy: 0.3966810966810967 Loss: 0.10872463640712557 Corrects: 2749\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.1144513413310051\n",
            "Train step - Step 1060, Loss 0.11085452884435654\n",
            "Train step - Step 1070, Loss 0.10810069739818573\n",
            "Train step - Step 1080, Loss 0.10778366029262543\n",
            "Train step - Step 1090, Loss 0.10574106127023697\n",
            "Train epoch - Accuracy: 0.4049062049062049 Loss: 0.10839163386417502 Corrects: 2806\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.11304909735918045\n",
            "Train step - Step 1110, Loss 0.10901368409395218\n",
            "Train step - Step 1120, Loss 0.10875110328197479\n",
            "Train step - Step 1130, Loss 0.10696336627006531\n",
            "Train step - Step 1140, Loss 0.10775462538003922\n",
            "Train step - Step 1150, Loss 0.1091332733631134\n",
            "Train epoch - Accuracy: 0.41789321789321787 Loss: 0.10849652401726655 Corrects: 2896\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10995805263519287\n",
            "Train step - Step 1170, Loss 0.10626213997602463\n",
            "Train step - Step 1180, Loss 0.10906684398651123\n",
            "Train step - Step 1190, Loss 0.10679514706134796\n",
            "Train step - Step 1200, Loss 0.10738798975944519\n",
            "Train epoch - Accuracy: 0.41515151515151516 Loss: 0.10800335240183455 Corrects: 2877\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.11241528391838074\n",
            "Train step - Step 1220, Loss 0.11339908093214035\n",
            "Train step - Step 1230, Loss 0.10563801229000092\n",
            "Train step - Step 1240, Loss 0.10321281850337982\n",
            "Train step - Step 1250, Loss 0.10639693588018417\n",
            "Train step - Step 1260, Loss 0.10667712241411209\n",
            "Train epoch - Accuracy: 0.4253968253968254 Loss: 0.10777314760726252 Corrects: 2948\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.10701082646846771\n",
            "Train step - Step 1280, Loss 0.10981550067663193\n",
            "Train step - Step 1290, Loss 0.10789947211742401\n",
            "Train step - Step 1300, Loss 0.10550685971975327\n",
            "Train step - Step 1310, Loss 0.11063897609710693\n",
            "Train epoch - Accuracy: 0.42943722943722945 Loss: 0.10805161799187268 Corrects: 2976\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.10839337855577469\n",
            "Train step - Step 1330, Loss 0.10778020322322845\n",
            "Train step - Step 1340, Loss 0.11013632267713547\n",
            "Train step - Step 1350, Loss 0.116443932056427\n",
            "Train step - Step 1360, Loss 0.11258546262979507\n",
            "Train step - Step 1370, Loss 0.10519254207611084\n",
            "Train epoch - Accuracy: 0.43232323232323233 Loss: 0.10796967132008953 Corrects: 2996\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.10821355879306793\n",
            "Train step - Step 1390, Loss 0.10565046966075897\n",
            "Train step - Step 1400, Loss 0.10578867793083191\n",
            "Train step - Step 1410, Loss 0.10440030694007874\n",
            "Train step - Step 1420, Loss 0.10802800208330154\n",
            "Train epoch - Accuracy: 0.43766233766233764 Loss: 0.10787330259277363 Corrects: 3033\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.10289034992456436\n",
            "Train step - Step 1440, Loss 0.10576336830854416\n",
            "Train step - Step 1450, Loss 0.10997714102268219\n",
            "Train step - Step 1460, Loss 0.1127198114991188\n",
            "Train step - Step 1470, Loss 0.1128322184085846\n",
            "Train step - Step 1480, Loss 0.11070875823497772\n",
            "Train epoch - Accuracy: 0.44126984126984126 Loss: 0.1077462325514997 Corrects: 3058\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.10998949408531189\n",
            "Train step - Step 1500, Loss 0.1076158881187439\n",
            "Train step - Step 1510, Loss 0.1073748767375946\n",
            "Train step - Step 1520, Loss 0.10838884860277176\n",
            "Train step - Step 1530, Loss 0.10815633088350296\n",
            "Train epoch - Accuracy: 0.4393939393939394 Loss: 0.10708453486234079 Corrects: 3045\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.10431989282369614\n",
            "Train step - Step 1550, Loss 0.11196158826351166\n",
            "Train step - Step 1560, Loss 0.10388132929801941\n",
            "Train step - Step 1570, Loss 0.1096690222620964\n",
            "Train step - Step 1580, Loss 0.11067643761634827\n",
            "Train step - Step 1590, Loss 0.10823731124401093\n",
            "Train epoch - Accuracy: 0.45411255411255413 Loss: 0.10736609005824828 Corrects: 3147\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.10425244271755219\n",
            "Train step - Step 1610, Loss 0.10105762630701065\n",
            "Train step - Step 1620, Loss 0.11152853071689606\n",
            "Train step - Step 1630, Loss 0.10519065707921982\n",
            "Train step - Step 1640, Loss 0.11110170930624008\n",
            "Train epoch - Accuracy: 0.4595959595959596 Loss: 0.10702860284849335 Corrects: 3185\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.10256393253803253\n",
            "Train step - Step 1660, Loss 0.10618829727172852\n",
            "Train step - Step 1670, Loss 0.10897865146398544\n",
            "Train step - Step 1680, Loss 0.10508022457361221\n",
            "Train step - Step 1690, Loss 0.10394091159105301\n",
            "Train step - Step 1700, Loss 0.11025106906890869\n",
            "Train epoch - Accuracy: 0.46075036075036074 Loss: 0.10704873146389814 Corrects: 3193\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.09884168952703476\n",
            "Train step - Step 1720, Loss 0.10587407648563385\n",
            "Train step - Step 1730, Loss 0.10317546874284744\n",
            "Train step - Step 1740, Loss 0.10856867581605911\n",
            "Train step - Step 1750, Loss 0.10710632055997849\n",
            "Train epoch - Accuracy: 0.46305916305916306 Loss: 0.10694219245539083 Corrects: 3209\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.10161668062210083\n",
            "Train step - Step 1770, Loss 0.11103038489818573\n",
            "Train step - Step 1780, Loss 0.10585368424654007\n",
            "Train step - Step 1790, Loss 0.10127800703048706\n",
            "Train step - Step 1800, Loss 0.10415468364953995\n",
            "Train step - Step 1810, Loss 0.11205240339040756\n",
            "Train epoch - Accuracy: 0.47243867243867244 Loss: 0.10686724371079243 Corrects: 3274\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.11081277579069138\n",
            "Train step - Step 1830, Loss 0.10269741714000702\n",
            "Train step - Step 1840, Loss 0.10338123887777328\n",
            "Train step - Step 1850, Loss 0.10689769685268402\n",
            "Train step - Step 1860, Loss 0.10357385128736496\n",
            "Train epoch - Accuracy: 0.4683982683982684 Loss: 0.10683873234502164 Corrects: 3246\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.10198719799518585\n",
            "Train step - Step 1880, Loss 0.10935787856578827\n",
            "Train step - Step 1890, Loss 0.10575399547815323\n",
            "Train step - Step 1900, Loss 0.10324915498495102\n",
            "Train step - Step 1910, Loss 0.10089002549648285\n",
            "Train step - Step 1920, Loss 0.10693200677633286\n",
            "Train epoch - Accuracy: 0.4746031746031746 Loss: 0.1066380439877166 Corrects: 3289\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.10639332234859467\n",
            "Train step - Step 1940, Loss 0.10353408008813858\n",
            "Train step - Step 1950, Loss 0.10655278712511063\n",
            "Train step - Step 1960, Loss 0.10740801692008972\n",
            "Train step - Step 1970, Loss 0.10025153309106827\n",
            "Train epoch - Accuracy: 0.4797979797979798 Loss: 0.10652030332693978 Corrects: 3325\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.10611645877361298\n",
            "Train step - Step 1990, Loss 0.10515622794628143\n",
            "Train step - Step 2000, Loss 0.10242252051830292\n",
            "Train step - Step 2010, Loss 0.10736774653196335\n",
            "Train step - Step 2020, Loss 0.11215643584728241\n",
            "Train step - Step 2030, Loss 0.10772458463907242\n",
            "Train epoch - Accuracy: 0.47748917748917746 Loss: 0.10678250009535367 Corrects: 3309\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.10718108713626862\n",
            "Train step - Step 2050, Loss 0.1057184487581253\n",
            "Train step - Step 2060, Loss 0.11550445854663849\n",
            "Train step - Step 2070, Loss 0.1058071106672287\n",
            "Train step - Step 2080, Loss 0.10773719847202301\n",
            "Train epoch - Accuracy: 0.48614718614718616 Loss: 0.1064291080828181 Corrects: 3369\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.10447964072227478\n",
            "Train step - Step 2100, Loss 0.11008112877607346\n",
            "Train step - Step 2110, Loss 0.1018894761800766\n",
            "Train step - Step 2120, Loss 0.10331763327121735\n",
            "Train step - Step 2130, Loss 0.1089651882648468\n",
            "Train step - Step 2140, Loss 0.10584106296300888\n",
            "Train epoch - Accuracy: 0.49393939393939396 Loss: 0.10630071486143494 Corrects: 3423\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.09896495193243027\n",
            "Train step - Step 2160, Loss 0.10604452341794968\n",
            "Train step - Step 2170, Loss 0.10163701325654984\n",
            "Train step - Step 2180, Loss 0.11562404781579971\n",
            "Train step - Step 2190, Loss 0.10878325998783112\n",
            "Train epoch - Accuracy: 0.5004329004329005 Loss: 0.10612257264049194 Corrects: 3468\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.10988491773605347\n",
            "Train step - Step 2210, Loss 0.10649748146533966\n",
            "Train step - Step 2220, Loss 0.10439891368150711\n",
            "Train step - Step 2230, Loss 0.10680896043777466\n",
            "Train step - Step 2240, Loss 0.1106579452753067\n",
            "Train step - Step 2250, Loss 0.11473838239908218\n",
            "Train epoch - Accuracy: 0.49783549783549785 Loss: 0.10617659925597399 Corrects: 3450\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.10635926574468613\n",
            "Train step - Step 2270, Loss 0.10813280940055847\n",
            "Train step - Step 2280, Loss 0.10370296984910965\n",
            "Train step - Step 2290, Loss 0.10786183923482895\n",
            "Train step - Step 2300, Loss 0.09901989251375198\n",
            "Train epoch - Accuracy: 0.5001443001443001 Loss: 0.10597402616625741 Corrects: 3466\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.11125518381595612\n",
            "Train step - Step 2320, Loss 0.11249256879091263\n",
            "Train step - Step 2330, Loss 0.10346037149429321\n",
            "Train step - Step 2340, Loss 0.10717558860778809\n",
            "Train step - Step 2350, Loss 0.10855519771575928\n",
            "Train step - Step 2360, Loss 0.10409640520811081\n",
            "Train epoch - Accuracy: 0.49927849927849927 Loss: 0.10631056823814758 Corrects: 3460\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.10899213701486588\n",
            "Train step - Step 2380, Loss 0.10731590539216995\n",
            "Train step - Step 2390, Loss 0.10260216891765594\n",
            "Train step - Step 2400, Loss 0.10718115419149399\n",
            "Train step - Step 2410, Loss 0.11027161777019501\n",
            "Train epoch - Accuracy: 0.5057720057720058 Loss: 0.10575102491984292 Corrects: 3505\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.10854968428611755\n",
            "Train step - Step 2430, Loss 0.10278372466564178\n",
            "Train step - Step 2440, Loss 0.10564633458852768\n",
            "Train step - Step 2450, Loss 0.1069747805595398\n",
            "Train step - Step 2460, Loss 0.10168442875146866\n",
            "Train step - Step 2470, Loss 0.1055731475353241\n",
            "Train epoch - Accuracy: 0.5086580086580087 Loss: 0.10600061210848036 Corrects: 3525\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.11042329668998718\n",
            "Train step - Step 2490, Loss 0.11596393585205078\n",
            "Train step - Step 2500, Loss 0.11067581921815872\n",
            "Train step - Step 2510, Loss 0.10628949850797653\n",
            "Train step - Step 2520, Loss 0.10779070109128952\n",
            "Train epoch - Accuracy: 0.5128427128427129 Loss: 0.10595361838093052 Corrects: 3554\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.10264168679714203\n",
            "Train step - Step 2540, Loss 0.11068255454301834\n",
            "Train step - Step 2550, Loss 0.10390178859233856\n",
            "Train step - Step 2560, Loss 0.10603687167167664\n",
            "Train step - Step 2570, Loss 0.10893668234348297\n",
            "Train step - Step 2580, Loss 0.10955298691987991\n",
            "Train epoch - Accuracy: 0.5112554112554113 Loss: 0.10563884916759672 Corrects: 3543\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.10258053988218307\n",
            "Train step - Step 2600, Loss 0.11176891624927521\n",
            "Train step - Step 2610, Loss 0.10395380854606628\n",
            "Train step - Step 2620, Loss 0.10672685503959656\n",
            "Train step - Step 2630, Loss 0.1096639484167099\n",
            "Train epoch - Accuracy: 0.5202020202020202 Loss: 0.10603604104267743 Corrects: 3605\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.11125434190034866\n",
            "Train step - Step 2650, Loss 0.10737638175487518\n",
            "Train step - Step 2660, Loss 0.10580949485301971\n",
            "Train step - Step 2670, Loss 0.10017507523298264\n",
            "Train step - Step 2680, Loss 0.10049451142549515\n",
            "Train step - Step 2690, Loss 0.11181621253490448\n",
            "Train epoch - Accuracy: 0.5215007215007215 Loss: 0.10551904204904947 Corrects: 3614\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.1046944186091423\n",
            "Train step - Step 2710, Loss 0.10553071647882462\n",
            "Train step - Step 2720, Loss 0.0966661348938942\n",
            "Train step - Step 2730, Loss 0.10687565058469772\n",
            "Train step - Step 2740, Loss 0.09585339576005936\n",
            "Train epoch - Accuracy: 0.5236652236652236 Loss: 0.1051586053717188 Corrects: 3629\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.10482180863618851\n",
            "Train step - Step 2760, Loss 0.11281059682369232\n",
            "Train step - Step 2770, Loss 0.10245628654956818\n",
            "Train step - Step 2780, Loss 0.10945157706737518\n",
            "Train step - Step 2790, Loss 0.11003544926643372\n",
            "Train step - Step 2800, Loss 0.1110195443034172\n",
            "Train epoch - Accuracy: 0.5275613275613276 Loss: 0.10490371373953758 Corrects: 3656\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.10154745727777481\n",
            "Train step - Step 2820, Loss 0.10026615113019943\n",
            "Train step - Step 2830, Loss 0.10709238052368164\n",
            "Train step - Step 2840, Loss 0.10711055248975754\n",
            "Train step - Step 2850, Loss 0.10659123957157135\n",
            "Train epoch - Accuracy: 0.5301587301587302 Loss: 0.1043826165416884 Corrects: 3674\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.10712731629610062\n",
            "Train step - Step 2870, Loss 0.10479109734296799\n",
            "Train step - Step 2880, Loss 0.1030910536646843\n",
            "Train step - Step 2890, Loss 0.1048911064863205\n",
            "Train step - Step 2900, Loss 0.10438178479671478\n",
            "Train step - Step 2910, Loss 0.10160492360591888\n",
            "Train epoch - Accuracy: 0.5311688311688312 Loss: 0.10494440690564559 Corrects: 3681\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.10392527282238007\n",
            "Train step - Step 2930, Loss 0.11251617223024368\n",
            "Train step - Step 2940, Loss 0.1021588072180748\n",
            "Train step - Step 2950, Loss 0.11659739166498184\n",
            "Train step - Step 2960, Loss 0.10634540766477585\n",
            "Train epoch - Accuracy: 0.529004329004329 Loss: 0.10457623665251946 Corrects: 3666\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.10470376908779144\n",
            "Train step - Step 2980, Loss 0.1091940626502037\n",
            "Train step - Step 2990, Loss 0.10007716715335846\n",
            "Train step - Step 3000, Loss 0.1044813022017479\n",
            "Train step - Step 3010, Loss 0.10308678448200226\n",
            "Train step - Step 3020, Loss 0.10858317464590073\n",
            "Train epoch - Accuracy: 0.5412698412698412 Loss: 0.10480361206061913 Corrects: 3751\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.10084272921085358\n",
            "Train step - Step 3040, Loss 0.10405027121305466\n",
            "Train step - Step 3050, Loss 0.10826906561851501\n",
            "Train step - Step 3060, Loss 0.10797958076000214\n",
            "Train step - Step 3070, Loss 0.10353735834360123\n",
            "Train epoch - Accuracy: 0.53997113997114 Loss: 0.10472353563938307 Corrects: 3742\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.10816606879234314\n",
            "Train step - Step 3090, Loss 0.10134518891572952\n",
            "Train step - Step 3100, Loss 0.10429619252681732\n",
            "Train step - Step 3110, Loss 0.10445444285869598\n",
            "Train step - Step 3120, Loss 0.10108164697885513\n",
            "Train step - Step 3130, Loss 0.10371673852205276\n",
            "Train epoch - Accuracy: 0.5298701298701298 Loss: 0.1046231310733985 Corrects: 3672\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.10796496272087097\n",
            "Train step - Step 3150, Loss 0.11430291086435318\n",
            "Train step - Step 3160, Loss 0.10232485830783844\n",
            "Train step - Step 3170, Loss 0.10538000613451004\n",
            "Train step - Step 3180, Loss 0.10346361994743347\n",
            "Train epoch - Accuracy: 0.5326118326118326 Loss: 0.10483229202023488 Corrects: 3691\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.1090182438492775\n",
            "Train step - Step 3200, Loss 0.10338924080133438\n",
            "Train step - Step 3210, Loss 0.10786817967891693\n",
            "Train step - Step 3220, Loss 0.10188180953264236\n",
            "Train step - Step 3230, Loss 0.11008356511592865\n",
            "Train step - Step 3240, Loss 0.10436062514781952\n",
            "Train epoch - Accuracy: 0.5321789321789322 Loss: 0.10433732117667342 Corrects: 3688\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.10669965296983719\n",
            "Train step - Step 3260, Loss 0.10147160291671753\n",
            "Train step - Step 3270, Loss 0.1087813675403595\n",
            "Train step - Step 3280, Loss 0.1048331931233406\n",
            "Train step - Step 3290, Loss 0.10381671786308289\n",
            "Train epoch - Accuracy: 0.5356421356421357 Loss: 0.10451547643667003 Corrects: 3712\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.11279385536909103\n",
            "Train step - Step 3310, Loss 0.10632986575365067\n",
            "Train step - Step 3320, Loss 0.10983812063932419\n",
            "Train step - Step 3330, Loss 0.10311813652515411\n",
            "Train step - Step 3340, Loss 0.10282918810844421\n",
            "Train step - Step 3350, Loss 0.1050148457288742\n",
            "Train epoch - Accuracy: 0.5424242424242425 Loss: 0.10465279817151128 Corrects: 3759\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.10383272916078568\n",
            "Train step - Step 3370, Loss 0.10365050286054611\n",
            "Train step - Step 3380, Loss 0.1047142967581749\n",
            "Train step - Step 3390, Loss 0.10084125399589539\n",
            "Train step - Step 3400, Loss 0.1033913791179657\n",
            "Train epoch - Accuracy: 0.536075036075036 Loss: 0.10451003381442198 Corrects: 3715\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.10040982067584991\n",
            "Train step - Step 3420, Loss 0.10430309176445007\n",
            "Train step - Step 3430, Loss 0.10234085470438004\n",
            "Train step - Step 3440, Loss 0.10904815047979355\n",
            "Train step - Step 3450, Loss 0.10271835327148438\n",
            "Train step - Step 3460, Loss 0.1046258956193924\n",
            "Train epoch - Accuracy: 0.5311688311688312 Loss: 0.10449123301289298 Corrects: 3681\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.10730429738759995\n",
            "Train step - Step 3480, Loss 0.10719473659992218\n",
            "Train step - Step 3490, Loss 0.101272352039814\n",
            "Train step - Step 3500, Loss 0.09932433813810349\n",
            "Train step - Step 3510, Loss 0.10069054365158081\n",
            "Train epoch - Accuracy: 0.5323232323232323 Loss: 0.1040618646648023 Corrects: 3689\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.10769639909267426\n",
            "Train step - Step 3530, Loss 0.10920839756727219\n",
            "Train step - Step 3540, Loss 0.10240297019481659\n",
            "Train step - Step 3550, Loss 0.1027071624994278\n",
            "Train step - Step 3560, Loss 0.10178165137767792\n",
            "Train step - Step 3570, Loss 0.1098657175898552\n",
            "Train epoch - Accuracy: 0.53997113997114 Loss: 0.10418473498043732 Corrects: 3742\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.1006518229842186\n",
            "Train step - Step 3590, Loss 0.10373644530773163\n",
            "Train step - Step 3600, Loss 0.1029662936925888\n",
            "Train step - Step 3610, Loss 0.10270141810178757\n",
            "Train step - Step 3620, Loss 0.1070220023393631\n",
            "Train epoch - Accuracy: 0.537950937950938 Loss: 0.10437883705459804 Corrects: 3728\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.10502548515796661\n",
            "Train step - Step 3640, Loss 0.09845352172851562\n",
            "Train step - Step 3650, Loss 0.10320541262626648\n",
            "Train step - Step 3660, Loss 0.10346918553113937\n",
            "Train step - Step 3670, Loss 0.10055244714021683\n",
            "Train step - Step 3680, Loss 0.10746888816356659\n",
            "Train epoch - Accuracy: 0.5378066378066378 Loss: 0.10427338610393833 Corrects: 3727\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.09659495204687119\n",
            "Train step - Step 3700, Loss 0.09950689971446991\n",
            "Train step - Step 3710, Loss 0.10046959668397903\n",
            "Train step - Step 3720, Loss 0.10236267745494843\n",
            "Train step - Step 3730, Loss 0.10506940633058548\n",
            "Train epoch - Accuracy: 0.5437229437229437 Loss: 0.10419789149733677 Corrects: 3768\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.10439011454582214\n",
            "Train step - Step 3750, Loss 0.104548878967762\n",
            "Train step - Step 3760, Loss 0.10422754287719727\n",
            "Train step - Step 3770, Loss 0.0955454483628273\n",
            "Train step - Step 3780, Loss 0.10125228762626648\n",
            "Train step - Step 3790, Loss 0.1028129830956459\n",
            "Train epoch - Accuracy: 0.5375180375180375 Loss: 0.10422714937257904 Corrects: 3725\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.10145926475524902\n",
            "Train step - Step 3810, Loss 0.10413898527622223\n",
            "Train step - Step 3820, Loss 0.1026320829987526\n",
            "Train step - Step 3830, Loss 0.10568582266569138\n",
            "Train step - Step 3840, Loss 0.10493743419647217\n",
            "Train epoch - Accuracy: 0.5363636363636364 Loss: 0.1043229717454869 Corrects: 3717\n",
            "Training finished in 397.1611394882202 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0, 79, 71, 63, 26, 14, 2, 73, 41, 56, 48, 99, 23, 66, 22, 6, 93, 17, 5, 1, 28]\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2241406a10>\n",
            "Constructing exemplars of class 99\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [41278, 27753, 49261, 32723, 41751, 27067, 5931, 33087, 44587, 38559, 28680, 15258, 22598, 43583, 9401, 3081, 39683, 42350, 10306, 45821, 40593, 21881, 11202, 13864, 44968, 42070, 27288, 25410]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a87710>\n",
            "Constructing exemplars of class 23\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [41454, 32156, 28839, 27202, 18055, 256, 24932, 2606, 12315, 42121, 4426, 10143, 14649, 36279, 31095, 24696, 14782, 7436, 27017, 49695, 14756, 39458, 5561, 17618, 48551, 16805, 6146, 27041]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22443a58d0>\n",
            "Constructing exemplars of class 66\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [41607, 31927, 45544, 38589, 18810, 22020, 10408, 43654, 28150, 3860, 17786, 6454, 13652, 36732, 16672, 49442, 40583, 8948, 19376, 18442, 29485, 5027, 37684, 48867, 47055, 11759, 14357, 24865]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a7e090>\n",
            "Constructing exemplars of class 22\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [32505, 36684, 46997, 6771, 24204, 41791, 632, 8603, 6019, 48778, 35337, 5250, 36675, 33877, 39093, 6300, 18902, 49084, 32505, 11429, 30505, 30411, 22566, 25300, 8638, 16135, 26474, 17646]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223295da50>\n",
            "Constructing exemplars of class 6\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [27003, 49107, 18887, 43542, 5877, 19935, 40740, 29444, 44515, 15088, 24833, 42459, 31184, 714, 21428, 36992, 11211, 21330, 17836, 17637, 38520, 12070, 41015, 25473, 36354, 17226, 27003, 23658]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e9eb10>\n",
            "Constructing exemplars of class 93\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [12110, 1224, 20401, 37430, 44161, 27722, 37996, 22741, 13042, 19033, 16419, 42140, 29068, 19116, 42068, 20940, 37796, 37405, 37996, 10409, 2142, 29069, 38898, 23982, 34734, 23224, 11099, 35203]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a7e090>\n",
            "Constructing exemplars of class 17\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [11578, 1960, 47695, 10730, 42609, 35889, 12582, 29296, 16365, 39926, 23599, 39997, 29864, 2784, 7213, 27474, 47497, 4224, 34726, 38005, 16128, 7457, 20513, 32188, 15967, 25170, 14769, 20283]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223295da50>\n",
            "Constructing exemplars of class 5\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [43966, 33951, 42520, 49813, 7054, 41562, 18916, 35545, 29948, 16480, 1092, 1479, 14402, 5302, 10319, 41324, 721, 26316, 16708, 31979, 1352, 9597, 7203, 44905, 5575, 23315, 9413, 26187]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232eedf10>\n",
            "Constructing exemplars of class 1\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [42934, 29098, 43670, 32884, 35107, 45011, 15859, 15940, 28112, 44775, 41114, 9164, 28293, 41141, 17084, 35826, 39673, 49682, 27655, 12796, 18023, 20090, 13221, 8713, 23518, 5356, 31177, 4348]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22414062d0>\n",
            "Constructing exemplars of class 28\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [36349, 1460, 10363, 15853, 38756, 30041, 16589, 5425, 26455, 36193, 44769, 26121, 40812, 3324, 8144, 32408, 3756, 29347, 35895, 14546, 21493, 35647, 18027, 42992, 33693, 227, 11356, 35169]\n",
            "x train:  [ 0.02063525 -0.07719998 -0.09806208 -0.04323838 -0.08704853 -0.09753458\n",
            " -0.10799194 -0.08815765 -0.12436156 -0.05206656 -0.07654451 -0.06554949\n",
            " -0.04939351 -0.11713692 -0.07628976 -0.02910401 -0.0928578  -0.07540951\n",
            " -0.13081011 -0.16031636 -0.14090507 -0.09848408 -0.2610185  -0.14227481\n",
            " -0.01296751 -0.08655286 -0.12768988 -0.06214459 -0.18437424 -0.03962271\n",
            " -0.08045632 -0.09128373 -0.14082584 -0.09455372 -0.23422652 -0.0765347\n",
            " -0.10741762 -0.14461088 -0.05755614 -0.07072841 -0.03873661 -0.06289662\n",
            " -0.03610712 -0.17738171 -0.14502038 -0.15018372 -0.10695073 -0.07248995\n",
            " -0.16612123 -0.2012673  -0.10491044 -0.12183402 -0.13097905 -0.06426029\n",
            " -0.08714031 -0.18953446 -0.09918445 -0.06584251 -0.09759878 -0.11384255\n",
            " -0.16286543 -0.16630115 -0.16555203 -0.14795946 -0.07279668 -0.01462855\n",
            " -0.13117328 -0.1380324  -0.11128438 -0.24369204]\n",
            "y_train:  [tensor([7]), tensor([33]), tensor([23]), tensor([93]), tensor([92]), tensor([49]), tensor([92]), tensor([98]), tensor([62]), tensor([57]), tensor([5]), tensor([77]), tensor([72]), tensor([26]), tensor([49]), tensor([56]), tensor([56]), tensor([93]), tensor([91]), tensor([72]), tensor([46]), tensor([16]), tensor([7]), tensor([98]), tensor([33]), tensor([91]), tensor([11]), tensor([67]), tensor([80]), tensor([91]), tensor([71]), tensor([66]), tensor([56]), tensor([26]), tensor([48]), tensor([24]), tensor([58]), tensor([79]), tensor([53]), tensor([14]), tensor([30]), tensor([94]), tensor([52]), tensor([58]), tensor([62]), tensor([45]), tensor([67]), tensor([49]), tensor([10]), tensor([7]), tensor([73]), tensor([98]), tensor([41]), tensor([28]), tensor([67]), tensor([22]), tensor([12]), tensor([35]), tensor([48]), tensor([16]), tensor([1]), tensor([53]), tensor([57]), tensor([14]), tensor([88]), tensor([50]), tensor([37]), tensor([45]), tensor([3]), tensor([23]), tensor([66]), tensor([63]), tensor([65]), tensor([2]), tensor([12]), tensor([24]), tensor([66]), tensor([32]), tensor([58]), tensor([80]), tensor([34]), tensor([11]), tensor([16]), tensor([95]), tensor([95]), tensor([12]), tensor([48]), tensor([49]), tensor([17]), tensor([7]), tensor([5]), tensor([17]), tensor([94]), tensor([31]), tensor([52]), tensor([24]), tensor([11]), tensor([25]), tensor([93]), tensor([31]), tensor([45]), tensor([16]), tensor([66]), tensor([63]), tensor([46]), tensor([77]), tensor([96]), tensor([53]), tensor([1]), tensor([53]), tensor([2]), tensor([79]), tensor([0]), tensor([17]), tensor([57]), tensor([41]), tensor([34]), tensor([23]), tensor([41]), tensor([53]), tensor([65]), tensor([26]), tensor([71]), tensor([27]), tensor([67]), tensor([63]), tensor([5]), tensor([96]), tensor([56]), tensor([98]), tensor([97]), tensor([23]), tensor([59]), tensor([82]), tensor([34]), tensor([88]), tensor([71]), tensor([33]), tensor([33]), tensor([13]), tensor([66]), tensor([98]), tensor([14]), tensor([96]), tensor([96]), tensor([62]), tensor([73]), tensor([69]), tensor([17]), tensor([31]), tensor([21]), tensor([46]), tensor([26]), tensor([59]), tensor([21]), tensor([14]), tensor([52]), tensor([45]), tensor([1]), tensor([60]), tensor([6]), tensor([34]), tensor([77]), tensor([48]), tensor([11]), tensor([30]), tensor([53]), tensor([56]), tensor([52]), tensor([94]), tensor([56]), tensor([67]), tensor([68]), tensor([48]), tensor([36]), tensor([63]), tensor([36]), tensor([81]), tensor([77]), tensor([80]), tensor([35]), tensor([98]), tensor([41]), tensor([62]), tensor([82]), tensor([35]), tensor([59]), tensor([12]), tensor([50]), tensor([86]), tensor([62]), tensor([41]), tensor([36]), tensor([59]), tensor([98]), tensor([79]), tensor([16]), tensor([1]), tensor([62]), tensor([77]), tensor([0]), tensor([94]), tensor([58]), tensor([37]), tensor([24]), tensor([12]), tensor([2]), tensor([37]), tensor([95]), tensor([56]), tensor([57]), tensor([97]), tensor([34]), tensor([78]), tensor([92]), tensor([26]), tensor([0]), tensor([22]), tensor([10]), tensor([79]), tensor([3]), tensor([6]), tensor([73]), tensor([57]), tensor([46]), tensor([97]), tensor([58]), tensor([0]), tensor([16]), tensor([67]), tensor([37]), tensor([97]), tensor([46]), tensor([97]), tensor([53]), tensor([7]), tensor([77]), tensor([25]), tensor([50]), tensor([66]), tensor([31]), tensor([67]), tensor([65]), tensor([66]), tensor([33]), tensor([99]), tensor([86]), tensor([50]), tensor([98]), tensor([97]), tensor([71]), tensor([53]), tensor([71]), tensor([22]), tensor([81]), tensor([27]), tensor([33]), tensor([28]), tensor([80]), tensor([88]), tensor([27]), tensor([0]), tensor([93]), tensor([30]), tensor([60]), tensor([35]), tensor([88]), tensor([48]), tensor([63]), tensor([68]), tensor([59]), tensor([7]), tensor([95]), tensor([69]), tensor([93]), tensor([92]), tensor([36]), tensor([14]), tensor([86]), tensor([77]), tensor([45]), tensor([26]), tensor([28]), tensor([5]), tensor([78]), tensor([36]), tensor([53]), tensor([69]), tensor([66]), tensor([65]), tensor([13]), tensor([98]), tensor([71]), tensor([92]), tensor([58]), tensor([82]), tensor([32]), tensor([25]), tensor([82]), tensor([99]), tensor([34]), tensor([57]), tensor([93]), tensor([97]), tensor([68]), tensor([86]), tensor([68]), tensor([92]), tensor([34]), tensor([34]), tensor([80]), tensor([79]), tensor([58]), tensor([91]), tensor([37]), tensor([7]), tensor([78]), tensor([79]), tensor([11]), tensor([80]), tensor([56]), tensor([59]), tensor([35]), tensor([56]), tensor([88]), tensor([73]), tensor([0]), tensor([34]), tensor([17]), tensor([48]), tensor([50]), tensor([78]), tensor([94]), tensor([2]), tensor([12]), tensor([91]), tensor([79]), tensor([0]), tensor([95]), tensor([65]), tensor([2]), tensor([91]), tensor([71]), tensor([57]), tensor([11]), tensor([14]), tensor([99]), tensor([45]), tensor([81]), tensor([81]), tensor([46]), tensor([7]), tensor([79]), tensor([16]), tensor([27]), tensor([56]), tensor([26]), tensor([5]), tensor([73]), tensor([94]), tensor([72]), tensor([10]), tensor([62]), tensor([98]), tensor([36]), tensor([46]), tensor([49]), tensor([10]), tensor([57]), tensor([99]), tensor([69]), tensor([69]), tensor([59]), tensor([91]), tensor([77]), tensor([36]), tensor([60]), tensor([46]), tensor([81]), tensor([52]), tensor([92]), tensor([88]), tensor([32]), tensor([0]), tensor([49]), tensor([88]), tensor([32]), tensor([50]), tensor([5]), tensor([73]), tensor([95]), tensor([73]), tensor([57]), tensor([36]), tensor([48]), tensor([53]), tensor([30]), tensor([36]), tensor([30]), tensor([77]), tensor([49]), tensor([16]), tensor([22]), tensor([37]), tensor([81]), tensor([77]), tensor([12]), tensor([86]), tensor([23]), tensor([68]), tensor([25]), tensor([57]), tensor([16]), tensor([12]), tensor([93]), tensor([69]), tensor([50]), tensor([94]), tensor([63]), tensor([94]), tensor([97]), tensor([59]), tensor([63]), tensor([23]), tensor([32]), tensor([25]), tensor([13]), tensor([97]), tensor([79]), tensor([88]), tensor([35]), tensor([6]), tensor([6]), tensor([60]), tensor([73]), tensor([3]), tensor([79]), tensor([33]), tensor([92]), tensor([14]), tensor([67]), tensor([68]), tensor([69]), tensor([36]), tensor([60]), tensor([3]), tensor([23]), tensor([31]), tensor([21]), tensor([95]), tensor([80]), tensor([58]), tensor([41]), tensor([97]), tensor([36]), tensor([88]), tensor([3]), tensor([13]), tensor([33]), tensor([13]), tensor([14]), tensor([31]), tensor([23]), tensor([41]), tensor([12]), tensor([99]), tensor([37]), tensor([10]), tensor([57]), tensor([62]), tensor([11]), tensor([6]), tensor([2]), tensor([35]), tensor([52]), tensor([91]), tensor([77]), tensor([48]), tensor([41]), tensor([5]), tensor([23]), tensor([22]), tensor([14]), tensor([71]), tensor([59]), tensor([73]), tensor([25]), tensor([66]), tensor([56]), tensor([68]), tensor([45]), tensor([53]), tensor([33]), tensor([27]), tensor([71]), tensor([60]), tensor([36]), tensor([67]), tensor([53]), tensor([41]), tensor([60]), tensor([62]), tensor([2]), tensor([7]), tensor([24]), tensor([35]), tensor([77]), tensor([71]), tensor([27]), tensor([27]), tensor([65]), tensor([49]), tensor([12]), tensor([33]), tensor([62]), tensor([68]), tensor([48]), tensor([88]), tensor([78]), tensor([88]), tensor([58]), tensor([81]), tensor([92]), tensor([56]), tensor([79]), tensor([50]), tensor([37]), tensor([45]), tensor([56]), tensor([26]), tensor([10]), tensor([86]), tensor([80]), tensor([62]), tensor([63]), tensor([2]), tensor([13]), tensor([82]), tensor([5]), tensor([53]), tensor([45]), tensor([16]), tensor([10]), tensor([68]), tensor([46]), tensor([2]), tensor([30]), tensor([49]), tensor([79]), tensor([33]), tensor([10]), tensor([21]), tensor([0]), tensor([56]), tensor([69]), tensor([14]), tensor([37]), tensor([3]), tensor([46]), tensor([98]), tensor([13]), tensor([62]), tensor([28]), tensor([22]), tensor([1]), tensor([92]), tensor([35]), tensor([91]), tensor([27]), tensor([78]), tensor([82]), tensor([80]), tensor([27]), tensor([91]), tensor([67]), tensor([3]), tensor([45]), tensor([50]), tensor([0]), tensor([24]), tensor([81]), tensor([59]), tensor([72]), tensor([88]), tensor([92]), tensor([67]), tensor([69]), tensor([17]), tensor([92]), tensor([41]), tensor([1]), tensor([5]), tensor([78]), tensor([13]), tensor([25]), tensor([71]), tensor([7]), tensor([80]), tensor([86]), tensor([52]), tensor([26]), tensor([66]), tensor([6]), tensor([31]), tensor([6]), tensor([72]), tensor([99]), tensor([94]), tensor([60]), tensor([65]), tensor([25]), tensor([59]), tensor([73]), tensor([7]), tensor([26]), tensor([12]), tensor([62]), tensor([37]), tensor([50]), tensor([82]), tensor([33]), tensor([49]), tensor([62]), tensor([7]), tensor([31]), tensor([50]), tensor([2]), tensor([91]), tensor([78]), tensor([91]), tensor([63]), tensor([22]), tensor([58]), tensor([32]), tensor([49]), tensor([60]), tensor([37]), tensor([37]), tensor([10]), tensor([36]), tensor([59]), tensor([96]), tensor([31]), tensor([93]), tensor([82]), tensor([86]), tensor([80]), tensor([71]), tensor([69]), tensor([88]), tensor([24]), tensor([95]), tensor([73]), tensor([52]), tensor([28]), tensor([7]), tensor([94]), tensor([14]), tensor([48]), tensor([13]), tensor([52]), tensor([77]), tensor([26]), tensor([10]), tensor([32]), tensor([3]), tensor([2]), tensor([95]), tensor([52]), tensor([48]), tensor([79]), tensor([99]), tensor([66]), tensor([41]), tensor([5]), tensor([11]), tensor([56]), tensor([24]), tensor([80]), tensor([99]), tensor([72]), tensor([31]), tensor([6]), tensor([95]), tensor([37]), tensor([6]), tensor([59]), tensor([30]), tensor([53]), tensor([98]), tensor([86]), tensor([88]), tensor([58]), tensor([41]), tensor([98]), tensor([35]), tensor([91]), tensor([60]), tensor([80]), tensor([28]), tensor([60]), tensor([14]), tensor([82]), tensor([16]), tensor([41]), tensor([68]), tensor([97]), tensor([65]), tensor([22]), tensor([95]), tensor([34]), tensor([96]), tensor([98]), tensor([80]), tensor([3]), tensor([2]), tensor([11]), tensor([91]), tensor([34]), tensor([30]), tensor([60]), tensor([82]), tensor([81]), tensor([59]), tensor([78]), tensor([52]), tensor([2]), tensor([45]), tensor([1]), tensor([5]), tensor([24]), tensor([48]), tensor([48]), tensor([11]), tensor([21]), tensor([65]), tensor([94]), tensor([93]), tensor([21]), tensor([33]), tensor([78]), tensor([21]), tensor([69]), tensor([93]), tensor([2]), tensor([37]), tensor([93]), tensor([6]), tensor([24]), tensor([71]), tensor([46]), tensor([53]), tensor([46]), tensor([65]), tensor([23]), tensor([53]), tensor([69]), tensor([59]), tensor([77]), tensor([10]), tensor([65]), tensor([5]), tensor([23]), tensor([46]), tensor([6]), tensor([34]), tensor([41]), tensor([79]), tensor([48]), tensor([95]), tensor([82]), tensor([31]), tensor([0]), tensor([49]), tensor([28]), tensor([99]), tensor([1]), tensor([7]), tensor([72]), tensor([92]), tensor([13]), tensor([17]), tensor([46]), tensor([91]), tensor([63]), tensor([99]), tensor([30]), tensor([11]), tensor([68]), tensor([26]), tensor([21]), tensor([17]), tensor([72]), tensor([67]), tensor([30]), tensor([52]), tensor([13]), tensor([81]), tensor([25]), tensor([81]), tensor([53]), tensor([7]), tensor([21]), tensor([34]), tensor([46]), tensor([92]), tensor([32]), tensor([50]), tensor([62]), tensor([97]), tensor([63]), tensor([92]), tensor([97]), tensor([60]), tensor([0]), tensor([32]), tensor([12]), tensor([7]), tensor([97]), tensor([48]), tensor([25]), tensor([88]), tensor([22]), tensor([56]), tensor([30]), tensor([96]), tensor([94]), tensor([88]), tensor([95]), tensor([0]), tensor([3]), tensor([13]), tensor([27]), tensor([79]), tensor([22]), tensor([30]), tensor([63]), tensor([14]), tensor([53]), tensor([97]), tensor([37]), tensor([27]), tensor([36]), tensor([41]), tensor([31]), tensor([5]), tensor([11]), tensor([25]), tensor([95]), tensor([41]), tensor([52]), tensor([25]), tensor([81]), tensor([25]), tensor([7]), tensor([69]), tensor([80]), tensor([73]), tensor([36]), tensor([5]), tensor([97]), tensor([16]), tensor([41]), tensor([23]), tensor([57]), tensor([37]), tensor([28]), tensor([30]), tensor([69]), tensor([28]), tensor([0]), tensor([11]), tensor([22]), tensor([46]), tensor([69]), tensor([17]), tensor([6]), tensor([50]), tensor([97]), tensor([24]), tensor([81]), tensor([66]), tensor([14]), tensor([1]), tensor([25]), tensor([21]), tensor([30]), tensor([36]), tensor([80]), tensor([71]), tensor([65]), tensor([30]), tensor([67]), tensor([48]), tensor([91]), tensor([25]), tensor([81]), tensor([94]), tensor([66]), tensor([33]), tensor([46]), tensor([92]), tensor([28]), tensor([72]), tensor([14]), tensor([23]), tensor([97]), tensor([17]), tensor([16]), tensor([68]), tensor([88]), tensor([1]), tensor([88]), tensor([86]), tensor([77]), tensor([88]), tensor([46]), tensor([22]), tensor([13]), tensor([63]), tensor([30]), tensor([41]), tensor([58]), tensor([86]), tensor([63]), tensor([72]), tensor([14]), tensor([1]), tensor([82]), tensor([1]), tensor([94]), tensor([50]), tensor([7]), tensor([73]), tensor([58]), tensor([48]), tensor([27]), tensor([37]), tensor([79]), tensor([22]), tensor([66]), tensor([80]), tensor([92]), tensor([24]), tensor([72]), tensor([49]), tensor([98]), tensor([92]), tensor([49]), tensor([98]), tensor([2]), tensor([16]), tensor([60]), tensor([78]), tensor([7]), tensor([69]), tensor([71]), tensor([82]), tensor([31]), tensor([96]), tensor([97]), tensor([57]), tensor([63]), tensor([63]), tensor([82]), tensor([31]), tensor([35]), tensor([63]), tensor([81]), tensor([93]), tensor([27]), tensor([23]), tensor([98]), tensor([69]), tensor([5]), tensor([57]), tensor([72]), tensor([86]), tensor([5]), tensor([59]), tensor([23]), tensor([10]), tensor([31]), tensor([12]), tensor([45]), tensor([21]), tensor([28]), tensor([32]), tensor([35]), tensor([99]), tensor([36]), tensor([66]), tensor([49]), tensor([45]), tensor([17]), tensor([13]), tensor([33]), tensor([12]), tensor([32]), tensor([63]), tensor([22]), tensor([92]), tensor([60]), tensor([53]), tensor([45]), tensor([65]), tensor([17]), tensor([72]), tensor([52]), tensor([52]), tensor([79]), tensor([12]), tensor([13]), tensor([34]), tensor([11]), tensor([45]), tensor([82]), tensor([22]), tensor([6]), tensor([46]), tensor([13]), tensor([99]), tensor([99]), tensor([82]), tensor([62]), tensor([27]), tensor([24]), tensor([63]), tensor([96]), tensor([28]), tensor([33]), tensor([80]), tensor([99]), tensor([82]), tensor([3]), tensor([99]), tensor([17]), tensor([58]), tensor([79]), tensor([57]), tensor([30]), tensor([99]), tensor([78]), tensor([65]), tensor([93]), tensor([95]), tensor([66]), tensor([53]), tensor([25]), tensor([13]), tensor([22]), tensor([5]), tensor([59]), tensor([37]), tensor([95]), tensor([66]), tensor([60]), tensor([3]), tensor([48]), tensor([6]), tensor([96]), tensor([50]), tensor([48]), tensor([91]), tensor([98]), tensor([28]), tensor([81]), tensor([56]), tensor([30]), tensor([3]), tensor([68]), tensor([22]), tensor([23]), tensor([63]), tensor([72]), tensor([58]), tensor([65]), tensor([67]), tensor([10]), tensor([91]), tensor([36]), tensor([24]), tensor([68]), tensor([78]), tensor([5]), tensor([69]), tensor([34]), tensor([94]), tensor([96]), tensor([24]), tensor([98]), tensor([34]), tensor([2]), tensor([33]), tensor([52]), tensor([69]), tensor([81]), tensor([28]), tensor([14]), tensor([96]), tensor([21]), tensor([33]), tensor([21]), tensor([24]), tensor([2]), tensor([72]), tensor([32]), tensor([16]), tensor([27]), tensor([31]), tensor([25]), tensor([66]), tensor([49]), tensor([41]), tensor([17]), tensor([1]), tensor([63]), tensor([62]), tensor([1]), tensor([28]), tensor([10]), tensor([65]), tensor([73]), tensor([11]), tensor([1]), tensor([81]), tensor([65]), tensor([28]), tensor([81]), tensor([45]), tensor([57]), tensor([30]), tensor([49]), tensor([35]), tensor([26]), tensor([59]), tensor([10]), tensor([57]), tensor([77]), tensor([7]), tensor([96]), tensor([36]), tensor([77]), tensor([31]), tensor([73]), tensor([49]), tensor([45]), tensor([35]), tensor([56]), tensor([6]), tensor([6]), tensor([21]), tensor([92]), tensor([86]), tensor([53]), tensor([50]), tensor([92]), tensor([33]), tensor([13]), tensor([34]), tensor([69]), tensor([25]), tensor([27]), tensor([71]), tensor([66]), tensor([35]), tensor([7]), tensor([80]), tensor([45]), tensor([94]), tensor([49]), tensor([96]), tensor([73]), tensor([72]), tensor([53]), tensor([73]), tensor([17]), tensor([48]), tensor([45]), tensor([0]), tensor([63]), tensor([1]), tensor([97]), tensor([32]), tensor([81]), tensor([58]), tensor([78]), tensor([49]), tensor([45]), tensor([77]), tensor([17]), tensor([24]), tensor([50]), tensor([25]), tensor([16]), tensor([26]), tensor([6]), tensor([77]), tensor([67]), tensor([73]), tensor([32]), tensor([69]), tensor([17]), tensor([3]), tensor([91]), tensor([22]), tensor([86]), tensor([88]), tensor([78]), tensor([96]), tensor([21]), tensor([50]), tensor([2]), tensor([94]), tensor([28]), tensor([59]), tensor([60]), tensor([86]), tensor([30]), tensor([46]), tensor([95]), tensor([23]), tensor([26]), tensor([81]), tensor([79]), tensor([7]), tensor([73]), tensor([33]), tensor([25]), tensor([27]), tensor([59]), tensor([10]), tensor([22]), tensor([52]), tensor([26]), tensor([72]), tensor([49]), tensor([12]), tensor([56]), tensor([21]), tensor([80]), tensor([86]), tensor([34]), tensor([94]), tensor([34]), tensor([49]), tensor([35]), tensor([56]), tensor([50]), tensor([58]), tensor([82]), tensor([45]), tensor([48]), tensor([36]), tensor([93]), tensor([52]), tensor([67]), tensor([3]), tensor([62]), tensor([60]), tensor([2]), tensor([88]), tensor([49]), tensor([79]), tensor([86]), tensor([45]), tensor([35]), tensor([65]), tensor([22]), tensor([93]), tensor([30]), tensor([49]), tensor([91]), tensor([0]), tensor([34]), tensor([2]), tensor([16]), tensor([21]), tensor([36]), tensor([45]), tensor([5]), tensor([99]), tensor([97]), tensor([53]), tensor([78]), tensor([25]), tensor([30]), tensor([32]), tensor([33]), tensor([77]), tensor([27]), tensor([73]), tensor([67]), tensor([68]), tensor([17]), tensor([60]), tensor([52]), tensor([6]), tensor([27]), tensor([94]), tensor([93]), tensor([23]), tensor([10]), tensor([32]), tensor([32]), tensor([41]), tensor([7]), tensor([21]), tensor([22]), tensor([7]), tensor([60]), tensor([80]), tensor([28]), tensor([2]), tensor([17]), tensor([96]), tensor([86]), tensor([17]), tensor([32]), tensor([35]), tensor([36]), tensor([23]), tensor([12]), tensor([3]), tensor([88]), tensor([17]), tensor([71]), tensor([57]), tensor([52]), tensor([6]), tensor([95]), tensor([60]), tensor([56]), tensor([14]), tensor([3]), tensor([79]), tensor([60]), tensor([88]), tensor([86]), tensor([0]), tensor([45]), tensor([93]), tensor([94]), tensor([26]), tensor([91]), tensor([59]), tensor([57]), tensor([28]), tensor([62]), tensor([65]), tensor([95]), tensor([11]), tensor([82]), tensor([91]), tensor([3]), tensor([93]), tensor([16]), tensor([30]), tensor([14]), tensor([98]), tensor([91]), tensor([91]), tensor([69]), tensor([26]), tensor([24]), tensor([24]), tensor([98]), tensor([25]), tensor([81]), tensor([60]), tensor([68]), tensor([86]), tensor([63]), tensor([16]), tensor([73]), tensor([32]), tensor([52]), tensor([92]), tensor([13]), tensor([10]), tensor([99]), tensor([56]), tensor([37]), tensor([52]), tensor([58]), tensor([6]), tensor([52]), tensor([34]), tensor([71]), tensor([65]), tensor([98]), tensor([14]), tensor([65]), tensor([52]), tensor([35]), tensor([65]), tensor([93]), tensor([72]), tensor([66]), tensor([12]), tensor([35]), tensor([56]), tensor([99]), tensor([48]), tensor([68]), tensor([21]), tensor([28]), tensor([58]), tensor([82]), tensor([60]), tensor([0]), tensor([52]), tensor([5]), tensor([99]), tensor([63]), tensor([41]), tensor([1]), tensor([37]), tensor([33]), tensor([82]), tensor([11]), tensor([67]), tensor([91]), tensor([71]), tensor([62]), tensor([23]), tensor([22]), tensor([66]), tensor([93]), tensor([3]), tensor([93]), tensor([2]), tensor([0]), tensor([95]), tensor([52]), tensor([65]), tensor([80]), tensor([12]), tensor([73]), tensor([66]), tensor([24]), tensor([14]), tensor([66]), tensor([21]), tensor([12]), tensor([23]), tensor([32]), tensor([67]), tensor([78]), tensor([72]), tensor([14]), tensor([34]), tensor([49]), tensor([58]), tensor([10]), tensor([0]), tensor([1]), tensor([23]), tensor([86]), tensor([94]), tensor([21]), tensor([26]), tensor([10]), tensor([30]), tensor([97]), tensor([48]), tensor([59]), tensor([16]), tensor([24]), tensor([96]), tensor([22]), tensor([53]), tensor([59]), tensor([17]), tensor([41]), tensor([79]), tensor([99]), tensor([1]), tensor([35]), tensor([50]), tensor([80]), tensor([93]), tensor([57]), tensor([60]), tensor([48]), tensor([24]), tensor([31]), tensor([31]), tensor([50]), tensor([17]), tensor([52]), tensor([26]), tensor([67]), tensor([25]), tensor([14]), tensor([46]), tensor([66]), tensor([22]), tensor([79]), tensor([49]), tensor([24]), tensor([52]), tensor([26]), tensor([71]), tensor([24]), tensor([33]), tensor([33]), tensor([31]), tensor([12]), tensor([35]), tensor([41]), tensor([24]), tensor([11]), tensor([68]), tensor([62]), tensor([95]), tensor([1]), tensor([32]), tensor([5]), tensor([98]), tensor([68]), tensor([67]), tensor([99]), tensor([41]), tensor([82]), tensor([63]), tensor([62]), tensor([14]), tensor([98]), tensor([96]), tensor([3]), tensor([69]), tensor([97]), tensor([32]), tensor([17]), tensor([2]), tensor([36]), tensor([16]), tensor([33]), tensor([1]), tensor([69]), tensor([62]), tensor([94]), tensor([57]), tensor([34]), tensor([37]), tensor([77]), tensor([3]), tensor([86]), tensor([16]), tensor([57]), tensor([36]), tensor([22]), tensor([95]), tensor([68]), tensor([78]), tensor([58]), tensor([22]), tensor([22]), tensor([58]), tensor([7]), tensor([21]), tensor([63]), tensor([60]), tensor([69]), tensor([96]), tensor([92]), tensor([16]), tensor([92]), tensor([88]), tensor([72]), tensor([59]), tensor([34]), tensor([34]), tensor([27]), tensor([45]), tensor([6]), tensor([96]), tensor([37]), tensor([10]), tensor([1]), tensor([82]), tensor([34]), tensor([25]), tensor([35]), tensor([27]), tensor([86]), tensor([24]), tensor([27]), tensor([57]), tensor([6]), tensor([12]), tensor([13]), tensor([5]), tensor([11]), tensor([95]), tensor([97]), tensor([88]), tensor([6]), tensor([23]), tensor([72]), tensor([96]), tensor([0]), tensor([21]), tensor([78]), tensor([71]), tensor([65]), tensor([57]), tensor([96]), tensor([80]), tensor([5]), tensor([32]), tensor([13]), tensor([95]), tensor([71]), tensor([79]), tensor([28]), tensor([82]), tensor([13]), tensor([96]), tensor([78]), tensor([13]), tensor([81]), tensor([98]), tensor([0]), tensor([94]), tensor([21]), tensor([92]), tensor([73]), tensor([25]), tensor([82]), tensor([72]), tensor([11]), tensor([86]), tensor([26]), tensor([88]), tensor([71]), tensor([1]), tensor([93]), tensor([27]), tensor([98]), tensor([30]), tensor([99]), tensor([80]), tensor([58]), tensor([2]), tensor([77]), tensor([69]), tensor([3]), tensor([31]), tensor([72]), tensor([71]), tensor([1]), tensor([7]), tensor([10]), tensor([66]), tensor([96]), tensor([92]), tensor([67]), tensor([45]), tensor([25]), tensor([65]), tensor([50]), tensor([14]), tensor([23]), tensor([57]), tensor([68]), tensor([77]), tensor([26]), tensor([11]), tensor([73]), tensor([57]), tensor([98]), tensor([78]), tensor([14]), tensor([96]), tensor([68]), tensor([78]), tensor([72]), tensor([26]), tensor([92]), tensor([86]), tensor([49]), tensor([5]), tensor([13]), tensor([6]), tensor([23]), tensor([79]), tensor([35]), tensor([62]), tensor([67]), tensor([11]), tensor([46]), tensor([69]), tensor([96]), tensor([34]), tensor([1]), tensor([93]), tensor([28]), tensor([16]), tensor([27]), tensor([99]), tensor([10]), tensor([86]), tensor([26]), tensor([68]), tensor([3]), tensor([23]), tensor([78]), tensor([53]), tensor([93]), tensor([0]), tensor([32]), tensor([0]), tensor([5]), tensor([3]), tensor([80]), tensor([81]), tensor([28]), tensor([35]), tensor([2]), tensor([91]), tensor([62]), tensor([71]), tensor([45]), tensor([59]), tensor([96]), tensor([58]), tensor([97]), tensor([0]), tensor([63]), tensor([56]), tensor([46]), tensor([67]), tensor([37]), tensor([71]), tensor([48]), tensor([81]), tensor([3]), tensor([35]), tensor([31]), tensor([2]), tensor([37]), tensor([65]), tensor([16]), tensor([94]), tensor([67]), tensor([65]), tensor([10]), tensor([80]), tensor([13]), tensor([95]), tensor([53]), tensor([28]), tensor([28]), tensor([32]), tensor([11]), tensor([73]), tensor([32]), tensor([46]), tensor([86]), tensor([60]), tensor([6]), tensor([12]), tensor([82]), tensor([10]), tensor([94]), tensor([68]), tensor([86]), tensor([14]), tensor([27]), tensor([67]), tensor([16]), tensor([77]), tensor([21]), tensor([56]), tensor([67]), tensor([0]), tensor([93]), tensor([71]), tensor([60]), tensor([31]), tensor([17]), tensor([37]), tensor([78]), tensor([53]), tensor([12]), tensor([73]), tensor([65]), tensor([79]), tensor([91]), tensor([73]), tensor([1]), tensor([30]), tensor([41]), tensor([96]), tensor([50]), tensor([31]), tensor([3]), tensor([48]), tensor([31]), tensor([0]), tensor([68]), tensor([53]), tensor([95]), tensor([17]), tensor([67]), tensor([12]), tensor([77]), tensor([36]), tensor([46]), tensor([33]), tensor([93]), tensor([7]), tensor([28]), tensor([32]), tensor([11]), tensor([58]), tensor([31]), tensor([37]), tensor([25]), tensor([12]), tensor([24]), tensor([3]), tensor([59]), tensor([34]), tensor([71]), tensor([97]), tensor([66]), tensor([1]), tensor([73]), tensor([80]), tensor([45]), tensor([81]), tensor([41]), tensor([36]), tensor([62]), tensor([17]), tensor([46]), tensor([24]), tensor([13]), tensor([28]), tensor([97]), tensor([68]), tensor([32]), tensor([97]), tensor([35]), tensor([28]), tensor([49]), tensor([99]), tensor([66]), tensor([62]), tensor([99]), tensor([3]), tensor([50]), tensor([94]), tensor([56]), tensor([56]), tensor([23]), tensor([25]), tensor([99]), tensor([82]), tensor([11]), tensor([68]), tensor([77]), tensor([37]), tensor([2]), tensor([50]), tensor([6]), tensor([50]), tensor([21]), tensor([27]), tensor([58]), tensor([57]), tensor([78]), tensor([68]), tensor([26]), tensor([78]), tensor([59]), tensor([23]), tensor([11]), tensor([56]), tensor([66]), tensor([98]), tensor([13]), tensor([88]), tensor([50]), tensor([33]), tensor([5]), tensor([77]), tensor([36]), tensor([78]), tensor([10]), tensor([36]), tensor([30]), tensor([21]), tensor([63]), tensor([22]), tensor([77]), tensor([58]), tensor([14]), tensor([82]), tensor([57]), tensor([27]), tensor([30]), tensor([41]), tensor([2]), tensor([50]), tensor([35]), tensor([59]), tensor([10]), tensor([78]), tensor([6]), tensor([6]), tensor([91]), tensor([95]), tensor([5]), tensor([79]), tensor([10]), tensor([12]), tensor([58]), tensor([21]), tensor([69]), tensor([16]), tensor([27]), tensor([72]), tensor([46]), tensor([72]), tensor([81]), tensor([26]), tensor([46]), tensor([92]), tensor([95]), tensor([7]), tensor([12]), tensor([79]), tensor([62]), tensor([81]), tensor([31]), tensor([48]), tensor([26]), tensor([93]), tensor([17]), tensor([72]), tensor([31]), tensor([0]), tensor([88]), tensor([41]), tensor([94]), tensor([11]), tensor([16]), tensor([99]), tensor([5]), tensor([13]), tensor([94]), tensor([33]), tensor([1]), tensor([11]), tensor([72]), tensor([37])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.54 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.53\n",
            "TEST ALL:  0.47\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  8000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [99, 95, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 65, 69, 73, 77, 81, 93, 13, 5, 1, 48, 4, 12, 16, 24, 28, 32, 36, 52, 96, 56, 60, 68, 72, 80, 88, 92, 97, 2, 6, 55, 11, 23, 27, 31, 35, 39, 47, 59, 3, 63, 67, 71, 75, 79, 87, 91, 7, 98, 10, 46, 14, 22, 26, 30, 34, 38, 42, 50, 94, 58, 62, 66, 70, 78, 82, 86, 0]\n",
            "TRAIN_SET CLASSES:  [87, 75, 55, 47, 39, 70, 42, 38, 29, 4]\n",
            "VALIDATION CLASSES:  [55, 47, 42, 39, 38, 29, 87, 75, 70, 4]\n",
            "GROUP:  8\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [87, 75, 55, 47, 39, 70, 42, 38, 29, 4]\n",
            "Len TOTAL train susbset:  6910\n",
            "training\n",
            "num classes till now:  80\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.18877437710762024\n",
            "Train step - Step 10, Loss 0.13897837698459625\n",
            "Train step - Step 20, Loss 0.13506563007831573\n",
            "Train step - Step 30, Loss 0.12165265530347824\n",
            "Train step - Step 40, Loss 0.11963336914777756\n",
            "Train step - Step 50, Loss 0.11664227396249771\n",
            "Train epoch - Accuracy: 0.15224312590448624 Loss: 0.13181550753789426 Corrects: 1052\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12257011979818344\n",
            "Train step - Step 70, Loss 0.11895585060119629\n",
            "Train step - Step 80, Loss 0.11448218673467636\n",
            "Train step - Step 90, Loss 0.11810316145420074\n",
            "Train step - Step 100, Loss 0.12154944986104965\n",
            "Train epoch - Accuracy: 0.17206946454413893 Loss: 0.11720298824969318 Corrects: 1189\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11729583889245987\n",
            "Train step - Step 120, Loss 0.1137746199965477\n",
            "Train step - Step 130, Loss 0.11708809435367584\n",
            "Train step - Step 140, Loss 0.10993082821369171\n",
            "Train step - Step 150, Loss 0.11672241985797882\n",
            "Train step - Step 160, Loss 0.11470808833837509\n",
            "Train epoch - Accuracy: 0.19942112879884225 Loss: 0.11555831765731751 Corrects: 1378\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.11224160343408585\n",
            "Train step - Step 180, Loss 0.11342840641736984\n",
            "Train step - Step 190, Loss 0.11865609139204025\n",
            "Train step - Step 200, Loss 0.11169041693210602\n",
            "Train step - Step 210, Loss 0.11365269869565964\n",
            "Train epoch - Accuracy: 0.22416787264833574 Loss: 0.11384046411333139 Corrects: 1549\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11927373707294464\n",
            "Train step - Step 230, Loss 0.11188855022192001\n",
            "Train step - Step 240, Loss 0.11578192561864853\n",
            "Train step - Step 250, Loss 0.11404675245285034\n",
            "Train step - Step 260, Loss 0.1136101707816124\n",
            "Train epoch - Accuracy: 0.23950795947901593 Loss: 0.11325907235380192 Corrects: 1655\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 270, Loss 0.114049032330513\n",
            "Train step - Step 280, Loss 0.11596884578466415\n",
            "Train step - Step 290, Loss 0.11020974069833755\n",
            "Train step - Step 300, Loss 0.1126345619559288\n",
            "Train step - Step 310, Loss 0.10639949887990952\n",
            "Train step - Step 320, Loss 0.11154326051473618\n",
            "Train epoch - Accuracy: 0.25007235890014473 Loss: 0.11291875900692602 Corrects: 1728\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11057982593774796\n",
            "Train step - Step 340, Loss 0.11529035866260529\n",
            "Train step - Step 350, Loss 0.11318513005971909\n",
            "Train step - Step 360, Loss 0.10862749069929123\n",
            "Train step - Step 370, Loss 0.10545235127210617\n",
            "Train epoch - Accuracy: 0.2635311143270622 Loss: 0.11264116633602576 Corrects: 1821\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 380, Loss 0.1068003922700882\n",
            "Train step - Step 390, Loss 0.11561935395002365\n",
            "Train step - Step 400, Loss 0.11791229248046875\n",
            "Train step - Step 410, Loss 0.1124526783823967\n",
            "Train step - Step 420, Loss 0.11663001030683517\n",
            "Train step - Step 430, Loss 0.11091581732034683\n",
            "Train epoch - Accuracy: 0.27554269175108537 Loss: 0.11200389404459041 Corrects: 1904\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10814151912927628\n",
            "Train step - Step 450, Loss 0.10851352661848068\n",
            "Train step - Step 460, Loss 0.10829134285449982\n",
            "Train step - Step 470, Loss 0.1099938303232193\n",
            "Train step - Step 480, Loss 0.10776803642511368\n",
            "Train epoch - Accuracy: 0.287698986975398 Loss: 0.11196243667223002 Corrects: 1988\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 490, Loss 0.11525364220142365\n",
            "Train step - Step 500, Loss 0.11156954616308212\n",
            "Train step - Step 510, Loss 0.1119389757514\n",
            "Train step - Step 520, Loss 0.10570404678583145\n",
            "Train step - Step 530, Loss 0.10835955291986465\n",
            "Train epoch - Accuracy: 0.29088277858176553 Loss: 0.11163510792550406 Corrects: 2010\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 540, Loss 0.11030705273151398\n",
            "Train step - Step 550, Loss 0.11722107976675034\n",
            "Train step - Step 560, Loss 0.10809045284986496\n",
            "Train step - Step 570, Loss 0.11295860260725021\n",
            "Train step - Step 580, Loss 0.10874038189649582\n",
            "Train step - Step 590, Loss 0.11010672152042389\n",
            "Train epoch - Accuracy: 0.30274963820549927 Loss: 0.11145392227837042 Corrects: 2092\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 600, Loss 0.11958513408899307\n",
            "Train step - Step 610, Loss 0.10859810560941696\n",
            "Train step - Step 620, Loss 0.11197829246520996\n",
            "Train step - Step 630, Loss 0.10683133453130722\n",
            "Train step - Step 640, Loss 0.11118190735578537\n",
            "Train epoch - Accuracy: 0.3114327062228654 Loss: 0.1112434090671767 Corrects: 2152\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 650, Loss 0.10845135897397995\n",
            "Train step - Step 660, Loss 0.1129557266831398\n",
            "Train step - Step 670, Loss 0.1106153056025505\n",
            "Train step - Step 680, Loss 0.10845481604337692\n",
            "Train step - Step 690, Loss 0.1044943556189537\n",
            "Train step - Step 700, Loss 0.11082782596349716\n",
            "Train epoch - Accuracy: 0.3118668596237337 Loss: 0.11107524241793863 Corrects: 2155\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.11582052707672119\n",
            "Train step - Step 720, Loss 0.113172248005867\n",
            "Train step - Step 730, Loss 0.10862045735120773\n",
            "Train step - Step 740, Loss 0.109480120241642\n",
            "Train step - Step 750, Loss 0.10628721863031387\n",
            "Train epoch - Accuracy: 0.3251808972503618 Loss: 0.11104260347082714 Corrects: 2247\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 760, Loss 0.1101221814751625\n",
            "Train step - Step 770, Loss 0.11296570301055908\n",
            "Train step - Step 780, Loss 0.11297651380300522\n",
            "Train step - Step 790, Loss 0.10916514694690704\n",
            "Train step - Step 800, Loss 0.10859381407499313\n",
            "Train epoch - Accuracy: 0.32141823444283646 Loss: 0.11082374317256138 Corrects: 2221\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 810, Loss 0.11223359405994415\n",
            "Train step - Step 820, Loss 0.10595257580280304\n",
            "Train step - Step 830, Loss 0.10799863189458847\n",
            "Train step - Step 840, Loss 0.10465947538614273\n",
            "Train step - Step 850, Loss 0.11538256704807281\n",
            "Train step - Step 860, Loss 0.11219199001789093\n",
            "Train epoch - Accuracy: 0.331548480463097 Loss: 0.1104595955606825 Corrects: 2291\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 870, Loss 0.10834157466888428\n",
            "Train step - Step 880, Loss 0.10818839073181152\n",
            "Train step - Step 890, Loss 0.10934609174728394\n",
            "Train step - Step 900, Loss 0.10556007921695709\n",
            "Train step - Step 910, Loss 0.10915684700012207\n",
            "Train epoch - Accuracy: 0.3344428364688857 Loss: 0.11048388579043569 Corrects: 2311\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 920, Loss 0.10807575285434723\n",
            "Train step - Step 930, Loss 0.11338410526514053\n",
            "Train step - Step 940, Loss 0.10647459328174591\n",
            "Train step - Step 950, Loss 0.10687962919473648\n",
            "Train step - Step 960, Loss 0.11056814342737198\n",
            "Train step - Step 970, Loss 0.10420294106006622\n",
            "Train epoch - Accuracy: 0.34327062228654126 Loss: 0.11043839996785745 Corrects: 2372\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.11108999699354172\n",
            "Train step - Step 990, Loss 0.10737981647253036\n",
            "Train step - Step 1000, Loss 0.11469914764165878\n",
            "Train step - Step 1010, Loss 0.1078031063079834\n",
            "Train step - Step 1020, Loss 0.10543491691350937\n",
            "Train epoch - Accuracy: 0.3512301013024602 Loss: 0.11007602762339257 Corrects: 2427\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1030, Loss 0.10897510498762131\n",
            "Train step - Step 1040, Loss 0.10563447326421738\n",
            "Train step - Step 1050, Loss 0.10892342776060104\n",
            "Train step - Step 1060, Loss 0.10885541886091232\n",
            "Train step - Step 1070, Loss 0.11035964637994766\n",
            "Train epoch - Accuracy: 0.3577424023154848 Loss: 0.10994110212657282 Corrects: 2472\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1080, Loss 0.11072786897420883\n",
            "Train step - Step 1090, Loss 0.10603529214859009\n",
            "Train step - Step 1100, Loss 0.1095859557390213\n",
            "Train step - Step 1110, Loss 0.11078665405511856\n",
            "Train step - Step 1120, Loss 0.11021792143583298\n",
            "Train step - Step 1130, Loss 0.1121165081858635\n",
            "Train epoch - Accuracy: 0.356150506512301 Loss: 0.10994941983614576 Corrects: 2461\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.10715055465698242\n",
            "Train step - Step 1150, Loss 0.11241938173770905\n",
            "Train step - Step 1160, Loss 0.10953662544488907\n",
            "Train step - Step 1170, Loss 0.1085326224565506\n",
            "Train step - Step 1180, Loss 0.10765276104211807\n",
            "Train epoch - Accuracy: 0.36005788712011577 Loss: 0.1098044949873484 Corrects: 2488\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1190, Loss 0.10631263256072998\n",
            "Train step - Step 1200, Loss 0.11345267295837402\n",
            "Train step - Step 1210, Loss 0.11322277784347534\n",
            "Train step - Step 1220, Loss 0.11319706588983536\n",
            "Train step - Step 1230, Loss 0.1112300381064415\n",
            "Train step - Step 1240, Loss 0.11705620586872101\n",
            "Train epoch - Accuracy: 0.36642547033285094 Loss: 0.10962316451214157 Corrects: 2532\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.11533010005950928\n",
            "Train step - Step 1260, Loss 0.10831663757562637\n",
            "Train step - Step 1270, Loss 0.1098090410232544\n",
            "Train step - Step 1280, Loss 0.10402288287878036\n",
            "Train step - Step 1290, Loss 0.10968201607465744\n",
            "Train epoch - Accuracy: 0.37062228654124457 Loss: 0.10971309324206216 Corrects: 2561\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1300, Loss 0.11268782615661621\n",
            "Train step - Step 1310, Loss 0.10507091134786606\n",
            "Train step - Step 1320, Loss 0.11588820070028305\n",
            "Train step - Step 1330, Loss 0.10493246465921402\n",
            "Train step - Step 1340, Loss 0.10745327919721603\n",
            "Train epoch - Accuracy: 0.373082489146165 Loss: 0.10967385459437902 Corrects: 2578\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1350, Loss 0.10829194635152817\n",
            "Train step - Step 1360, Loss 0.11170685291290283\n",
            "Train step - Step 1370, Loss 0.1088087186217308\n",
            "Train step - Step 1380, Loss 0.10379566252231598\n",
            "Train step - Step 1390, Loss 0.10730981081724167\n",
            "Train step - Step 1400, Loss 0.1109725832939148\n",
            "Train epoch - Accuracy: 0.37858176555716355 Loss: 0.1095731199564016 Corrects: 2616\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.10803400725126266\n",
            "Train step - Step 1420, Loss 0.10524880886077881\n",
            "Train step - Step 1430, Loss 0.10485787689685822\n",
            "Train step - Step 1440, Loss 0.11164146661758423\n",
            "Train step - Step 1450, Loss 0.11047635227441788\n",
            "Train epoch - Accuracy: 0.3782923299565847 Loss: 0.10922025880963689 Corrects: 2614\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1460, Loss 0.11089690774679184\n",
            "Train step - Step 1470, Loss 0.10622145980596542\n",
            "Train step - Step 1480, Loss 0.1070624515414238\n",
            "Train step - Step 1490, Loss 0.11028041690587997\n",
            "Train step - Step 1500, Loss 0.10512547940015793\n",
            "Train step - Step 1510, Loss 0.10645315796136856\n",
            "Train epoch - Accuracy: 0.38219971056439944 Loss: 0.10957249499609432 Corrects: 2641\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1520, Loss 0.10542085021734238\n",
            "Train step - Step 1530, Loss 0.11223254352807999\n",
            "Train step - Step 1540, Loss 0.11242518573999405\n",
            "Train step - Step 1550, Loss 0.11040160804986954\n",
            "Train step - Step 1560, Loss 0.10928987711668015\n",
            "Train epoch - Accuracy: 0.38683068017366135 Loss: 0.10936445188850122 Corrects: 2673\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1570, Loss 0.11210619658231735\n",
            "Train step - Step 1580, Loss 0.1090092808008194\n",
            "Train step - Step 1590, Loss 0.10928191244602203\n",
            "Train step - Step 1600, Loss 0.10851879417896271\n",
            "Train step - Step 1610, Loss 0.11433210223913193\n",
            "Train epoch - Accuracy: 0.39088277858176557 Loss: 0.10947046637017543 Corrects: 2701\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1620, Loss 0.10509796440601349\n",
            "Train step - Step 1630, Loss 0.1100102886557579\n",
            "Train step - Step 1640, Loss 0.10840769857168198\n",
            "Train step - Step 1650, Loss 0.10919354110956192\n",
            "Train step - Step 1660, Loss 0.10850479453802109\n",
            "Train step - Step 1670, Loss 0.10972002893686295\n",
            "Train epoch - Accuracy: 0.40144717800289437 Loss: 0.1089628111587247 Corrects: 2774\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.11038612574338913\n",
            "Train step - Step 1690, Loss 0.11051007360219955\n",
            "Train step - Step 1700, Loss 0.11793017387390137\n",
            "Train step - Step 1710, Loss 0.10449446737766266\n",
            "Train step - Step 1720, Loss 0.11086455732584\n",
            "Train epoch - Accuracy: 0.39652677279305354 Loss: 0.1090566926275942 Corrects: 2740\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1730, Loss 0.10255267471075058\n",
            "Train step - Step 1740, Loss 0.11300136893987656\n",
            "Train step - Step 1750, Loss 0.11322528123855591\n",
            "Train step - Step 1760, Loss 0.10796016454696655\n",
            "Train step - Step 1770, Loss 0.1062893271446228\n",
            "Train step - Step 1780, Loss 0.1146787628531456\n",
            "Train epoch - Accuracy: 0.40434153400868306 Loss: 0.10914624687688568 Corrects: 2794\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1790, Loss 0.10983055084943771\n",
            "Train step - Step 1800, Loss 0.11186392605304718\n",
            "Train step - Step 1810, Loss 0.11308268457651138\n",
            "Train step - Step 1820, Loss 0.10518407821655273\n",
            "Train step - Step 1830, Loss 0.11259114742279053\n",
            "Train epoch - Accuracy: 0.40144717800289437 Loss: 0.10882311843324846 Corrects: 2774\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.10614154487848282\n",
            "Train step - Step 1850, Loss 0.10855139791965485\n",
            "Train step - Step 1860, Loss 0.10690879821777344\n",
            "Train step - Step 1870, Loss 0.1038115993142128\n",
            "Train step - Step 1880, Loss 0.10353495925664902\n",
            "Train epoch - Accuracy: 0.4015918958031838 Loss: 0.1088831196839661 Corrects: 2775\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1890, Loss 0.10594115406274796\n",
            "Train step - Step 1900, Loss 0.10554666817188263\n",
            "Train step - Step 1910, Loss 0.11125282198190689\n",
            "Train step - Step 1920, Loss 0.1065271869301796\n",
            "Train step - Step 1930, Loss 0.1052015945315361\n",
            "Train step - Step 1940, Loss 0.1070900708436966\n",
            "Train epoch - Accuracy: 0.41490593342981186 Loss: 0.10914533437807894 Corrects: 2867\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1950, Loss 0.10353125631809235\n",
            "Train step - Step 1960, Loss 0.10475434362888336\n",
            "Train step - Step 1970, Loss 0.10481033474206924\n",
            "Train step - Step 1980, Loss 0.10306191444396973\n",
            "Train step - Step 1990, Loss 0.11231887340545654\n",
            "Train epoch - Accuracy: 0.4136034732272069 Loss: 0.10904318046164409 Corrects: 2858\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2000, Loss 0.11126649379730225\n",
            "Train step - Step 2010, Loss 0.11040347069501877\n",
            "Train step - Step 2020, Loss 0.1060752421617508\n",
            "Train step - Step 2030, Loss 0.10729757696390152\n",
            "Train step - Step 2040, Loss 0.11145815998315811\n",
            "Train step - Step 2050, Loss 0.1143593117594719\n",
            "Train epoch - Accuracy: 0.4117221418234443 Loss: 0.10877090965097098 Corrects: 2845\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2060, Loss 0.11150465160608292\n",
            "Train step - Step 2070, Loss 0.10821395367383957\n",
            "Train step - Step 2080, Loss 0.11115238815546036\n",
            "Train step - Step 2090, Loss 0.11085646599531174\n",
            "Train step - Step 2100, Loss 0.10894143581390381\n",
            "Train epoch - Accuracy: 0.4199710564399421 Loss: 0.10875911150240863 Corrects: 2902\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2110, Loss 0.10740971565246582\n",
            "Train step - Step 2120, Loss 0.11012375354766846\n",
            "Train step - Step 2130, Loss 0.1123194471001625\n",
            "Train step - Step 2140, Loss 0.10707942396402359\n",
            "Train step - Step 2150, Loss 0.11062528938055038\n",
            "Train epoch - Accuracy: 0.42054992764109983 Loss: 0.10894448050498617 Corrects: 2906\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2160, Loss 0.10771413147449493\n",
            "Train step - Step 2170, Loss 0.11991439014673233\n",
            "Train step - Step 2180, Loss 0.10783066600561142\n",
            "Train step - Step 2190, Loss 0.11294998973608017\n",
            "Train step - Step 2200, Loss 0.10779903084039688\n",
            "Train step - Step 2210, Loss 0.11632449924945831\n",
            "Train epoch - Accuracy: 0.4309696092619392 Loss: 0.10885551737678034 Corrects: 2978\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2220, Loss 0.11107319593429565\n",
            "Train step - Step 2230, Loss 0.1098228469491005\n",
            "Train step - Step 2240, Loss 0.11266177892684937\n",
            "Train step - Step 2250, Loss 0.10748203098773956\n",
            "Train step - Step 2260, Loss 0.1050996333360672\n",
            "Train epoch - Accuracy: 0.4295224312590449 Loss: 0.10831964895797708 Corrects: 2968\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2270, Loss 0.10853938013315201\n",
            "Train step - Step 2280, Loss 0.10705020278692245\n",
            "Train step - Step 2290, Loss 0.11018174886703491\n",
            "Train step - Step 2300, Loss 0.1109001412987709\n",
            "Train step - Step 2310, Loss 0.10648321360349655\n",
            "Train step - Step 2320, Loss 0.10880694538354874\n",
            "Train epoch - Accuracy: 0.4248914616497829 Loss: 0.10859000256282376 Corrects: 2936\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2330, Loss 0.1040068045258522\n",
            "Train step - Step 2340, Loss 0.11039745807647705\n",
            "Train step - Step 2350, Loss 0.10842572897672653\n",
            "Train step - Step 2360, Loss 0.11005287617444992\n",
            "Train step - Step 2370, Loss 0.11254000663757324\n",
            "Train epoch - Accuracy: 0.43082489146164976 Loss: 0.10883189015597235 Corrects: 2977\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2380, Loss 0.11025303602218628\n",
            "Train step - Step 2390, Loss 0.10774703323841095\n",
            "Train step - Step 2400, Loss 0.10464074462652206\n",
            "Train step - Step 2410, Loss 0.10740718990564346\n",
            "Train step - Step 2420, Loss 0.11451790481805801\n",
            "Train epoch - Accuracy: 0.42995658465991315 Loss: 0.10831076953716458 Corrects: 2971\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2430, Loss 0.10952261835336685\n",
            "Train step - Step 2440, Loss 0.10931211709976196\n",
            "Train step - Step 2450, Loss 0.10760162025690079\n",
            "Train step - Step 2460, Loss 0.10530320554971695\n",
            "Train step - Step 2470, Loss 0.10403630882501602\n",
            "Train step - Step 2480, Loss 0.11036133766174316\n",
            "Train epoch - Accuracy: 0.4409551374819103 Loss: 0.10830740338641552 Corrects: 3047\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2490, Loss 0.10893507301807404\n",
            "Train step - Step 2500, Loss 0.10722947120666504\n",
            "Train step - Step 2510, Loss 0.10945906490087509\n",
            "Train step - Step 2520, Loss 0.10905923694372177\n",
            "Train step - Step 2530, Loss 0.10888027399778366\n",
            "Train epoch - Accuracy: 0.4402315484804631 Loss: 0.1083176534876292 Corrects: 3042\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2540, Loss 0.11360355466604233\n",
            "Train step - Step 2550, Loss 0.10532187670469284\n",
            "Train step - Step 2560, Loss 0.10508526861667633\n",
            "Train step - Step 2570, Loss 0.11947988718748093\n",
            "Train step - Step 2580, Loss 0.11363186687231064\n",
            "Train step - Step 2590, Loss 0.1054558977484703\n",
            "Train epoch - Accuracy: 0.4419681620839363 Loss: 0.10823938841843915 Corrects: 3054\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2600, Loss 0.10315728187561035\n",
            "Train step - Step 2610, Loss 0.10928789526224136\n",
            "Train step - Step 2620, Loss 0.10957520455121994\n",
            "Train step - Step 2630, Loss 0.10598862171173096\n",
            "Train step - Step 2640, Loss 0.09749634563922882\n",
            "Train epoch - Accuracy: 0.4492040520984081 Loss: 0.10789807808683853 Corrects: 3104\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2650, Loss 0.10539300739765167\n",
            "Train step - Step 2660, Loss 0.10121240466833115\n",
            "Train step - Step 2670, Loss 0.1064654141664505\n",
            "Train step - Step 2680, Loss 0.10785946995019913\n",
            "Train step - Step 2690, Loss 0.10855045169591904\n",
            "Train epoch - Accuracy: 0.44775687409551373 Loss: 0.10778220959665462 Corrects: 3094\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2700, Loss 0.11404644697904587\n",
            "Train step - Step 2710, Loss 0.10787781327962875\n",
            "Train step - Step 2720, Loss 0.11284613609313965\n",
            "Train step - Step 2730, Loss 0.11447751522064209\n",
            "Train step - Step 2740, Loss 0.10457050800323486\n",
            "Train step - Step 2750, Loss 0.10540292412042618\n",
            "Train epoch - Accuracy: 0.45180897250361796 Loss: 0.10790789050795407 Corrects: 3122\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2760, Loss 0.10687129944562912\n",
            "Train step - Step 2770, Loss 0.10919064283370972\n",
            "Train step - Step 2780, Loss 0.11385159939527512\n",
            "Train step - Step 2790, Loss 0.10481937974691391\n",
            "Train step - Step 2800, Loss 0.10321535915136337\n",
            "Train epoch - Accuracy: 0.4483357452966715 Loss: 0.10727206095298361 Corrects: 3098\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.1111454963684082\n",
            "Train step - Step 2820, Loss 0.10872495174407959\n",
            "Train step - Step 2830, Loss 0.1066020280122757\n",
            "Train step - Step 2840, Loss 0.10440094769001007\n",
            "Train step - Step 2850, Loss 0.1044958233833313\n",
            "Train step - Step 2860, Loss 0.1055370345711708\n",
            "Train epoch - Accuracy: 0.4487698986975398 Loss: 0.10757111412528282 Corrects: 3101\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2870, Loss 0.106661856174469\n",
            "Train step - Step 2880, Loss 0.1058851033449173\n",
            "Train step - Step 2890, Loss 0.11543821543455124\n",
            "Train step - Step 2900, Loss 0.10607268661260605\n",
            "Train step - Step 2910, Loss 0.10729475319385529\n",
            "Train epoch - Accuracy: 0.45557163531114325 Loss: 0.10762992240758087 Corrects: 3148\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.10743852704763412\n",
            "Train step - Step 2930, Loss 0.10356413573026657\n",
            "Train step - Step 2940, Loss 0.10491534322500229\n",
            "Train step - Step 2950, Loss 0.10885139554738998\n",
            "Train step - Step 2960, Loss 0.10779096931219101\n",
            "Train epoch - Accuracy: 0.4479015918958032 Loss: 0.10751303128154165 Corrects: 3095\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.10326772928237915\n",
            "Train step - Step 2980, Loss 0.10481622070074081\n",
            "Train step - Step 2990, Loss 0.1070701852440834\n",
            "Train step - Step 3000, Loss 0.10892193764448166\n",
            "Train step - Step 3010, Loss 0.10579414665699005\n",
            "Train step - Step 3020, Loss 0.11294833570718765\n",
            "Train epoch - Accuracy: 0.4539797395079595 Loss: 0.10725142111093366 Corrects: 3137\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.10801757872104645\n",
            "Train step - Step 3040, Loss 0.11148524284362793\n",
            "Train step - Step 3050, Loss 0.10401294380426407\n",
            "Train step - Step 3060, Loss 0.11264457553625107\n",
            "Train step - Step 3070, Loss 0.10624740272760391\n",
            "Train epoch - Accuracy: 0.45470332850940665 Loss: 0.1073606700697097 Corrects: 3142\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.11044790595769882\n",
            "Train step - Step 3090, Loss 0.10943951457738876\n",
            "Train step - Step 3100, Loss 0.10873037576675415\n",
            "Train step - Step 3110, Loss 0.10277865082025528\n",
            "Train step - Step 3120, Loss 0.11274080723524094\n",
            "Train step - Step 3130, Loss 0.10662104189395905\n",
            "Train epoch - Accuracy: 0.45470332850940665 Loss: 0.10751446538999698 Corrects: 3142\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.1099236011505127\n",
            "Train step - Step 3150, Loss 0.11399133503437042\n",
            "Train step - Step 3160, Loss 0.10541540384292603\n",
            "Train step - Step 3170, Loss 0.1068492904305458\n",
            "Train step - Step 3180, Loss 0.1084199920296669\n",
            "Train epoch - Accuracy: 0.45991316931982634 Loss: 0.10726926414519763 Corrects: 3178\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.10630112141370773\n",
            "Train step - Step 3200, Loss 0.10636565834283829\n",
            "Train step - Step 3210, Loss 0.10998298972845078\n",
            "Train step - Step 3220, Loss 0.10455924272537231\n",
            "Train step - Step 3230, Loss 0.10346678644418716\n",
            "Train epoch - Accuracy: 0.4541244573082489 Loss: 0.10782339807745345 Corrects: 3138\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3240, Loss 0.10675330460071564\n",
            "Train step - Step 3250, Loss 0.11241636425256729\n",
            "Train step - Step 3260, Loss 0.10814215987920761\n",
            "Train step - Step 3270, Loss 0.1096867099404335\n",
            "Train step - Step 3280, Loss 0.10584910213947296\n",
            "Train step - Step 3290, Loss 0.10295071452856064\n",
            "Train epoch - Accuracy: 0.45354558610709117 Loss: 0.10738723236210267 Corrects: 3134\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.10911338776350021\n",
            "Train step - Step 3310, Loss 0.10821957886219025\n",
            "Train step - Step 3320, Loss 0.11272753775119781\n",
            "Train step - Step 3330, Loss 0.10428237915039062\n",
            "Train step - Step 3340, Loss 0.10228051990270615\n",
            "Train epoch - Accuracy: 0.4565846599131693 Loss: 0.1072554809721955 Corrects: 3155\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3350, Loss 0.105851911008358\n",
            "Train step - Step 3360, Loss 0.10401220619678497\n",
            "Train step - Step 3370, Loss 0.1059749498963356\n",
            "Train step - Step 3380, Loss 0.1039453074336052\n",
            "Train step - Step 3390, Loss 0.1089467778801918\n",
            "Train step - Step 3400, Loss 0.11085090786218643\n",
            "Train epoch - Accuracy: 0.4548480463096961 Loss: 0.1073643955282123 Corrects: 3143\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3410, Loss 0.10408871620893478\n",
            "Train step - Step 3420, Loss 0.11319589614868164\n",
            "Train step - Step 3430, Loss 0.10307580232620239\n",
            "Train step - Step 3440, Loss 0.10657985508441925\n",
            "Train step - Step 3450, Loss 0.10392655432224274\n",
            "Train epoch - Accuracy: 0.45904486251808974 Loss: 0.10724895731714458 Corrects: 3172\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3460, Loss 0.10959501564502716\n",
            "Train step - Step 3470, Loss 0.11050980538129807\n",
            "Train step - Step 3480, Loss 0.10619913786649704\n",
            "Train step - Step 3490, Loss 0.10528912395238876\n",
            "Train step - Step 3500, Loss 0.10905641317367554\n",
            "Train epoch - Accuracy: 0.45267727930535456 Loss: 0.10751829433717189 Corrects: 3128\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3510, Loss 0.10699620097875595\n",
            "Train step - Step 3520, Loss 0.10938739776611328\n",
            "Train step - Step 3530, Loss 0.1052592545747757\n",
            "Train step - Step 3540, Loss 0.10684066265821457\n",
            "Train step - Step 3550, Loss 0.10743862390518188\n",
            "Train step - Step 3560, Loss 0.10817237943410873\n",
            "Train epoch - Accuracy: 0.4548480463096961 Loss: 0.1071051071008449 Corrects: 3143\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3570, Loss 0.1067626029253006\n",
            "Train step - Step 3580, Loss 0.10605865716934204\n",
            "Train step - Step 3590, Loss 0.1034398302435875\n",
            "Train step - Step 3600, Loss 0.10463196039199829\n",
            "Train step - Step 3610, Loss 0.1103852167725563\n",
            "Train epoch - Accuracy: 0.46367583212735164 Loss: 0.10711385254706385 Corrects: 3204\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3620, Loss 0.11610686033964157\n",
            "Train step - Step 3630, Loss 0.10438181459903717\n",
            "Train step - Step 3640, Loss 0.10472645610570908\n",
            "Train step - Step 3650, Loss 0.11115533113479614\n",
            "Train step - Step 3660, Loss 0.10930373519659042\n",
            "Train step - Step 3670, Loss 0.11275317519903183\n",
            "Train epoch - Accuracy: 0.46078147612156295 Loss: 0.10747062923594253 Corrects: 3184\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3680, Loss 0.10595567524433136\n",
            "Train step - Step 3690, Loss 0.10541880130767822\n",
            "Train step - Step 3700, Loss 0.10916761308908463\n",
            "Train step - Step 3710, Loss 0.10631784051656723\n",
            "Train step - Step 3720, Loss 0.10734996944665909\n",
            "Train epoch - Accuracy: 0.4593342981186686 Loss: 0.10713442157086 Corrects: 3174\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3730, Loss 0.1040586605668068\n",
            "Train step - Step 3740, Loss 0.10452532768249512\n",
            "Train step - Step 3750, Loss 0.1113550215959549\n",
            "Train step - Step 3760, Loss 0.10842668265104294\n",
            "Train step - Step 3770, Loss 0.11005140841007233\n",
            "Train epoch - Accuracy: 0.46150506512301015 Loss: 0.10723449138846308 Corrects: 3189\n",
            "Training finished in 393.83966064453125 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0, 79, 71, 63, 26, 14, 2, 73, 41, 56, 48, 99, 23, 66, 22, 6, 93, 17, 5, 1, 28, 87, 75, 55, 47, 39, 70, 42, 38, 29, 4]\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232d98850>\n",
            "Constructing exemplars of class 87\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [20762, 24931, 9880, 33460, 18171, 853, 45397, 43597, 26053, 39479, 43217, 49355, 34255, 30799, 38014, 24774, 20201, 24513, 36251, 2032, 21454, 34055, 10328, 34075, 23546]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223823c690>\n",
            "Constructing exemplars of class 75\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [32029, 11163, 23676, 23226, 27697, 37753, 42449, 36890, 31322, 48585, 1589, 4358, 96, 14503, 37476, 8879, 23467, 18600, 41614, 31890, 5110, 22887, 37294, 37005, 49259]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a5fbd0>\n",
            "Constructing exemplars of class 55\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [32965, 10517, 7753, 41524, 6551, 11085, 45587, 13034, 40380, 9630, 49332, 8751, 23991, 14118, 44470, 38423, 19579, 438, 43357, 32204, 13273, 31233, 43357, 29630, 21390]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22443a5050>\n",
            "Constructing exemplars of class 47\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [20277, 36111, 42412, 30852, 12824, 43979, 48131, 37911, 24080, 19613, 32268, 23074, 26618, 28400, 3659, 38194, 7166, 45938, 21846, 3281, 48326, 49096, 21998, 14039, 48495]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223802a090>\n",
            "Constructing exemplars of class 39\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [27868, 10953, 19357, 20919, 28163, 42909, 44285, 11046, 3745, 20430, 32839, 21409, 13219, 12135, 9660, 24427, 21575, 41736, 44085, 8683, 37881, 18749, 21409, 4519, 3524]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223156ec50>\n",
            "Constructing exemplars of class 70\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [14317, 12548, 28446, 48358, 31013, 4054, 18543, 126, 49805, 31345, 17917, 37948, 42512, 20367, 1731, 37331, 16392, 20944, 32729, 41538, 15271, 17639, 9492, 12548, 36353]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232ec3850>\n",
            "Constructing exemplars of class 42\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [42517, 2037, 33165, 6142, 14686, 48984, 44038, 7937, 4095, 26109, 48431, 20825, 48144, 48595, 15695, 24728, 5749, 4182, 6713, 36697, 16667, 18899, 45125, 31453, 45963]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e8f490>\n",
            "Constructing exemplars of class 38\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [31276, 32737, 42757, 37908, 15513, 44953, 16866, 23861, 10214, 7788, 2650, 39910, 12004, 43523, 37752, 11868, 7670, 41816, 44284, 25638, 24077, 2675, 17462, 13858, 14901]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223295dcd0>\n",
            "Constructing exemplars of class 29\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [21217, 24325, 3396, 22691, 26854, 15187, 44731, 31686, 15060, 26458, 20116, 45196, 12047, 906, 35936, 21153, 36146, 41805, 30766, 4699, 1, 15740, 2446, 10682, 238]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238ac7e90>\n",
            "Constructing exemplars of class 4\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [21103, 916, 38786, 995, 40069, 27761, 21342, 45436, 34888, 45151, 30771, 2866, 47125, 7932, 22736, 28682, 24228, 34743, 22732, 15423, 5490, 2769, 37238, 17704, 30853]\n",
            "x train:  [-0.05520144 -0.0836652  -0.10323135 -0.02721385 -0.02918455 -0.1120626\n",
            " -0.12979832 -0.12500185 -0.00429738 -0.09824383 -0.10781135 -0.08598163\n",
            " -0.16633403 -0.06511641 -0.15195408 -0.12378561 -0.11994181 -0.102626\n",
            " -0.13085021 -0.15222587 -0.09312256 -0.07569142 -0.2164393  -0.06925848\n",
            " -0.06925847 -0.16110592 -0.03238026 -0.04341089 -0.07223613 -0.11191054\n",
            " -0.09977462 -0.08788153 -0.09841274 -0.00690635 -0.11196378 -0.15723044\n",
            " -0.05530297 -0.04475538 -0.1449207  -0.14093961 -0.09636339 -0.04226979\n",
            " -0.06441239 -0.06842959 -0.13234515 -0.07226862 -0.13750406 -0.00523175\n",
            " -0.16723022 -0.07739527 -0.10356999 -0.19809324 -0.09305768 -0.06129831\n",
            " -0.0341458  -0.06918787 -0.09491684 -0.09253453 -0.14487137 -0.18383802\n",
            " -0.06647409 -0.13855769 -0.11994819 -0.06819361 -0.08411522 -0.02877828\n",
            " -0.20634301 -0.16023658 -0.08718515 -0.17709658 -0.21633835 -0.16553575\n",
            " -0.05409345 -0.13465029 -0.06863572  0.00553011 -0.14270005 -0.13462523\n",
            " -0.06726891 -0.05804823]\n",
            "y_train:  [tensor([39]), tensor([2]), tensor([60]), tensor([45]), tensor([55]), tensor([96]), tensor([68]), tensor([45]), tensor([34]), tensor([87]), tensor([37]), tensor([47]), tensor([6]), tensor([95]), tensor([45]), tensor([25]), tensor([98]), tensor([97]), tensor([82]), tensor([96]), tensor([57]), tensor([94]), tensor([46]), tensor([25]), tensor([70]), tensor([72]), tensor([42]), tensor([38]), tensor([79]), tensor([5]), tensor([95]), tensor([97]), tensor([39]), tensor([26]), tensor([93]), tensor([82]), tensor([80]), tensor([14]), tensor([73]), tensor([56]), tensor([17]), tensor([53]), tensor([24]), tensor([27]), tensor([97]), tensor([95]), tensor([13]), tensor([53]), tensor([39]), tensor([23]), tensor([75]), tensor([22]), tensor([42]), tensor([30]), tensor([91]), tensor([58]), tensor([63]), tensor([56]), tensor([33]), tensor([70]), tensor([16]), tensor([7]), tensor([12]), tensor([37]), tensor([79]), tensor([69]), tensor([12]), tensor([33]), tensor([27]), tensor([6]), tensor([87]), tensor([95]), tensor([57]), tensor([10]), tensor([62]), tensor([7]), tensor([68]), tensor([32]), tensor([5]), tensor([42]), tensor([94]), tensor([53]), tensor([26]), tensor([12]), tensor([94]), tensor([78]), tensor([52]), tensor([13]), tensor([24]), tensor([68]), tensor([57]), tensor([6]), tensor([12]), tensor([55]), tensor([67]), tensor([33]), tensor([41]), tensor([52]), tensor([3]), tensor([66]), tensor([63]), tensor([30]), tensor([72]), tensor([16]), tensor([2]), tensor([47]), tensor([30]), tensor([6]), tensor([69]), tensor([70]), tensor([0]), tensor([36]), tensor([28]), tensor([94]), tensor([2]), tensor([70]), tensor([11]), tensor([33]), tensor([2]), tensor([10]), tensor([52]), tensor([42]), tensor([24]), tensor([25]), tensor([98]), tensor([60]), tensor([33]), tensor([82]), tensor([80]), tensor([7]), tensor([81]), tensor([49]), tensor([96]), tensor([37]), tensor([97]), tensor([45]), tensor([91]), tensor([70]), tensor([62]), tensor([36]), tensor([38]), tensor([96]), tensor([80]), tensor([28]), tensor([48]), tensor([93]), tensor([41]), tensor([97]), tensor([49]), tensor([95]), tensor([46]), tensor([29]), tensor([32]), tensor([82]), tensor([69]), tensor([50]), tensor([5]), tensor([70]), tensor([94]), tensor([23]), tensor([24]), tensor([57]), tensor([34]), tensor([72]), tensor([68]), tensor([58]), tensor([86]), tensor([35]), tensor([86]), tensor([79]), tensor([96]), tensor([6]), tensor([11]), tensor([22]), tensor([46]), tensor([24]), tensor([95]), tensor([59]), tensor([68]), tensor([50]), tensor([23]), tensor([87]), tensor([26]), tensor([36]), tensor([94]), tensor([35]), tensor([56]), tensor([41]), tensor([63]), tensor([77]), tensor([5]), tensor([50]), tensor([67]), tensor([33]), tensor([25]), tensor([39]), tensor([17]), tensor([47]), tensor([97]), tensor([56]), tensor([45]), tensor([80]), tensor([10]), tensor([99]), tensor([21]), tensor([57]), tensor([91]), tensor([88]), tensor([94]), tensor([92]), tensor([45]), tensor([66]), tensor([50]), tensor([71]), tensor([45]), tensor([86]), tensor([79]), tensor([39]), tensor([28]), tensor([27]), tensor([71]), tensor([69]), tensor([60]), tensor([17]), tensor([98]), tensor([23]), tensor([50]), tensor([53]), tensor([5]), tensor([30]), tensor([95]), tensor([11]), tensor([60]), tensor([65]), tensor([88]), tensor([92]), tensor([97]), tensor([52]), tensor([60]), tensor([28]), tensor([71]), tensor([26]), tensor([37]), tensor([36]), tensor([75]), tensor([42]), tensor([67]), tensor([47]), tensor([70]), tensor([81]), tensor([62]), tensor([78]), tensor([52]), tensor([38]), tensor([75]), tensor([50]), tensor([10]), tensor([1]), tensor([37]), tensor([66]), tensor([92]), tensor([72]), tensor([14]), tensor([13]), tensor([39]), tensor([72]), tensor([1]), tensor([16]), tensor([3]), tensor([75]), tensor([65]), tensor([45]), tensor([52]), tensor([97]), tensor([50]), tensor([21]), tensor([35]), tensor([58]), tensor([86]), tensor([14]), tensor([49]), tensor([58]), tensor([46]), tensor([34]), tensor([14]), tensor([56]), tensor([62]), tensor([63]), tensor([16]), tensor([91]), tensor([10]), tensor([33]), tensor([97]), tensor([75]), tensor([27]), tensor([36]), tensor([37]), tensor([69]), tensor([22]), tensor([26]), tensor([80]), tensor([46]), tensor([17]), tensor([66]), tensor([36]), tensor([77]), tensor([12]), tensor([33]), tensor([53]), tensor([0]), tensor([0]), tensor([75]), tensor([95]), tensor([33]), tensor([81]), tensor([5]), tensor([46]), tensor([21]), tensor([62]), tensor([75]), tensor([12]), tensor([80]), tensor([48]), tensor([55]), tensor([14]), tensor([33]), tensor([28]), tensor([12]), tensor([93]), tensor([42]), tensor([37]), tensor([24]), tensor([78]), tensor([1]), tensor([72]), tensor([87]), tensor([94]), tensor([73]), tensor([13]), tensor([72]), tensor([69]), tensor([65]), tensor([42]), tensor([1]), tensor([3]), tensor([13]), tensor([24]), tensor([30]), tensor([69]), tensor([58]), tensor([6]), tensor([17]), tensor([65]), tensor([30]), tensor([99]), tensor([22]), tensor([80]), tensor([26]), tensor([21]), tensor([6]), tensor([32]), tensor([36]), tensor([26]), tensor([26]), tensor([69]), tensor([23]), tensor([2]), tensor([21]), tensor([32]), tensor([67]), tensor([81]), tensor([70]), tensor([59]), tensor([4]), tensor([62]), tensor([46]), tensor([13]), tensor([4]), tensor([31]), tensor([93]), tensor([39]), tensor([12]), tensor([41]), tensor([95]), tensor([24]), tensor([59]), tensor([93]), tensor([25]), tensor([95]), tensor([77]), tensor([60]), tensor([10]), tensor([80]), tensor([63]), tensor([67]), tensor([77]), tensor([80]), tensor([86]), tensor([14]), tensor([22]), tensor([4]), tensor([23]), tensor([59]), tensor([55]), tensor([5]), tensor([1]), tensor([4]), tensor([2]), tensor([53]), tensor([87]), tensor([91]), tensor([50]), tensor([92]), tensor([63]), tensor([13]), tensor([11]), tensor([58]), tensor([23]), tensor([31]), tensor([17]), tensor([99]), tensor([71]), tensor([57]), tensor([32]), tensor([31]), tensor([28]), tensor([58]), tensor([0]), tensor([88]), tensor([42]), tensor([50]), tensor([94]), tensor([0]), tensor([88]), tensor([46]), tensor([75]), tensor([60]), tensor([39]), tensor([81]), tensor([1]), tensor([97]), tensor([63]), tensor([58]), tensor([92]), tensor([72]), tensor([1]), tensor([38]), tensor([72]), tensor([79]), tensor([32]), tensor([22]), tensor([5]), tensor([31]), tensor([3]), tensor([45]), tensor([66]), tensor([42]), tensor([31]), tensor([12]), tensor([97]), tensor([65]), tensor([14]), tensor([5]), tensor([68]), tensor([36]), tensor([1]), tensor([78]), tensor([94]), tensor([67]), tensor([53]), tensor([33]), tensor([36]), tensor([78]), tensor([60]), tensor([66]), tensor([41]), tensor([13]), tensor([95]), tensor([0]), tensor([69]), tensor([7]), tensor([96]), tensor([81]), tensor([47]), tensor([0]), tensor([63]), tensor([65]), tensor([32]), tensor([72]), tensor([73]), tensor([98]), tensor([26]), tensor([36]), tensor([28]), tensor([63]), tensor([67]), tensor([67]), tensor([39]), tensor([28]), tensor([88]), tensor([5]), tensor([59]), tensor([55]), tensor([68]), tensor([46]), tensor([71]), tensor([47]), tensor([21]), tensor([56]), tensor([13]), tensor([86]), tensor([22]), tensor([59]), tensor([56]), tensor([58]), tensor([38]), tensor([36]), tensor([11]), tensor([32]), tensor([56]), tensor([77]), tensor([88]), tensor([3]), tensor([91]), tensor([66]), tensor([59]), tensor([93]), tensor([65]), tensor([55]), tensor([77]), tensor([25]), tensor([17]), tensor([94]), tensor([75]), tensor([65]), tensor([14]), tensor([98]), tensor([53]), tensor([82]), tensor([3]), tensor([12]), tensor([13]), tensor([10]), tensor([27]), tensor([32]), tensor([82]), tensor([66]), tensor([49]), tensor([29]), tensor([53]), tensor([33]), tensor([24]), tensor([60]), tensor([21]), tensor([86]), tensor([97]), tensor([68]), tensor([67]), tensor([66]), tensor([80]), tensor([33]), tensor([16]), tensor([11]), tensor([25]), tensor([94]), tensor([49]), tensor([99]), tensor([41]), tensor([29]), tensor([26]), tensor([69]), tensor([34]), tensor([46]), tensor([79]), tensor([7]), tensor([87]), tensor([75]), tensor([21]), tensor([72]), tensor([98]), tensor([94]), tensor([67]), tensor([2]), tensor([97]), tensor([69]), tensor([45]), tensor([22]), tensor([41]), tensor([21]), tensor([93]), tensor([92]), tensor([56]), tensor([57]), tensor([7]), tensor([93]), tensor([52]), tensor([32]), tensor([69]), tensor([98]), tensor([30]), tensor([16]), tensor([48]), tensor([28]), tensor([28]), tensor([2]), tensor([11]), tensor([88]), tensor([27]), tensor([66]), tensor([98]), tensor([24]), tensor([78]), tensor([48]), tensor([52]), tensor([62]), tensor([79]), tensor([27]), tensor([72]), tensor([79]), tensor([80]), tensor([73]), tensor([77]), tensor([14]), tensor([35]), tensor([47]), tensor([42]), tensor([97]), tensor([67]), tensor([86]), tensor([16]), tensor([21]), tensor([86]), tensor([39]), tensor([62]), tensor([21]), tensor([45]), tensor([60]), tensor([52]), tensor([48]), tensor([46]), tensor([78]), tensor([36]), tensor([60]), tensor([92]), tensor([86]), tensor([96]), tensor([6]), tensor([22]), tensor([26]), tensor([80]), tensor([86]), tensor([91]), tensor([56]), tensor([59]), tensor([65]), tensor([70]), tensor([5]), tensor([50]), tensor([57]), tensor([62]), tensor([94]), tensor([22]), tensor([2]), tensor([1]), tensor([25]), tensor([14]), tensor([47]), tensor([7]), tensor([98]), tensor([50]), tensor([1]), tensor([21]), tensor([47]), tensor([49]), tensor([68]), tensor([32]), tensor([60]), tensor([94]), tensor([29]), tensor([60]), tensor([38]), tensor([71]), tensor([36]), tensor([26]), tensor([63]), tensor([94]), tensor([72]), tensor([71]), tensor([3]), tensor([73]), tensor([82]), tensor([79]), tensor([77]), tensor([65]), tensor([99]), tensor([77]), tensor([62]), tensor([4]), tensor([39]), tensor([14]), tensor([1]), tensor([67]), tensor([48]), tensor([79]), tensor([79]), tensor([35]), tensor([50]), tensor([82]), tensor([17]), tensor([23]), tensor([55]), tensor([93]), tensor([67]), tensor([63]), tensor([38]), tensor([12]), tensor([55]), tensor([26]), tensor([16]), tensor([95]), tensor([28]), tensor([48]), tensor([10]), tensor([48]), tensor([81]), tensor([80]), tensor([3]), tensor([75]), tensor([58]), tensor([65]), tensor([42]), tensor([62]), tensor([12]), tensor([45]), tensor([96]), tensor([53]), tensor([96]), tensor([92]), tensor([59]), tensor([23]), tensor([23]), tensor([73]), tensor([0]), tensor([58]), tensor([41]), tensor([75]), tensor([62]), tensor([31]), tensor([12]), tensor([4]), tensor([78]), tensor([99]), tensor([33]), tensor([30]), tensor([62]), tensor([27]), tensor([97]), tensor([39]), tensor([59]), tensor([65]), tensor([35]), tensor([1]), tensor([69]), tensor([86]), tensor([41]), tensor([98]), tensor([48]), tensor([13]), tensor([79]), tensor([30]), tensor([91]), tensor([96]), tensor([78]), tensor([96]), tensor([5]), tensor([11]), tensor([82]), tensor([7]), tensor([49]), tensor([12]), tensor([33]), tensor([26]), tensor([11]), tensor([48]), tensor([5]), tensor([81]), tensor([53]), tensor([3]), tensor([52]), tensor([7]), tensor([93]), tensor([98]), tensor([69]), tensor([69]), tensor([57]), tensor([68]), tensor([69]), tensor([47]), tensor([72]), tensor([7]), tensor([6]), tensor([57]), tensor([78]), tensor([49]), tensor([11]), tensor([70]), tensor([82]), tensor([5]), tensor([66]), tensor([80]), tensor([92]), tensor([33]), tensor([91]), tensor([26]), tensor([63]), tensor([87]), tensor([47]), tensor([11]), tensor([49]), tensor([23]), tensor([82]), tensor([91]), tensor([35]), tensor([22]), tensor([34]), tensor([17]), tensor([17]), tensor([63]), tensor([70]), tensor([30]), tensor([57]), tensor([88]), tensor([24]), tensor([92]), tensor([41]), tensor([35]), tensor([26]), tensor([73]), tensor([22]), tensor([34]), tensor([25]), tensor([5]), tensor([31]), tensor([13]), tensor([29]), tensor([22]), tensor([38]), tensor([57]), tensor([25]), tensor([36]), tensor([25]), tensor([50]), tensor([36]), tensor([99]), tensor([5]), tensor([87]), tensor([30]), tensor([77]), tensor([60]), tensor([2]), tensor([59]), tensor([3]), tensor([12]), tensor([31]), tensor([5]), tensor([13]), tensor([95]), tensor([79]), tensor([38]), tensor([36]), tensor([58]), tensor([56]), tensor([92]), tensor([81]), tensor([36]), tensor([42]), tensor([47]), tensor([96]), tensor([97]), tensor([27]), tensor([68]), tensor([65]), tensor([16]), tensor([87]), tensor([70]), tensor([6]), tensor([22]), tensor([87]), tensor([80]), tensor([68]), tensor([55]), tensor([60]), tensor([66]), tensor([91]), tensor([88]), tensor([95]), tensor([49]), tensor([34]), tensor([63]), tensor([98]), tensor([87]), tensor([57]), tensor([41]), tensor([4]), tensor([21]), tensor([2]), tensor([75]), tensor([52]), tensor([21]), tensor([67]), tensor([91]), tensor([80]), tensor([57]), tensor([73]), tensor([7]), tensor([94]), tensor([96]), tensor([71]), tensor([28]), tensor([2]), tensor([86]), tensor([77]), tensor([25]), tensor([87]), tensor([45]), tensor([68]), tensor([92]), tensor([88]), tensor([57]), tensor([62]), tensor([62]), tensor([81]), tensor([22]), tensor([81]), tensor([41]), tensor([93]), tensor([30]), tensor([1]), tensor([80]), tensor([33]), tensor([87]), tensor([86]), tensor([26]), tensor([29]), tensor([60]), tensor([46]), tensor([0]), tensor([31]), tensor([88]), tensor([70]), tensor([3]), tensor([80]), tensor([16]), tensor([38]), tensor([99]), tensor([63]), tensor([49]), tensor([78]), tensor([23]), tensor([75]), tensor([7]), tensor([63]), tensor([0]), tensor([27]), tensor([77]), tensor([0]), tensor([21]), tensor([38]), tensor([35]), tensor([68]), tensor([60]), tensor([78]), tensor([66]), tensor([93]), tensor([48]), tensor([41]), tensor([77]), tensor([34]), tensor([48]), tensor([26]), tensor([32]), tensor([34]), tensor([39]), tensor([21]), tensor([26]), tensor([65]), tensor([97]), tensor([66]), tensor([50]), tensor([47]), tensor([98]), tensor([78]), tensor([92]), tensor([46]), tensor([42]), tensor([86]), tensor([27]), tensor([87]), tensor([38]), tensor([52]), tensor([17]), tensor([12]), tensor([62]), tensor([31]), tensor([92]), tensor([47]), tensor([25]), tensor([48]), tensor([79]), tensor([12]), tensor([50]), tensor([88]), tensor([25]), tensor([82]), tensor([22]), tensor([53]), tensor([73]), tensor([0]), tensor([58]), tensor([11]), tensor([79]), tensor([50]), tensor([37]), tensor([17]), tensor([13]), tensor([86]), tensor([32]), tensor([56]), tensor([29]), tensor([75]), tensor([93]), tensor([58]), tensor([21]), tensor([77]), tensor([28]), tensor([58]), tensor([27]), tensor([58]), tensor([42]), tensor([94]), tensor([56]), tensor([86]), tensor([39]), tensor([7]), tensor([78]), tensor([22]), tensor([78]), tensor([92]), tensor([71]), tensor([49]), tensor([46]), tensor([93]), tensor([35]), tensor([91]), tensor([28]), tensor([14]), tensor([52]), tensor([98]), tensor([14]), tensor([39]), tensor([27]), tensor([48]), tensor([35]), tensor([41]), tensor([12]), tensor([55]), tensor([59]), tensor([17]), tensor([21]), tensor([55]), tensor([32]), tensor([39]), tensor([16]), tensor([38]), tensor([37]), tensor([57]), tensor([86]), tensor([33]), tensor([23]), tensor([33]), tensor([55]), tensor([71]), tensor([71]), tensor([62]), tensor([59]), tensor([28]), tensor([17]), tensor([70]), tensor([7]), tensor([70]), tensor([91]), tensor([63]), tensor([80]), tensor([24]), tensor([59]), tensor([46]), tensor([66]), tensor([77]), tensor([87]), tensor([24]), tensor([14]), tensor([13]), tensor([2]), tensor([62]), tensor([12]), tensor([80]), tensor([25]), tensor([92]), tensor([69]), tensor([48]), tensor([93]), tensor([73]), tensor([57]), tensor([59]), tensor([11]), tensor([81]), tensor([1]), tensor([91]), tensor([32]), tensor([62]), tensor([10]), tensor([2]), tensor([48]), tensor([66]), tensor([91]), tensor([0]), tensor([42]), tensor([53]), tensor([37]), tensor([27]), tensor([4]), tensor([33]), tensor([49]), tensor([27]), tensor([16]), tensor([30]), tensor([25]), tensor([24]), tensor([32]), tensor([48]), tensor([86]), tensor([2]), tensor([46]), tensor([10]), tensor([88]), tensor([92]), tensor([97]), tensor([75]), tensor([7]), tensor([30]), tensor([0]), tensor([81]), tensor([35]), tensor([50]), tensor([10]), tensor([69]), tensor([88]), tensor([94]), tensor([5]), tensor([53]), tensor([0]), tensor([34]), tensor([42]), tensor([0]), tensor([39]), tensor([47]), tensor([3]), tensor([3]), tensor([57]), tensor([13]), tensor([50]), tensor([41]), tensor([13]), tensor([30]), tensor([60]), tensor([17]), tensor([35]), tensor([55]), tensor([78]), tensor([12]), tensor([38]), tensor([38]), tensor([96]), tensor([4]), tensor([41]), tensor([34]), tensor([7]), tensor([34]), tensor([81]), tensor([13]), tensor([98]), tensor([17]), tensor([62]), tensor([7]), tensor([73]), tensor([47]), tensor([37]), tensor([77]), tensor([36]), tensor([29]), tensor([49]), tensor([68]), tensor([13]), tensor([52]), tensor([13]), tensor([98]), tensor([88]), tensor([77]), tensor([1]), tensor([70]), tensor([46]), tensor([62]), tensor([52]), tensor([3]), tensor([60]), tensor([98]), tensor([70]), tensor([71]), tensor([42]), tensor([55]), tensor([22]), tensor([81]), tensor([96]), tensor([22]), tensor([10]), tensor([32]), tensor([38]), tensor([63]), tensor([37]), tensor([65]), tensor([4]), tensor([60]), tensor([87]), tensor([23]), tensor([4]), tensor([66]), tensor([4]), tensor([82]), tensor([91]), tensor([82]), tensor([60]), tensor([96]), tensor([48]), tensor([66]), tensor([55]), tensor([47]), tensor([86]), tensor([71]), tensor([46]), tensor([72]), tensor([32]), tensor([93]), tensor([24]), tensor([80]), tensor([11]), tensor([16]), tensor([88]), tensor([93]), tensor([55]), tensor([39]), tensor([37]), tensor([79]), tensor([96]), tensor([92]), tensor([1]), tensor([95]), tensor([25]), tensor([3]), tensor([47]), tensor([31]), tensor([56]), tensor([93]), tensor([17]), tensor([24]), tensor([58]), tensor([87]), tensor([53]), tensor([34]), tensor([55]), tensor([11]), tensor([38]), tensor([95]), tensor([52]), tensor([13]), tensor([50]), tensor([3]), tensor([58]), tensor([59]), tensor([52]), tensor([23]), tensor([56]), tensor([95]), tensor([32]), tensor([68]), tensor([80]), tensor([97]), tensor([86]), tensor([93]), tensor([81]), tensor([99]), tensor([81]), tensor([27]), tensor([47]), tensor([97]), tensor([14]), tensor([10]), tensor([59]), tensor([2]), tensor([14]), tensor([24]), tensor([23]), tensor([21]), tensor([81]), tensor([48]), tensor([27]), tensor([53]), tensor([32]), tensor([73]), tensor([6]), tensor([11]), tensor([77]), tensor([10]), tensor([96]), tensor([73]), tensor([82]), tensor([36]), tensor([3]), tensor([99]), tensor([2]), tensor([11]), tensor([92]), tensor([70]), tensor([14]), tensor([81]), tensor([56]), tensor([26]), tensor([10]), tensor([55]), tensor([53]), tensor([45]), tensor([16]), tensor([65]), tensor([63]), tensor([58]), tensor([14]), tensor([77]), tensor([66]), tensor([22]), tensor([3]), tensor([99]), tensor([92]), tensor([95]), tensor([35]), tensor([21]), tensor([22]), tensor([70]), tensor([55]), tensor([38]), tensor([93]), tensor([91]), tensor([14]), tensor([99]), tensor([26]), tensor([96]), tensor([67]), tensor([71]), tensor([65]), tensor([28]), tensor([48]), tensor([94]), tensor([6]), tensor([78]), tensor([67]), tensor([16]), tensor([59]), tensor([95]), tensor([97]), tensor([4]), tensor([69]), tensor([87]), tensor([25]), tensor([25]), tensor([21]), tensor([5]), tensor([34]), tensor([36]), tensor([72]), tensor([39]), tensor([91]), tensor([4]), tensor([26]), tensor([2]), tensor([88]), tensor([81]), tensor([2]), tensor([32]), tensor([56]), tensor([3]), tensor([28]), tensor([24]), tensor([79]), tensor([11]), tensor([88]), tensor([87]), tensor([75]), tensor([30]), tensor([86]), tensor([34]), tensor([88]), tensor([82]), tensor([67]), tensor([78]), tensor([58]), tensor([34]), tensor([93]), tensor([25]), tensor([6]), tensor([88]), tensor([78]), tensor([53]), tensor([57]), tensor([56]), tensor([75]), tensor([31]), tensor([56]), tensor([66]), tensor([28]), tensor([27]), tensor([24]), tensor([88]), tensor([69]), tensor([62]), tensor([46]), tensor([48]), tensor([34]), tensor([49]), tensor([10]), tensor([25]), tensor([6]), tensor([47]), tensor([31]), tensor([97]), tensor([11]), tensor([22]), tensor([70]), tensor([68]), tensor([60]), tensor([79]), tensor([49]), tensor([53]), tensor([72]), tensor([62]), tensor([14]), tensor([55]), tensor([33]), tensor([59]), tensor([4]), tensor([99]), tensor([75]), tensor([2]), tensor([7]), tensor([7]), tensor([68]), tensor([6]), tensor([22]), tensor([56]), tensor([53]), tensor([92]), tensor([34]), tensor([94]), tensor([30]), tensor([17]), tensor([17]), tensor([25]), tensor([6]), tensor([24]), tensor([14]), tensor([21]), tensor([80]), tensor([29]), tensor([2]), tensor([70]), tensor([23]), tensor([23]), tensor([88]), tensor([47]), tensor([30]), tensor([71]), tensor([65]), tensor([16]), tensor([37]), tensor([35]), tensor([86]), tensor([33]), tensor([35]), tensor([68]), tensor([30]), tensor([95]), tensor([68]), tensor([29]), tensor([63]), tensor([56]), tensor([12]), tensor([73]), tensor([12]), tensor([45]), tensor([99]), tensor([93]), tensor([17]), tensor([45]), tensor([93]), tensor([52]), tensor([35]), tensor([46]), tensor([34]), tensor([29]), tensor([95]), tensor([41]), tensor([71]), tensor([11]), tensor([52]), tensor([69]), tensor([99]), tensor([75]), tensor([78]), tensor([63]), tensor([35]), tensor([47]), tensor([27]), tensor([3]), tensor([31]), tensor([67]), tensor([52]), tensor([99]), tensor([25]), tensor([35]), tensor([91]), tensor([69]), tensor([35]), tensor([28]), tensor([95]), tensor([82]), tensor([17]), tensor([66]), tensor([68]), tensor([79]), tensor([41]), tensor([13]), tensor([88]), tensor([75]), tensor([35]), tensor([34]), tensor([38]), tensor([77]), tensor([4]), tensor([14]), tensor([70]), tensor([6]), tensor([93]), tensor([55]), tensor([10]), tensor([6]), tensor([36]), tensor([80]), tensor([50]), tensor([87]), tensor([77]), tensor([14]), tensor([59]), tensor([7]), tensor([47]), tensor([42]), tensor([52]), tensor([45]), tensor([59]), tensor([73]), tensor([63]), tensor([7]), tensor([6]), tensor([34]), tensor([55]), tensor([37]), tensor([32]), tensor([99]), tensor([4]), tensor([12]), tensor([58]), tensor([0]), tensor([71]), tensor([39]), tensor([2]), tensor([62]), tensor([4]), tensor([39]), tensor([49]), tensor([72]), tensor([53]), tensor([1]), tensor([70]), tensor([79]), tensor([69]), tensor([0]), tensor([28]), tensor([87]), tensor([67]), tensor([36]), tensor([1]), tensor([2]), tensor([86]), tensor([17]), tensor([16]), tensor([88]), tensor([95]), tensor([67]), tensor([73]), tensor([35]), tensor([65]), tensor([23]), tensor([66]), tensor([4]), tensor([75]), tensor([36]), tensor([71]), tensor([58]), tensor([67]), tensor([34]), tensor([72]), tensor([4]), tensor([29]), tensor([41]), tensor([38]), tensor([79]), tensor([1]), tensor([49]), tensor([33]), tensor([98]), tensor([46]), tensor([47]), tensor([3]), tensor([30]), tensor([5]), tensor([82]), tensor([3]), tensor([1]), tensor([23]), tensor([73]), tensor([91]), tensor([49]), tensor([58]), tensor([45]), tensor([99]), tensor([57]), tensor([28]), tensor([28]), tensor([31]), tensor([82]), tensor([30]), tensor([16]), tensor([2]), tensor([78]), tensor([75]), tensor([71]), tensor([16]), tensor([82]), tensor([53]), tensor([27]), tensor([26]), tensor([56]), tensor([93]), tensor([94]), tensor([10]), tensor([50]), tensor([79]), tensor([30]), tensor([11]), tensor([37]), tensor([0]), tensor([59]), tensor([16]), tensor([37]), tensor([57]), tensor([16]), tensor([67]), tensor([28]), tensor([29]), tensor([10]), tensor([77]), tensor([29]), tensor([3]), tensor([38]), tensor([4]), tensor([23]), tensor([48]), tensor([96]), tensor([57]), tensor([25]), tensor([73]), tensor([3]), tensor([7]), tensor([60]), tensor([24]), tensor([52]), tensor([27]), tensor([5]), tensor([97]), tensor([29]), tensor([46]), tensor([29]), tensor([66]), tensor([96]), tensor([10]), tensor([48]), tensor([82]), tensor([16]), tensor([71]), tensor([49]), tensor([33]), tensor([45]), tensor([28]), tensor([37]), tensor([39]), tensor([60]), tensor([72]), tensor([50]), tensor([65]), tensor([12]), tensor([10]), tensor([21]), tensor([96]), tensor([23]), tensor([33]), tensor([99]), tensor([65]), tensor([81]), tensor([95]), tensor([56]), tensor([4]), tensor([96]), tensor([7]), tensor([57]), tensor([68]), tensor([13]), tensor([63]), tensor([91]), tensor([98]), tensor([14]), tensor([81]), tensor([77]), tensor([24]), tensor([94]), tensor([94]), tensor([29]), tensor([30]), tensor([10]), tensor([59]), tensor([45]), tensor([81]), tensor([11]), tensor([65]), tensor([96]), tensor([99]), tensor([50]), tensor([6]), tensor([53]), tensor([6]), tensor([31]), tensor([77]), tensor([60]), tensor([31]), tensor([98]), tensor([46]), tensor([41]), tensor([29]), tensor([27]), tensor([97]), tensor([79]), tensor([91]), tensor([42]), tensor([25]), tensor([39]), tensor([55]), tensor([99]), tensor([37]), tensor([2]), tensor([34]), tensor([24]), tensor([13]), tensor([32]), tensor([49]), tensor([41]), tensor([69]), tensor([94]), tensor([65]), tensor([10]), tensor([30]), tensor([1]), tensor([92]), tensor([72]), tensor([11]), tensor([27]), tensor([73]), tensor([11]), tensor([10]), tensor([0]), tensor([45]), tensor([53]), tensor([77]), tensor([31]), tensor([72]), tensor([22]), tensor([41]), tensor([31]), tensor([36]), tensor([42]), tensor([58]), tensor([95]), tensor([45]), tensor([4]), tensor([56]), tensor([98]), tensor([6]), tensor([92]), tensor([30]), tensor([57]), tensor([29]), tensor([82]), tensor([53]), tensor([32]), tensor([59]), tensor([81]), tensor([68]), tensor([71]), tensor([67]), tensor([31]), tensor([37]), tensor([48]), tensor([1]), tensor([73]), tensor([31]), tensor([79]), tensor([71]), tensor([14]), tensor([71]), tensor([0]), tensor([41]), tensor([67]), tensor([13]), tensor([49]), tensor([41]), tensor([46]), tensor([91]), tensor([38]), tensor([35]), tensor([88]), tensor([72]), tensor([39]), tensor([23]), tensor([45]), tensor([99]), tensor([52]), tensor([0]), tensor([73]), tensor([27]), tensor([55]), tensor([36]), tensor([5]), tensor([38]), tensor([11]), tensor([1]), tensor([26]), tensor([29]), tensor([68]), tensor([31]), tensor([31]), tensor([37]), tensor([92]), tensor([99]), tensor([99]), tensor([58]), tensor([0]), tensor([37]), tensor([32]), tensor([70]), tensor([37]), tensor([0]), tensor([23]), tensor([23]), tensor([82]), tensor([67]), tensor([29]), tensor([65]), tensor([98]), tensor([68]), tensor([87]), tensor([7]), tensor([98]), tensor([5]), tensor([50]), tensor([24]), tensor([17]), tensor([59]), tensor([29]), tensor([87]), tensor([34]), tensor([21]), tensor([10]), tensor([4]), tensor([63]), tensor([96]), tensor([31]), tensor([46]), tensor([45]), tensor([41]), tensor([98]), tensor([48]), tensor([71]), tensor([26]), tensor([72]), tensor([78]), tensor([79]), tensor([55]), tensor([82]), tensor([97]), tensor([73]), tensor([49]), tensor([69]), tensor([31]), tensor([0]), tensor([66]), tensor([42]), tensor([27]), tensor([50]), tensor([12]), tensor([35]), tensor([78]), tensor([66]), tensor([16]), tensor([73]), tensor([87]), tensor([73]), tensor([75]), tensor([86]), tensor([52]), tensor([1]), tensor([45]), tensor([7]), tensor([42]), tensor([78]), tensor([92]), tensor([6]), tensor([70]), tensor([16]), tensor([28]), tensor([91]), tensor([17]), tensor([49]), tensor([37]), tensor([73]), tensor([5]), tensor([24]), tensor([3]), tensor([5]), tensor([38]), tensor([17]), tensor([29]), tensor([72]), tensor([42]), tensor([34]), tensor([47]), tensor([35]), tensor([37]), tensor([62]), tensor([80]), tensor([87]), tensor([21]), tensor([75]), tensor([99]), tensor([81]), tensor([38]), tensor([29]), tensor([63]), tensor([82]), tensor([57]), tensor([52]), tensor([65]), tensor([6]), tensor([98]), tensor([71]), tensor([42]), tensor([78]), tensor([39]), tensor([56]), tensor([42]), tensor([4]), tensor([16]), tensor([49]), tensor([1]), tensor([22]), tensor([6]), tensor([29])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.44 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.436\n",
            "TEST ALL:  0.441875\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  9000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 81, 97, 2, 10, 18, 26, 34, 42, 50, 58, 66, 82, 98, 3, 11, 27, 35, 51, 59, 67, 89, 73, 87, 65, 8, 16, 24, 32, 48, 56, 64, 72, 80, 88, 96, 1, 9, 17, 25, 33, 41, 49, 57, 75, 83, 91, 99, 14, 22, 30, 38, 46, 62, 70, 78, 86, 94, 7, 23, 31, 39, 47, 55, 63, 71, 79, 6, 93, 85, 84, 4, 12, 20, 28, 36, 52, 60, 68, 92, 77, 5, 13, 21, 29, 37, 45, 53, 69, 0]\n",
            "TRAIN_SET CLASSES:  [83, 51, 18, 89, 85, 9, 84, 64, 20, 8]\n",
            "VALIDATION CLASSES:  [20, 51, 89, 85, 84, 83, 18, 9, 8, 64]\n",
            "GROUP:  9\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [83, 51, 18, 89, 85, 9, 84, 64, 20, 8]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  90\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.19942538440227509\n",
            "Train step - Step 10, Loss 0.13514085114002228\n",
            "Train step - Step 20, Loss 0.12644003331661224\n",
            "Train step - Step 30, Loss 0.1262558251619339\n",
            "Train step - Step 40, Loss 0.11731797456741333\n",
            "Train step - Step 50, Loss 0.12036912888288498\n",
            "Train epoch - Accuracy: 0.13798561151079136 Loss: 0.13427696183859872 Corrects: 959\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.11954716593027115\n",
            "Train step - Step 70, Loss 0.11889682710170746\n",
            "Train step - Step 80, Loss 0.1151750385761261\n",
            "Train step - Step 90, Loss 0.12264082580804825\n",
            "Train step - Step 100, Loss 0.11317749321460724\n",
            "Train epoch - Accuracy: 0.16618705035971224 Loss: 0.11875045987985117 Corrects: 1155\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11756113171577454\n",
            "Train step - Step 120, Loss 0.11730414628982544\n",
            "Train step - Step 130, Loss 0.11552576720714569\n",
            "Train step - Step 140, Loss 0.11537028849124908\n",
            "Train step - Step 150, Loss 0.11485763639211655\n",
            "Train step - Step 160, Loss 0.11286706477403641\n",
            "Train epoch - Accuracy: 0.18129496402877698 Loss: 0.1171990903097091 Corrects: 1260\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.1216273233294487\n",
            "Train step - Step 180, Loss 0.1142764613032341\n",
            "Train step - Step 190, Loss 0.1254003942012787\n",
            "Train step - Step 200, Loss 0.1140199825167656\n",
            "Train step - Step 210, Loss 0.11429524421691895\n",
            "Train epoch - Accuracy: 0.20676258992805754 Loss: 0.11578718427059462 Corrects: 1437\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11654850095510483\n",
            "Train step - Step 230, Loss 0.11354200541973114\n",
            "Train step - Step 240, Loss 0.11923739314079285\n",
            "Train step - Step 250, Loss 0.11700757592916489\n",
            "Train step - Step 260, Loss 0.11001159250736237\n",
            "Train step - Step 270, Loss 0.11452047526836395\n",
            "Train epoch - Accuracy: 0.23093525179856114 Loss: 0.11525057683316924 Corrects: 1605\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.11056678742170334\n",
            "Train step - Step 290, Loss 0.11832010746002197\n",
            "Train step - Step 300, Loss 0.116853728890419\n",
            "Train step - Step 310, Loss 0.11543288826942444\n",
            "Train step - Step 320, Loss 0.1121039018034935\n",
            "Train epoch - Accuracy: 0.239568345323741 Loss: 0.11485367940484191 Corrects: 1665\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.10666099190711975\n",
            "Train step - Step 340, Loss 0.112849660217762\n",
            "Train step - Step 350, Loss 0.11724250018596649\n",
            "Train step - Step 360, Loss 0.11462025344371796\n",
            "Train step - Step 370, Loss 0.11669430136680603\n",
            "Train step - Step 380, Loss 0.11310174316167831\n",
            "Train epoch - Accuracy: 0.2568345323741007 Loss: 0.1143492324210757 Corrects: 1785\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.11536680907011032\n",
            "Train step - Step 400, Loss 0.112955242395401\n",
            "Train step - Step 410, Loss 0.11194909363985062\n",
            "Train step - Step 420, Loss 0.11770566552877426\n",
            "Train step - Step 430, Loss 0.11373474448919296\n",
            "Train epoch - Accuracy: 0.2745323741007194 Loss: 0.11376347850123755 Corrects: 1908\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.11306513100862503\n",
            "Train step - Step 450, Loss 0.11350329220294952\n",
            "Train step - Step 460, Loss 0.11620602011680603\n",
            "Train step - Step 470, Loss 0.11100805550813675\n",
            "Train step - Step 480, Loss 0.1117439717054367\n",
            "Train step - Step 490, Loss 0.10749253630638123\n",
            "Train epoch - Accuracy: 0.2896402877697842 Loss: 0.11368241737643592 Corrects: 2013\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.11623327434062958\n",
            "Train step - Step 510, Loss 0.11686524748802185\n",
            "Train step - Step 520, Loss 0.11276733130216599\n",
            "Train step - Step 530, Loss 0.11707252264022827\n",
            "Train step - Step 540, Loss 0.1120595932006836\n",
            "Train epoch - Accuracy: 0.29884892086330933 Loss: 0.11344423043642113 Corrects: 2077\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.11201664060354233\n",
            "Train step - Step 560, Loss 0.10931085050106049\n",
            "Train step - Step 570, Loss 0.11441201716661453\n",
            "Train step - Step 580, Loss 0.11249684542417526\n",
            "Train step - Step 590, Loss 0.11337482184171677\n",
            "Train step - Step 600, Loss 0.11615990847349167\n",
            "Train epoch - Accuracy: 0.31107913669064746 Loss: 0.11340075375579244 Corrects: 2162\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.11100871115922928\n",
            "Train step - Step 620, Loss 0.11423536390066147\n",
            "Train step - Step 630, Loss 0.11838186532258987\n",
            "Train step - Step 640, Loss 0.11230996996164322\n",
            "Train step - Step 650, Loss 0.11297067999839783\n",
            "Train epoch - Accuracy: 0.32258992805755393 Loss: 0.112846786167553 Corrects: 2242\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10740680992603302\n",
            "Train step - Step 670, Loss 0.11527968943119049\n",
            "Train step - Step 680, Loss 0.12042137235403061\n",
            "Train step - Step 690, Loss 0.11054392904043198\n",
            "Train step - Step 700, Loss 0.1095813438296318\n",
            "Train step - Step 710, Loss 0.1049724817276001\n",
            "Train epoch - Accuracy: 0.3254676258992806 Loss: 0.11275973085233633 Corrects: 2262\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.1136636957526207\n",
            "Train step - Step 730, Loss 0.11523266136646271\n",
            "Train step - Step 740, Loss 0.1136997863650322\n",
            "Train step - Step 750, Loss 0.11923065781593323\n",
            "Train step - Step 760, Loss 0.11311086267232895\n",
            "Train epoch - Accuracy: 0.33482014388489206 Loss: 0.1126469631139323 Corrects: 2327\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.11281684786081314\n",
            "Train step - Step 780, Loss 0.11650063842535019\n",
            "Train step - Step 790, Loss 0.11473600566387177\n",
            "Train step - Step 800, Loss 0.11251138150691986\n",
            "Train step - Step 810, Loss 0.11765903234481812\n",
            "Train step - Step 820, Loss 0.11238054186105728\n",
            "Train epoch - Accuracy: 0.3454676258992806 Loss: 0.11262122858342508 Corrects: 2401\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.11919401586055756\n",
            "Train step - Step 840, Loss 0.11357080191373825\n",
            "Train step - Step 850, Loss 0.11343303322792053\n",
            "Train step - Step 860, Loss 0.1116923913359642\n",
            "Train step - Step 870, Loss 0.1131042093038559\n",
            "Train epoch - Accuracy: 0.35122302158273383 Loss: 0.11207550795601426 Corrects: 2441\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.11052924394607544\n",
            "Train step - Step 890, Loss 0.10482358932495117\n",
            "Train step - Step 900, Loss 0.10636837780475616\n",
            "Train step - Step 910, Loss 0.11081810295581818\n",
            "Train step - Step 920, Loss 0.11191578209400177\n",
            "Train step - Step 930, Loss 0.10955432802438736\n",
            "Train epoch - Accuracy: 0.36071942446043165 Loss: 0.11235991797644457 Corrects: 2507\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.11238051950931549\n",
            "Train step - Step 950, Loss 0.11739576607942581\n",
            "Train step - Step 960, Loss 0.11427891254425049\n",
            "Train step - Step 970, Loss 0.10874515026807785\n",
            "Train step - Step 980, Loss 0.11270540207624435\n",
            "Train epoch - Accuracy: 0.36431654676258995 Loss: 0.11205563925367465 Corrects: 2532\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.11111820489168167\n",
            "Train step - Step 1000, Loss 0.11154694110155106\n",
            "Train step - Step 1010, Loss 0.11184340715408325\n",
            "Train step - Step 1020, Loss 0.10627755522727966\n",
            "Train step - Step 1030, Loss 0.11793708801269531\n",
            "Train step - Step 1040, Loss 0.11133959144353867\n",
            "Train epoch - Accuracy: 0.3697841726618705 Loss: 0.11179144210309433 Corrects: 2570\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.11280340701341629\n",
            "Train step - Step 1060, Loss 0.11264730989933014\n",
            "Train step - Step 1070, Loss 0.1092832162976265\n",
            "Train step - Step 1080, Loss 0.10720846801996231\n",
            "Train step - Step 1090, Loss 0.11332501471042633\n",
            "Train epoch - Accuracy: 0.38244604316546765 Loss: 0.11147232895703625 Corrects: 2658\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.11254601180553436\n",
            "Train step - Step 1110, Loss 0.10876584053039551\n",
            "Train step - Step 1120, Loss 0.111696258187294\n",
            "Train step - Step 1130, Loss 0.11131282895803452\n",
            "Train step - Step 1140, Loss 0.11205687373876572\n",
            "Train step - Step 1150, Loss 0.11034910380840302\n",
            "Train epoch - Accuracy: 0.38446043165467625 Loss: 0.11138639373744992 Corrects: 2672\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10955408215522766\n",
            "Train step - Step 1170, Loss 0.1127733662724495\n",
            "Train step - Step 1180, Loss 0.10707147419452667\n",
            "Train step - Step 1190, Loss 0.11030612885951996\n",
            "Train step - Step 1200, Loss 0.11347579956054688\n",
            "Train epoch - Accuracy: 0.3814388489208633 Loss: 0.1115611317839554 Corrects: 2651\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.10884632915258408\n",
            "Train step - Step 1220, Loss 0.11366831511259079\n",
            "Train step - Step 1230, Loss 0.11168120056390762\n",
            "Train step - Step 1240, Loss 0.11466444283723831\n",
            "Train step - Step 1250, Loss 0.11257053166627884\n",
            "Train step - Step 1260, Loss 0.10899703949689865\n",
            "Train epoch - Accuracy: 0.3893525179856115 Loss: 0.11159475109774432 Corrects: 2706\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.11124502867460251\n",
            "Train step - Step 1280, Loss 0.11553194373846054\n",
            "Train step - Step 1290, Loss 0.11085567623376846\n",
            "Train step - Step 1300, Loss 0.11326587945222855\n",
            "Train step - Step 1310, Loss 0.10981163382530212\n",
            "Train epoch - Accuracy: 0.39741007194244604 Loss: 0.11160456498106607 Corrects: 2762\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.10753488540649414\n",
            "Train step - Step 1330, Loss 0.11150896549224854\n",
            "Train step - Step 1340, Loss 0.11015025526285172\n",
            "Train step - Step 1350, Loss 0.11132258921861649\n",
            "Train step - Step 1360, Loss 0.10968342423439026\n",
            "Train step - Step 1370, Loss 0.11144110560417175\n",
            "Train epoch - Accuracy: 0.41050359712230217 Loss: 0.11125937593069007 Corrects: 2853\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.11515199393033981\n",
            "Train step - Step 1390, Loss 0.11044837534427643\n",
            "Train step - Step 1400, Loss 0.10305175185203552\n",
            "Train step - Step 1410, Loss 0.10848773270845413\n",
            "Train step - Step 1420, Loss 0.11145687103271484\n",
            "Train epoch - Accuracy: 0.41007194244604317 Loss: 0.11109499747590196 Corrects: 2850\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.1070035994052887\n",
            "Train step - Step 1440, Loss 0.11277913302183151\n",
            "Train step - Step 1450, Loss 0.11234478652477264\n",
            "Train step - Step 1460, Loss 0.10928717255592346\n",
            "Train step - Step 1470, Loss 0.11461663991212845\n",
            "Train step - Step 1480, Loss 0.11687944829463959\n",
            "Train epoch - Accuracy: 0.4181294964028777 Loss: 0.11123203638860647 Corrects: 2906\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.11128254234790802\n",
            "Train step - Step 1500, Loss 0.11456124484539032\n",
            "Train step - Step 1510, Loss 0.11715060472488403\n",
            "Train step - Step 1520, Loss 0.10762407630681992\n",
            "Train step - Step 1530, Loss 0.111262746155262\n",
            "Train epoch - Accuracy: 0.4153956834532374 Loss: 0.11090628285845407 Corrects: 2887\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.11120828986167908\n",
            "Train step - Step 1550, Loss 0.11126578599214554\n",
            "Train step - Step 1560, Loss 0.11340449750423431\n",
            "Train step - Step 1570, Loss 0.11019483208656311\n",
            "Train step - Step 1580, Loss 0.11286943405866623\n",
            "Train step - Step 1590, Loss 0.1149769052863121\n",
            "Train epoch - Accuracy: 0.4162589928057554 Loss: 0.1111660575116281 Corrects: 2893\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.11194290965795517\n",
            "Train step - Step 1610, Loss 0.11284130066633224\n",
            "Train step - Step 1620, Loss 0.1077054813504219\n",
            "Train step - Step 1630, Loss 0.11191894114017487\n",
            "Train step - Step 1640, Loss 0.10993587225675583\n",
            "Train epoch - Accuracy: 0.4312230215827338 Loss: 0.11079684528086683 Corrects: 2997\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.1080731526017189\n",
            "Train step - Step 1660, Loss 0.10984128713607788\n",
            "Train step - Step 1670, Loss 0.11096865683794022\n",
            "Train step - Step 1680, Loss 0.11229459941387177\n",
            "Train step - Step 1690, Loss 0.11068448424339294\n",
            "Train step - Step 1700, Loss 0.11012537777423859\n",
            "Train epoch - Accuracy: 0.4300719424460432 Loss: 0.11065561116384945 Corrects: 2989\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.10809464007616043\n",
            "Train step - Step 1720, Loss 0.10567010194063187\n",
            "Train step - Step 1730, Loss 0.1107209101319313\n",
            "Train step - Step 1740, Loss 0.11146298050880432\n",
            "Train step - Step 1750, Loss 0.1132354587316513\n",
            "Train epoch - Accuracy: 0.441294964028777 Loss: 0.11079422479696412 Corrects: 3067\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.10851992666721344\n",
            "Train step - Step 1770, Loss 0.11201462149620056\n",
            "Train step - Step 1780, Loss 0.11066582053899765\n",
            "Train step - Step 1790, Loss 0.10723243653774261\n",
            "Train step - Step 1800, Loss 0.11054770648479462\n",
            "Train step - Step 1810, Loss 0.11226419359445572\n",
            "Train epoch - Accuracy: 0.4322302158273381 Loss: 0.11074140697288856 Corrects: 3004\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.10644669085741043\n",
            "Train step - Step 1830, Loss 0.11139556020498276\n",
            "Train step - Step 1840, Loss 0.10414527356624603\n",
            "Train step - Step 1850, Loss 0.10871492326259613\n",
            "Train step - Step 1860, Loss 0.11148341000080109\n",
            "Train epoch - Accuracy: 0.44402877697841725 Loss: 0.11070941352158141 Corrects: 3086\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.11019007861614227\n",
            "Train step - Step 1880, Loss 0.10894083976745605\n",
            "Train step - Step 1890, Loss 0.11385776102542877\n",
            "Train step - Step 1900, Loss 0.11300301551818848\n",
            "Train step - Step 1910, Loss 0.11296828091144562\n",
            "Train step - Step 1920, Loss 0.10726422816514969\n",
            "Train epoch - Accuracy: 0.4470503597122302 Loss: 0.11047847422764456 Corrects: 3107\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.10640314221382141\n",
            "Train step - Step 1940, Loss 0.10740918666124344\n",
            "Train step - Step 1950, Loss 0.11404767632484436\n",
            "Train step - Step 1960, Loss 0.10807978361845016\n",
            "Train step - Step 1970, Loss 0.1120174452662468\n",
            "Train epoch - Accuracy: 0.45884892086330936 Loss: 0.11021171030380743 Corrects: 3189\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.11276271939277649\n",
            "Train step - Step 1990, Loss 0.1098063588142395\n",
            "Train step - Step 2000, Loss 0.09906284511089325\n",
            "Train step - Step 2010, Loss 0.11249485611915588\n",
            "Train step - Step 2020, Loss 0.11432306468486786\n",
            "Train step - Step 2030, Loss 0.11648441106081009\n",
            "Train epoch - Accuracy: 0.4571223021582734 Loss: 0.1105662069625134 Corrects: 3177\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.11344647407531738\n",
            "Train step - Step 2050, Loss 0.11283976584672928\n",
            "Train step - Step 2060, Loss 0.1111178919672966\n",
            "Train step - Step 2070, Loss 0.11041387170553207\n",
            "Train step - Step 2080, Loss 0.11094225943088531\n",
            "Train epoch - Accuracy: 0.4566906474820144 Loss: 0.11020371581367451 Corrects: 3174\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.11312046647071838\n",
            "Train step - Step 2100, Loss 0.10876409709453583\n",
            "Train step - Step 2110, Loss 0.10700599104166031\n",
            "Train step - Step 2120, Loss 0.11417405307292938\n",
            "Train step - Step 2130, Loss 0.10538226366043091\n",
            "Train step - Step 2140, Loss 0.10739479213953018\n",
            "Train epoch - Accuracy: 0.4623021582733813 Loss: 0.10991667085628716 Corrects: 3213\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.1111285537481308\n",
            "Train step - Step 2160, Loss 0.11043570935726166\n",
            "Train step - Step 2170, Loss 0.1132730096578598\n",
            "Train step - Step 2180, Loss 0.10797766596078873\n",
            "Train step - Step 2190, Loss 0.1075136736035347\n",
            "Train epoch - Accuracy: 0.4646043165467626 Loss: 0.11013465460041444 Corrects: 3229\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.10778594762086868\n",
            "Train step - Step 2210, Loss 0.11285849660634995\n",
            "Train step - Step 2220, Loss 0.11016436666250229\n",
            "Train step - Step 2230, Loss 0.10984543710947037\n",
            "Train step - Step 2240, Loss 0.11526627093553543\n",
            "Train step - Step 2250, Loss 0.11246607452630997\n",
            "Train epoch - Accuracy: 0.46446043165467626 Loss: 0.10992793844758178 Corrects: 3228\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.11123178899288177\n",
            "Train step - Step 2270, Loss 0.11217700690031052\n",
            "Train step - Step 2280, Loss 0.10743870586156845\n",
            "Train step - Step 2290, Loss 0.11287301778793335\n",
            "Train step - Step 2300, Loss 0.11007250100374222\n",
            "Train epoch - Accuracy: 0.47784172661870505 Loss: 0.10995096202805746 Corrects: 3321\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.11518025398254395\n",
            "Train step - Step 2320, Loss 0.10609492659568787\n",
            "Train step - Step 2330, Loss 0.1085001677274704\n",
            "Train step - Step 2340, Loss 0.1124579980969429\n",
            "Train step - Step 2350, Loss 0.10852067172527313\n",
            "Train step - Step 2360, Loss 0.11443451792001724\n",
            "Train epoch - Accuracy: 0.46964028776978417 Loss: 0.11011540832922613 Corrects: 3264\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.10985345393419266\n",
            "Train step - Step 2380, Loss 0.11144489794969559\n",
            "Train step - Step 2390, Loss 0.11819238215684891\n",
            "Train step - Step 2400, Loss 0.11074376851320267\n",
            "Train step - Step 2410, Loss 0.11191879212856293\n",
            "Train epoch - Accuracy: 0.47870503597122305 Loss: 0.1100232518383925 Corrects: 3327\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.10789848119020462\n",
            "Train step - Step 2430, Loss 0.10747861862182617\n",
            "Train step - Step 2440, Loss 0.1044626384973526\n",
            "Train step - Step 2450, Loss 0.10420090705156326\n",
            "Train step - Step 2460, Loss 0.11318976432085037\n",
            "Train step - Step 2470, Loss 0.10898756980895996\n",
            "Train epoch - Accuracy: 0.47899280575539566 Loss: 0.10999306189070503 Corrects: 3329\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.10656280815601349\n",
            "Train step - Step 2490, Loss 0.11154776811599731\n",
            "Train step - Step 2500, Loss 0.10699678957462311\n",
            "Train step - Step 2510, Loss 0.10411857068538666\n",
            "Train step - Step 2520, Loss 0.10885108262300491\n",
            "Train epoch - Accuracy: 0.48345323741007196 Loss: 0.10958167232197823 Corrects: 3360\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.11255710572004318\n",
            "Train step - Step 2540, Loss 0.11555091291666031\n",
            "Train step - Step 2550, Loss 0.10906470566987991\n",
            "Train step - Step 2560, Loss 0.10887506604194641\n",
            "Train step - Step 2570, Loss 0.11346904188394547\n",
            "Train step - Step 2580, Loss 0.11148615926504135\n",
            "Train epoch - Accuracy: 0.48503597122302156 Loss: 0.10982423549504589 Corrects: 3371\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.10993651300668716\n",
            "Train step - Step 2600, Loss 0.10985113680362701\n",
            "Train step - Step 2610, Loss 0.10834859311580658\n",
            "Train step - Step 2620, Loss 0.10697795450687408\n",
            "Train step - Step 2630, Loss 0.1103862076997757\n",
            "Train epoch - Accuracy: 0.48733812949640287 Loss: 0.10975674594692189 Corrects: 3387\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.11245259642601013\n",
            "Train step - Step 2650, Loss 0.11202692985534668\n",
            "Train step - Step 2660, Loss 0.11235354095697403\n",
            "Train step - Step 2670, Loss 0.1054014042019844\n",
            "Train step - Step 2680, Loss 0.10780233144760132\n",
            "Train step - Step 2690, Loss 0.10777811706066132\n",
            "Train epoch - Accuracy: 0.48589928057553955 Loss: 0.10947825575904023 Corrects: 3377\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.11478321254253387\n",
            "Train step - Step 2710, Loss 0.11138267815113068\n",
            "Train step - Step 2720, Loss 0.10327067971229553\n",
            "Train step - Step 2730, Loss 0.11553207039833069\n",
            "Train step - Step 2740, Loss 0.11064747720956802\n",
            "Train epoch - Accuracy: 0.4925179856115108 Loss: 0.10923080480141606 Corrects: 3423\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.10803645849227905\n",
            "Train step - Step 2760, Loss 0.1095532476902008\n",
            "Train step - Step 2770, Loss 0.11082753539085388\n",
            "Train step - Step 2780, Loss 0.10915699601173401\n",
            "Train step - Step 2790, Loss 0.10794468969106674\n",
            "Train step - Step 2800, Loss 0.10739375650882721\n",
            "Train epoch - Accuracy: 0.5064748201438849 Loss: 0.10879517895926674 Corrects: 3520\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.11303187906742096\n",
            "Train step - Step 2820, Loss 0.11210775375366211\n",
            "Train step - Step 2830, Loss 0.11195001006126404\n",
            "Train step - Step 2840, Loss 0.1091427430510521\n",
            "Train step - Step 2850, Loss 0.10330520570278168\n",
            "Train epoch - Accuracy: 0.49798561151079135 Loss: 0.10895825965799016 Corrects: 3461\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.11089502274990082\n",
            "Train step - Step 2870, Loss 0.10892418771982193\n",
            "Train step - Step 2880, Loss 0.10656187683343887\n",
            "Train step - Step 2890, Loss 0.10784585773944855\n",
            "Train step - Step 2900, Loss 0.11091858893632889\n",
            "Train step - Step 2910, Loss 0.11401791125535965\n",
            "Train epoch - Accuracy: 0.5041726618705036 Loss: 0.10892918633685696 Corrects: 3504\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.10582082718610764\n",
            "Train step - Step 2930, Loss 0.10994654148817062\n",
            "Train step - Step 2940, Loss 0.10541077703237534\n",
            "Train step - Step 2950, Loss 0.10537267476320267\n",
            "Train step - Step 2960, Loss 0.11198296397924423\n",
            "Train epoch - Accuracy: 0.5053237410071942 Loss: 0.10881994242076394 Corrects: 3512\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.11389835178852081\n",
            "Train step - Step 2980, Loss 0.10447391867637634\n",
            "Train step - Step 2990, Loss 0.11201303452253342\n",
            "Train step - Step 3000, Loss 0.1058720275759697\n",
            "Train step - Step 3010, Loss 0.10759922116994858\n",
            "Train step - Step 3020, Loss 0.11055205017328262\n",
            "Train epoch - Accuracy: 0.5001438848920863 Loss: 0.10904232826807517 Corrects: 3476\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.10850048065185547\n",
            "Train step - Step 3040, Loss 0.11058203876018524\n",
            "Train step - Step 3050, Loss 0.10442548245191574\n",
            "Train step - Step 3060, Loss 0.107340008020401\n",
            "Train step - Step 3070, Loss 0.10866621136665344\n",
            "Train epoch - Accuracy: 0.5024460431654676 Loss: 0.1090829867060236 Corrects: 3492\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.11722080409526825\n",
            "Train step - Step 3090, Loss 0.11135861277580261\n",
            "Train step - Step 3100, Loss 0.1118389293551445\n",
            "Train step - Step 3110, Loss 0.10675091296434402\n",
            "Train step - Step 3120, Loss 0.10600626468658447\n",
            "Train step - Step 3130, Loss 0.10650550574064255\n",
            "Train epoch - Accuracy: 0.5064748201438849 Loss: 0.109238086565793 Corrects: 3520\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.10684335976839066\n",
            "Train step - Step 3150, Loss 0.11048531532287598\n",
            "Train step - Step 3160, Loss 0.10643172264099121\n",
            "Train step - Step 3170, Loss 0.10603352636098862\n",
            "Train step - Step 3180, Loss 0.11114582419395447\n",
            "Train epoch - Accuracy: 0.5044604316546762 Loss: 0.10907553566445549 Corrects: 3506\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.10778507590293884\n",
            "Train step - Step 3200, Loss 0.10976175218820572\n",
            "Train step - Step 3210, Loss 0.11332609504461288\n",
            "Train step - Step 3220, Loss 0.10442261397838593\n",
            "Train step - Step 3230, Loss 0.10723082721233368\n",
            "Train step - Step 3240, Loss 0.10427755117416382\n",
            "Train epoch - Accuracy: 0.49366906474820144 Loss: 0.10929572949092164 Corrects: 3431\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.10524629056453705\n",
            "Train step - Step 3260, Loss 0.10819964855909348\n",
            "Train step - Step 3270, Loss 0.10620851069688797\n",
            "Train step - Step 3280, Loss 0.11090235412120819\n",
            "Train step - Step 3290, Loss 0.10308661311864853\n",
            "Train epoch - Accuracy: 0.5035971223021583 Loss: 0.10893354634158045 Corrects: 3500\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.11024394631385803\n",
            "Train step - Step 3310, Loss 0.10673773288726807\n",
            "Train step - Step 3320, Loss 0.1092933863401413\n",
            "Train step - Step 3330, Loss 0.10927380621433258\n",
            "Train step - Step 3340, Loss 0.10730932652950287\n",
            "Train step - Step 3350, Loss 0.10309305787086487\n",
            "Train epoch - Accuracy: 0.5079136690647482 Loss: 0.10902472330297497 Corrects: 3530\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.11129717528820038\n",
            "Train step - Step 3370, Loss 0.10978084057569504\n",
            "Train step - Step 3380, Loss 0.11048128455877304\n",
            "Train step - Step 3390, Loss 0.10566554963588715\n",
            "Train step - Step 3400, Loss 0.11031026393175125\n",
            "Train epoch - Accuracy: 0.5050359712230216 Loss: 0.10893415376007985 Corrects: 3510\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.10509617626667023\n",
            "Train step - Step 3420, Loss 0.10180573910474777\n",
            "Train step - Step 3430, Loss 0.1096348911523819\n",
            "Train step - Step 3440, Loss 0.11126618087291718\n",
            "Train step - Step 3450, Loss 0.11009401082992554\n",
            "Train step - Step 3460, Loss 0.11095483601093292\n",
            "Train epoch - Accuracy: 0.5037410071942446 Loss: 0.1087986209366819 Corrects: 3501\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.10735827684402466\n",
            "Train step - Step 3480, Loss 0.10729442536830902\n",
            "Train step - Step 3490, Loss 0.10839799791574478\n",
            "Train step - Step 3500, Loss 0.11079046875238419\n",
            "Train step - Step 3510, Loss 0.10532907396554947\n",
            "Train epoch - Accuracy: 0.5074820143884892 Loss: 0.10872635985878731 Corrects: 3527\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.1121063232421875\n",
            "Train step - Step 3530, Loss 0.1089276447892189\n",
            "Train step - Step 3540, Loss 0.11132317036390305\n",
            "Train step - Step 3550, Loss 0.11112483590841293\n",
            "Train step - Step 3560, Loss 0.10539624840021133\n",
            "Train step - Step 3570, Loss 0.11309166252613068\n",
            "Train epoch - Accuracy: 0.5086330935251798 Loss: 0.10875009624434889 Corrects: 3535\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.10463821887969971\n",
            "Train step - Step 3590, Loss 0.1083456501364708\n",
            "Train step - Step 3600, Loss 0.11208462715148926\n",
            "Train step - Step 3610, Loss 0.10617833584547043\n",
            "Train step - Step 3620, Loss 0.10648415982723236\n",
            "Train epoch - Accuracy: 0.5054676258992806 Loss: 0.10883055514140094 Corrects: 3513\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.1134973093867302\n",
            "Train step - Step 3640, Loss 0.1046425849199295\n",
            "Train step - Step 3650, Loss 0.10849392414093018\n",
            "Train step - Step 3660, Loss 0.10880487412214279\n",
            "Train step - Step 3670, Loss 0.11140649765729904\n",
            "Train step - Step 3680, Loss 0.11175882816314697\n",
            "Train epoch - Accuracy: 0.5048920863309353 Loss: 0.10882648020982742 Corrects: 3509\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.11110129952430725\n",
            "Train step - Step 3700, Loss 0.10738219320774078\n",
            "Train step - Step 3710, Loss 0.10985168069601059\n",
            "Train step - Step 3720, Loss 0.10823574662208557\n",
            "Train step - Step 3730, Loss 0.11040154099464417\n",
            "Train epoch - Accuracy: 0.5066187050359712 Loss: 0.10888175349441363 Corrects: 3521\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.10949043929576874\n",
            "Train step - Step 3750, Loss 0.1051686555147171\n",
            "Train step - Step 3760, Loss 0.10337143391370773\n",
            "Train step - Step 3770, Loss 0.10748476535081863\n",
            "Train step - Step 3780, Loss 0.11219697445631027\n",
            "Train step - Step 3790, Loss 0.1050342246890068\n",
            "Train epoch - Accuracy: 0.5046043165467626 Loss: 0.10853031008363628 Corrects: 3507\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.10541073977947235\n",
            "Train step - Step 3810, Loss 0.11422277241945267\n",
            "Train step - Step 3820, Loss 0.10719963163137436\n",
            "Train step - Step 3830, Loss 0.10253601521253586\n",
            "Train step - Step 3840, Loss 0.11046653240919113\n",
            "Train epoch - Accuracy: 0.5063309352517985 Loss: 0.10856530608461915 Corrects: 3519\n",
            "Training finished in 401.8233149051666 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0, 79, 71, 63, 26, 14, 2, 73, 41, 56, 48, 99, 23, 66, 22, 6, 93, 17, 5, 1, 28, 87, 75, 55, 47, 39, 70, 42, 38, 29, 4, 83, 51, 18, 89, 85, 9, 84, 64, 20, 8]\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238225590>\n",
            "Constructing exemplars of class 83\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [26992, 39204, 30835, 34520, 49226, 32336, 43773, 49484, 4849, 49689, 48391, 49905, 311, 28036, 14953, 1857, 5618, 4408, 4991, 28652, 23548, 37021]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22413d9350>\n",
            "Constructing exemplars of class 51\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [4839, 26436, 19938, 8857, 10522, 47680, 2926, 7525, 48532, 12540, 1994, 713, 25056, 17143, 33969, 43362, 4778, 21189, 2882, 40224, 26797, 2381]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232ec3850>\n",
            "Constructing exemplars of class 18\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [44655, 46884, 9654, 437, 32203, 18847, 46329, 22976, 89, 41094, 2435, 1242, 11061, 2554, 10359, 20584, 21942, 14633, 4384, 38338, 32775, 8405]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238225590>\n",
            "Constructing exemplars of class 89\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [4633, 46031, 47744, 28360, 42165, 16618, 15526, 36363, 9168, 43324, 20340, 21688, 35994, 26335, 13415, 41994, 24089, 29224, 43373, 42624, 34007, 26716]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a7e090>\n",
            "Constructing exemplars of class 85\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [34165, 7911, 13562, 25446, 43576, 43692, 40029, 803, 49501, 32231, 12601, 5799, 6105, 27580, 13562, 10604, 16094, 36380, 11452, 46988, 46360, 40390]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232935650>\n",
            "Constructing exemplars of class 9\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [19148, 40902, 27408, 36125, 47217, 45314, 31463, 35341, 46470, 37927, 9131, 29221, 22462, 26685, 49020, 4477, 31474, 4425, 31777, 10860, 17401, 22513]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232f25850>\n",
            "Constructing exemplars of class 84\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [42878, 46177, 43425, 14973, 45186, 3808, 15662, 28012, 34961, 20195, 21532, 23425, 25895, 9022, 7949, 45212, 37019, 32817, 39707, 611, 40602, 31447]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223802bd50>\n",
            "Constructing exemplars of class 64\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [35131, 5876, 47777, 11339, 7605, 3583, 1820, 28199, 20867, 34735, 38038, 37690, 19318, 36455, 34476, 12388, 4665, 44105, 10418, 29199, 4137, 2134]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223823c3d0>\n",
            "Constructing exemplars of class 20\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [25504, 45876, 41499, 19315, 32522, 49630, 48234, 37538, 33814, 5231, 37657, 24540, 37898, 34447, 7813, 26019, 10329, 36557, 19222, 15143, 1827, 44914]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22318f5750>\n",
            "Constructing exemplars of class 8\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [5757, 45769, 1023, 16, 42539, 35210, 45233, 13295, 43052, 43343, 11388, 41735, 3954, 22403, 28530, 9066, 9094, 33226, 22948, 1555, 37517, 8222]\n",
            "x train:  [-0.09352785 -0.05136676 -0.04969806 -0.05857512 -0.05539732 -0.13016284\n",
            " -0.06102295 -0.08115377 -0.00628711 -0.04326263 -0.16178523 -0.10003998\n",
            " -0.05314515 -0.0704823  -0.07987405 -0.09192872 -0.06082618 -0.09903963\n",
            " -0.17164305 -0.11467971 -0.01610225 -0.08194405 -0.07670053 -0.08898933\n",
            "  0.00143447 -0.10025608 -0.12557998 -0.08642658 -0.08870966 -0.1502889\n",
            " -0.0842734  -0.06207918 -0.06425309 -0.14676146 -0.06749631 -0.07467256\n",
            " -0.09115262  0.05204477 -0.14654665 -0.13180998 -0.1246314  -0.12980564\n",
            " -0.085691    0.03679985 -0.0724887  -0.12419569 -0.1173562  -0.09047754\n",
            " -0.10537238 -0.03525478 -0.11926162 -0.13276824 -0.17137156 -0.06704759\n",
            " -0.05119825 -0.07478905 -0.1407187  -0.11746655 -0.1142459  -0.11001165\n",
            " -0.07796169 -0.12239216 -0.13335155 -0.09141608 -0.04167837 -0.16745898\n",
            " -0.14706273 -0.14847566 -0.04259048 -0.12654921 -0.12332562 -0.14890416\n",
            " -0.12440994 -0.10057245 -0.09354613  0.02922344 -0.16116078 -0.14937773\n",
            " -0.0837955  -0.10856777  0.05189877 -0.08517921 -0.12563795 -0.11871632\n",
            " -0.18916553 -0.09354958 -0.09550085 -0.08747629 -0.16797033 -0.13474862]\n",
            "y_train:  [tensor([92]), tensor([77]), tensor([25]), tensor([70]), tensor([70]), tensor([89]), tensor([23]), tensor([28]), tensor([4]), tensor([23]), tensor([75]), tensor([93]), tensor([18]), tensor([23]), tensor([3]), tensor([31]), tensor([99]), tensor([99]), tensor([45]), tensor([28]), tensor([59]), tensor([14]), tensor([92]), tensor([64]), tensor([47]), tensor([8]), tensor([32]), tensor([42]), tensor([25]), tensor([66]), tensor([21]), tensor([41]), tensor([99]), tensor([45]), tensor([49]), tensor([3]), tensor([24]), tensor([48]), tensor([4]), tensor([21]), tensor([42]), tensor([42]), tensor([93]), tensor([41]), tensor([46]), tensor([4]), tensor([9]), tensor([49]), tensor([94]), tensor([96]), tensor([93]), tensor([60]), tensor([8]), tensor([84]), tensor([30]), tensor([22]), tensor([27]), tensor([86]), tensor([39]), tensor([86]), tensor([31]), tensor([85]), tensor([66]), tensor([66]), tensor([47]), tensor([32]), tensor([79]), tensor([25]), tensor([1]), tensor([23]), tensor([48]), tensor([93]), tensor([53]), tensor([58]), tensor([73]), tensor([37]), tensor([71]), tensor([67]), tensor([65]), tensor([5]), tensor([8]), tensor([46]), tensor([6]), tensor([11]), tensor([39]), tensor([52]), tensor([50]), tensor([88]), tensor([73]), tensor([36]), tensor([68]), tensor([12]), tensor([0]), tensor([95]), tensor([14]), tensor([80]), tensor([32]), tensor([51]), tensor([29]), tensor([85]), tensor([83]), tensor([10]), tensor([52]), tensor([4]), tensor([47]), tensor([1]), tensor([36]), tensor([33]), tensor([26]), tensor([93]), tensor([53]), tensor([81]), tensor([2]), tensor([20]), tensor([35]), tensor([56]), tensor([72]), tensor([31]), tensor([45]), tensor([51]), tensor([24]), tensor([99]), tensor([3]), tensor([30]), tensor([72]), tensor([82]), tensor([97]), tensor([18]), tensor([26]), tensor([4]), tensor([37]), tensor([33]), tensor([28]), tensor([60]), tensor([72]), tensor([91]), tensor([1]), tensor([30]), tensor([50]), tensor([85]), tensor([79]), tensor([84]), tensor([59]), tensor([10]), tensor([91]), tensor([17]), tensor([84]), tensor([41]), tensor([16]), tensor([11]), tensor([98]), tensor([3]), tensor([57]), tensor([81]), tensor([46]), tensor([25]), tensor([55]), tensor([94]), tensor([64]), tensor([34]), tensor([55]), tensor([46]), tensor([98]), tensor([73]), tensor([18]), tensor([14]), tensor([59]), tensor([58]), tensor([99]), tensor([49]), tensor([21]), tensor([94]), tensor([18]), tensor([2]), tensor([98]), tensor([23]), tensor([47]), tensor([31]), tensor([3]), tensor([25]), tensor([80]), tensor([96]), tensor([12]), tensor([86]), tensor([85]), tensor([24]), tensor([46]), tensor([38]), tensor([84]), tensor([39]), tensor([91]), tensor([82]), tensor([83]), tensor([25]), tensor([28]), tensor([81]), tensor([46]), tensor([27]), tensor([22]), tensor([85]), tensor([31]), tensor([94]), tensor([41]), tensor([48]), tensor([67]), tensor([67]), tensor([81]), tensor([11]), tensor([83]), tensor([13]), tensor([18]), tensor([85]), tensor([22]), tensor([33]), tensor([5]), tensor([14]), tensor([65]), tensor([98]), tensor([6]), tensor([57]), tensor([38]), tensor([16]), tensor([77]), tensor([5]), tensor([56]), tensor([95]), tensor([12]), tensor([39]), tensor([2]), tensor([38]), tensor([70]), tensor([38]), tensor([67]), tensor([28]), tensor([95]), tensor([62]), tensor([82]), tensor([11]), tensor([82]), tensor([51]), tensor([99]), tensor([99]), tensor([70]), tensor([17]), tensor([94]), tensor([34]), tensor([66]), tensor([52]), tensor([12]), tensor([59]), tensor([8]), tensor([81]), tensor([56]), tensor([98]), tensor([63]), tensor([86]), tensor([36]), tensor([66]), tensor([31]), tensor([62]), tensor([51]), tensor([45]), tensor([42]), tensor([60]), tensor([32]), tensor([80]), tensor([58]), tensor([33]), tensor([16]), tensor([20]), tensor([78]), tensor([47]), tensor([49]), tensor([8]), tensor([32]), tensor([95]), tensor([69]), tensor([0]), tensor([33]), tensor([12]), tensor([83]), tensor([94]), tensor([75]), tensor([62]), tensor([48]), tensor([70]), tensor([63]), tensor([29]), tensor([8]), tensor([62]), tensor([65]), tensor([97]), tensor([16]), tensor([92]), tensor([70]), tensor([66]), tensor([80]), tensor([29]), tensor([97]), tensor([98]), tensor([60]), tensor([82]), tensor([31]), tensor([23]), tensor([35]), tensor([71]), tensor([66]), tensor([5]), tensor([42]), tensor([52]), tensor([18]), tensor([95]), tensor([95]), tensor([79]), tensor([82]), tensor([17]), tensor([29]), tensor([81]), tensor([63]), tensor([82]), tensor([65]), tensor([26]), tensor([38]), tensor([16]), tensor([62]), tensor([47]), tensor([25]), tensor([47]), tensor([45]), tensor([78]), tensor([91]), tensor([99]), tensor([55]), tensor([62]), tensor([26]), tensor([95]), tensor([8]), tensor([51]), tensor([71]), tensor([42]), tensor([21]), tensor([2]), tensor([1]), tensor([30]), tensor([93]), tensor([87]), tensor([3]), tensor([68]), tensor([65]), tensor([77]), tensor([88]), tensor([34]), tensor([94]), tensor([12]), tensor([41]), tensor([63]), tensor([72]), tensor([32]), tensor([69]), tensor([69]), tensor([86]), tensor([47]), tensor([70]), tensor([56]), tensor([80]), tensor([21]), tensor([27]), tensor([97]), tensor([20]), tensor([1]), tensor([97]), tensor([53]), tensor([24]), tensor([45]), tensor([92]), tensor([0]), tensor([88]), tensor([92]), tensor([78]), tensor([33]), tensor([85]), tensor([79]), tensor([7]), tensor([14]), tensor([80]), tensor([94]), tensor([71]), tensor([33]), tensor([81]), tensor([30]), tensor([36]), tensor([53]), tensor([5]), tensor([97]), tensor([39]), tensor([86]), tensor([25]), tensor([86]), tensor([12]), tensor([96]), tensor([7]), tensor([82]), tensor([10]), tensor([58]), tensor([87]), tensor([33]), tensor([66]), tensor([6]), tensor([36]), tensor([45]), tensor([99]), tensor([30]), tensor([11]), tensor([0]), tensor([17]), tensor([47]), tensor([63]), tensor([49]), tensor([25]), tensor([55]), tensor([88]), tensor([66]), tensor([77]), tensor([98]), tensor([31]), tensor([92]), tensor([85]), tensor([27]), tensor([36]), tensor([98]), tensor([79]), tensor([81]), tensor([66]), tensor([84]), tensor([98]), tensor([17]), tensor([2]), tensor([55]), tensor([16]), tensor([2]), tensor([42]), tensor([30]), tensor([89]), tensor([0]), tensor([72]), tensor([98]), tensor([8]), tensor([10]), tensor([87]), tensor([73]), tensor([23]), tensor([16]), tensor([10]), tensor([70]), tensor([31]), tensor([12]), tensor([88]), tensor([34]), tensor([60]), tensor([75]), tensor([87]), tensor([6]), tensor([37]), tensor([50]), tensor([41]), tensor([51]), tensor([24]), tensor([7]), tensor([64]), tensor([21]), tensor([1]), tensor([23]), tensor([79]), tensor([58]), tensor([83]), tensor([52]), tensor([32]), tensor([89]), tensor([1]), tensor([67]), tensor([95]), tensor([25]), tensor([65]), tensor([50]), tensor([53]), tensor([68]), tensor([23]), tensor([86]), tensor([95]), tensor([94]), tensor([11]), tensor([38]), tensor([48]), tensor([32]), tensor([60]), tensor([68]), tensor([93]), tensor([20]), tensor([13]), tensor([35]), tensor([49]), tensor([88]), tensor([38]), tensor([35]), tensor([75]), tensor([8]), tensor([18]), tensor([34]), tensor([97]), tensor([82]), tensor([9]), tensor([87]), tensor([21]), tensor([13]), tensor([45]), tensor([10]), tensor([84]), tensor([67]), tensor([94]), tensor([10]), tensor([67]), tensor([34]), tensor([45]), tensor([75]), tensor([5]), tensor([99]), tensor([84]), tensor([31]), tensor([64]), tensor([96]), tensor([41]), tensor([79]), tensor([93]), tensor([71]), tensor([47]), tensor([48]), tensor([93]), tensor([72]), tensor([80]), tensor([72]), tensor([70]), tensor([88]), tensor([41]), tensor([98]), tensor([45]), tensor([16]), tensor([37]), tensor([50]), tensor([26]), tensor([30]), tensor([42]), tensor([79]), tensor([92]), tensor([87]), tensor([66]), tensor([78]), tensor([8]), tensor([0]), tensor([98]), tensor([49]), tensor([72]), tensor([53]), tensor([2]), tensor([67]), tensor([71]), tensor([77]), tensor([46]), tensor([84]), tensor([91]), tensor([83]), tensor([64]), tensor([72]), tensor([68]), tensor([38]), tensor([87]), tensor([46]), tensor([91]), tensor([86]), tensor([9]), tensor([60]), tensor([37]), tensor([11]), tensor([4]), tensor([22]), tensor([73]), tensor([89]), tensor([32]), tensor([56]), tensor([1]), tensor([80]), tensor([72]), tensor([10]), tensor([51]), tensor([23]), tensor([98]), tensor([53]), tensor([55]), tensor([71]), tensor([46]), tensor([65]), tensor([95]), tensor([67]), tensor([63]), tensor([29]), tensor([27]), tensor([1]), tensor([87]), tensor([49]), tensor([12]), tensor([20]), tensor([12]), tensor([5]), tensor([75]), tensor([25]), tensor([38]), tensor([48]), tensor([31]), tensor([33]), tensor([92]), tensor([9]), tensor([89]), tensor([34]), tensor([51]), tensor([29]), tensor([11]), tensor([70]), tensor([20]), tensor([55]), tensor([86]), tensor([60]), tensor([46]), tensor([45]), tensor([65]), tensor([87]), tensor([59]), tensor([69]), tensor([83]), tensor([11]), tensor([20]), tensor([25]), tensor([85]), tensor([38]), tensor([7]), tensor([34]), tensor([92]), tensor([35]), tensor([2]), tensor([38]), tensor([53]), tensor([5]), tensor([81]), tensor([78]), tensor([7]), tensor([96]), tensor([70]), tensor([28]), tensor([80]), tensor([63]), tensor([72]), tensor([57]), tensor([45]), tensor([60]), tensor([33]), tensor([4]), tensor([5]), tensor([96]), tensor([87]), tensor([83]), tensor([46]), tensor([37]), tensor([78]), tensor([27]), tensor([78]), tensor([41]), tensor([77]), tensor([47]), tensor([71]), tensor([36]), tensor([71]), tensor([81]), tensor([69]), tensor([63]), tensor([77]), tensor([97]), tensor([62]), tensor([39]), tensor([22]), tensor([65]), tensor([92]), tensor([68]), tensor([41]), tensor([57]), tensor([21]), tensor([0]), tensor([92]), tensor([96]), tensor([30]), tensor([51]), tensor([21]), tensor([63]), tensor([92]), tensor([42]), tensor([72]), tensor([34]), tensor([53]), tensor([31]), tensor([11]), tensor([39]), tensor([57]), tensor([9]), tensor([80]), tensor([34]), tensor([2]), tensor([5]), tensor([46]), tensor([4]), tensor([23]), tensor([56]), tensor([4]), tensor([56]), tensor([22]), tensor([29]), tensor([18]), tensor([26]), tensor([20]), tensor([16]), tensor([0]), tensor([25]), tensor([7]), tensor([99]), tensor([98]), tensor([37]), tensor([57]), tensor([98]), tensor([99]), tensor([87]), tensor([13]), tensor([38]), tensor([50]), tensor([29]), tensor([69]), tensor([91]), tensor([80]), tensor([57]), tensor([69]), tensor([30]), tensor([6]), tensor([23]), tensor([46]), tensor([26]), tensor([93]), tensor([94]), tensor([60]), tensor([94]), tensor([62]), tensor([56]), tensor([32]), tensor([42]), tensor([48]), tensor([31]), tensor([62]), tensor([45]), tensor([39]), tensor([24]), tensor([35]), tensor([23]), tensor([28]), tensor([13]), tensor([52]), tensor([57]), tensor([24]), tensor([80]), tensor([41]), tensor([51]), tensor([1]), tensor([20]), tensor([88]), tensor([51]), tensor([72]), tensor([58]), tensor([88]), tensor([18]), tensor([97]), tensor([23]), tensor([18]), tensor([27]), tensor([77]), tensor([82]), tensor([47]), tensor([6]), tensor([28]), tensor([50]), tensor([25]), tensor([5]), tensor([2]), tensor([37]), tensor([8]), tensor([0]), tensor([96]), tensor([85]), tensor([67]), tensor([59]), tensor([98]), tensor([11]), tensor([0]), tensor([60]), tensor([11]), tensor([7]), tensor([24]), tensor([99]), tensor([53]), tensor([49]), tensor([33]), tensor([89]), tensor([52]), tensor([81]), tensor([72]), tensor([95]), tensor([56]), tensor([69]), tensor([21]), tensor([22]), tensor([60]), tensor([5]), tensor([5]), tensor([10]), tensor([86]), tensor([83]), tensor([16]), tensor([58]), tensor([28]), tensor([3]), tensor([59]), tensor([55]), tensor([2]), tensor([59]), tensor([75]), tensor([22]), tensor([88]), tensor([42]), tensor([58]), tensor([27]), tensor([68]), tensor([94]), tensor([87]), tensor([66]), tensor([59]), tensor([63]), tensor([39]), tensor([14]), tensor([0]), tensor([11]), tensor([16]), tensor([48]), tensor([14]), tensor([36]), tensor([7]), tensor([58]), tensor([79]), tensor([67]), tensor([27]), tensor([29]), tensor([45]), tensor([81]), tensor([71]), tensor([7]), tensor([10]), tensor([37]), tensor([55]), tensor([87]), tensor([95]), tensor([26]), tensor([82]), tensor([30]), tensor([91]), tensor([73]), tensor([78]), tensor([65]), tensor([87]), tensor([51]), tensor([69]), tensor([86]), tensor([24]), tensor([22]), tensor([73]), tensor([48]), tensor([73]), tensor([68]), tensor([14]), tensor([14]), tensor([56]), tensor([12]), tensor([62]), tensor([68]), tensor([55]), tensor([89]), tensor([45]), tensor([6]), tensor([48]), tensor([53]), tensor([10]), tensor([55]), tensor([80]), tensor([51]), tensor([86]), tensor([20]), tensor([24]), tensor([4]), tensor([51]), tensor([64]), tensor([73]), tensor([1]), tensor([18]), tensor([26]), tensor([31]), tensor([84]), tensor([39]), tensor([1]), tensor([41]), tensor([68]), tensor([2]), tensor([46]), tensor([48]), tensor([31]), tensor([42]), tensor([85]), tensor([64]), tensor([26]), tensor([93]), tensor([50]), tensor([3]), tensor([8]), tensor([67]), tensor([97]), tensor([18]), tensor([63]), tensor([49]), tensor([91]), tensor([12]), tensor([8]), tensor([89]), tensor([12]), tensor([58]), tensor([88]), tensor([34]), tensor([86]), tensor([66]), tensor([78]), tensor([58]), tensor([10]), tensor([36]), tensor([97]), tensor([59]), tensor([18]), tensor([86]), tensor([53]), tensor([28]), tensor([3]), tensor([27]), tensor([62]), tensor([56]), tensor([23]), tensor([42]), tensor([29]), tensor([33]), tensor([97]), tensor([28]), tensor([73]), tensor([96]), tensor([79]), tensor([67]), tensor([99]), tensor([85]), tensor([47]), tensor([65]), tensor([50]), tensor([23]), tensor([64]), tensor([36]), tensor([60]), tensor([63]), tensor([67]), tensor([79]), tensor([62]), tensor([89]), tensor([22]), tensor([24]), tensor([28]), tensor([10]), tensor([84]), tensor([67]), tensor([18]), tensor([88]), tensor([35]), tensor([97]), tensor([16]), tensor([26]), tensor([81]), tensor([96]), tensor([58]), tensor([22]), tensor([20]), tensor([80]), tensor([60]), tensor([59]), tensor([81]), tensor([27]), tensor([84]), tensor([6]), tensor([27]), tensor([39]), tensor([48]), tensor([82]), tensor([98]), tensor([8]), tensor([27]), tensor([6]), tensor([65]), tensor([39]), tensor([99]), tensor([53]), tensor([73]), tensor([4]), tensor([50]), tensor([4]), tensor([17]), tensor([81]), tensor([45]), tensor([77]), tensor([22]), tensor([41]), tensor([24]), tensor([37]), tensor([58]), tensor([24]), tensor([13]), tensor([93]), tensor([93]), tensor([89]), tensor([50]), tensor([57]), tensor([68]), tensor([99]), tensor([42]), tensor([69]), tensor([71]), tensor([77]), tensor([18]), tensor([27]), tensor([41]), tensor([78]), tensor([28]), tensor([29]), tensor([9]), tensor([25]), tensor([79]), tensor([98]), tensor([1]), tensor([24]), tensor([68]), tensor([60]), tensor([96]), tensor([29]), tensor([56]), tensor([6]), tensor([31]), tensor([94]), tensor([69]), tensor([34]), tensor([73]), tensor([91]), tensor([12]), tensor([55]), tensor([32]), tensor([32]), tensor([88]), tensor([21]), tensor([79]), tensor([62]), tensor([0]), tensor([89]), tensor([59]), tensor([92]), tensor([65]), tensor([21]), tensor([85]), tensor([30]), tensor([99]), tensor([38]), tensor([9]), tensor([96]), tensor([46]), tensor([66]), tensor([53]), tensor([50]), tensor([82]), tensor([14]), tensor([84]), tensor([9]), tensor([18]), tensor([50]), tensor([3]), tensor([52]), tensor([0]), tensor([17]), tensor([36]), tensor([70]), tensor([38]), tensor([97]), tensor([55]), tensor([50]), tensor([14]), tensor([89]), tensor([11]), tensor([77]), tensor([82]), tensor([21]), tensor([98]), tensor([64]), tensor([42]), tensor([33]), tensor([42]), tensor([1]), tensor([56]), tensor([96]), tensor([10]), tensor([39]), tensor([3]), tensor([36]), tensor([56]), tensor([78]), tensor([91]), tensor([29]), tensor([57]), tensor([7]), tensor([20]), tensor([87]), tensor([21]), tensor([78]), tensor([72]), tensor([55]), tensor([1]), tensor([30]), tensor([92]), tensor([18]), tensor([25]), tensor([97]), tensor([28]), tensor([38]), tensor([52]), tensor([96]), tensor([35]), tensor([7]), tensor([65]), tensor([91]), tensor([2]), tensor([53]), tensor([57]), tensor([35]), tensor([70]), tensor([3]), tensor([93]), tensor([26]), tensor([99]), tensor([12]), tensor([16]), tensor([36]), tensor([6]), tensor([87]), tensor([42]), tensor([35]), tensor([82]), tensor([36]), tensor([13]), tensor([23]), tensor([29]), tensor([21]), tensor([97]), tensor([91]), tensor([57]), tensor([42]), tensor([9]), tensor([71]), tensor([39]), tensor([6]), tensor([83]), tensor([9]), tensor([64]), tensor([73]), tensor([14]), tensor([60]), tensor([68]), tensor([64]), tensor([47]), tensor([79]), tensor([16]), tensor([62]), tensor([89]), tensor([99]), tensor([78]), tensor([83]), tensor([22]), tensor([25]), tensor([9]), tensor([72]), tensor([37]), tensor([9]), tensor([57]), tensor([11]), tensor([5]), tensor([52]), tensor([13]), tensor([77]), tensor([91]), tensor([63]), tensor([3]), tensor([97]), tensor([59]), tensor([2]), tensor([22]), tensor([22]), tensor([13]), tensor([32]), tensor([49]), tensor([28]), tensor([27]), tensor([34]), tensor([48]), tensor([94]), tensor([63]), tensor([6]), tensor([27]), tensor([79]), tensor([23]), tensor([17]), tensor([22]), tensor([24]), tensor([86]), tensor([1]), tensor([52]), tensor([13]), tensor([35]), tensor([79]), tensor([77]), tensor([88]), tensor([65]), tensor([55]), tensor([66]), tensor([34]), tensor([86]), tensor([51]), tensor([5]), tensor([23]), tensor([34]), tensor([92]), tensor([42]), tensor([82]), tensor([9]), tensor([52]), tensor([58]), tensor([70]), tensor([66]), tensor([9]), tensor([86]), tensor([52]), tensor([45]), tensor([85]), tensor([93]), tensor([45]), tensor([97]), tensor([23]), tensor([5]), tensor([96]), tensor([88]), tensor([70]), tensor([51]), tensor([17]), tensor([2]), tensor([3]), tensor([28]), tensor([21]), tensor([92]), tensor([56]), tensor([55]), tensor([85]), tensor([87]), tensor([52]), tensor([29]), tensor([86]), tensor([29]), tensor([36]), tensor([7]), tensor([22]), tensor([71]), tensor([62]), tensor([17]), tensor([62]), tensor([38]), tensor([91]), tensor([77]), tensor([16]), tensor([18]), tensor([50]), tensor([78]), tensor([58]), tensor([5]), tensor([35]), tensor([1]), tensor([79]), tensor([97]), tensor([89]), tensor([67]), tensor([58]), tensor([73]), tensor([84]), tensor([92]), tensor([32]), tensor([47]), tensor([68]), tensor([17]), tensor([79]), tensor([33]), tensor([16]), tensor([3]), tensor([96]), tensor([50]), tensor([69]), tensor([10]), tensor([7]), tensor([91]), tensor([72]), tensor([98]), tensor([52]), tensor([11]), tensor([45]), tensor([83]), tensor([10]), tensor([3]), tensor([87]), tensor([65]), tensor([39]), tensor([9]), tensor([26]), tensor([75]), tensor([79]), tensor([66]), tensor([26]), tensor([80]), tensor([48]), tensor([64]), tensor([39]), tensor([94]), tensor([13]), tensor([2]), tensor([41]), tensor([13]), tensor([69]), tensor([97]), tensor([28]), tensor([34]), tensor([67]), tensor([27]), tensor([87]), tensor([36]), tensor([10]), tensor([7]), tensor([78]), tensor([35]), tensor([89]), tensor([7]), tensor([75]), tensor([56]), tensor([35]), tensor([65]), tensor([32]), tensor([89]), tensor([37]), tensor([75]), tensor([45]), tensor([13]), tensor([73]), tensor([75]), tensor([35]), tensor([56]), tensor([93]), tensor([18]), tensor([3]), tensor([89]), tensor([10]), tensor([60]), tensor([86]), tensor([5]), tensor([24]), tensor([14]), tensor([71]), tensor([91]), tensor([57]), tensor([11]), tensor([77]), tensor([69]), tensor([75]), tensor([21]), tensor([91]), tensor([56]), tensor([70]), tensor([98]), tensor([0]), tensor([37]), tensor([62]), tensor([10]), tensor([68]), tensor([7]), tensor([71]), tensor([81]), tensor([20]), tensor([8]), tensor([85]), tensor([65]), tensor([69]), tensor([70]), tensor([63]), tensor([30]), tensor([73]), tensor([24]), tensor([75]), tensor([34]), tensor([73]), tensor([37]), tensor([71]), tensor([41]), tensor([9]), tensor([88]), tensor([78]), tensor([78]), tensor([68]), tensor([55]), tensor([24]), tensor([59]), tensor([88]), tensor([31]), tensor([80]), tensor([41]), tensor([31]), tensor([91]), tensor([67]), tensor([6]), tensor([30]), tensor([41]), tensor([13]), tensor([33]), tensor([4]), tensor([4]), tensor([95]), tensor([32]), tensor([41]), tensor([14]), tensor([63]), tensor([77]), tensor([7]), tensor([8]), tensor([77]), tensor([22]), tensor([84]), tensor([22]), tensor([53]), tensor([25]), tensor([53]), tensor([67]), tensor([37]), tensor([26]), tensor([39]), tensor([17]), tensor([8]), tensor([42]), tensor([37]), tensor([51]), tensor([8]), tensor([64]), tensor([12]), tensor([59]), tensor([37]), tensor([83]), tensor([24]), tensor([62]), tensor([95]), tensor([20]), tensor([11]), tensor([33]), tensor([34]), tensor([25]), tensor([9]), tensor([22]), tensor([78]), tensor([84]), tensor([70]), tensor([49]), tensor([94]), tensor([26]), tensor([72]), tensor([27]), tensor([96]), tensor([51]), tensor([7]), tensor([29]), tensor([64]), tensor([1]), tensor([66]), tensor([28]), tensor([82]), tensor([99]), tensor([24]), tensor([62]), tensor([36]), tensor([75]), tensor([5]), tensor([52]), tensor([20]), tensor([56]), tensor([85]), tensor([64]), tensor([91]), tensor([30]), tensor([39]), tensor([47]), tensor([47]), tensor([94]), tensor([59]), tensor([65]), tensor([91]), tensor([48]), tensor([92]), tensor([75]), tensor([80]), tensor([21]), tensor([0]), tensor([30]), tensor([57]), tensor([97]), tensor([27]), tensor([94]), tensor([88]), tensor([63]), tensor([39]), tensor([84]), tensor([14]), tensor([20]), tensor([59]), tensor([82]), tensor([73]), tensor([92]), tensor([36]), tensor([13]), tensor([83]), tensor([7]), tensor([37]), tensor([51]), tensor([10]), tensor([71]), tensor([66]), tensor([17]), tensor([83]), tensor([47]), tensor([52]), tensor([9]), tensor([88]), tensor([11]), tensor([62]), tensor([81]), tensor([3]), tensor([13]), tensor([35]), tensor([14]), tensor([84]), tensor([56]), tensor([93]), tensor([50]), tensor([83]), tensor([16]), tensor([83]), tensor([84]), tensor([92]), tensor([55]), tensor([37]), tensor([22]), tensor([49]), tensor([95]), tensor([84]), tensor([49]), tensor([58]), tensor([50]), tensor([47]), tensor([0]), tensor([33]), tensor([57]), tensor([6]), tensor([89]), tensor([59]), tensor([4]), tensor([32]), tensor([26]), tensor([3]), tensor([11]), tensor([29]), tensor([2]), tensor([87]), tensor([13]), tensor([30]), tensor([21]), tensor([33]), tensor([0]), tensor([4]), tensor([95]), tensor([77]), tensor([38]), tensor([53]), tensor([38]), tensor([80]), tensor([71]), tensor([73]), tensor([82]), tensor([67]), tensor([12]), tensor([63]), tensor([30]), tensor([71]), tensor([26]), tensor([49]), tensor([52]), tensor([1]), tensor([68]), tensor([71]), tensor([8]), tensor([51]), tensor([39]), tensor([83]), tensor([2]), tensor([92]), tensor([87]), tensor([13]), tensor([36]), tensor([8]), tensor([80]), tensor([49]), tensor([46]), tensor([70]), tensor([72]), tensor([69]), tensor([36]), tensor([31]), tensor([48]), tensor([96]), tensor([4]), tensor([25]), tensor([6]), tensor([93]), tensor([84]), tensor([29]), tensor([48]), tensor([83]), tensor([73]), tensor([55]), tensor([48]), tensor([64]), tensor([13]), tensor([77]), tensor([36]), tensor([93]), tensor([45]), tensor([42]), tensor([0]), tensor([9]), tensor([85]), tensor([55]), tensor([89]), tensor([26]), tensor([32]), tensor([17]), tensor([75]), tensor([63]), tensor([6]), tensor([75]), tensor([59]), tensor([58]), tensor([46]), tensor([17]), tensor([17]), tensor([81]), tensor([18]), tensor([41]), tensor([30]), tensor([41]), tensor([85]), tensor([12]), tensor([29]), tensor([24]), tensor([70]), tensor([47]), tensor([57]), tensor([36]), tensor([10]), tensor([56]), tensor([35]), tensor([51]), tensor([95]), tensor([1]), tensor([45]), tensor([31]), tensor([89]), tensor([12]), tensor([78]), tensor([65]), tensor([6]), tensor([13]), tensor([60]), tensor([59]), tensor([22]), tensor([96]), tensor([72]), tensor([38]), tensor([39]), tensor([17]), tensor([60]), tensor([77]), tensor([21]), tensor([46]), tensor([66]), tensor([35]), tensor([71]), tensor([75]), tensor([32]), tensor([49]), tensor([0]), tensor([7]), tensor([58]), tensor([24]), tensor([41]), tensor([2]), tensor([49]), tensor([11]), tensor([75]), tensor([51]), tensor([69]), tensor([23]), tensor([64]), tensor([20]), tensor([53]), tensor([6]), tensor([4]), tensor([59]), tensor([64]), tensor([34]), tensor([25]), tensor([58]), tensor([6]), tensor([80]), tensor([38]), tensor([27]), tensor([65]), tensor([62]), tensor([4]), tensor([88]), tensor([91]), tensor([0]), tensor([32]), tensor([14]), tensor([49]), tensor([64]), tensor([57]), tensor([66]), tensor([25]), tensor([20]), tensor([55]), tensor([99]), tensor([49]), tensor([93]), tensor([81]), tensor([34]), tensor([6]), tensor([49]), tensor([35]), tensor([35]), tensor([79]), tensor([69]), tensor([60]), tensor([28]), tensor([65]), tensor([33]), tensor([13]), tensor([23]), tensor([20]), tensor([38]), tensor([17]), tensor([77]), tensor([52]), tensor([28]), tensor([84]), tensor([83]), tensor([11]), tensor([29]), tensor([47]), tensor([27]), tensor([17]), tensor([78]), tensor([59]), tensor([75]), tensor([4]), tensor([97]), tensor([47]), tensor([3]), tensor([57]), tensor([3]), tensor([30]), tensor([78]), tensor([14]), tensor([27]), tensor([58]), tensor([82]), tensor([93]), tensor([13]), tensor([75]), tensor([70]), tensor([26]), tensor([52]), tensor([9]), tensor([49]), tensor([83]), tensor([73]), tensor([52]), tensor([9]), tensor([33]), tensor([70]), tensor([18]), tensor([16]), tensor([4]), tensor([0]), tensor([82]), tensor([33]), tensor([67]), tensor([69]), tensor([26]), tensor([75]), tensor([63]), tensor([72]), tensor([48]), tensor([84]), tensor([2]), tensor([67]), tensor([7]), tensor([66]), tensor([14]), tensor([86]), tensor([50]), tensor([1]), tensor([8]), tensor([37]), tensor([95]), tensor([93]), tensor([46]), tensor([79]), tensor([53]), tensor([46]), tensor([17]), tensor([60]), tensor([71]), tensor([46]), tensor([50]), tensor([57]), tensor([57]), tensor([88]), tensor([16]), tensor([88]), tensor([48]), tensor([86]), tensor([7]), tensor([5]), tensor([62]), tensor([5]), tensor([12]), tensor([35]), tensor([17]), tensor([85]), tensor([37]), tensor([5]), tensor([34]), tensor([53]), tensor([3]), tensor([9]), tensor([81]), tensor([21]), tensor([87]), tensor([77]), tensor([94]), tensor([89]), tensor([50]), tensor([2]), tensor([89]), tensor([80]), tensor([42]), tensor([28]), tensor([85]), tensor([85]), tensor([32]), tensor([80]), tensor([48]), tensor([32]), tensor([95]), tensor([16]), tensor([0]), tensor([1]), tensor([34]), tensor([72]), tensor([96]), tensor([14]), tensor([98]), tensor([35]), tensor([96]), tensor([64]), tensor([8]), tensor([81]), tensor([68]), tensor([46]), tensor([68]), tensor([31]), tensor([52]), tensor([68]), tensor([38]), tensor([55]), tensor([96]), tensor([30]), tensor([33]), tensor([94]), tensor([69]), tensor([14]), tensor([63]), tensor([95]), tensor([95]), tensor([2]), tensor([78]), tensor([81]), tensor([31]), tensor([69]), tensor([82]), tensor([68]), tensor([12]), tensor([12]), tensor([17]), tensor([26]), tensor([56]), tensor([16]), tensor([10]), tensor([58]), tensor([20]), tensor([60]), tensor([63]), tensor([94]), tensor([50]), tensor([16]), tensor([73]), tensor([79]), tensor([53]), tensor([28]), tensor([20]), tensor([21]), tensor([83]), tensor([20]), tensor([57]), tensor([95]), tensor([69]), tensor([99]), tensor([64]), tensor([92]), tensor([4]), tensor([29]), tensor([37]), tensor([14]), tensor([68]), tensor([39]), tensor([18]), tensor([64]), tensor([6])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.46 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.443\n",
            "TEST ALL:  0.4142222222222222\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  10000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 82, 2, 10, 18, 26, 34, 42, 50, 58, 66, 74, 90, 87, 98, 3, 11, 19, 27, 35, 43, 51, 59, 67, 97, 89, 81, 73, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 1, 9, 17, 25, 33, 41, 49, 57, 65, 75, 83, 91, 93, 14, 22, 30, 38, 46, 54, 62, 70, 78, 86, 94, 7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 6, 85, 99, 77, 4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92, 5, 13, 21, 29, 37, 45, 53, 61, 69, 0]\n",
            "TRAIN_SET CLASSES:  [43, 19, 15, 90, 74, 54, 61, 76, 44, 40]\n",
            "VALIDATION CLASSES:  [61, 54, 44, 43, 40, 90, 19, 15, 76, 74]\n",
            "GROUP:  10\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [43, 19, 15, 90, 74, 54, 61, 76, 44, 40]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "num classes till now:  100\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.20515647530555725\n",
            "Train step - Step 10, Loss 0.13973425328731537\n",
            "Train step - Step 20, Loss 0.13115456700325012\n",
            "Train step - Step 30, Loss 0.12848767638206482\n",
            "Train step - Step 40, Loss 0.12299854308366776\n",
            "Train step - Step 50, Loss 0.12956057488918304\n",
            "Train epoch - Accuracy: 0.13924963924963926 Loss: 0.13828148774340382 Corrects: 965\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12331976741552353\n",
            "Train step - Step 70, Loss 0.1187295913696289\n",
            "Train step - Step 80, Loss 0.11812479048967361\n",
            "Train step - Step 90, Loss 0.11942512542009354\n",
            "Train step - Step 100, Loss 0.119901143014431\n",
            "Train epoch - Accuracy: 0.15367965367965367 Loss: 0.12223051423462267 Corrects: 1065\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11695580929517746\n",
            "Train step - Step 120, Loss 0.12129579484462738\n",
            "Train step - Step 130, Loss 0.12145427614450455\n",
            "Train step - Step 140, Loss 0.11733583360910416\n",
            "Train step - Step 150, Loss 0.119474858045578\n",
            "Train step - Step 160, Loss 0.11809077858924866\n",
            "Train epoch - Accuracy: 0.1717171717171717 Loss: 0.12039049495264222 Corrects: 1190\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.1199551522731781\n",
            "Train step - Step 180, Loss 0.12079092115163803\n",
            "Train step - Step 190, Loss 0.1189141497015953\n",
            "Train step - Step 200, Loss 0.11680307239294052\n",
            "Train step - Step 210, Loss 0.11820166558027267\n",
            "Train epoch - Accuracy: 0.19336219336219337 Loss: 0.11929318951106863 Corrects: 1340\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.12057662755250931\n",
            "Train step - Step 230, Loss 0.12109102308750153\n",
            "Train step - Step 240, Loss 0.11602922528982162\n",
            "Train step - Step 250, Loss 0.11512701958417892\n",
            "Train step - Step 260, Loss 0.12291865795850754\n",
            "Train step - Step 270, Loss 0.11736612021923065\n",
            "Train epoch - Accuracy: 0.2222222222222222 Loss: 0.11877823774561738 Corrects: 1540\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1148950383067131\n",
            "Train step - Step 290, Loss 0.11641759425401688\n",
            "Train step - Step 300, Loss 0.12453174591064453\n",
            "Train step - Step 310, Loss 0.11912957578897476\n",
            "Train step - Step 320, Loss 0.12025173008441925\n",
            "Train epoch - Accuracy: 0.24733044733044732 Loss: 0.11817513107393383 Corrects: 1714\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.1206008791923523\n",
            "Train step - Step 340, Loss 0.11408208310604095\n",
            "Train step - Step 350, Loss 0.12369592487812042\n",
            "Train step - Step 360, Loss 0.12381793558597565\n",
            "Train step - Step 370, Loss 0.11948974430561066\n",
            "Train step - Step 380, Loss 0.11832679808139801\n",
            "Train epoch - Accuracy: 0.2623376623376623 Loss: 0.11798701340501959 Corrects: 1818\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.1181098073720932\n",
            "Train step - Step 400, Loss 0.12044373154640198\n",
            "Train step - Step 410, Loss 0.11805488169193268\n",
            "Train step - Step 420, Loss 0.11970300227403641\n",
            "Train step - Step 430, Loss 0.11126331984996796\n",
            "Train epoch - Accuracy: 0.2813852813852814 Loss: 0.11775882789840945 Corrects: 1950\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.11729207634925842\n",
            "Train step - Step 450, Loss 0.11614049971103668\n",
            "Train step - Step 460, Loss 0.1175064817070961\n",
            "Train step - Step 470, Loss 0.1179024875164032\n",
            "Train step - Step 480, Loss 0.11482397466897964\n",
            "Train step - Step 490, Loss 0.11403806507587433\n",
            "Train epoch - Accuracy: 0.28773448773448773 Loss: 0.11742301561357656 Corrects: 1994\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.11985864490270615\n",
            "Train step - Step 510, Loss 0.11544673889875412\n",
            "Train step - Step 520, Loss 0.1193414181470871\n",
            "Train step - Step 530, Loss 0.12025269865989685\n",
            "Train step - Step 540, Loss 0.11816059052944183\n",
            "Train epoch - Accuracy: 0.298989898989899 Loss: 0.11735829816752182 Corrects: 2072\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.11776319146156311\n",
            "Train step - Step 560, Loss 0.1173877865076065\n",
            "Train step - Step 570, Loss 0.11418984830379486\n",
            "Train step - Step 580, Loss 0.11465027183294296\n",
            "Train step - Step 590, Loss 0.11830779910087585\n",
            "Train step - Step 600, Loss 0.1227138489484787\n",
            "Train epoch - Accuracy: 0.30966810966810965 Loss: 0.11688897553159389 Corrects: 2146\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.11697197705507278\n",
            "Train step - Step 620, Loss 0.11220494657754898\n",
            "Train step - Step 630, Loss 0.11762423068284988\n",
            "Train step - Step 640, Loss 0.11773614585399628\n",
            "Train step - Step 650, Loss 0.12386830896139145\n",
            "Train epoch - Accuracy: 0.3202020202020202 Loss: 0.11716341900318043 Corrects: 2219\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.12031761556863785\n",
            "Train step - Step 670, Loss 0.11219771951436996\n",
            "Train step - Step 680, Loss 0.11989694088697433\n",
            "Train step - Step 690, Loss 0.11805829405784607\n",
            "Train step - Step 700, Loss 0.11529523879289627\n",
            "Train step - Step 710, Loss 0.11920066922903061\n",
            "Train epoch - Accuracy: 0.31861471861471863 Loss: 0.11665843845617892 Corrects: 2208\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.11757568269968033\n",
            "Train step - Step 730, Loss 0.11449664831161499\n",
            "Train step - Step 740, Loss 0.116350457072258\n",
            "Train step - Step 750, Loss 0.11911731213331223\n",
            "Train step - Step 760, Loss 0.12102480977773666\n",
            "Train epoch - Accuracy: 0.3443001443001443 Loss: 0.11689491145323537 Corrects: 2386\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.11427862197160721\n",
            "Train step - Step 780, Loss 0.11574437469244003\n",
            "Train step - Step 790, Loss 0.11634501814842224\n",
            "Train step - Step 800, Loss 0.11730712652206421\n",
            "Train step - Step 810, Loss 0.11726737767457962\n",
            "Train step - Step 820, Loss 0.11582225561141968\n",
            "Train epoch - Accuracy: 0.3500721500721501 Loss: 0.11662793467399189 Corrects: 2426\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.12019597738981247\n",
            "Train step - Step 840, Loss 0.11323960870504379\n",
            "Train step - Step 850, Loss 0.1200643926858902\n",
            "Train step - Step 860, Loss 0.11219167709350586\n",
            "Train step - Step 870, Loss 0.11622937768697739\n",
            "Train epoch - Accuracy: 0.3528138528138528 Loss: 0.11614584294705507 Corrects: 2445\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.11781468987464905\n",
            "Train step - Step 890, Loss 0.11744517087936401\n",
            "Train step - Step 900, Loss 0.11109040677547455\n",
            "Train step - Step 910, Loss 0.11027180403470993\n",
            "Train step - Step 920, Loss 0.11367997527122498\n",
            "Train step - Step 930, Loss 0.11437284201383591\n",
            "Train epoch - Accuracy: 0.35223665223665224 Loss: 0.11644562101596362 Corrects: 2441\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.11497344821691513\n",
            "Train step - Step 950, Loss 0.11598257720470428\n",
            "Train step - Step 960, Loss 0.11893834918737411\n",
            "Train step - Step 970, Loss 0.11737676709890366\n",
            "Train step - Step 980, Loss 0.11421018093824387\n",
            "Train epoch - Accuracy: 0.3595959595959596 Loss: 0.1162454421859111 Corrects: 2492\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.1179003193974495\n",
            "Train step - Step 1000, Loss 0.11486894637346268\n",
            "Train step - Step 1010, Loss 0.11565852910280228\n",
            "Train step - Step 1020, Loss 0.11134208738803864\n",
            "Train step - Step 1030, Loss 0.11387208849191666\n",
            "Train step - Step 1040, Loss 0.11274600028991699\n",
            "Train epoch - Accuracy: 0.366955266955267 Loss: 0.11633400416907466 Corrects: 2543\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.11929309368133545\n",
            "Train step - Step 1060, Loss 0.12088174372911453\n",
            "Train step - Step 1070, Loss 0.12054216116666794\n",
            "Train step - Step 1080, Loss 0.11283393949270248\n",
            "Train step - Step 1090, Loss 0.11828723549842834\n",
            "Train epoch - Accuracy: 0.37777777777777777 Loss: 0.11633153153747364 Corrects: 2618\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.11202960461378098\n",
            "Train step - Step 1110, Loss 0.11347892135381699\n",
            "Train step - Step 1120, Loss 0.11221760511398315\n",
            "Train step - Step 1130, Loss 0.11525694280862808\n",
            "Train step - Step 1140, Loss 0.12299496680498123\n",
            "Train step - Step 1150, Loss 0.11913187801837921\n",
            "Train epoch - Accuracy: 0.37777777777777777 Loss: 0.11612291252200221 Corrects: 2618\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.11754342168569565\n",
            "Train step - Step 1170, Loss 0.11346706002950668\n",
            "Train step - Step 1180, Loss 0.11285319924354553\n",
            "Train step - Step 1190, Loss 0.1192445382475853\n",
            "Train step - Step 1200, Loss 0.1146794930100441\n",
            "Train epoch - Accuracy: 0.3759018759018759 Loss: 0.11600268177612863 Corrects: 2605\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.12150537222623825\n",
            "Train step - Step 1220, Loss 0.1174275353550911\n",
            "Train step - Step 1230, Loss 0.1155790314078331\n",
            "Train step - Step 1240, Loss 0.11303365230560303\n",
            "Train step - Step 1250, Loss 0.11980033665895462\n",
            "Train step - Step 1260, Loss 0.12060736119747162\n",
            "Train epoch - Accuracy: 0.3904761904761905 Loss: 0.11595651987741176 Corrects: 2706\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.11882142722606659\n",
            "Train step - Step 1280, Loss 0.11224672198295593\n",
            "Train step - Step 1290, Loss 0.11533067375421524\n",
            "Train step - Step 1300, Loss 0.11538612097501755\n",
            "Train step - Step 1310, Loss 0.11366469413042068\n",
            "Train epoch - Accuracy: 0.3926406926406926 Loss: 0.11605893862883938 Corrects: 2721\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.11674235761165619\n",
            "Train step - Step 1330, Loss 0.11808092147111893\n",
            "Train step - Step 1340, Loss 0.11691871285438538\n",
            "Train step - Step 1350, Loss 0.11860507726669312\n",
            "Train step - Step 1360, Loss 0.11800242215394974\n",
            "Train step - Step 1370, Loss 0.11712589859962463\n",
            "Train epoch - Accuracy: 0.3927849927849928 Loss: 0.11609377687713629 Corrects: 2722\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.10996823757886887\n",
            "Train step - Step 1390, Loss 0.11541585624217987\n",
            "Train step - Step 1400, Loss 0.11627250164747238\n",
            "Train step - Step 1410, Loss 0.11855892091989517\n",
            "Train step - Step 1420, Loss 0.11455516517162323\n",
            "Train epoch - Accuracy: 0.39826839826839827 Loss: 0.1155013294554548 Corrects: 2760\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.12235084176063538\n",
            "Train step - Step 1440, Loss 0.11563364416360855\n",
            "Train step - Step 1450, Loss 0.11434682458639145\n",
            "Train step - Step 1460, Loss 0.1184031069278717\n",
            "Train step - Step 1470, Loss 0.11795402318239212\n",
            "Train step - Step 1480, Loss 0.11582902818918228\n",
            "Train epoch - Accuracy: 0.40663780663780663 Loss: 0.11597850874259874 Corrects: 2818\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.11553803831338882\n",
            "Train step - Step 1500, Loss 0.1162339448928833\n",
            "Train step - Step 1510, Loss 0.11170360445976257\n",
            "Train step - Step 1520, Loss 0.11330676823854446\n",
            "Train step - Step 1530, Loss 0.11194053292274475\n",
            "Train epoch - Accuracy: 0.4090909090909091 Loss: 0.11564902486051144 Corrects: 2835\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.11543828994035721\n",
            "Train step - Step 1550, Loss 0.11552716046571732\n",
            "Train step - Step 1560, Loss 0.11716022342443466\n",
            "Train step - Step 1570, Loss 0.11805038154125214\n",
            "Train step - Step 1580, Loss 0.12147002667188644\n",
            "Train step - Step 1590, Loss 0.11206980794668198\n",
            "Train epoch - Accuracy: 0.4111111111111111 Loss: 0.1156536946307013 Corrects: 2849\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.11186164617538452\n",
            "Train step - Step 1610, Loss 0.11573861539363861\n",
            "Train step - Step 1620, Loss 0.11312414705753326\n",
            "Train step - Step 1630, Loss 0.11707967519760132\n",
            "Train step - Step 1640, Loss 0.11159928888082504\n",
            "Train epoch - Accuracy: 0.4113997113997114 Loss: 0.11564302356513204 Corrects: 2851\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.11491338163614273\n",
            "Train step - Step 1660, Loss 0.11319974809885025\n",
            "Train step - Step 1670, Loss 0.114434152841568\n",
            "Train step - Step 1680, Loss 0.11836066842079163\n",
            "Train step - Step 1690, Loss 0.11783530563116074\n",
            "Train step - Step 1700, Loss 0.11912312358617783\n",
            "Train epoch - Accuracy: 0.4171717171717172 Loss: 0.11549160992256319 Corrects: 2891\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.11535317450761795\n",
            "Train step - Step 1720, Loss 0.11972539871931076\n",
            "Train step - Step 1730, Loss 0.1143476590514183\n",
            "Train step - Step 1740, Loss 0.11446858942508698\n",
            "Train step - Step 1750, Loss 0.11688948422670364\n",
            "Train epoch - Accuracy: 0.4261183261183261 Loss: 0.11576156807797296 Corrects: 2953\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.11579287052154541\n",
            "Train step - Step 1770, Loss 0.12329939752817154\n",
            "Train step - Step 1780, Loss 0.11917193979024887\n",
            "Train step - Step 1790, Loss 0.11375865340232849\n",
            "Train step - Step 1800, Loss 0.11735055595636368\n",
            "Train step - Step 1810, Loss 0.10998492687940598\n",
            "Train epoch - Accuracy: 0.4196248196248196 Loss: 0.11522437802382877 Corrects: 2908\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.11271171271800995\n",
            "Train step - Step 1830, Loss 0.11292250454425812\n",
            "Train step - Step 1840, Loss 0.11642467230558395\n",
            "Train step - Step 1850, Loss 0.11132734268903732\n",
            "Train step - Step 1860, Loss 0.1170365959405899\n",
            "Train epoch - Accuracy: 0.43477633477633476 Loss: 0.11552890028702405 Corrects: 3013\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.11969444900751114\n",
            "Train step - Step 1880, Loss 0.11261510848999023\n",
            "Train step - Step 1890, Loss 0.1173606738448143\n",
            "Train step - Step 1900, Loss 0.1118399053812027\n",
            "Train step - Step 1910, Loss 0.11666545271873474\n",
            "Train step - Step 1920, Loss 0.11461003869771957\n",
            "Train epoch - Accuracy: 0.42424242424242425 Loss: 0.11546701339072135 Corrects: 2940\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.11429120600223541\n",
            "Train step - Step 1940, Loss 0.11565076559782028\n",
            "Train step - Step 1950, Loss 0.11289569735527039\n",
            "Train step - Step 1960, Loss 0.11573562026023865\n",
            "Train step - Step 1970, Loss 0.1132044568657875\n",
            "Train epoch - Accuracy: 0.4341991341991342 Loss: 0.11531227236831343 Corrects: 3009\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.11373872309923172\n",
            "Train step - Step 1990, Loss 0.11797172576189041\n",
            "Train step - Step 2000, Loss 0.11990701407194138\n",
            "Train step - Step 2010, Loss 0.11003230512142181\n",
            "Train step - Step 2020, Loss 0.11566244810819626\n",
            "Train step - Step 2030, Loss 0.11355193704366684\n",
            "Train epoch - Accuracy: 0.43593073593073595 Loss: 0.1151630892618566 Corrects: 3021\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.11760410666465759\n",
            "Train step - Step 2050, Loss 0.11216526478528976\n",
            "Train step - Step 2060, Loss 0.11886508762836456\n",
            "Train step - Step 2070, Loss 0.11158519238233566\n",
            "Train step - Step 2080, Loss 0.11340958625078201\n",
            "Train epoch - Accuracy: 0.43564213564213566 Loss: 0.11527303072797272 Corrects: 3019\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.11323156207799911\n",
            "Train step - Step 2100, Loss 0.11451224982738495\n",
            "Train step - Step 2110, Loss 0.1205471009016037\n",
            "Train step - Step 2120, Loss 0.11720018088817596\n",
            "Train step - Step 2130, Loss 0.11684074252843857\n",
            "Train step - Step 2140, Loss 0.11833017319440842\n",
            "Train epoch - Accuracy: 0.4393939393939394 Loss: 0.1152923686808838 Corrects: 3045\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.11640428006649017\n",
            "Train step - Step 2160, Loss 0.11803880333900452\n",
            "Train step - Step 2170, Loss 0.11094491928815842\n",
            "Train step - Step 2180, Loss 0.11625707149505615\n",
            "Train step - Step 2190, Loss 0.11421290040016174\n",
            "Train epoch - Accuracy: 0.4458874458874459 Loss: 0.11494445176136614 Corrects: 3090\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.11336293816566467\n",
            "Train step - Step 2210, Loss 0.11529647558927536\n",
            "Train step - Step 2220, Loss 0.1143600270152092\n",
            "Train step - Step 2230, Loss 0.11472313851118088\n",
            "Train step - Step 2240, Loss 0.11605554074048996\n",
            "Train step - Step 2250, Loss 0.11697307229042053\n",
            "Train epoch - Accuracy: 0.44545454545454544 Loss: 0.11497996310565035 Corrects: 3087\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.11667134612798691\n",
            "Train step - Step 2270, Loss 0.11579078435897827\n",
            "Train step - Step 2280, Loss 0.117215096950531\n",
            "Train step - Step 2290, Loss 0.11175279319286346\n",
            "Train step - Step 2300, Loss 0.1129140630364418\n",
            "Train epoch - Accuracy: 0.44516594516594515 Loss: 0.11504706190348016 Corrects: 3085\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.1144077479839325\n",
            "Train step - Step 2320, Loss 0.11332406103610992\n",
            "Train step - Step 2330, Loss 0.1146463006734848\n",
            "Train step - Step 2340, Loss 0.11135588586330414\n",
            "Train step - Step 2350, Loss 0.11328194290399551\n",
            "Train step - Step 2360, Loss 0.11536908894777298\n",
            "Train epoch - Accuracy: 0.44617604617604617 Loss: 0.11525930981185357 Corrects: 3092\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.11484795063734055\n",
            "Train step - Step 2380, Loss 0.1133682057261467\n",
            "Train step - Step 2390, Loss 0.11672420054674149\n",
            "Train step - Step 2400, Loss 0.10827378183603287\n",
            "Train step - Step 2410, Loss 0.11854410916566849\n",
            "Train epoch - Accuracy: 0.4505050505050505 Loss: 0.11482931727712804 Corrects: 3122\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.11735206097364426\n",
            "Train step - Step 2430, Loss 0.1161511018872261\n",
            "Train step - Step 2440, Loss 0.11029649525880814\n",
            "Train step - Step 2450, Loss 0.11898083239793777\n",
            "Train step - Step 2460, Loss 0.11564517766237259\n",
            "Train step - Step 2470, Loss 0.11426476389169693\n",
            "Train epoch - Accuracy: 0.4479076479076479 Loss: 0.11507369759794954 Corrects: 3104\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.11764376610517502\n",
            "Train step - Step 2490, Loss 0.11776076257228851\n",
            "Train step - Step 2500, Loss 0.11441193521022797\n",
            "Train step - Step 2510, Loss 0.11677360534667969\n",
            "Train step - Step 2520, Loss 0.11470348387956619\n",
            "Train epoch - Accuracy: 0.4572871572871573 Loss: 0.11467200711175993 Corrects: 3169\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.1110173687338829\n",
            "Train step - Step 2540, Loss 0.11488892883062363\n",
            "Train step - Step 2550, Loss 0.11423870176076889\n",
            "Train step - Step 2560, Loss 0.10907470434904099\n",
            "Train step - Step 2570, Loss 0.11821319907903671\n",
            "Train step - Step 2580, Loss 0.1168350875377655\n",
            "Train epoch - Accuracy: 0.46002886002886 Loss: 0.11463489120699799 Corrects: 3188\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.1141110360622406\n",
            "Train step - Step 2600, Loss 0.11389579623937607\n",
            "Train step - Step 2610, Loss 0.11831745505332947\n",
            "Train step - Step 2620, Loss 0.1111767366528511\n",
            "Train step - Step 2630, Loss 0.11287263035774231\n",
            "Train epoch - Accuracy: 0.4595959595959596 Loss: 0.1148412225566385 Corrects: 3185\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.11441829055547714\n",
            "Train step - Step 2650, Loss 0.11873365938663483\n",
            "Train step - Step 2660, Loss 0.11675702780485153\n",
            "Train step - Step 2670, Loss 0.11346647143363953\n",
            "Train step - Step 2680, Loss 0.1160617396235466\n",
            "Train step - Step 2690, Loss 0.11465464532375336\n",
            "Train epoch - Accuracy: 0.4691197691197691 Loss: 0.1150537829077433 Corrects: 3251\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.1135544627904892\n",
            "Train step - Step 2710, Loss 0.11586343497037888\n",
            "Train step - Step 2720, Loss 0.11646459251642227\n",
            "Train step - Step 2730, Loss 0.1140405610203743\n",
            "Train step - Step 2740, Loss 0.1163986548781395\n",
            "Train epoch - Accuracy: 0.47243867243867244 Loss: 0.11417686301908452 Corrects: 3274\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.11391141265630722\n",
            "Train step - Step 2760, Loss 0.11564429849386215\n",
            "Train step - Step 2770, Loss 0.11147750914096832\n",
            "Train step - Step 2780, Loss 0.1120099201798439\n",
            "Train step - Step 2790, Loss 0.11414685100317001\n",
            "Train step - Step 2800, Loss 0.11197071522474289\n",
            "Train epoch - Accuracy: 0.4733044733044733 Loss: 0.11414041329943944 Corrects: 3280\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.11483026295900345\n",
            "Train step - Step 2820, Loss 0.11439813673496246\n",
            "Train step - Step 2830, Loss 0.11499595642089844\n",
            "Train step - Step 2840, Loss 0.11171675473451614\n",
            "Train step - Step 2850, Loss 0.11439710110425949\n",
            "Train epoch - Accuracy: 0.46998556998557 Loss: 0.11458170489229337 Corrects: 3257\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.11648529022932053\n",
            "Train step - Step 2870, Loss 0.11459732055664062\n",
            "Train step - Step 2880, Loss 0.11323688179254532\n",
            "Train step - Step 2890, Loss 0.10811412334442139\n",
            "Train step - Step 2900, Loss 0.11242149025201797\n",
            "Train step - Step 2910, Loss 0.11857146769762039\n",
            "Train epoch - Accuracy: 0.46854256854256854 Loss: 0.11440623860381555 Corrects: 3247\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.11057431995868683\n",
            "Train step - Step 2930, Loss 0.11020534485578537\n",
            "Train step - Step 2940, Loss 0.1164248138666153\n",
            "Train step - Step 2950, Loss 0.11141300946474075\n",
            "Train step - Step 2960, Loss 0.11681662499904633\n",
            "Train epoch - Accuracy: 0.4727272727272727 Loss: 0.11443514775748205 Corrects: 3276\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.11509692668914795\n",
            "Train step - Step 2980, Loss 0.11343797296285629\n",
            "Train step - Step 2990, Loss 0.11060883104801178\n",
            "Train step - Step 3000, Loss 0.11516328901052475\n",
            "Train step - Step 3010, Loss 0.11420615762472153\n",
            "Train step - Step 3020, Loss 0.10841840505599976\n",
            "Train epoch - Accuracy: 0.4702741702741703 Loss: 0.11436558324107188 Corrects: 3259\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.10927579551935196\n",
            "Train step - Step 3040, Loss 0.11513537913560867\n",
            "Train step - Step 3050, Loss 0.11210725456476212\n",
            "Train step - Step 3060, Loss 0.11397016048431396\n",
            "Train step - Step 3070, Loss 0.11575730890035629\n",
            "Train epoch - Accuracy: 0.4715728715728716 Loss: 0.11427905876485128 Corrects: 3268\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.11380397528409958\n",
            "Train step - Step 3090, Loss 0.1156754121184349\n",
            "Train step - Step 3100, Loss 0.1129990816116333\n",
            "Train step - Step 3110, Loss 0.11894714087247849\n",
            "Train step - Step 3120, Loss 0.11315218359231949\n",
            "Train step - Step 3130, Loss 0.1179957166314125\n",
            "Train epoch - Accuracy: 0.4733044733044733 Loss: 0.11437968716218874 Corrects: 3280\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.11645985394716263\n",
            "Train step - Step 3150, Loss 0.11752095818519592\n",
            "Train step - Step 3160, Loss 0.11421556025743484\n",
            "Train step - Step 3170, Loss 0.11103449761867523\n",
            "Train step - Step 3180, Loss 0.11596159636974335\n",
            "Train epoch - Accuracy: 0.4701298701298701 Loss: 0.11437089871319514 Corrects: 3258\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.11118876934051514\n",
            "Train step - Step 3200, Loss 0.11453399062156677\n",
            "Train step - Step 3210, Loss 0.11405282467603683\n",
            "Train step - Step 3220, Loss 0.10750210285186768\n",
            "Train step - Step 3230, Loss 0.11600560694932938\n",
            "Train step - Step 3240, Loss 0.11145826429128647\n",
            "Train epoch - Accuracy: 0.47056277056277057 Loss: 0.11431617684995629 Corrects: 3261\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.11883079260587692\n",
            "Train step - Step 3260, Loss 0.10886847972869873\n",
            "Train step - Step 3270, Loss 0.11266481876373291\n",
            "Train step - Step 3280, Loss 0.11313356459140778\n",
            "Train step - Step 3290, Loss 0.10866374522447586\n",
            "Train epoch - Accuracy: 0.4683982683982684 Loss: 0.11452383145066394 Corrects: 3246\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.11128439009189606\n",
            "Train step - Step 3310, Loss 0.11235146969556808\n",
            "Train step - Step 3320, Loss 0.11262435466051102\n",
            "Train step - Step 3330, Loss 0.11433707922697067\n",
            "Train step - Step 3340, Loss 0.11043090373277664\n",
            "Train step - Step 3350, Loss 0.11000490188598633\n",
            "Train epoch - Accuracy: 0.4717171717171717 Loss: 0.11383334122393898 Corrects: 3269\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.11199995875358582\n",
            "Train step - Step 3370, Loss 0.11839363723993301\n",
            "Train step - Step 3380, Loss 0.1122889295220375\n",
            "Train step - Step 3390, Loss 0.11215192824602127\n",
            "Train step - Step 3400, Loss 0.11764860153198242\n",
            "Train epoch - Accuracy: 0.48095238095238096 Loss: 0.11452389108971256 Corrects: 3333\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.11151646077632904\n",
            "Train step - Step 3420, Loss 0.11012792587280273\n",
            "Train step - Step 3430, Loss 0.115610271692276\n",
            "Train step - Step 3440, Loss 0.11983852833509445\n",
            "Train step - Step 3450, Loss 0.11072488874197006\n",
            "Train step - Step 3460, Loss 0.10993586480617523\n",
            "Train epoch - Accuracy: 0.47965367965367967 Loss: 0.11419396837810417 Corrects: 3324\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.11552832275629044\n",
            "Train step - Step 3480, Loss 0.11544854193925858\n",
            "Train step - Step 3490, Loss 0.11276134103536606\n",
            "Train step - Step 3500, Loss 0.11668416857719421\n",
            "Train step - Step 3510, Loss 0.11272110790014267\n",
            "Train epoch - Accuracy: 0.481962481962482 Loss: 0.11417821748303128 Corrects: 3340\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.11430332809686661\n",
            "Train step - Step 3530, Loss 0.11454149335622787\n",
            "Train step - Step 3540, Loss 0.1065295934677124\n",
            "Train step - Step 3550, Loss 0.11553466320037842\n",
            "Train step - Step 3560, Loss 0.11106251180171967\n",
            "Train step - Step 3570, Loss 0.11465189605951309\n",
            "Train epoch - Accuracy: 0.4790764790764791 Loss: 0.11398669959920825 Corrects: 3320\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.112025685608387\n",
            "Train step - Step 3590, Loss 0.11458180844783783\n",
            "Train step - Step 3600, Loss 0.11411602795124054\n",
            "Train step - Step 3610, Loss 0.1058979406952858\n",
            "Train step - Step 3620, Loss 0.11349192261695862\n",
            "Train epoch - Accuracy: 0.4767676767676768 Loss: 0.11419581016592821 Corrects: 3304\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.11490560322999954\n",
            "Train step - Step 3640, Loss 0.11841193586587906\n",
            "Train step - Step 3650, Loss 0.11169448494911194\n",
            "Train step - Step 3660, Loss 0.11777128279209137\n",
            "Train step - Step 3670, Loss 0.11488260328769684\n",
            "Train step - Step 3680, Loss 0.11902904510498047\n",
            "Train epoch - Accuracy: 0.4767676767676768 Loss: 0.11413901000139862 Corrects: 3304\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.11766919493675232\n",
            "Train step - Step 3700, Loss 0.11005038768053055\n",
            "Train step - Step 3710, Loss 0.11462552845478058\n",
            "Train step - Step 3720, Loss 0.11450164765119553\n",
            "Train step - Step 3730, Loss 0.11651621758937836\n",
            "Train epoch - Accuracy: 0.47748917748917746 Loss: 0.11430842065793956 Corrects: 3309\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.11407773941755295\n",
            "Train step - Step 3750, Loss 0.11708127707242966\n",
            "Train step - Step 3760, Loss 0.11147367209196091\n",
            "Train step - Step 3770, Loss 0.11585172265768051\n",
            "Train step - Step 3780, Loss 0.11272601038217545\n",
            "Train step - Step 3790, Loss 0.11291061341762543\n",
            "Train epoch - Accuracy: 0.4704184704184704 Loss: 0.1142318940523899 Corrects: 3260\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.108919158577919\n",
            "Train step - Step 3810, Loss 0.10937093198299408\n",
            "Train step - Step 3820, Loss 0.11579043418169022\n",
            "Train step - Step 3830, Loss 0.11309321969747543\n",
            "Train step - Step 3840, Loss 0.1103334054350853\n",
            "Train epoch - Accuracy: 0.4782106782106782 Loss: 0.11416355182254125 Corrects: 3314\n",
            "Training finished in 404.2914321422577 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0, 79, 71, 63, 26, 14, 2, 73, 41, 56, 48, 99, 23, 66, 22, 6, 93, 17, 5, 1, 28, 87, 75, 55, 47, 39, 70, 42, 38, 29, 4, 83, 51, 18, 89, 85, 9, 84, 64, 20, 8, 43, 19, 15, 90, 74, 54, 61, 76, 44, 40]\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232ed8050>\n",
            "Constructing exemplars of class 43\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [31008, 27402, 45916, 13088, 26807, 25860, 30952, 14912, 31582, 42868, 14937, 37241, 3030, 28984, 49892, 28617, 31262, 33510, 39562, 2388]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2241476e50>\n",
            "Constructing exemplars of class 19\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [21314, 18443, 48684, 47129, 34860, 18019, 41450, 28223, 9482, 16162, 19008, 20489, 38116, 18134, 28927, 10093, 26037, 30909, 7698, 44475]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22414762d0>\n",
            "Constructing exemplars of class 15\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [4488, 31377, 3244, 19608, 45908, 28315, 13598, 25799, 11715, 7123, 17164, 37178, 18148, 35927, 10697, 21220, 33158, 2724, 15355, 19604]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a87e50>\n",
            "Constructing exemplars of class 90\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [39115, 1607, 39041, 44403, 32236, 29806, 15048, 39540, 26155, 1034, 41330, 32407, 18880, 31363, 26536, 10871, 37973, 7392, 3843, 1095]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a877d0>\n",
            "Constructing exemplars of class 74\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [21759, 49523, 28050, 44615, 48493, 11865, 13556, 1075, 38112, 20463, 16149, 45905, 29340, 24924, 9763, 47070, 1583, 33044, 22501, 6885]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238035ad0>\n",
            "Constructing exemplars of class 54\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [12166, 15609, 20153, 31725, 5071, 49847, 49741, 47082, 6048, 49087, 29521, 40111, 22148, 14325, 47633, 13587, 2642, 47082, 24730, 31630]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232f25850>\n",
            "Constructing exemplars of class 61\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [44699, 31406, 41996, 46714, 11401, 16119, 49846, 21261, 38599, 37139, 33306, 15565, 24156, 8865, 39247, 48165, 7575, 20306, 24977, 11946]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223822c790>\n",
            "Constructing exemplars of class 76\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [5101, 28994, 4629, 11120, 44302, 2885, 18783, 34295, 6467, 10228, 16345, 13182, 20353, 35830, 32760, 48950, 6811, 18783, 30768, 21990]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a7da90>\n",
            "Constructing exemplars of class 44\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [43555, 17570, 32508, 980, 2576, 1393, 10431, 49674, 39668, 48771, 2506, 37316, 19866, 15720, 12980, 40010, 36742, 10606, 18521, 661]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22318d2850>\n",
            "Constructing exemplars of class 40\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [4717, 5156, 17538, 6843, 15961, 48663, 933, 19450, 46868, 32561, 14963, 24708, 4256, 6857, 37567, 16977, 24803, 1282, 44399, 24645]\n",
            "x train:  [-0.08496322 -0.07788854 -0.046113   -0.07923395 -0.04362911 -0.13332394\n",
            " -0.08848953 -0.09046408 -0.04019038  0.01666227 -0.12081163 -0.11035814\n",
            " -0.04287349 -0.07399985 -0.095273   -0.14041409 -0.10174412 -0.08174222\n",
            " -0.11468966 -0.12497511 -0.00806111 -0.11015553 -0.0660091  -0.0511431\n",
            " -0.06363802 -0.13148184 -0.10468461 -0.10428838 -0.04916965 -0.06203615\n",
            " -0.11129855 -0.01797907 -0.01567997 -0.09317721 -0.03820258 -0.11543341\n",
            " -0.12162873 -0.11939751 -0.12605608 -0.12055225 -0.09531885 -0.09854062\n",
            " -0.09122235 -0.10358276 -0.02497499 -0.10807543 -0.13844107 -0.06017898\n",
            " -0.09491657 -0.04132134 -0.11194368 -0.10152763 -0.09306684 -0.06513982\n",
            " -0.1146904  -0.0097174  -0.12202356 -0.12605217 -0.10635399 -0.18038246\n",
            " -0.13104962 -0.12183479 -0.15104206 -0.07464983 -0.0844496  -0.07215464\n",
            " -0.17880975 -0.02217012 -0.10435662 -0.04914587 -0.08183428 -0.15717053\n",
            " -0.10722704 -0.08447871 -0.09671955 -0.07894989 -0.11027104 -0.1013234\n",
            " -0.07671177 -0.07828113 -0.05364253 -0.0942109  -0.12825455 -0.12780488\n",
            " -0.10752824 -0.09702511 -0.03437965 -0.10617768 -0.04584351 -0.17069468\n",
            " -0.09792249 -0.11123469 -0.07254475 -0.13060096 -0.11944653 -0.09955826\n",
            " -0.10184825 -0.21622214 -0.07354517 -0.0385037 ]\n",
            "y_train:  [tensor([46]), tensor([88]), tensor([9]), tensor([13]), tensor([6]), tensor([1]), tensor([1]), tensor([43]), tensor([36]), tensor([67]), tensor([87]), tensor([38]), tensor([4]), tensor([71]), tensor([98]), tensor([14]), tensor([60]), tensor([20]), tensor([41]), tensor([29]), tensor([83]), tensor([90]), tensor([57]), tensor([31]), tensor([22]), tensor([78]), tensor([17]), tensor([92]), tensor([47]), tensor([25]), tensor([13]), tensor([77]), tensor([15]), tensor([57]), tensor([40]), tensor([70]), tensor([61]), tensor([33]), tensor([16]), tensor([72]), tensor([61]), tensor([38]), tensor([54]), tensor([17]), tensor([52]), tensor([28]), tensor([44]), tensor([75]), tensor([92]), tensor([41]), tensor([67]), tensor([91]), tensor([75]), tensor([36]), tensor([78]), tensor([80]), tensor([69]), tensor([58]), tensor([53]), tensor([62]), tensor([63]), tensor([5]), tensor([37]), tensor([94]), tensor([92]), tensor([33]), tensor([29]), tensor([74]), tensor([98]), tensor([80]), tensor([13]), tensor([67]), tensor([91]), tensor([23]), tensor([68]), tensor([33]), tensor([20]), tensor([43]), tensor([78]), tensor([70]), tensor([81]), tensor([38]), tensor([48]), tensor([71]), tensor([52]), tensor([1]), tensor([69]), tensor([23]), tensor([15]), tensor([50]), tensor([34]), tensor([83]), tensor([78]), tensor([66]), tensor([23]), tensor([30]), tensor([88]), tensor([15]), tensor([95]), tensor([76]), tensor([76]), tensor([7]), tensor([12]), tensor([88]), tensor([78]), tensor([77]), tensor([82]), tensor([86]), tensor([75]), tensor([50]), tensor([70]), tensor([66]), tensor([78]), tensor([25]), tensor([52]), tensor([56]), tensor([68]), tensor([85]), tensor([93]), tensor([3]), tensor([17]), tensor([83]), tensor([62]), tensor([17]), tensor([37]), tensor([66]), tensor([62]), tensor([10]), tensor([39]), tensor([61]), tensor([65]), tensor([73]), tensor([5]), tensor([75]), tensor([39]), tensor([43]), tensor([33]), tensor([28]), tensor([81]), tensor([54]), tensor([88]), tensor([12]), tensor([91]), tensor([11]), tensor([56]), tensor([38]), tensor([48]), tensor([34]), tensor([3]), tensor([85]), tensor([22]), tensor([73]), tensor([21]), tensor([90]), tensor([72]), tensor([22]), tensor([95]), tensor([38]), tensor([51]), tensor([87]), tensor([3]), tensor([51]), tensor([25]), tensor([52]), tensor([25]), tensor([42]), tensor([22]), tensor([74]), tensor([9]), tensor([73]), tensor([69]), tensor([23]), tensor([46]), tensor([68]), tensor([21]), tensor([85]), tensor([23]), tensor([37]), tensor([27]), tensor([77]), tensor([88]), tensor([23]), tensor([92]), tensor([67]), tensor([81]), tensor([26]), tensor([47]), tensor([61]), tensor([42]), tensor([41]), tensor([62]), tensor([3]), tensor([58]), tensor([80]), tensor([37]), tensor([9]), tensor([4]), tensor([62]), tensor([17]), tensor([77]), tensor([42]), tensor([92]), tensor([7]), tensor([21]), tensor([3]), tensor([55]), tensor([43]), tensor([5]), tensor([71]), tensor([26]), tensor([19]), tensor([80]), tensor([82]), tensor([84]), tensor([87]), tensor([97]), tensor([81]), tensor([92]), tensor([83]), tensor([82]), tensor([62]), tensor([50]), tensor([56]), tensor([55]), tensor([55]), tensor([87]), tensor([24]), tensor([87]), tensor([96]), tensor([67]), tensor([8]), tensor([63]), tensor([40]), tensor([49]), tensor([73]), tensor([19]), tensor([10]), tensor([42]), tensor([63]), tensor([31]), tensor([27]), tensor([31]), tensor([28]), tensor([32]), tensor([68]), tensor([51]), tensor([9]), tensor([59]), tensor([67]), tensor([18]), tensor([48]), tensor([23]), tensor([47]), tensor([21]), tensor([64]), tensor([82]), tensor([90]), tensor([45]), tensor([77]), tensor([89]), tensor([73]), tensor([96]), tensor([37]), tensor([31]), tensor([44]), tensor([84]), tensor([45]), tensor([89]), tensor([93]), tensor([65]), tensor([64]), tensor([35]), tensor([71]), tensor([56]), tensor([93]), tensor([67]), tensor([38]), tensor([22]), tensor([64]), tensor([47]), tensor([78]), tensor([84]), tensor([27]), tensor([15]), tensor([66]), tensor([13]), tensor([29]), tensor([34]), tensor([19]), tensor([29]), tensor([84]), tensor([95]), tensor([4]), tensor([71]), tensor([59]), tensor([12]), tensor([15]), tensor([53]), tensor([63]), tensor([45]), tensor([61]), tensor([7]), tensor([16]), tensor([12]), tensor([45]), tensor([0]), tensor([36]), tensor([29]), tensor([88]), tensor([29]), tensor([89]), tensor([54]), tensor([37]), tensor([68]), tensor([92]), tensor([20]), tensor([52]), tensor([11]), tensor([29]), tensor([56]), tensor([23]), tensor([84]), tensor([84]), tensor([1]), tensor([83]), tensor([35]), tensor([66]), tensor([59]), tensor([37]), tensor([91]), tensor([43]), tensor([58]), tensor([10]), tensor([38]), tensor([2]), tensor([21]), tensor([23]), tensor([27]), tensor([66]), tensor([22]), tensor([80]), tensor([0]), tensor([71]), tensor([65]), tensor([44]), tensor([1]), tensor([39]), tensor([14]), tensor([56]), tensor([18]), tensor([70]), tensor([94]), tensor([54]), tensor([60]), tensor([62]), tensor([39]), tensor([23]), tensor([33]), tensor([14]), tensor([4]), tensor([78]), tensor([8]), tensor([63]), tensor([80]), tensor([77]), tensor([34]), tensor([93]), tensor([10]), tensor([14]), tensor([41]), tensor([23]), tensor([1]), tensor([90]), tensor([23]), tensor([34]), tensor([79]), tensor([54]), tensor([9]), tensor([36]), tensor([94]), tensor([80]), tensor([64]), tensor([65]), tensor([90]), tensor([29]), tensor([48]), tensor([16]), tensor([43]), tensor([50]), tensor([95]), tensor([5]), tensor([88]), tensor([90]), tensor([68]), tensor([7]), tensor([39]), tensor([13]), tensor([21]), tensor([92]), tensor([9]), tensor([81]), tensor([31]), tensor([11]), tensor([50]), tensor([30]), tensor([6]), tensor([26]), tensor([80]), tensor([84]), tensor([67]), tensor([28]), tensor([84]), tensor([68]), tensor([48]), tensor([60]), tensor([81]), tensor([7]), tensor([95]), tensor([49]), tensor([86]), tensor([74]), tensor([15]), tensor([89]), tensor([46]), tensor([33]), tensor([37]), tensor([98]), tensor([21]), tensor([98]), tensor([7]), tensor([83]), tensor([98]), tensor([57]), tensor([89]), tensor([21]), tensor([44]), tensor([77]), tensor([46]), tensor([65]), tensor([33]), tensor([54]), tensor([80]), tensor([45]), tensor([86]), tensor([9]), tensor([32]), tensor([14]), tensor([59]), tensor([8]), tensor([70]), tensor([41]), tensor([8]), tensor([52]), tensor([79]), tensor([43]), tensor([6]), tensor([89]), tensor([51]), tensor([67]), tensor([62]), tensor([21]), tensor([59]), tensor([61]), tensor([10]), tensor([61]), tensor([75]), tensor([44]), tensor([62]), tensor([19]), tensor([83]), tensor([8]), tensor([64]), tensor([53]), tensor([19]), tensor([34]), tensor([61]), tensor([2]), tensor([65]), tensor([54]), tensor([89]), tensor([86]), tensor([88]), tensor([96]), tensor([16]), tensor([91]), tensor([0]), tensor([11]), tensor([0]), tensor([0]), tensor([97]), tensor([77]), tensor([81]), tensor([60]), tensor([9]), tensor([84]), tensor([61]), tensor([97]), tensor([45]), tensor([63]), tensor([14]), tensor([94]), tensor([30]), tensor([89]), tensor([5]), tensor([71]), tensor([47]), tensor([70]), tensor([45]), tensor([38]), tensor([35]), tensor([75]), tensor([26]), tensor([54]), tensor([11]), tensor([20]), tensor([38]), tensor([8]), tensor([72]), tensor([34]), tensor([44]), tensor([27]), tensor([39]), tensor([95]), tensor([25]), tensor([93]), tensor([40]), tensor([52]), tensor([88]), tensor([7]), tensor([9]), tensor([85]), tensor([90]), tensor([53]), tensor([16]), tensor([12]), tensor([99]), tensor([98]), tensor([82]), tensor([22]), tensor([35]), tensor([3]), tensor([68]), tensor([66]), tensor([65]), tensor([34]), tensor([55]), tensor([1]), tensor([72]), tensor([73]), tensor([49]), tensor([69]), tensor([69]), tensor([54]), tensor([1]), tensor([78]), tensor([52]), tensor([46]), tensor([33]), tensor([25]), tensor([43]), tensor([63]), tensor([55]), tensor([69]), tensor([22]), tensor([49]), tensor([58]), tensor([42]), tensor([48]), tensor([94]), tensor([11]), tensor([36]), tensor([93]), tensor([74]), tensor([26]), tensor([4]), tensor([17]), tensor([7]), tensor([67]), tensor([82]), tensor([44]), tensor([42]), tensor([9]), tensor([44]), tensor([60]), tensor([97]), tensor([31]), tensor([2]), tensor([42]), tensor([82]), tensor([78]), tensor([19]), tensor([2]), tensor([31]), tensor([10]), tensor([15]), tensor([13]), tensor([61]), tensor([14]), tensor([3]), tensor([52]), tensor([7]), tensor([30]), tensor([41]), tensor([59]), tensor([12]), tensor([88]), tensor([99]), tensor([43]), tensor([27]), tensor([46]), tensor([69]), tensor([94]), tensor([17]), tensor([68]), tensor([1]), tensor([59]), tensor([2]), tensor([71]), tensor([56]), tensor([13]), tensor([56]), tensor([37]), tensor([82]), tensor([22]), tensor([64]), tensor([68]), tensor([5]), tensor([97]), tensor([52]), tensor([47]), tensor([21]), tensor([12]), tensor([98]), tensor([33]), tensor([62]), tensor([87]), tensor([4]), tensor([43]), tensor([15]), tensor([95]), tensor([86]), tensor([11]), tensor([87]), tensor([51]), tensor([85]), tensor([85]), tensor([63]), tensor([28]), tensor([59]), tensor([36]), tensor([3]), tensor([70]), tensor([31]), tensor([57]), tensor([65]), tensor([4]), tensor([87]), tensor([22]), tensor([40]), tensor([31]), tensor([18]), tensor([5]), tensor([98]), tensor([54]), tensor([83]), tensor([32]), tensor([77]), tensor([36]), tensor([12]), tensor([28]), tensor([86]), tensor([78]), tensor([26]), tensor([75]), tensor([6]), tensor([99]), tensor([27]), tensor([63]), tensor([18]), tensor([24]), tensor([65]), tensor([76]), tensor([72]), tensor([22]), tensor([64]), tensor([34]), tensor([50]), tensor([12]), tensor([88]), tensor([53]), tensor([14]), tensor([23]), tensor([56]), tensor([92]), tensor([98]), tensor([17]), tensor([12]), tensor([20]), tensor([96]), tensor([75]), tensor([4]), tensor([90]), tensor([40]), tensor([30]), tensor([34]), tensor([41]), tensor([53]), tensor([79]), tensor([45]), tensor([18]), tensor([95]), tensor([49]), tensor([80]), tensor([28]), tensor([37]), tensor([42]), tensor([2]), tensor([37]), tensor([56]), tensor([17]), tensor([17]), tensor([84]), tensor([1]), tensor([81]), tensor([24]), tensor([57]), tensor([32]), tensor([88]), tensor([39]), tensor([72]), tensor([86]), tensor([40]), tensor([41]), tensor([34]), tensor([71]), tensor([65]), tensor([36]), tensor([85]), tensor([25]), tensor([62]), tensor([80]), tensor([2]), tensor([65]), tensor([93]), tensor([74]), tensor([20]), tensor([88]), tensor([5]), tensor([70]), tensor([35]), tensor([4]), tensor([76]), tensor([69]), tensor([40]), tensor([68]), tensor([93]), tensor([43]), tensor([61]), tensor([13]), tensor([92]), tensor([52]), tensor([96]), tensor([9]), tensor([57]), tensor([39]), tensor([31]), tensor([49]), tensor([27]), tensor([66]), tensor([59]), tensor([30]), tensor([80]), tensor([72]), tensor([87]), tensor([38]), tensor([36]), tensor([40]), tensor([49]), tensor([22]), tensor([46]), tensor([68]), tensor([14]), tensor([96]), tensor([54]), tensor([2]), tensor([70]), tensor([95]), tensor([44]), tensor([21]), tensor([75]), tensor([15]), tensor([88]), tensor([53]), tensor([4]), tensor([48]), tensor([18]), tensor([95]), tensor([15]), tensor([79]), tensor([55]), tensor([31]), tensor([1]), tensor([76]), tensor([90]), tensor([71]), tensor([91]), tensor([27]), tensor([2]), tensor([0]), tensor([33]), tensor([60]), tensor([47]), tensor([22]), tensor([49]), tensor([58]), tensor([99]), tensor([3]), tensor([72]), tensor([72]), tensor([95]), tensor([6]), tensor([93]), tensor([47]), tensor([13]), tensor([23]), tensor([73]), tensor([50]), tensor([57]), tensor([25]), tensor([10]), tensor([83]), tensor([94]), tensor([92]), tensor([70]), tensor([74]), tensor([4]), tensor([70]), tensor([26]), tensor([93]), tensor([29]), tensor([41]), tensor([73]), tensor([72]), tensor([2]), tensor([36]), tensor([0]), tensor([77]), tensor([41]), tensor([60]), tensor([51]), tensor([32]), tensor([21]), tensor([44]), tensor([99]), tensor([64]), tensor([73]), tensor([91]), tensor([67]), tensor([14]), tensor([89]), tensor([51]), tensor([32]), tensor([25]), tensor([66]), tensor([62]), tensor([44]), tensor([16]), tensor([55]), tensor([30]), tensor([48]), tensor([96]), tensor([24]), tensor([86]), tensor([84]), tensor([3]), tensor([26]), tensor([89]), tensor([2]), tensor([58]), tensor([78]), tensor([63]), tensor([0]), tensor([95]), tensor([52]), tensor([83]), tensor([99]), tensor([82]), tensor([51]), tensor([0]), tensor([69]), tensor([86]), tensor([72]), tensor([12]), tensor([6]), tensor([72]), tensor([28]), tensor([61]), tensor([98]), tensor([10]), tensor([52]), tensor([82]), tensor([93]), tensor([30]), tensor([84]), tensor([58]), tensor([66]), tensor([57]), tensor([80]), tensor([46]), tensor([13]), tensor([28]), tensor([59]), tensor([63]), tensor([35]), tensor([35]), tensor([27]), tensor([50]), tensor([12]), tensor([13]), tensor([68]), tensor([36]), tensor([24]), tensor([91]), tensor([64]), tensor([85]), tensor([2]), tensor([53]), tensor([36]), tensor([96]), tensor([27]), tensor([89]), tensor([91]), tensor([12]), tensor([42]), tensor([5]), tensor([45]), tensor([18]), tensor([34]), tensor([10]), tensor([40]), tensor([14]), tensor([19]), tensor([32]), tensor([15]), tensor([81]), tensor([24]), tensor([8]), tensor([74]), tensor([41]), tensor([6]), tensor([43]), tensor([34]), tensor([1]), tensor([71]), tensor([79]), tensor([94]), tensor([80]), tensor([3]), tensor([58]), tensor([11]), tensor([81]), tensor([47]), tensor([26]), tensor([0]), tensor([50]), tensor([39]), tensor([17]), tensor([37]), tensor([88]), tensor([22]), tensor([41]), tensor([14]), tensor([57]), tensor([1]), tensor([96]), tensor([88]), tensor([20]), tensor([29]), tensor([87]), tensor([24]), tensor([36]), tensor([5]), tensor([2]), tensor([40]), tensor([55]), tensor([38]), tensor([33]), tensor([72]), tensor([95]), tensor([6]), tensor([27]), tensor([8]), tensor([20]), tensor([21]), tensor([44]), tensor([65]), tensor([20]), tensor([36]), tensor([32]), tensor([8]), tensor([86]), tensor([47]), tensor([27]), tensor([91]), tensor([24]), tensor([30]), tensor([8]), tensor([10]), tensor([43]), tensor([98]), tensor([63]), tensor([6]), tensor([71]), tensor([86]), tensor([99]), tensor([13]), tensor([75]), tensor([74]), tensor([59]), tensor([66]), tensor([47]), tensor([70]), tensor([10]), tensor([71]), tensor([55]), tensor([47]), tensor([88]), tensor([94]), tensor([87]), tensor([7]), tensor([82]), tensor([39]), tensor([92]), tensor([11]), tensor([92]), tensor([49]), tensor([90]), tensor([68]), tensor([64]), tensor([57]), tensor([97]), tensor([58]), tensor([96]), tensor([25]), tensor([17]), tensor([55]), tensor([30]), tensor([7]), tensor([13]), tensor([3]), tensor([49]), tensor([60]), tensor([39]), tensor([49]), tensor([0]), tensor([66]), tensor([91]), tensor([75]), tensor([0]), tensor([39]), tensor([41]), tensor([22]), tensor([18]), tensor([30]), tensor([14]), tensor([99]), tensor([98]), tensor([54]), tensor([37]), tensor([27]), tensor([20]), tensor([89]), tensor([58]), tensor([1]), tensor([36]), tensor([98]), tensor([79]), tensor([99]), tensor([43]), tensor([85]), tensor([55]), tensor([26]), tensor([76]), tensor([21]), tensor([39]), tensor([86]), tensor([47]), tensor([50]), tensor([65]), tensor([16]), tensor([35]), tensor([15]), tensor([82]), tensor([41]), tensor([22]), tensor([25]), tensor([45]), tensor([51]), tensor([57]), tensor([63]), tensor([99]), tensor([47]), tensor([14]), tensor([48]), tensor([54]), tensor([85]), tensor([23]), tensor([60]), tensor([83]), tensor([48]), tensor([79]), tensor([57]), tensor([9]), tensor([38]), tensor([24]), tensor([50]), tensor([44]), tensor([69]), tensor([8]), tensor([25]), tensor([94]), tensor([53]), tensor([81]), tensor([43]), tensor([10]), tensor([43]), tensor([77]), tensor([41]), tensor([69]), tensor([60]), tensor([46]), tensor([6]), tensor([4]), tensor([23]), tensor([74]), tensor([68]), tensor([66]), tensor([25]), tensor([62]), tensor([34]), tensor([93]), tensor([65]), tensor([99]), tensor([17]), tensor([54]), tensor([75]), tensor([79]), tensor([48]), tensor([27]), tensor([59]), tensor([64]), tensor([69]), tensor([98]), tensor([69]), tensor([64]), tensor([7]), tensor([91]), tensor([28]), tensor([40]), tensor([32]), tensor([29]), tensor([13]), tensor([55]), tensor([67]), tensor([42]), tensor([51]), tensor([31]), tensor([19]), tensor([11]), tensor([61]), tensor([20]), tensor([91]), tensor([74]), tensor([41]), tensor([53]), tensor([24]), tensor([79]), tensor([86]), tensor([72]), tensor([37]), tensor([79]), tensor([62]), tensor([19]), tensor([8]), tensor([51]), tensor([53]), tensor([5]), tensor([53]), tensor([20]), tensor([37]), tensor([98]), tensor([87]), tensor([38]), tensor([32]), tensor([90]), tensor([51]), tensor([70]), tensor([62]), tensor([38]), tensor([83]), tensor([59]), tensor([51]), tensor([79]), tensor([66]), tensor([32]), tensor([3]), tensor([78]), tensor([21]), tensor([26]), tensor([10]), tensor([32]), tensor([96]), tensor([76]), tensor([73]), tensor([77]), tensor([75]), tensor([33]), tensor([1]), tensor([31]), tensor([16]), tensor([48]), tensor([8]), tensor([21]), tensor([82]), tensor([22]), tensor([34]), tensor([17]), tensor([76]), tensor([35]), tensor([59]), tensor([75]), tensor([6]), tensor([85]), tensor([56]), tensor([72]), tensor([87]), tensor([67]), tensor([72]), tensor([52]), tensor([76]), tensor([40]), tensor([49]), tensor([73]), tensor([52]), tensor([15]), tensor([56]), tensor([75]), tensor([58]), tensor([81]), tensor([97]), tensor([80]), tensor([40]), tensor([85]), tensor([16]), tensor([97]), tensor([57]), tensor([8]), tensor([71]), tensor([44]), tensor([33]), tensor([47]), tensor([89]), tensor([28]), tensor([93]), tensor([74]), tensor([59]), tensor([72]), tensor([95]), tensor([26]), tensor([91]), tensor([87]), tensor([50]), tensor([96]), tensor([66]), tensor([30]), tensor([41]), tensor([11]), tensor([57]), tensor([1]), tensor([74]), tensor([40]), tensor([97]), tensor([33]), tensor([7]), tensor([51]), tensor([96]), tensor([48]), tensor([90]), tensor([9]), tensor([49]), tensor([60]), tensor([75]), tensor([13]), tensor([93]), tensor([32]), tensor([66]), tensor([90]), tensor([85]), tensor([83]), tensor([16]), tensor([77]), tensor([3]), tensor([87]), tensor([63]), tensor([40]), tensor([5]), tensor([14]), tensor([21]), tensor([65]), tensor([84]), tensor([94]), tensor([33]), tensor([11]), tensor([89]), tensor([22]), tensor([83]), tensor([49]), tensor([97]), tensor([77]), tensor([59]), tensor([22]), tensor([65]), tensor([86]), tensor([16]), tensor([16]), tensor([74]), tensor([48]), tensor([42]), tensor([68]), tensor([20]), tensor([56]), tensor([79]), tensor([92]), tensor([79]), tensor([79]), tensor([64]), tensor([19]), tensor([79]), tensor([2]), tensor([74]), tensor([61]), tensor([6]), tensor([28]), tensor([27]), tensor([58]), tensor([90]), tensor([38]), tensor([35]), tensor([91]), tensor([77]), tensor([50]), tensor([40]), tensor([86]), tensor([14]), tensor([12]), tensor([97]), tensor([12]), tensor([15]), tensor([94]), tensor([50]), tensor([30]), tensor([68]), tensor([16]), tensor([67]), tensor([84]), tensor([68]), tensor([56]), tensor([45]), tensor([50]), tensor([99]), tensor([16]), tensor([44]), tensor([88]), tensor([11]), tensor([58]), tensor([81]), tensor([9]), tensor([67]), tensor([17]), tensor([78]), tensor([44]), tensor([74]), tensor([81]), tensor([6]), tensor([3]), tensor([18]), tensor([27]), tensor([18]), tensor([40]), tensor([18]), tensor([16]), tensor([68]), tensor([19]), tensor([27]), tensor([97]), tensor([78]), tensor([8]), tensor([72]), tensor([89]), tensor([4]), tensor([92]), tensor([12]), tensor([30]), tensor([46]), tensor([45]), tensor([43]), tensor([23]), tensor([32]), tensor([61]), tensor([74]), tensor([55]), tensor([8]), tensor([53]), tensor([95]), tensor([53]), tensor([61]), tensor([45]), tensor([88]), tensor([73]), tensor([73]), tensor([90]), tensor([7]), tensor([24]), tensor([48]), tensor([59]), tensor([79]), tensor([31]), tensor([35]), tensor([6]), tensor([77]), tensor([48]), tensor([24]), tensor([97]), tensor([46]), tensor([17]), tensor([54]), tensor([43]), tensor([76]), tensor([74]), tensor([95]), tensor([60]), tensor([0]), tensor([1]), tensor([94]), tensor([33]), tensor([34]), tensor([57]), tensor([30]), tensor([70]), tensor([63]), tensor([63]), tensor([42]), tensor([58]), tensor([18]), tensor([73]), tensor([70]), tensor([46]), tensor([1]), tensor([61]), tensor([2]), tensor([39]), tensor([69]), tensor([24]), tensor([84]), tensor([18]), tensor([38]), tensor([78]), tensor([98]), tensor([31]), tensor([1]), tensor([39]), tensor([24]), tensor([79]), tensor([46]), tensor([13]), tensor([74]), tensor([93]), tensor([60]), tensor([33]), tensor([84]), tensor([15]), tensor([49]), tensor([99]), tensor([99]), tensor([71]), tensor([45]), tensor([77]), tensor([29]), tensor([4]), tensor([20]), tensor([87]), tensor([56]), tensor([17]), tensor([29]), tensor([63]), tensor([3]), tensor([64]), tensor([79]), tensor([20]), tensor([46]), tensor([53]), tensor([12]), tensor([14]), tensor([18]), tensor([80]), tensor([0]), tensor([9]), tensor([35]), tensor([53]), tensor([54]), tensor([31]), tensor([50]), tensor([83]), tensor([91]), tensor([68]), tensor([54]), tensor([71]), tensor([18]), tensor([46]), tensor([20]), tensor([26]), tensor([84]), tensor([16]), tensor([78]), tensor([81]), tensor([10]), tensor([96]), tensor([96]), tensor([70]), tensor([60]), tensor([89]), tensor([37]), tensor([10]), tensor([13]), tensor([48]), tensor([58]), tensor([73]), tensor([14]), tensor([43]), tensor([71]), tensor([89]), tensor([16]), tensor([56]), tensor([21]), tensor([50]), tensor([62]), tensor([52]), tensor([39]), tensor([61]), tensor([67]), tensor([83]), tensor([11]), tensor([75]), tensor([74]), tensor([10]), tensor([25]), tensor([69]), tensor([79]), tensor([97]), tensor([26]), tensor([3]), tensor([44]), tensor([20]), tensor([6]), tensor([78]), tensor([78]), tensor([28]), tensor([52]), tensor([26]), tensor([35]), tensor([37]), tensor([51]), tensor([7]), tensor([15]), tensor([90]), tensor([14]), tensor([69]), tensor([6]), tensor([96]), tensor([96]), tensor([99]), tensor([42]), tensor([64]), tensor([58]), tensor([6]), tensor([76]), tensor([87]), tensor([22]), tensor([7]), tensor([23]), tensor([99]), tensor([75]), tensor([49]), tensor([53]), tensor([53]), tensor([60]), tensor([80]), tensor([41]), tensor([82]), tensor([25]), tensor([19]), tensor([60]), tensor([60]), tensor([69]), tensor([94]), tensor([13]), tensor([5]), tensor([29]), tensor([11]), tensor([92]), tensor([96]), tensor([97]), tensor([66]), tensor([76]), tensor([60]), tensor([10]), tensor([40]), tensor([29]), tensor([0]), tensor([40]), tensor([23]), tensor([0]), tensor([49]), tensor([75]), tensor([90]), tensor([0]), tensor([98]), tensor([5]), tensor([80]), tensor([31]), tensor([75]), tensor([32]), tensor([81]), tensor([27]), tensor([76]), tensor([26]), tensor([83]), tensor([55]), tensor([11]), tensor([88]), tensor([29]), tensor([28]), tensor([35]), tensor([76]), tensor([61]), tensor([30]), tensor([94]), tensor([34]), tensor([46]), tensor([90]), tensor([25]), tensor([84]), tensor([26]), tensor([46]), tensor([58]), tensor([32]), tensor([51]), tensor([97]), tensor([67]), tensor([77]), tensor([26]), tensor([93]), tensor([13]), tensor([82]), tensor([76]), tensor([64]), tensor([11]), tensor([65]), tensor([53]), tensor([72]), tensor([57]), tensor([45]), tensor([58]), tensor([80]), tensor([7]), tensor([94]), tensor([19]), tensor([42]), tensor([24]), tensor([48]), tensor([93]), tensor([97]), tensor([8]), tensor([17]), tensor([42]), tensor([36]), tensor([6]), tensor([82]), tensor([9]), tensor([47]), tensor([87]), tensor([62]), tensor([63]), tensor([28]), tensor([20]), tensor([51]), tensor([5]), tensor([19]), tensor([21]), tensor([52]), tensor([24]), tensor([61]), tensor([90]), tensor([56]), tensor([35]), tensor([45]), tensor([57]), tensor([77]), tensor([65]), tensor([71]), tensor([55]), tensor([26]), tensor([97]), tensor([82]), tensor([7]), tensor([9]), tensor([4]), tensor([49]), tensor([8]), tensor([56]), tensor([38]), tensor([31]), tensor([64]), tensor([18]), tensor([23]), tensor([87]), tensor([30]), tensor([97]), tensor([51]), tensor([35]), tensor([36]), tensor([96]), tensor([28]), tensor([19]), tensor([94]), tensor([4]), tensor([36]), tensor([18]), tensor([86]), tensor([38]), tensor([62]), tensor([63]), tensor([81]), tensor([74]), tensor([86]), tensor([37]), tensor([42]), tensor([92]), tensor([33]), tensor([37]), tensor([39]), tensor([92]), tensor([2]), tensor([17]), tensor([83]), tensor([39]), tensor([82]), tensor([57]), tensor([11]), tensor([32]), tensor([1]), tensor([30]), tensor([81]), tensor([18]), tensor([52]), tensor([85]), tensor([9]), tensor([94]), tensor([83]), tensor([16]), tensor([49]), tensor([55]), tensor([59]), tensor([64]), tensor([82]), tensor([89]), tensor([41]), tensor([86]), tensor([52]), tensor([18]), tensor([34]), tensor([98]), tensor([21]), tensor([42]), tensor([11]), tensor([53]), tensor([82]), tensor([99]), tensor([57]), tensor([99]), tensor([44]), tensor([70]), tensor([4]), tensor([72]), tensor([3]), tensor([9]), tensor([19]), tensor([25]), tensor([5]), tensor([50]), tensor([71]), tensor([73]), tensor([5]), tensor([86]), tensor([33]), tensor([70]), tensor([95]), tensor([49]), tensor([84]), tensor([85]), tensor([17]), tensor([69]), tensor([99]), tensor([13]), tensor([39]), tensor([3]), tensor([29]), tensor([0]), tensor([66]), tensor([34]), tensor([30]), tensor([76]), tensor([14]), tensor([26]), tensor([45]), tensor([65]), tensor([77]), tensor([7]), tensor([64]), tensor([4]), tensor([81]), tensor([46]), tensor([91]), tensor([94]), tensor([38]), tensor([24]), tensor([19]), tensor([38]), tensor([25]), tensor([12]), tensor([40]), tensor([47]), tensor([71]), tensor([45]), tensor([24]), tensor([24]), tensor([69]), tensor([46]), tensor([91]), tensor([92]), tensor([96]), tensor([95]), tensor([98]), tensor([86]), tensor([50]), tensor([20]), tensor([35]), tensor([36]), tensor([55]), tensor([2]), tensor([2]), tensor([19]), tensor([42]), tensor([62]), tensor([44]), tensor([93]), tensor([54]), tensor([59]), tensor([50]), tensor([85]), tensor([6]), tensor([10]), tensor([41]), tensor([48]), tensor([93]), tensor([2]), tensor([87]), tensor([37]), tensor([69]), tensor([93]), tensor([28]), tensor([35]), tensor([80]), tensor([51]), tensor([83]), tensor([48]), tensor([57]), tensor([36]), tensor([66]), tensor([66]), tensor([3]), tensor([18]), tensor([85]), tensor([65]), tensor([47]), tensor([73]), tensor([89]), tensor([84]), tensor([60]), tensor([67]), tensor([32]), tensor([7]), tensor([55]), tensor([85]), tensor([73]), tensor([59]), tensor([2]), tensor([70]), tensor([20]), tensor([79]), tensor([5]), tensor([95]), tensor([39]), tensor([81]), tensor([29]), tensor([73]), tensor([32]), tensor([15]), tensor([46]), tensor([67]), tensor([28]), tensor([91]), tensor([25]), tensor([63]), tensor([15]), tensor([89]), tensor([97]), tensor([10]), tensor([27]), tensor([62]), tensor([90]), tensor([64]), tensor([60]), tensor([85]), tensor([91]), tensor([44]), tensor([4]), tensor([73]), tensor([70]), tensor([35]), tensor([99]), tensor([6]), tensor([58]), tensor([34]), tensor([45]), tensor([98]), tensor([11]), tensor([9]), tensor([78]), tensor([95]), tensor([55]), tensor([12]), tensor([51]), tensor([33]), tensor([42]), tensor([92]), tensor([15]), tensor([45]), tensor([4]), tensor([29]), tensor([47]), tensor([30]), tensor([76]), tensor([12]), tensor([16]), tensor([31]), tensor([19]), tensor([47]), tensor([35]), tensor([25]), tensor([58]), tensor([15]), tensor([56]), tensor([43]), tensor([56]), tensor([0]), tensor([8]), tensor([16]), tensor([29]), tensor([24]), tensor([76]), tensor([5]), tensor([76]), tensor([54]), tensor([28]), tensor([54]), tensor([32]), tensor([0]), tensor([85]), tensor([55]), tensor([5]), tensor([19]), tensor([10]), tensor([42]), tensor([28]), tensor([67]), tensor([31]), tensor([94]), tensor([8]), tensor([76])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.34 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.445\n",
            "TEST ALL:  0.3876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fYJBfJyvpng"
      },
      "source": [
        "### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MDs9xRsqrc1R",
        "outputId": "87d493da-0022-4af0-cede-fc8c0ed5cb6a"
      },
      "source": [
        "method = \"iCaRL with SVM\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics FINETUNING for seed 31\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAJcCAYAAAAb0rWEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdebwcVZn/8c8TAkZkiYGQBAkGhyAqATQR0SiLUUYFRQfFBRk2jYor4g+BmZ/bqIPihjMuv7iBTARRQNEIwqDIyCiYIBIUJAiRAAkJ0bAHSPL8/qgKdJ8+t+vcSlV1377f9+t1X7lVXXXq1NJ9K9XPcx5zd0REREQGzZhed0BERESkDrrJERERkYGkmxwREREZSLrJERERkYGkmxwREREZSLrJERERkYGkmxxpnJk92cx+Ymb3mtkPNqGdI8zs0ir71gtmdrGZHVVivRG9/2Z2qpl9s8vrR5vZr5vs00ilYyUSp5scGZKZvcXMFprZA2a2PP9j/OIKmn49MAnYzt3fULYRd5/v7gdV0J82ZnaAmbmZXRjM3yuff0ViOx8zs/8qWs7dX+nuZw23n+H+W+Z9ZnaDmT1oZneY2Q/MbEZCX6fl+/ZA/rPUzE4OlllqZi8bbj+79P/T7v62YPtjy7ZnZi82s//Nb57/ZmZXmdnzzWzf/HhsFVnn92b2npbt/z54fXsze9TMlpbtV7/J9+kqM1ttZmvM7DdmNrvl9T3M7Odmdo+ZaSA1GdF0kyNRZvZB4EvAp8luSHYGvgocWkHzTwdudvd1FbRVl1XAC81su5Z5RwE3V7WB/KakyvfgGcD7gfcBE4DdgB8BBw+jjfHuvhXZjej/NbOXV9i/2pjZNsBPgf8g2/enAR8HHnH33wJ3kO1T6zp7AM8GzmmZvWU+f6O3ALfV2PVeeAA4FpgIPBX4DPCTlhvMx4DzgON60z2R6ugmRzqY2bbAJ4B3u/sF7v6guz/m7j9x9/+TL/MkM/uSmd2V/3zJzJ6Uv3ZA/hThRDNbmT8FOiZ/7ePAR4A35k8MjgufeIT/q88fxd9qZveb2W1mdkTL/F+3rPciM/td/j/535nZi1peu8LM/i3/H+z9ZnapmW3f5TA8SnaD8KZ8/c2ANwLzg2N1hpktM7P7zGyRmb0kn/8K4NSW/fxDSz8+ZWZXAQ8Bz8jnbXyi8TUzO7+l/c+Y2eVmZpHz9Pj+m9l04N3Am939F+7+iLs/lD/tOS1f5uD8ycV9eZ8/NtTOu/tC4I/A3l2OUZSZ/dXMZua/H5Gfy+fk08eZ2Y/y31vP+5X5v2vy4/XClvY+Z2Z/z8/9K4fY7G55v89x9/Xu/rC7X+ru1+evnwX8c7DOPwM/c/fVLfPOJruZbV3mu8M6AIGhrt/8tWPN7MZ8/35uZk9veW13M7vMsqdSfzazw1te287MLsrP5TXAP6T2x93Xuvuf3X0DYMB6spudCfnrf3b3b5Gdf5ERTTc5EvNCYBxwYZdl/gXYl+yP4F7APsC/trw+GdiW7H/UxwFfMbOnuvtHyZ4Ofd/dt8o/TIdkZk8Bvgy80t23Bl4EXBdZbgKwIF92O+ALwILgScxbgGOAHYAtgA912zbZH7eNfxj/EbgBuCtY5ndkx2AC8D3gB2Y2zt0vCfZzr5Z1jgTmAlsDfw3aOxGYkf9hfAnZsTvKi+uvzAHucPdruizzYL4/48me7rzLzF4bW9DM9gX2AG4p2G7Mr4AD8t/3B24F9muZ/lVknY2vj8+P12/y6RcAfwa2Bz4LfCt2w0f2hG29mZ1lZq80s6cGr58N7GdmUwHyJ2hvIbv5afVfwJvMbDMzezawFXB10Q4Ppdv1a2aHkt0I/xPZU5X/IX+qlK93Gdk1tQPZzfZX8z4BfAVYC0wheypzbLDdn1rwdWOkb9fnbVwEfNPdV5bdT5F+pZscidkOuKfg66QjgE+4+0p3X0X21cCRLa8/lr/+mLv/jOwR+TNL9mcDsIeZPdndl7t77H+YBwNL3P1sd1/n7ucANwGvblnmO+5+s7s/TPY4vutTCnf/X2CCmT2TIf5H7+7/5e6r821+HngSxft5prv/MV/nsaC9h8iO4xfI/uC+193vKGgPsnO2vGB/rnD3xe6+IX/CcQ7ZTUere8zsYeA3ZF9P/ihh26FftbT7EuDfW6aHuskZyl/d/Rvuvp7shmQK2denbdz9PuDFgAPfAFblTzom5a8vA67giWt0Dtm5WhA0dQfZTdXLyM752cPo61CGun7fCfy7u9+Yv9c+DeydP805BFjq7t/Jr5PfA+cDb8ifKh4GfCR/ynoDwc2aux+y8QneUNx9T2Absps9BS3LQNJNjsSsBra37kGgO9L+FOKv+bzH2whukh4i+1/xsLj7g2RfE70TWG5mC8xs94T+bOzT01qmV5Toz9nAe4ADiTzZMrMP5V833Gtma8ieXnX7GgxgWbcX3f1qsqcfRnYzlmI12Q3AkMzsBWb2SzNbZWb3kh3TsK/bkx2XE8mexmyeuP1WvwJeYmZTgM3I9mG2mU0jOz4dT+K6ePyc5TeAMMR5y28Wjnb3ncieQu1IFle20Vk8cZNzJHBueJOZ+y5wNPBmCm5yLMsQ2xis/fVIn7pdv08HzrAs+HcN8Deyc/60/LUXbHwtf/0IsiekE4GxtF9H4bWfJP/q6hzgZDPbq3AFkRFGNzkS8xvgESD6VUbuLrIP4o12pvOrnFQPAlu2TE9ufdHdf+7uLyf7I34T2f/Ui/qzsU93luzTRmcDx5PFbjzU+kL+ddJJwOHAU919PHAv2R8qyJ4qxHT96snM3k32lOGuvP0UlwM7mdmsLst8j+yrianuvi3w9Za+PtG5LKblC2RfZRyfuP3W9W8hu4l8L3Bl/pRlBdlXdL/OY0E6Vhvudgr6cBNwJtnNzkYXkB2jA8m+Ihoqo+18sieDt7r77QXb+XT+9dpW7v7OIZYZ6vpdBrzD3ce3/Dw5f4K4DPhV8NpW7v4usqD4dcDUls3s3K2fCTYHnrGJbYj0Hd3kSAd3v5csOPgrZvZaM9vSzDbPYx0+my92DvCvZjbRsgDej5B9vVLGdWTxEjtbFvR8ysYXzGySmR2axyg8Qva1V+yP5M+A3SxLex9rZm8ky5z5ack+AeDut5F9xfIvkZe3JvtjswoYa2YfIXv8v9HdwDQbRgaVme0GfBJ4K9nThpPMrDD4192XkH29dI5lgd9bmNk4M3tTS2zG1sDf3H2tme1D9jVFN6fl2x/XMm/zvN2NP0M97fsV2ROwjV9NXRFMh1aRnddSf2jzIN0TzWynfHoq2ZOY325cJn+q8kPgO2Rfgy2MtZUv91LgbWX6EvSr2/X7deAUeyIoe1sz2zikwk/Jrucj8/fe5palwz8r/+ruAuBj+Xvz2bQHSxf1aV/L0u23sGzMqg+TfQV4df665ed8i3x6nOVJBSIjjW5yJCqPL/kgWTDxKrL/Wb6HJ2I0PgksBK4HFgPX5vPKbOsy4Pt5W4tovzEZk/fjLrLH+fsD74q0sZosjuFEsq9uTgIOcfd7yvQpaPvX7h57SvVz4BKyoNe/kj35aP0KYeNAh6vN7Nqi7eQ3DP8FfMbd/5DfuJwKnJ34R+Z9wH+SBaWuAf4CvA74Sf768cAnzOx+spvSoq/CFgB/B97eMu9nwMMtPx8bYt1fkd1UXTnEdJv8KdmngKvyr2f2Lehb6H6yIOWrzexBspubG8iuh1ZnkT3x65ox5e4L3f0vw+xDzJDXr7tfSJa+fa6Z3Zf395X5a/cDB5EFHN9F9iTsM2RP+CB7L26Vzz+T7MbtcZaNaXXqEH16Etk1sprsSeergINbrvGnk53bjbFDD5PFKYmMOFactCEiIiIy8uhJjoiIiAwk3eSIiIhIXzGz91tWouaPZvaBfN4EywbIXJL/G46H1UE3OSIiItI3LCut8nayQWb3Ag4xs12Bk4HL3X06WUZp1wEvQTc5IiIi0l+eBVztWWmadWSJC/9EVjtx49APZ9F9mBMgG1CqcZbV9TmDbKCwbxaNzPnf35/fFh198ukP1ti7eqzfvH1Mtc0ei41BNnjC/YaRt+9l96HonI/EY/PI1lt3zHvS/fe3Td82Z07HMrtcfvmwt1XXe6bfjnsvPxti53Ps2rVt0/1+TQ6qhQvnxsqX1KmxLCQzewfZuFkbzXP3eS3TNwCfsqwsz8NkGYALgUnuvnFk9xVERj8PNX6Tkw9J/hXg5WRDqP/OzC5y9z813RcRERFpVn5DM6/L6zea2WeAS8kGi72OrJBs6zJuZoU3Zr34umof4BZ3v9XdHwXOJXsEJSIiIoK7f8vdZ7r7fmTjdd0M3J2XiyH/t7CobC9ucp5G+4Bpd9BeXwgAM5trZgvNbOGC//5FY50TEREZbTasX9/YTwoz2yH/d2eyeJyNZWk2ju59FPDjonZ6EpOTovVxVhiTIyIiIgPt/Dwm5zHg3e6+xsxOA84zs+PIRpk/vKiRXtzk3El7YbmdKCiieMLX2x84Xf7bdW3Tr9q3vt2IBSmGUgJKy2ynqoC/MLgwDBQdCXq5D+vGjeuYl3Juiq6LJgNpY8uE+xUGnMbaiS0T2nJl5xPke6dObZvedlnXQuzRbZcVXjux8/mUVau6rgNp11wv9zOUsg9VvY+q+pwscyzKfN6mCq+Vuj53Hpw4sZZ2h2PDhlhJwHqM2WyzwmXc/SWReauBzsyGbtsazsIV+R0w3cx2MbMtyGqzXNSDfoiIiMgAa/xJjruvM7P3kBU33Az4trv/sWA1ERERqUlqrEwlanz6FupJTI67/4ysmrGIiIhILfo28FhERESasWFDg09yGqSyDiIiIjKQejHi8VTgu2TDMTvZcM5ndFsnjHCf8/L2wqN/m7lbxzpTFi1qmy4bgV/F8P0Ai07cr2165uev3OTtxMTaKZMRUFcpg5RlYttuMptq0TuPbZuesOSvHcuEZQrqGh4/5TzE2q0qy6dMVtv4pUs75q2ZNm3Y/akqGy08D2WvpZT+pOxXmXbLSMmEC68TKLcPKe/zqlTxGZPSLnQew6oywsL3VZjd1wsb1jeXXdWkXnxdtQ440d2vNbOtgUVmdpnKOoiIDIY607pFhqMX2VXLgeX57/eb2Y1kIx7rJkdERKQHFJNTAzObBjwXuDry2uNlHdYsu7TpromIiMgI17ObHDPbCjgf+IC73xe+7u7z3H2Wu88aP/Wg5jsoIiIiI5q5N18Wysw2B34K/Nzdv1C0/KxZ89o6mRTo+7Ed26Z3/VbnMNJhgF3ZodyrUHZo/jLLLJs9u2OZqVdd1TadEpBYVbBt2E5KeYHYft6z++5t09vfdFNhO1Wd8zLnJuXY3HjYYR3znnX++cPsXXXqeo+UDaqvSplzExuKv64A0pT+LZ85s216m9tv71hm3Jo1hduqqvxCFQHVVX3mVaXMdRq7TsLzEOvvwoVzbZjd2yT3/W11YzcD20zYrrF9a/xJjpkZ8C3gxpQbHBERGVkUeCz9ohfZVbOBI4HFZnZdPu/UfBRkERERaViTBTqb1Ivsql8DjT6GExERkdFHZR1ERERGuUYLdDZoRN7kpASV7f6V9mX+ttsuHctUMTppTJmgt5R1qlpmx2uuSepTkVjAaRhkFwvELDqG68aNKwxmDUfBhs5A49ixCPsXC3Lu5TkOxUYPrmo015T17p4xo2160uLFheukSAnGbFIYtB477k2Ouh1KOVd3HTe/bXrK8bsPsWR3VQQM1xkkHlo7fnzHvJTA3hRlRlcOp8etWZMULC316NlNjpltBiwE7nT3Q3rVD+k/vfxjIr3RyxsckTo1ecO3KQb1SU4vBwN8P3BjD7cvIiIiA6wnT3LMbCfgYOBTwAd70QcRERHJDGp2Va+e5HwJOAkY8qi2lnVYterKoRYTERERiWr8SY6ZHQKsdPdFZnbAUMu5+zxgHnSOeCwiIiLVGdSYnMbLOpjZv5MNBrgOGAdsA1zg7m8dap3nvvA7bZ2sanjwl5x3a9v0/76us5RBSjtV9KfXwWnXz7u2bXrPuc+rpN2U8hD9rqpSBuGxSDkOZbMwqrqemszkqkKTpRZSlMnU6+XxK6vMcS+7n6unT2+b3m7JksJ1qsqYTCmJk7LtlG01XdZhxV+XNnYzMPnp0xrbt14MBngKcApA/iTnQ91ucEREZGTp5Y2llLNhw2A+yelldpWIiIhIbXo6GKC7XwFc0cs+iIiIjHaDGpOjJzkiIiIykBoPPC4jzK4KA0FTgkBTgr9e85PlHctc9OopSX3sZ00GOS+fObNtesqiRYX9CfV7kGWTwiBLgK1WrGibLnv9h6UxqhppOhao/cDkyW3TsbIJobqug34L7K3q/VnV52IoPHeQFjS/6NQ92qZnfvqGwnXKCEuPQHXlR6owUgKP77jl5sZuBnbadbfBDTwGMLPxwDeBPQAHjnX33/SiLyIiUq3wBkf636AOBtirmJwzgEvc/fVmtgWwZY/6ISIiIgOqF4MBbgvsBxwN4O6PAo823Q8RERHJKPC4OrsAq4DvmNnvzeybZvaUcCGVdRAREZFN0YubnLHA84CvuftzgQeBk8OF3H2eu89y91kTJ+7XdB9FRERGjQ3r1zf206RexOTcAdzh7lfn0z8kcpPTKoxOL5MFkhLNfvbx+3TMu+Wrl7VN7/3+zkj+ujIzypQBiAmzaGL9rWr4/rsO+2Pb9JTO5KrCYexj2Tlj164t7M9Iy8pKybpIyUJKaSd2bMLroiop78+wP7FzXrROWf2USQXV9Wft+PGFy6Scm8VHHNE2PWP+/GH3Zeanb+g4p3VlxcYyqcpsqy4j7XNp0PSirMMKM1tmZs909z8Dc4A/Nd0PERGpR8pNq/QXZVdV673A/Dyz6lbgmB71Q0RERAZUT25y3P06YFYvti0iIiLtlF0lIiIiMoL0asTjE4C3kY12vBg4xt2HjCxtanj3WGBvGGj8sWOv61jm3/7fc4a9rZR9KhNoHAvUSwm6SwlODsWWmXnqpp+rWH+bLE3RlLLHOJQyNH/MDXOf2zY98/P1DdVQdA3GAst7WdahqmWG+/qmKPN5EevP9AULNrkvZQN9qwpODoOwm7y+RqoNG/QkpxJm9jTgfcAsd98D2Ax4U9P9EBERkcHWq8DjscCTzewxspIOd/WoHyIiIqPehvWDmV3V+JMcd78T+BxwO7AcuNfdLw2X04jHIiIisil68XXVU4FDyco77Ag8xczeGi6nEY9FRESasWHD+sZ+mtSL7KqXAbe5+yp3fwy4AHhRD/ohIiIiA6wXMTm3A/ua2ZbAw2QjHi8cTgNVZdqUyW752Lf37ljm2t+1F1F//vNt2NsqO9z76unT26ZTygDEhFkMsRFL6xoavYqslaaVuQZvmzOnbXqH66/vWOYpq1YNuy9lj02d2VTDFduHuobmryqrrcxxXzZ7dse8qVddNex2ygo/L7ZbsqRjmTATKWU/q/q8SPkcStlO2RI4Re6e0VnWJ1ZWosggZov2q16UdbjazH4IXAusA34PzGu6HyIiUo9e1oqScgZ1MMBejXj8UeCjvdi2iIiIjA69SiEXERGRPjGoBTpV1kFEREQGkrl7PQ2bfRs4BFiZj2yMmU0Avg9MA5YCh7v734vaGvufj7Z18lnnn9/2etnvfx+cOLFtukzQZ0yTwYUpwZkpQW7hsYgJh0qPBTmHbccCB4sCG3sdnFmXqgJpU8o4pAQy1nX9x66lcWvWtE1XVa4ipZ1Fn24vWbL3RztjD1Ku25Tzde/UqW3TdQXAVqXJBIMyYtfAA5Mnt03XeYxTPjvrCiJeuHBuZwZLja694vJ6bgYinnfAnMb2rc4nOWcCrwjmnQxc7u7TgcvzaREREZHK1RaT4+5Xmtm0YPahwAH572cBVwAfrqsPIiIiUmxQs6uajsmZ5O7L899XAJOGWrC1rMOGq77ZTO9ERERkYPQsu8rd3cyG/A7Q3eeRj58TxuSIiIhIdfotu8rMTgDeBjiwGDgGmAKcC2wHLAKOdPdHh2yE5m9y7jazKe6+3MymACtTVtr7zDPbph/ZeuuO4LiUIMpYEFu4XNlA3nC92Gi2b1lw9+O/f+/gIR9iDVtKoGBKMFzrsXhw4kTWjRvXsUzrCKnrN988KRAv7N9Q67Xa5vbbC/tbp8VHHPH47zPmz0+6LuoaoTcMZt1qxYq245dyHoY63mFAcExKO6HwfbV+883bAtdTg5yr2s+Zpz4R7J5y/UH581dXIGyZ85ASFFt0Lfc6EDnW561WrChcr6p9CLcf/q0Zt2ZN4fmIfTa0vq/7PUC9aWb2NOB9wLPd/WEzOw94E/Aq4Ivufq6ZfR04Dvhat7aa/rrqIuCo/PejgB+XaaToBiemzA1OajvDvcGpUh03OEDhDc5Q7Rbd4KT0p59ucCDtukhZpozwBgc6j1/KeYgZ7g1OqtgNTJiZl6Kq/Wy9wRmqnVA/3+BUuU7KzXovVXGDU+X2Y39rhnuDA/H3da9tWL++sZ9EY4Enm9lYYEtgOfBS4If562cBry1qpLabHDM7B/gN8Ewzu8PMjgNOA15uZkvICnWeVtf2RUREpP+0xtzmP3NbX3f3O4HPkdW6XA7cS/b11Bp3X5cvdgfwtKJt1Zld9eYhXpozxHwRERHpgQ0bmsuuao25jTGzp5JlY+8CrAF+QOeQNEk04rGIiIj0k5cBt7n7Knd/DLgAmA2Mz7++AtgJuLOoIdWuEhERGeU2rO+r7KrbgX3NbEvgYbJvgBYCvwReT5ZhlRTX23RZh9OBVwOPAn8BjnH3wujHWbPm1dLJuobjTrHDube0Td/23hd2LFNmmP27Z8zomDdp8eJhtxMLWixzfKpqpypVnfMyJRHKbLvJ4xcLqkwJzq8i+yal3ZRyEbGA+V5mB62ePr1tOhbsHR7jcB3oDPov68bDDmubDkvklBULpC0Kwo6dz7DkS9kyNSn9CwOYy76vBqWsw69/8qPGhmp58atfW7hvZvZx4I3AOuD3ZOnkTyO7wZmQz3uruz/SrZ2myzpcBuzh7nsCNwOn1Lh9ERHpAaVEy6Zy94+6++7uvoe7H+nuj7j7re6+j7vv6u5vKLrBgYbLOrj7pS2TvyV77CQiIiI91GTgcZN6GXh8LHDxUC+2ppitWnVlg90SERGRQdCTwGMz+xey79nmD7VMa4pZXTE5IiIiMrgFOhu/yTGzo8kCkud4XVHPieoK4EwJFo0FGt90QnuQ3cxTi7cVBmyOX7q0eKURqKrgvqrO+X0779w2XSZIPOU6qTNIOwzGTImjqCuIN6Xd2/fbr2NeVYGzdSkTMFxVkHEsmLuu43XbnM7hz8Jthec49p4Jg65j10U4enasnVh/QmXihupKBCgzorWkafQmx8xeAZwE7O/uDzW57X4X3uCIiIxU/X7zKZ36rUBnVZou6/CfwNbAZWZ2XV5gS0RERKRyTZd1+FZd2xMREZFyBjUmR2UdREREZCCprIOIiMgoN6hPchot69Dy2olkZdQnuvs9RW2FKeThcOCxodLLDJkfy96YcPPNbdPhUOCp2ypj0Ynt/Zn5+XLjBTVZTiAlYyfM+ujlsPt1KjruZUskpGRipJyrus5DrCxBKCWDqK7h8suUIIj1p6oSElWdh/BzMcxCgs7jvnzmzI5lws+8qvoTfk5XdT5j+zBl0aK26areM01quqzDpd/7bmPZzge95Z8b27c6n+ScSRZo/N3WmWY2FTiIrACXiIgMmFhdKulvyq4aJne/Evhb5KUvkqWRa4A/ERERqU3T4+QcCtzp7n8w6/60yszmAnMBdt75CCZO7PwqSURERDbdoMbkNJZdZWZbAqcCH0lZ3t3nufssd5+lGxwREREZriaf5PwDsAuw8SnOTsC1ZraPu3dG83aRMoR+SpDg2LVr26Z3vrIzsLeqYLSiwLfYdsJA40Uf27FzmY/d1Ta95OCDO5aZvmBB23RKoN7KPfcsXCamTGmAMPAyPC8AD0yeXLidqgI4ywQpxoJtwyDPsN2U/sWGpw+v09i1nXLdpmy/TJB/LDg/3FZKUHHKOQ+lBMyntJMSFF7VZ0NVAd/h52LK5+QO11/fMS/cr5RjGi4zbs2aUsenzHsv5XOp7LmqKmmjqv7UaVCrkDd2k+Pui4EdNk6b2VJgVkp2lYiIjBz9+EdcRqemyzqIiIiINKLpsg6tr0+ra9siIiKSToHHIiIiIiOIyjqIiIiMcoM6GGDjZR3M7L3Au4H1wAJ3P6morRkHntPWybqGny8z1DzAdZ96uG165kmd945hlkpK5kOKF13Yninyv6/rHLI+xd0zZrRNT1q8uGOZuobZD5Utd9BvymTUpbTR70Gd/dbnMu/zFFXtZ1PvK2i2pEoV+xU7xn86/PC26Rnz5w+73bKavLabLuvwo3lfaWyA3tfOffdglnUwswOBQ4G93P0RM9thiHVFRGSESkmjlv6imJxhGqKsw7uA09z9kXyZlXVtX0REREa3pgOPdwNeYmZXm9mvzOz5Qy1oZnPNbKGZLfzbXZc32EUREZHRZcP69Y39NKnpm5yxwARgX+D/AOfZEEWsWss6TNixc9RXERERkW6azq66A7jAs2jna8xsA7A90DUKt67guDIBiNHyC5FA41AYaJxSdiJlv8NA46Nf+seOZc78xXMK24kFGoeaCh4dCUHGy2bPbpueetVVHctUcbyaDNitKqiy3wKjqwo0DqXsZ5myGHW6a5992qZ3ubyap+RVXTspgdFlAo0H9dqu0qBmVzX9JOdHwIEAZrYbsAWgsg4iIgNkkG8GZGSp7UlOXtbhAGB7M7sD+CjwbeDbZnYD8ChwlNeVwy4iIiJJBjW7qhdlHd5a1zZFRERENtKIxyIiIqPchg2D+SRHtatERERkINUZk9NR1sHM9ga+DowD1gHHu/s1w227zNDkKdH1YWkD6MyG2GrFisJtpQTdhcuUDdQL9yuWSXXfmTe0TW9z9B4dy1SlrqHcw3Z6XToglk01XLHyFWPXri1cb+348W3TZUuE1FVOIHZuHpg8uW1622Xt5UiqKuWRckyreq+ltFOUVVm23ZiUz8Udrr++bfreqZ1lYGKZnqHxS5e2Tcf6XFTKpqpznvJZEOtfuO/hNVl2++Hxi72n+zEweyW5vuoAACAASURBVMN6ZVcN15nAK4J5nwU+7u57Ax/Jp0VEZICENzgivVJn4PGVZjYtnA1sk/++LXBXXdsXERGRNIrJqcYHgNPNbBnwOeCUoRZsLeuwatWVjXVQREREBkPTNznvAk5w96nACcC3hlqwtazDxIn7NdZBERERGQxNp5AfBbw///0HwDfLNFI20LhomZTSBlUFy5UR+547DIxePX16xzLbHd0+feU1nQFm++3Tfr9bdj/rKmUQ9icWzFcmID12TFOCRavYz5RSHrHA0LIBkkXbj+1TmfIjsXaK+pwScJ2izvdiuO9hADiUCwKvKgg13PflM2d2LDNl0aJKtlUk5TjUea6Kgp6huvdRmSSSuoL+N8WgDgbY9JOcu4D9899fCtRTVEZERERGvabLOrwdOMPMxgJrgbl1bV9ERETSDGqBzl6Udeh8hioiIiJSMZV1EBERGeUGNSZnRNzklAnSKhP8lSIWINlUEFkseC7c9nZLisOcZs9+Use80//PFm3TJ59eLiiwTKBqqMmg55QAyarOZyx4uijgtqrgyJgyxzTlWJQ5f/0QeFkk3IcygbNN7ueEm2+upJ1ejy4eSvm8LTsKeJGqEk9GwvU+KOqMyZkKfBeYRDYI4Dx3P8PMJgDfB6YBS4HD3f3vdfVDpF9VlVEkIrKpBvVJTp3ZVeuAE9392cC+wLvN7NnAycDl7j4duDyfFhEREalUnYHHy4Hl+e/3m9mNwNOAQ8myrgDOAq4APlxXP0RERKS7Qc2uamScnLyG1XOBq4FJ+Q0QwAqyr7Ni6zxe1uGeu69oopsiIiIyQGoPPDazrYDzgQ+4+31m9vhr7u5m5rH13H0eMA/guS/8TnQZERER2XSDGpNT602OmW1OdoMz390vyGffbWZT3H25mU0BVha1UyYS/bY5c9qmd7n88lLtlsmcqisboc4sh5NPf7Bt+tA9Oo/Xj2+Y0zGvqD9ljlfZ4d77caj0VmX2q86h+es6XmXKrvT6XKVkWoZ97Leso5RSBmWU3acyZVZSroteHuOmSvhIderMrjKyApw3uvsXWl66iKyG1Wn5vz+uqw8iItK8WKq19LcNG/QkZ7hmA0cCi83sunzeqWQ3N+eZ2XHAX4HDa+yDiIiIjFJ1Zlf9GrAhXi7+3kNEREQasWG9sqtEREREamVmzzSz61p+7jOzD5jZBDO7zMyW5P8+taitEVnWIRQLRIsFGpcRtn3v1Kkdy4Qj18ZGsg3LG4QBbClBjLH9DANTU4JSY+2E248FGd942GFt0886//zCbZXdrzLqCkgsG2Ba5roN7XD99cNuN7XtNdOmtU2nlARJORYpQ9/XFfQfs8O5t7RNr3zTrh3LlGm718HSoZRA45RYmfDzq8x+xgJ0y1y3sc/b8LN0qxUrOpZJGU28zH5VVdZBunP3PwN7A5jZZsCdwIU8MZjwaWZ2cj7ddZy9XpR1OB14NfAo8BfgGHdfU1c/REREpLs+DjyeA/zF3f9qZsMeTLgXZR0uA/Zw9z2Bm4FTauyDiIiI9JHWwX7zn7ldFn8TcE7+e9Jgwq0aL+vg7pe2LPZb4PV19UFERESKNTkYYOtgv92Y2RbAa4g8DOk2mHCrXpR1aHUscPEQ66isg4iIyOj1SuBad787n747H0SY1MGEa7/JCcs6tMz/F7KvtObH1nP3ee4+y91nbT/pgLq7KSIiMmpt2LChsZ9heDNPfFUFTwwmDImDCfeirANmdjRwCDDH3WupS1XXsPHbLltWSTuhsv2raoj/lO2H2VRvf8VvO5b5xiX7VtKfflL23FRxzVV13cYyW8JsqlgmSyxzpUhV2SZVZb8sP/JZ7e3Su6yolOycRSfu17HMzM9fWcn2H5g8uW06JaOuKmXOZ12ft2Upk6o5ZvYU4OXAO1pmD3sw4cbLOpjZK4CTgP3d/aG6ti8iIiJp+q1Ap7s/CGwXzFvNMAcT7kVZhy8DTwIuyyuS/9bd31ljP0RERGQU6kVZh5/VtU0REREZvn57klMVlXUQERGRgdT4iMctr58IfA6Y6O73dGurriHXU4KTV0+f3jYdC9QLgwlThjTv5ZDwZcsUhGJBxh988+q26S+cs13HMkXHYjQPnZ5ynZS5llKWSQnyTBmav5cG4TrZ+8u/KbVeyrkZv3Rp4Tp1Bbv3WxmMfupfP7yvhpn1NGLUGZOzccTja81sa2CRmV3m7n/Kb4AOAm6vcfsiItID/fBHWwR6MOIx8Cfgi2QZVoU57iIiIlIvxeRsgtYRj/MCW3e6+x8K1nl8xONVq6oZI0JERERGj1oHA4T2EY/JvsI6leyrqq5aa1vMmjWvlgEDRUREpK+rkG+SWp/kREY8/gdgF+APZrYU2Am41swmD92KiIiIyPA1OuKxuy8GdmhZZikwqyi76sGJE9umn7Jq1bD7E8vYufk1r2mbnjG/s4xWyrDnY9eubZtOCbqL9SeUkilSpp1YFsGNhx3WNh2WcIC0bIQwm6rMEPVrx4/vmFdV1kyTGRXhtsL9il3HVWVKpagrS6suYaYjdL4/+z1bKPysqFK/ZQcV9Sf8XAcYt2ZNYRvLZ85sm04pbVM2EDpc76599ulYZupVV5Vqu9W6ceM2uQ2Ja3zEY3fXYIAiIgOs39LFpZgrhXx4uox43LrMtLq2LyIiIqNb7YHHIiIi0t/GbDaYBRAGc69ERERk1OtJWQczey/wbmA9sMDdT+rWVhigWVXw6LPPO69tOhYIF267bMmBlNIPZVTVzs5XtgcDxwL1wuC4lADmmZ/vDGBedOoe7ct8+oa26Vh5gXunTi1cJkXY5zoDVUNlAuZT+lc2IDclEDqU8h5JuXbKXLcpSQCxcxf2ORb8G/anrtIiVZXpiAn7HNvPsO3Y+QzXS2mnzH7FrrdwH2LthoHGZQOYw/7Fznn4HikTZJxyLfVDOZIxm3WNLhmxGi/rQHbTcyiwl7s/YmY7dG1FRERGFJV1kH7Ri7IObwdOc/dH8tdW1tUHERERKTZmzGA+yWm8rAOwG/ASM7vazH5lZs8fYh2VdRAREZHSGi3r4O73mdlYYAKwL/B84Dwze4a7t5VuUFkHERGRZigmp4RIWQeAO4AL8puaa8xsA7A9kByVmRKYF34n/MDkzsoR45cubZsOg9Vi7ZQNEOvliL1h0O5WK1Z0LJMSqJcStBuOlBxbJgw0XvSxHTuX+dhdbdOxQOMyx6KuAPAmA1XvnjGjbXrS4sUdy6QcmzJB9SnByf028m5Kn8sEtqcEhacEapc5XrFt33DEG9qm9/7W2YXtxD7zUhIMit5Hmz32WKm4nDIB6rF9SPmsuvWg9hKK0xcs6FimzHs43FbqKNeKY6pHo2Udcj8CDgR+aWa7AVsAXcs6SGa0vAnCG5yY0XIsZLCUybAbifT+HJ5+OF6DGpPTeFkH4NvAt83sBuBR4KjwqyoRERGRTdWrsg5vrWu7IiIiMjyDGpOjEY9FRERkIKl2lYiIyCg3qDE5Vlc4zFBlHcxsb+DrwDiyUZGPd/drurU1/XUXt3Wy7JD+dYllpYSqiNIvk+WQKswCKRsgWcUQ9b/6xaMd8/Z/6RZt0ylZKymaLOuw5OCD26Zj2RwpAYi9zF5KUVemWcq5ii2zZtq0tumU8hAp2y9zHuo6NmWlHNNe9jnWvzBTtsm/B1V9XqRcSwsXzm30ruOL7z+ysdjYE844u7F960VZh88CH3f3i83sVfn0ATX2Q0REREahXpR1cGCbfLFtgeJ8YREREamNAo83QVDW4QPA6Wa2DPgccMoQ6zxe1uHepRc30U0REREZILXf5IRlHYB3ASe4+1TgBLIBAzu4+zx3n+Xus7ad9sq6uykiIjJqjRljjf00qRdlHY4C3p///gPgm0XthIFlKUGyZUaQXDt+fMe8MkPfVyUlqDLcdsp+h0OnQ+fQ6GUD7KoI8gyDjAFe+qM72qZ/8drO9coEhsbKfdQVyBgGGseupXAI+Ng1GZ6r2PlMuSbD4xVrp0y7Zd4PsUDylFIjKcpcg1W9z8sO8V9GeAxj5Q5SNNnnMsqU3CibtJGyTNG2mkxukE69KOtwF7A/cAXwUqBcqoOIiIhUYlBjcnpR1uHtwBl5NfK1wNwa+yAiIiKjVK/KOsysa7siIiIyPIP6JEdlHURERGQgqayDiIjIKDeoZR3qDDweB1wJPCnfzg/d/aNmtgtwLrAdsAg40t07x/HvImX4/jLR6ynt1pVJlRKBn7JPVS2TIpYRU3bI/CK/eO1ObdNzL7mnY5l5r9h+2O2WydSAao5hyrVU17UeWy+WabbVihWl2h6usmVEQlVd22Xf52UybUIpmV2LvnpTxzIzjy/uX1VlYJpS1bUdE2YTxo5F+J6ILXPdu9rLtcz88o+6bifWvzLZwJKmzq+rHgFe6u57AXsDrzCzfYHPAF90912BvwPH1dgHERERKTBmM2vsp9H9qqthzzyQT26e/zhZ2vgP8/lnAZERT0REREQ2Ta2Bx2a2WZ4+vhK4DPgLsMbd1+WL3EFWzyq27uNlHVaturLOboqIiIxqY8aMaeyn0f2qs3F3X+/uewM7AfsAuw9j3cfLOkycuF9tfRQREZHB1Eh2lbuvMbNfAi8ExpvZ2Pxpzk7AnXVs896pU9umexlgmiJlO3fPmNExb9LixW3Tq6dP71gmDAZedOoeHcvM/PQNbdOxoOIwOLTssPFlhPs17xWdy2w5/89t0w8d8cw6u9QmvN6gXHmI8BqsquxEyrUda7eugMiUdlPeE2VKeVTVTkqAcEoJmqI2YmYe3/n/xbA/sXIMKcG2ZYzE0gUp5yL87Iyd8zDQOJRyjPvhWGmcnGEys4lmNj7//cnAy4EbgV8Cr88XOwr4cV19EBGR5vXDH20RqPdJzhTgLDPbjOxm6jx3/6mZ/Qk418w+CfyeIaqQi4iIiGyKOss6XA88NzL/VrL4HBEREekDgzoYoMo6iIiIyEBSWQcREZFRblADj3tR1mE+MAt4DLgGeIe7DytKLSUTIswUWTZ7dscyO15zzXA2u0mKsixSMjViQ+yHx2L80qWFfdn9G3cXLlO2nEC4nykZWClBiinlIu4/uj1r7PY5nUMP7HL55W3TsSyyMlljZTKeYlIynlKUyeqJXYOhMhlPUC6rJ2xn7fjxHcuknKswMy92LYVtx45X2J/YPhRd/ylZSGWXCfsTWybM1quznEaZEhJNZt2lZOCWOecpn+2hWCacVKPOJzkbyzo8YGabA782s4uB+cBb82W+B7wN+FqN/RARkQal/GGX/qInOcPk7g50lHVw959tXMbMriEbK0dERESkUo2WdXD3q1te2xw4ErhkiHVV1kFERKQBY8ZYYz+N7ledjYdlHcysNWjiq8CV7v4/Q6yrsg4iIiKjkJmNN7MfmtlNZnajmb3QzCaY2WVmtiT/96lF7TRd1uEVwA1m9lFgIvCOMu2VGU1z6lVXdcwrM2R92eHLiwL8YgFt4bZSggRTSj+ktFPXftYp7F8YZAyw6NPtAbAzT+1df2PKBAzHhOulXBdhcHDq9lMCOIuunZQyIrG+hLEftx50UMcy0xcs6LptKBcgn9LnMOg5JTEg5X1W1TIxZcpDxK6dMgH8VY2UnNJOLJGjqJ3lM2d2LDNl0aK26fC9Fr7P+lUfxuScAVzi7q83sy2ALYFTgcvd/TQzOxk4Gfhwt0aaLutwk5m9DfhH4M3uvqGu7YuISG80WddOBo+ZbQvsR14Rwd0fdfc1wKHAWfliZwGvLWqrF2Ud1gF/BX5jZgAXuPsnauyHiIiIdNFkrIyZzQXmtsya5+7zWqZ3AVYB3zGzvYBFwPuBSe6+PF9mBTCpaFu9KOugAQhFRERGqfyGZl6XRcYCzwPe6+5Xm9kZZF9NtbbhZuZF29INh4iIyCjXZzE5dwB3tGRk/5DsJuduM5vi7svNbApZ5nZXql0lIiIifcPdVwDLzOyZ+aw5wJ+Ai4Cj8nlHAT8uasuyMfuqN1RZh5bXvwwc6+5bFbU1a9a8tk6mZKCkDOsdSimtUFaZ/hS1EWsnZZkw4wM6sz7KZjlUlR0USsn4CKXsw4azb+yYN+bIZ7VN3zZnTscyYeZW2Wy0IinbrlNV57PM9V/Fe6aslM+ClHOect1WtV/htlIynmKlMlLOccp1UaasQxmxz7OUMjBNKft3ZeHCuY0+Wrnw6++r52Yg4nXv/HLhvpnZ3sA3gS2AW4FjyON7gZ3JYnsPd/e/dWun8bIO7v5bM5sFFOa3i4jIyKOyDrKp3P06sjqXoc7/+XVR29dVnuko65BnW50OnFTXtkVERER6UdbhPcBFLWlgQ62rsg4iIiINGLOZNfbT6H7V2XikrMN+wBuA/0hYV2UdREREpLSmyzocCOwK3JIPBLilmd3i7rt2W7+ovEHKUNspygbGpQRI/vErV7dN7zn3ecPeTlVDuacE5ZUNlksJCC4jJegzlLIPYZAxwK3fvK5teruPT+1YJpRy3MNgzdiosGE7O1x/fWG7MSmlFlKUGbk2dm4emDx52P2pKiC3TABzVYHtdQXbpmwrJci5bMBwmTI1oaqC9WOfZ70MWg/FgrvDeWXfn1VqunBmU5ou67DI3Se7+zR3nwY8VHSDIyIiIlJG42UdatyeiIiIlDBms8EcNq/xsg7BMoVj5IiIiIiUobIOIiIio1yflXWozEDc5MSCM8PAs9jon00GBT7zxP2DOcPfdhi4CtWNKBwqe2zqOqZlAgnLjKoL8PR3Pb9t+tS3LOhY5hMXzG6bTtnvMucqDNhNbaeqQMYyAcMxW61YUUV3SikTdBo77v0QHNpNFaNKQ7mg4TLHuK6RnqG+kepjivYjdu33MhB6tKntJmeosg6WpVV9kiyVfD3wNXf/cl39EBERke4GNbuq8bIOwLOAqcDu7r7BzHaosQ8iIiIyStUZeOxAR1kH4F3AW9x9Q75cYal0ERERqc+gxuT0oqzDPwBvzEs2XGxmnSVkaS/rcM/dV9TZTRERERlATZd12IMsRmetu88CvgF8e4h1Hy/rsP2kA+rspoiIyKg2Zow19tOkpss6vAK4A7ggf+lC4DtF6xdFopeNVK9q6O8yw7uHGQFlh2AP9+FPhx/escyM+fO7rhPbVlVDrteZ+VCFlH36zFl7dsz7f1e015d93wHDH/Ip5RhPWrx42O2WFetPSkZRFVk9KdmPTV63VWVSVZXxVFU2YVXlPsJtlSmtE/tsCMU+K8Is05Rsw9i2ws/c2PEL5909Y0bHMkXvUWVS9Vad2VUTgcfyG5yNZR0+A/yIrIbVbcD+wM119UFERJpXpnag9NagxuQ0XtbBzH4NzDezE8gCk99WYx9ERERklGq8rIO7rwEOrmu7IiIiIjAgIx6LiIhIeRoMcJi6jHg8Bzid7CusB4Cj3f2WqrdfxXDcsXZiAZIpAWyhqoJvw22FQcYxsX0I26kqWK7MEPGx/pWRsg9lAgmhM9B40fte27HMzC//qGsbdfavjFh/woDN2LkpU66iqjIAVSUPlJESVF+mLEbZfVh8xBFt088+77yOZcISAymlYsJgZei8DsrE4JT9DAz7t3p65ygk45cuLdxWeP5SjnvsvVd0DS45uPOLi+kLOkvFSD16MeLx14BD3f1GMzse+Ffg6Br7ISIiIl0o8HiYuox47MA2+fxtgbvq6oOIiIiMXrXG5OSZVYuAXYGvuPvVZvY24Gdm9jBwH7DvEOvOBeYC7LzzEUycuF+dXRURERm1BjUmpxcjHp8AvMrddyIbCPALQ6z7+IjHusERERGR4Wp6xONXAnvlNawAvg9c0kQfREREJE4xOcPUZcTjbc1sN3e/OZ93Yx3bL5M5lTKsd9lthcoMTR5TJossJdMgpcxELOsizB4pU9ahzrIP4fEqky0Bnecvlkn1ogvbj8X/vq7zeBWps6xDyn6G5yJ2XVShbOZUOC92vYWZQGXfa6HYdRr2OcxmSpHynollRaVkVob9ix2LZbNnt01PveqqwnZTPgtSlMlm3W7JkmFvB8p9zsQypZ5x6aVd1yl6HdJKXEg5vRjx+O3A+Wa2Afg7cGyNfRARkYZVVftLmqMnOcPUZcTjC8kKc4qIiIjURiMei4iIjHLKrhIREREZQWp/kpPH5CwE7nT3Q8xsF+BcYDuyMXSOdPdHu7VR19DtYTuxIN6UbYVBdynfR5cJfqyqVEXMyj33bJve8ZprCtdJ2c+U4L6wzymlDMoENMe2VXaZlPMXBhq/5ifL26YvevWUwjZiyrwfUq6dFCklQcq+j6poNxYYXVewdEzRfqYErqb0N+X6K3sewkDjlHbCAOv1m29euK2UazJ2LFJKZZS53lOOTexzsWi9lMSFqoLhN8WgxuQ08STn/bRnUH0G+KK770oWeHxcA30QEZGGNFlDTKSbWm9yzGwn4GDgm/m0AS8FfpgvchbQWdlQREREGjNmzJjGfhrdr5rb/xJwErAhn94OWOPu6/LpO4CnxVY0s7lmttDMFt5z9xU1d1NEREQGTW03OWZ2CLDS3ReVWb+1rMP2kw6otnMiIiIy8OoMPJ4NvMbMXgWMI6s8fgYw3szG5k9zdgLurLEPIiIiUmBQA4/N3evfiNkBwIfy7KofAOe7+7lm9nXgenf/arf1Z82aV0sny2SpxIZTryIyvqqMlJgwE6nOsgkpijIfyg7x36Qq+nPUAZ3D5Z91xezIksPrS2p/wiy2OktIhMocv5SMupRjUed7rZ+U/ayq6r3WVBZgrN0yGa+9FNvv3//mmEbvOv7w64/XfzOQ2+vFH21s33oxGOCHgXPN7JPA74Fv9aAPIiJSk6qGLJDmDOpggE1VIb8CuCL//VZgnya2KyIiIqOXyjqIiIiMcoMak6OyDiIiIjKQelHWYT4wC3gMuAZ4h7sPK7KtqoC2MuvFhrWvQlWBj7H9LBNoXLaERJlz02TQZ1VB2FX0ORZkvOirN7VN7/itIzqWmbKofVSGsn0JA43rLBtSRTBrnWU7wkDVcWvWFG6/riSEFLFztWbatLbp7ZYsKVwvdixS2ilS9joJP19jZR3CtjecfWPHMtseWWrzlSjzPuqHwHc9ySkvLOswH9gdmAE8GXhbA30QERGRUabRsg4A7v4zz5E9ydmpzj6IiIhId2PGWGM/je5Xze2HZR0eZ2abA0cCl8RWbC3rsGrVlfX2UkRERAZOL8s6fBW40t3/J/Zia1mHiRP3q6ubIiIio96YzayxnyY1WtbBzP7L3d9qZh8FJgLvqHH7lQgDwvp95MyqRo4t23aKMiPQVrXtqoKw6woU3Pv97aMQH/eS73YscybPqWXbsX0Kr5U6g/x7KXxfpwSPxoKTmxI7nlutWDHsdmL7OX7p0mGvF+tPGJgdC8oO2ynz/rz/1IM65m21efuxaPL66/drfbSp7SbH3U8BToG2sg5vNbO3Af8IzHH3jq+xRPQhITKyxTLPQhoVub8M6ojHvRgn5+vAJOA3ZnadmX2kB30QERGRAdeLsg4aZVlERKSPaJwcERERkRFENzkiIiIykBov69Ay/8vAse6+Vd19gPJZRynDjIfz6izREErZVjhkfUqGWEqmTSwT4p7dd2+bDksHQGdQYniMY/0L14llttQVsFy23fB4hfuZkp1z5i86M6lWT5/eNl12+P6idaDz2i6bORUei7DdWLmU8PqqqjRLVeVIUkq8pGQShu2kZBilfDakZFHG2rn1oPZspQk339yxTFEGVkomVYqUdcpklUFa9lfKZ17RtTNSsg37LfDYzJYC9wPrgXXuPsvMJgDfB6YBS4HD3f3v3drpRVkHzGwW8NQGti0iIiIj04Huvre7z8qnTwYud/fpwOX5dFeNl3XIn+ycTjYSsoiIiPTYCBkM8FDgrPz3s4DXFu7XpmwtQaysw3uAi9x9ebcVVdZBRERk8LT+fc9/5kYWc+BSM1vU8vqklnuHFWTD0XRVW0xOa1mHfDBAzGxH4A3AAUXru/s8YB7ArFnzvK5+ioiIjHZNxuS0/n3v4sXufqeZ7QBcZmY3BW24mRXeGzRa1gH4I/AIcIuZAWxpZre4+67dGgoDt8oE/MWC8MLgxwcmT+5YJgzqrCrwskwAW0zKOimBeSntpAQthgGJsWXWjh/fdZ2YcJ1YkGCKlGDDqoTHq8yQ9bHrNjxei07do2OZmZ++oW065TykBHOnBILG3kfhNVjVtZ0i/LyItVOm7VgSQtHxim0nPA8p7ab0p2zw9M5Xtj85T+lPSpBzSoB1mXOeMrpy7NqO7VeZZYr62I9BxiOBu9+Z/7vSzC4E9gHuNrMp7r7czKYAK4vaqe3rKnc/xd13cvdpwJuAX7j7U919srtPy+c/VHSDM1poiHMRGRRlbuilt/opJsfMnmJmW2/8HTgIuAG4CDgqX+wo4MdFbWn0YREREeknk4AL8298xgLfc/dLzOx3wHlmdhzwV+DwooYaL+sQzG9kjBwREREZ2pjN+mdsYHe/FdgrMn81MGc4bfXPXomIiIhUSF9XiYiIjHL9NuJxVYa8yTGz/yDLU49y9/elbCAs62DZl2yfJEslXw98zd2/3K2NMDo9JbI/XCYlkj82PH6KlIyAG05pnzfz1PbXy0bgVzWEeJj5cPt++3Uss8vllxe2m7KtMse5zDqxY5GSTbXk4IPbpsNsE+i8nmIZHuG2wnIMsayylOs2FGZSAdx42GFt0886//yOZcJzXiZrMSZWlqNMYH1VWSkpGTKhlPdRSv9S3p9lMvzKlukoc32lSGmnqhIhKSVewoy1lM+qOsuGhJbPnNk2PWXRosJ1pJxuT3IWVrSNjWUdtsmnjwamAru7+4Y8B15ERER6ZBNHIu5bQ97kuPtZrdNmtqW7PzScxlvKOnwK+GA++13AW9x9Q76dwjx3ERERkeEqDDw2sxea2Z+Am/Lpvczsq4ntx8o6/APwxnwo54vNbHpsRZV1EBERacaYMdbYV4KkXAAAIABJREFUT6P7lbDMl4B/BFYDuPsfgM6AjUBrWYfgpScBa/Oqot8Avh1b393nufssd581cWLh5kRERETamHv30g9mdrW7v8DMfu/uz83n/cHdO3LYg/X+HTgSWMcTZR0uAGYBr3T32/Ig5DXuvm23tqqoXVU2qGwQVDU8voxcX77igY557zugmmGqPvLf7YGfn3jZ+CGWHFpKWYCY2+a0D5kRBsf32kh871XV517uey+3HSYYlE1oWbhwbqOPPP6+8ozGakQ+dYf3N7ZvKSnky8zsRYCb2eY8EUjclbufApwCkBfo/JC7v9XMTgMOBG4D9gduLtl3ERHpQypTM/KMusDjFu8EzgCeBtwF/Bx49yZs8zRgvpmdADwAvG0T2hIRERGJKrzJcfd7gCM2ZSOtZR3cfQ1ZxpWIiIj0gUEdDDAlu+oZZvYTM1tlZivN7Mdm9owmOiciIiJSVsrXVd8DvgK8Lp9+E3AO8IK6OiUiIiLNGdSYnJTsquvdfc9gXmF2VcuyYVmHOcDpZE+RHgCOdvdburVRRXZVipQMrLtnzOhYZtLixY30p9eZGf3WH9k0Z/7nc9qmj37PH3vUk7T33mjOkByJwvILZcpXlHXv1Klt07FSI/2u6eyqB+/7z8ayq56yzXt6n11lZhPyXy82s5OBc8lqWb0R+NkwthGWdfgacKi732hmxwP/SlbqQUREBkCslpv0t0GNyen2ddUispuajXv+jpbXnDw9vJshyjo4T9zwbEuWsSUiIiJSqW61q3apoP2NZR1aR/p6G/AzM3sYuA/YN7aimc0F5gLsvPMRaNRjERGRegxqTE5KWQfMbA8zO9zM/nnjT8I6Q5V1OAF4lbvvBHwH+EJsfZV1EBERkU1RmF1lZh8FDgCeTRaL80rg18B3C1adDbzGzF5FXtbBzBYAu7v71fky3wcuGW6nwyHgU4Z/T5ESxFhXkHFMmaDKlJFGU9pNCfIMg/sgLcCvTACzgp6rFwYav2HWFR3L/GDhAY30JeV8piwTiwVpMuC1SMr7KiwLAJ2lAWLtrBs3rm069rm4bPbstumpV101dGc3Qa+PeV2fQzF1/T1q2mh+kvN6YA6wwt2PAfYii6Xpyt1Pcfed3H0aWdr5L4BDgW3NbLd8sZeTUCJCREREZLhSxsl52N03mNk6M9sGWAl0/hc+gbuvM7O3A+eb2Qbg78CxZdoSERGRaozG7KqNFprZeOAbZBlXDwC/Gc5GgrIOFwIXDquXIiIiIsOUUrvq+PzXr5vZJcA27n59vd0SERGRpgxqTE63wQCf1+01d7+2ni6JiIiIbLpuT3I+3+U1B15a1LiZLQXuB9YD69x9Vj6S8veBacBS4HB3/3tif4Fy0ev9nnWRIozih85j0WTW0VYrVnTM6+VQ7nVZPnNmx7wpi8KREeoRKyMSHvfYMS6TORLLpHrGebe2TS854pkdy/RTdlzZY1EmK7FMmYmU/Q4zqVLbWXzEEW3Tzzr//I5lwmyq2D6sHT++bbqu93BKhtjYtWs7limTjRYTbivluogtE34Gh5/TsX0I9UO2qFtzT3KafGbUbTDAAyvaxoHufk/L9MnA5e5+Wl4u4mTgwxVtS0RERARIHAywYocCZ+W/nwW8tgd9EBERkQFX902OA5ea2aK8TAPAJHdfnv++ApgUW9HM5prZQjNbuGrVlTV3U0REZPRa597YT5NSUsg3xYvd/U4z2wG4zMxuan3R3d3Monvs7vOAeQCzZs1r9qiIiIjIiJdS1sGAI4BnuPsnzGxnYLK7X1O0rrvfmf+70swuBPYB7jazKe6+3MymkA0u2FUY/JUSGBeukxI8lxLYG1smDCyrK4gsJYCtTilBeHUFKaYECaYI9yFsFzrP+Q7Xd46YUEVpipiUwMvwGKcEwJYJkgW49fBntE2/7ie3dyxz4T/tXNhu0flLee+lSDnGVZU+qarsSlWfFzPmz2+bDj8nofOYPjB5cscyVZVEiJ3Tbn0Zqp0iK/fcs2NeSuBxyvVVpj8jtaxDk09Ytmgw8jjl66qvAi8E3pxP3w98pWglM3uKmW298XfgIOAG4CLgqHyxo4AfD7PPIiLSx4pucESakvJ11Qvc/Xlm9nsAd/+7mW2RsN4k4MLsQRBjge+5+yVm9jvgPDM7DvgrcHjJvouIiEgFmo6VaUrKTc5jZrYZWRAxZjYR2FC0krvfSlbMM5y/mqzgp4iIiEhtUm5yvkxWa2oHM/sUWVXyf621VyIiItKYQX2SY56wY2a2O9nTFyMbyO/GujvWqorsqjoD/ppSZ4CpDLZ7p07tmJcSYJrivDP3bZs+/OjfDruNstdtOCL0pMWLh73tOjU10rP0j6rO+cKFcxstJvW3dV9t7C5nwtjjG9u3lOyqnYGHgJ+0znP3zjSLznWX0lnW4XTg1cCjwF+AY9x9Tbnui4iIyKZa1+sO1CTl66oFZPE4BowDdgH+DDwncRthWYfLgFPcfZ2ZfQY4BZV1EBERkYoV3uS4e9vz4Lw6+fFlN+jul7ZM/pYsxkdERER6ZFBjcoZd1sHdrwVekLo4nWUdWh0LXBxbUWUdREREZFOkxOR8sGVyDPA84K7E9jvKOrj7lXm7/0L2NeD82Ioq6yAiItKMQX2SkxKT0zp05TqyGJ3zUxofoqzDlWZ2NHAIMMcT0ruWzZ7dNj31qqtSNt+m7LD7/ZQNUddQ8zL46iwJEmZTfe8bMzuWecvbF3Vto+x1m5JN1csMp5GY/VjX8VKmmfRC15ucfBDArd39Q8NtOC/lMMbd728p6/AJM3sFcBKwv7s/VKbTIiIiUp1R9yTHzMbmGVCzh1qmwFBlHW4BnkT29RXAb939nSW3ISIiIhLV7UnONWTxN9eZ2UXAD4AHN77o7hd0a7hLWYddy3VVREREJF1KTM44YDXwUp4YL8eBrjc5IiIiMjKMuq+ryGpVfRC4gSdubjZKOhqxEY9bXjsR+BwwMRgssMOO11zTNh0L3guVCTReN25cYTuPbL11xzJhUOcgBNTVWUKiqQDEfgvyLLPfsevtSfff37XdmKesWlXJtlLEgoyPOqA9WeCsK4q/Ba/rePVSbB/qej+UvXbCz8Gy/Qm330/nIebBiRM75qW8b0IpfzP6/VgMkm43OZsBW9F+c7PRcG75whGPMbOpZIHIhaUhRERkZIn9YZf+NhrLOix390/UtN0vkmVY/bim9kVERGSU6zbicRVVQjtGPDazQ4E73f0P3VZsHfH4nruvqKArIiIiErPOvbGfJnV7kjOngvY7RjwGTiX7qqqr1hGPn/vC7wxmRJSIiIjUZsibHHf/26Y2HhnxeH+yKuZ/yMfI2Qm41sz2cfcVm7o9ERERGb7RmF21SYYa8djdd2hZZikwqyi7qkx0/+rp09umx61Z07FMOC829H2YfdDLqPiy0f9lstHu2X33jmVShtBPyagoyj6IZbmF52rlnnt2LDNlUXtWT9lSHqFYf1KyVMpct2E7sWuyrqyV2LbCay72PkrZzzCbqsx1kqKq8hUpWUex6yJ8P5bZz/CzC2C7JUuG7mxuycEHt02HWanQeXzKvs+LlL0mmyz9EJ6b2GdpSn/unTq1bXrbZcvaplM+PyQur7qwkCzE5RAz2wU4F9gOWAQc6e6Pdmtj2FXIh2ES8Gsz+wPZwIIL3P2SGrcnIiIiJfRpTM77gRtbpj8DfDEfVPjvwHFFDdR2k+Put7r7XvnPc9z9U5FlphU9xREREZHRxcx2Ag4GvplPG9mgxD/MFzkLeG1RO7V9XSUiIiIjQ5MxOXm29dyWWfPyZKNWXyIbambj94rbAWvcfeOQPncATyvalm5yREREpDGt2dMxZnYIsNLdF5nZAZuyrVpvcoYq62Bm7wXenc9f4O4nVb3tlEC9UErJhqpKSpQJHk0JMg6D4KAzEC5FSvBhbD/LBAqG+x47FuFxn3DzzYXLVDWEfpND8aeUzqgraDG2DynXXJng5HAf5s/r3M4RczsD7UN3z5jRNl1F0CykXdsp5yH8/Aj7C519LvPZBfCMSy9tm47tQ/i5k3K8Uj7zYpoqoVI26L+q99FWK9oTg1MCmvtRn414PBt4jZm9iqx+5jbAGcB4MxubP83ZCbizqKEmnuS0lXUwswOBQ4G93P2RfAwdEREZEINQv096x91PAU4ByJ/kfMjdjzCzHwCvJ8uwOoqEqgm9+LrqXcBp7v4IZGPo9KAPIiIikhsh4+R8GDjXzD4J/B74VtEKdaaQQ6SsA7Ab8BIzu9rMfmVmz4+t2FrWYdWqK2vupoiIiPQbd7/C3Q/Jf7/V3fdx913d/Q0bH5Z0U/eTnFhZh7HABGBf4PnAeWb2DPf228jWwKRZs+aNiFtMERER6R+13uREyjrsQ5b2dUF+U3ONmW0AtgdGRnSWiIjIgBkhX1cNW+NlHYAHgAOBX5rZbsAWQNcBAVd+pz0DYIdjOjMUqtDkUNtltpWSRVAmk6pKdZUcCPezbGBjXVlQMVVkYPVbAGfsGgyzRxZ9tjNPY49/e2rbdHhdxDKpPnhpe/m8Lxw0oWOZqrKpUlRxPstmM5XNAgylvB+ryhwM23lg8uS26dhnVb+9R1KyHfvtPSrt6nySMwm4MC/EORb4nrtfYmZbAN82sxuAR4Gjwq+qRERk5Cqbdi69oyc5w+TutwJ7ReY/Cry1ru2KiIiIgEY8FhERGfUG9UlO3SnkIiIiIj3ReFkHM9sb+DrZUM3rgOPd/Zpu7dQVaNxLYYDummnTOpYJgxTLBLumrldVwHAVAdUjIZAvpc9F+7Hk4IM75k1fsGDTOjYMYTmGlOHnU87NzJNiHyvDvy7CQOPvf2d1xzJvPGa7tulYaZa6gt/7XdljUWY/w2sJOq+nlKSIlDIdKe+9cN/DoGcoVz6jqgDwftRnZR0q03hZB+CzwMfd/eK8LsVngQMa6IeIiDRgpNRrksHXi5gcJyu2BbAtcFcP+iAiIiI5xeSUEyvr8AHgdDNbBnyOvAhXSGUdREREZFP0oqzD64ET3P18MzucrMDWy8IVVdZBRESkGYP6JMeaGofPzD5GNtrx/wXGu7tbNlLgve6+Tbd1y9zklAlmHYTRLAdhH8pKOef/v717D5ejKvO+/70hhoMMRCAJAcIbGBJAEwgm8KIOB4k6CI7BERkPICcniidgGBXwGUQdfVBUxPFVJyKHGfGAgsgzUQmDIsorxAQCCSBEJcohB0CDAxgiyf38UbWhe1V119pFVXXv3r/Pde0ru6pr1VpVvbp2pfpe6+73IOe62hcTGFq27/zglvaQxSMPGv7/ne554xsz6/a56qrCcktPPLFtecYVV2S2GS3vcczggVWzZrUtT1qyJLNN2L5nttwys01ds8OXCYbPEzMZYV394vHJkzPrysxEv3jxPKuiPbG+8YfzG7vLeev2ZzV2bL1I6/AwcChwI3A4MPwQdxGREUgzAY8+vU61E2tQn+T0Iq3DE8BFZjYGWA/M67IPERERkVJ6kdbh58CsbAkRERHphUF9kqMZj0VERGQg6SZHREREBlKto6vMbBxwMTCdZM6ck4F7gW8DU4CVwLHu/sdu+9EQ8pGrydFe/T5yqt/FjMDqd/M/89eZdfP++Tc9aEkiHPE0Zv36zDZhP+11uo9+N1o+502Prpr/6Ccb+zs7b8dzGju2up/kXAT8yN33JonPuQc4C7jB3acCN6TLIiIiIpWqcwj5dsAhwIkA7r4B2GBmc3kuV9XlJEPJP1RXO0RERKQ7BR4P3+7AI8ClZna7mV2czpcz0d1XpdusJhlqnqG0DiIiIvJ81HmTMwZ4KfBld98feJLgqylPAoJybx/dfb67z3b32ePHH1JjM0VEREa3Z9wb+2lSnZMBPgg86O63psvfJbnJWWNmk9x9lZlNAtYW7aiqqb6bkjet9zarV7cth8FyZQN0Y6ZyLyOvPY/uvXfb8o6/+lVmm7DNZVIrlD0XTQU0l62r3wImw36aNzNrmenxezmlfl6Q8b+886625Y//+0sy29T13pT5PMYEGVfVJ8PrR566UjjEGIRg+BiaCbs+dU4GuNrMHjCzvdz9XmAOcHf6cwJwfvrv9+tqg4iIiBQb1JicurOQvw+4wszGAr8FTiL5iuxKMzsF+B1wbM1tEBERkVGo1pscd18KzM55aU6d9YqIiEi8QX2SoxmPRUREZCDV/XWViIiI9Llnet2AmtR6k9MhrcPfA38HbAB+A5zk7uu67eeZLbdsW65rJERVkfzhSCoobmPZY6hr5ENeeyYuW1bbvofzetPC/gfl2tjL0V9rZszIbBPzfpYZObV+3LjMujBVwczLLivcT1XOu2Rm2/Jtv9yQ2eaAA3rX58qMkOzl6C+IuwaH7/keCxcWlgk1OZIq7/q/5bquf5qAat6LfrvmDZJepHW4Hpju7vsC9wFn19wGERFpUF7+LelvgzpPTm03OS1pHb4GSVoHd1/n7gvdfejJ2C3ArnW1QUREREavXqR1aHUy8MO8wq1pHR5fmbuJiIiIVEBPcoava1oHM/swSazTFXmFW9M6bDfltTU2U0RERAZRL9I6YGYnAq8D5qT5q7rKm26+yKpZs9qWt7/vvsw2Y9avb1vOC3IrE+Q8CEFkZaeN72Xqgpi6y7Qv7Cexqki5ERMwHHMMZYPGw/MVE4Sd9zlqMtC4yAEHWGbdpv+8p215zeePy2wzacmSWtpT1eCB8Bg2O36fzDYx72e4Lib4NjR1wYK+S2NSpKrrf9E+yu5Hymk8rYOZHQF8EDjU3Z+qq34REekN5WIaeQZ1MsBepHX4JbAFcL2ZAdzi7u+quR0iIiIyyvQircOeddYpIiIiwzOoT3KU1kFEREQGktI6iIiIjHJK61BCXloHd/9F+tqZwGeA8e7+aLf9lIlwn3DnnYVlYoLjehkF38v2PXzggZl1k2++ubBcFaMP8vYRTrmeNxKirpFvZdM6FI2aiXl/N40dW1iuzj66dt9925bLjjAqGtVTV3oSiDs/fnL7cT78iVsz22y9dve25bz0FVWlPikjHBE2iex7FZ6LdVOmZLYZt3Jl1zIAj0+e3LYcjoBt8roZk44h772KSRlRxXE8uvfemXW97CejTd1PcobSOhyTBh9vDWBmk4HXAL+vuX4REREpoJicYeqU1iF9+UKSYeSDeVZFRESk5xpP62Bmc4GH3P2OboVb0zo8uubGGpspIiIyuimtw/DlpXU4DzgHOLeocGtahx0nHlZjM0VERGQQNZ3W4TySJzx3pBMB7grcZmYHuvvqTjsKg7/CQLO8acdjAsbC4MeYMuFU/VBv0GRTwsDQnRctqmQ/McLznreP8D3Oex/C9AtVpXX4/SGHZNZNXbCg635jlAmgb1r4GckL8gwDOGPORcxnpqoA65j9hOtmfTB7aVz2tvZ+sMfChaXaU4W8c7zZhg1tyzHvVRhkDMXXW4CnJkxoW85LvVNXgHy437y0KzGpRmLSrsSkwQiF7dnxV78qLNMPFJMzTOlNywNmtle6ag5wm7tPcPcp7j6F5Ebopd1ucEREZGRRWgfpF71I6yAiIiJ9ZFCf5PQirUPr61PqrF9ERERGL814LCIiMsppxuM+EjNTZUzAWBholvc9chjAFrOfqtQVaBlznCuOOiqzTRhsm6dMXcN9fWibou/9y9SdZ7ebbhp2mbJ15bU5nJV2hxUrSrUn9NjUqYX7jZnlOhRz3DHvTVWBqlW9Dy++8sq25aUf2CuzzaxPFl+bYuoq8xkJZ8d+YqedMtvkDdIIhQG5edfbov2Ufe9iBnaE+86bzbhoH5AfsByqqk8uObM9aH3WZ8tdU2T4ak3QaWbjzOy7ZvYrM7vHzF6Wrn9fuu4uM/t0nW0YbUZLwN9oOc68afcHUS/Tp/QbnYvnDMLI1fAGR5rVeFoHM3slMBfYz92fNrMJ3XchIiIideqnwGMz2xK4CdiC5D7lu+7+ETPbHfgWsAOwBDje3Td03lNv0jqcCpzv7k+n69fW1QYREREZcZ4GDnf3/YCZwBFmdhDwKeBCd98T+CNwStGOGk/rAEwDDjazW83sp2Z2QF7h1rQOjzyi7y9FRETq0k9pHTzxRLr4gvTHgcNJJhYGuBw4umhfTad1OCtdvz1wEPAB4EpLpz9u1ZrWYfx4facpIiIyCFofYqQ/83K22dzMlgJrgeuB3wDr3H1oINiDwC5FdTWd1uGsdP3V7u7AIjPbBOxI8tQnV5nRQqG8CPwyQW1lRnZBc8GEsSOTiuSNpAqPa+2++2a2CdMQlEmVkfe+hNvETOVetq5QzDZ1jQTKG/FU1XT54b7D0VYA26wunoy8zOeoyeDaMtP3x5j1yeWZdUvedXL7Nl+5pLCeMufiT5dl657wj8X9oqqUG3UpM+I15pqcJ+a83z9nTtvy7jfckNmmaCTvzC/8orCefhhI0WRMjrvPB+YXbLMRmGlm44DvAXuXqavptA53A9cArwQws2nAWODRutohIiIiI1May/sT4GXAODMbejizK/BQUflepHV4ErjEzJYDG4AT0qc6IiIi0gN9NrpqPPAXd19nZlsBryYJOv4JcAzJCKsTgO8X7atXaR2Oq7NeERERGbEmAZeb2eYk3zhd6e7/ZWZ3A98ys38Fbicdvd3NiJzxWERERKrTT2kd3P1OYP+c9b8FDhzOvmwkfFM0e/b8/m9kn4iZFl36V1VBxVXUHVt/L9sc6mXQf57LvviStuUT33tXYZknx4/PrCsTXJt3LsKg2JhrQ5P9oszAgF6/53X1/8WL52VGHdfpyF9/sLG/sz/Y89ONHVutT3LSqOiLgekkY9xPBv4MfAXYkuTm8d3uvqjOdoiISHP6YbSQDE8/xeRUqfG0DsCVwEfd/YdmdiTwaeCwmtshIiIio0xtNzktaR1OhCStA7DBzBzYNt1sO+DhutogIiIixQb1SU4v0jqcDlxgZg8AnwHOziustA4iIiLyfPQircOpwBnuPhk4gw5DwJTWQUREpBn9lLuqSr1I6/A3wGnpuu+QBCaPOo9Pnty2vN0DD1Sy37x0ByNN2aDFXo6iyROOkol5b3o5Eq7s+eun895PbYHsaKr3viE7QesXv1eYficjZkRR3rlYN2VK23JdKUvKvg/9niKkTP0x71XeqFipRi/SOjwMHJquOxzIJucREREReZ56kdbh+8BFaf6J9UAm+6iIiIg0Z1ADj3uR1uHnwKw66xURERFRWgcREZFRrp/SOlSpznly9gK+3bJqD+Bc4D/S9VOAlcCx7v7HbvsKA7fKTE0ek+6gbEqEMlORh4HGMcFpZYMNY1Q1NXl4LvKCbYv23etAwlDZfhFOxV/X9O9l+876cePalmNSB8TUlZeW4C2X3Nm2fO3fTSqsqy5LzsyO1pz12WamqcgLMl5x1FFty1MXLCjcT9m+s8OK4hDIuvrpY1Onti2PW7kys01dn4mY/YaDQQC2Wb162PsJxZRR6p361Bl4fK+7z3T3mSRfTz0FfI9khNUN7j4VuCFdFhERkR4Z1CHkdc6T02oO8Bt3/x0wF7g8XX85cHRDbRAREZFRpKmYnDcD30x/n+juq9LfVwMT8wqY2TzSkVe77v52dpx4WN1tFBERGZUGdXRV7U9y0uHjryeZ+K+NuztJdvKM1hmPdYMjIiIiw9XEk5zXAre5+5p0eY2ZTXL3VWY2CVjbQBtERESkg0F9kmNe84GZ2beA69z90nT5AuAxdz/fzM4Ctnf3D3bbx/4vu7Stkf02+qaMXo60qcqaGTMy6yYuW1ZLXaG8kRBVpcYYrfJGRcWMuOonS96fDfGb9YVrCsvV9XkMhSOMIDvi6eOn3ZvZ5l8u2iuzroymjrOsJttXZlRsnrravHjxPKtkR5FefNdpjd3l3P2Sixo7tlqf5KRZx18NvLNl9fnAlWZ2CvA74Ng62yAiIiLdDeqTnLpnPH4S2CFY9xjJaCsRERGR2mjGYxERkVFuUGc8bmqeHBEREZFG9SKtwy7A3wEbgN8AJ7n7uqrrb3IK8TLC1BR56mpfXnBy6LeveU1mXTjdfF6QcZkgvDCAect12e4QTq+ep0wgYVWB2nXtJ6+fhKkyqkrBkXfew23y2lMmHURMSokyfWn6pTcUbpPniZ12alvOOxdl+1OrmLQK53xpembd5hSn6QiPIS8QP9wmTy8D+MukGskT03fyUs6EVs1qzyU9acmSzDZFbW5yMMjzMagxOb1I63A9MN3d9wXuA86uqw0iIiIyejWe1sHdF7r70Nd/twC7NtQGERERGUWauslpTevQ6mTgh3kFzGyemS02s8WPrrmxzraJiIiMakrQWVKntA5m9mGSgO4r8soprYOIiIg8H71I64CZnQi8DpjjdU+5LCIiIl0NauBxEzc5b6HlqyozOwL4IHCouz9VZofh9PN5EfjhKIZwlAhkI9ybHGkTjtSIGfEUI6a9MdvssXBh4TZVpQEokwoiHNEA5UaFVDXKIWY/MSM+wnV1jsII++D9c7JzdO68aFHbctnRLuFx5I1eKioTo+zU/FWNKCoapVXV5/P4ax7MrLv0ze0j3/KueeGIon5L2xEz4ilGzDl8dO+925bzrkObbdhQuJ8q2px3LZVq9CKtwxeBLYDrzQzgFnd/V53tEBERkc70JKeEDmkd9qyzThERERFQWgcREZFRT2kdREREREYQq2twU6e0Du7++fT1M4HPAOPd/dFu+5o9e/6wGxkGeT584IGZbcKgyjz9OP12E/KCFmOCOmNSKxQFWeed8zJT/Dep7Pkq0usp4Ze8/+i25ZlfXpDZpqn0I1XVk3dOw3QVecGkYf1VBd5X5ZKf/blt+Y1vnZbZJgywfnzy5Mw2YbB0Ff24TjGfkbKfozJ9sKp+u3jxPCtVsKStbz+1saCcp/b/cmPHVtvXVe5+LzATwMw2Bx4iSeuAmU0GXgP8vq76RUREZHRrKibn2bQO6fKFJMPIv99us9y3AAAgAElEQVRQ/SIiItLBoI6uajytg5nNBR5y9zu6FWhN6/DIIzc10UYREREZILU/yWlJ63C2mW0NnEPyVVVX7j4fmA/lYnJEREQkzqA+yWk0rYOZzQB2B+5IJwLcFbjNzA5099V1NmLyzTeXKhcTSDvSxARMlp3FM2Ym56KgwF4H25ZRtl8UBSnWedwxM4dPv/SGtuUwQBfqa2Nds43HzDQdo+xnpK6A6pMP3qpt+cNvuCWzzRcf2KVtOW+m56pmX29KnZ+RKgKN6xqUIHEaTevg7suACUMvmNlKYHbR6CqRQTTS/pjI86f3XPqV+2D2zVpjclrSOlxdZz0iIiIiocbTOgSvT6mzfhERERm9lNZBRERktNs0ttctqIXSOoiIiMhAqu1JTre0Dmb2PuA9wEZggbt/sOr6q4q4LzNaqJdiIvljpp6v6phi9hMzBXu/iRkhUyZ9RZPCfjAIo0CaPKdl0pz8YVo21cKkJUsqaU/Y3774vV0y25z7ro1tyx/7yuaZbcp8Hps872VGp5VtXxXHPmI+QwP6JKfxtA5m9kpgLrCfuz9tZhO67EZERESklMbTOpjZBcD57v40gLuvbagNIiIikmdAn+Q0ntYBmAYcbGa3mtlPzeyAvAJK6yAiIiLPR6NpHVrq3B44CDgAuNLM9nBvn1NaaR1EREQaMqBPchpN65AuPwhcnd7ULDKzTcCOQMdo2DKpFcKAsfXjxmW2iQm8DMvlTYMeo66p3EN556aq1BRhGoAt160rLJN3nGF7wlQBMYHRMUGydQaJ1xns2Ct5/eLxyZPblvM+RxOXLSvcdxWf4TJT7EO2f+XVXdfnM0z9UFWQcZ7wOPOOIQw0/sEtz2S2+dsgPUTeex7z2a9LTGB0Ve9fTN8p0u+DVQZdo2kdUtcArwR+YmbTgLGA0jqIiIj0yoA+yelFWodLgD3MbDnwLeCE8KsqERERGZ3MbLKZ/cTM7jazu8zstHT99mZ2vZmtSP99UdG+Gk/r4O4bgOPqrFdERESGob+e5DwDnOnut5nZXwFLzOx64ETgBnc/38zOAs4CPtRtR5rxWERERPqGu69y99vS3/8HuAfYhWSOvcvTzS4Hji7al3JXiYiIjHYNPskxs3nAvJZV89MR1XnbTgH2B24FJrr7qvSl1cDEoroaT+sA3Ah8BdiS5JHUu919Ubd9lYloD6PXwyj5POFICMiOIghHGEF9aRJiRqTEjAqJGSGQN1opFJ6LMqkMAJ7Yaae25R1WrBj2PmL6REz76hzlUDQarexooapGl4Tty+vH4WjCbVavLtxvnqL3q9cjUKqqq8n+FSozujAcSQVw1Y1PtC2/8bBsXWX6bplzUbZfVJF2BaoZBVjmszjoWqeI6cbMtgGuAk539z+ZWes+3MwK43kbT+sAfBX4qLv/0MyOBD4NHFZXO0REpFkjIf+cBPorJgczewHJDc4V7j40eGmNmU1y91VmNgkozJjQVEzOs2kdAAe2TddvBzzcUBtERESkz1nyyOZrwD3u/rmWl64FTkh/PwH4ftG+morJaU3rcDpwnZl9huQm6+V5BVq/s9ttt7cxfvwhTbRTREREeusVwPHAMjNbmq47BzifJEvCKcDvgGOLdtSLtA6nAme4+1VmdizJ3dqrwnJK6yAiItKQPvq6yt1/DliHl+cMZ1+9SOtwAnBa+vt3gIsbaENUOoa1++6bWTfhzjvraE6UssG1oZig67KpHorqymvfuJUru+4jr0yYXqBseo2Y9lUlJiA9FAaA5wXDxwQMx8jbd6iqQNqqUosUyWtfFUH/kD1fZeqKSUcSo6qYl7yUDXOPaP+MnHTNrzPbXPrmPduWw2PY/C9/qaSNMWlh8o6hzPUhr66Yz1rRQICYlEJlP8NSrBdpHR4GDiUZZXU4sCKnjIiIjFAKPB6BNvbPk5wq1XqT05LW4Z0tq/8RuMjMxgDraR8rLyIiIlKJXqR1+Dkwq856RUREZBh8MJ/kKK2DiIiIDCSldRARERnt+mh0VZXqjsk5A3gHyQSAy4CTgEnAt0i+xloCHJ9mJq9UTOBbGBWfN5KqzBTdvZR33OFU/FVNoZ+3nzIjwsqM4MmrOyZ9RThCJiZNRziyC7KjN2K2iRG2L29kRpgeoqyY9yqsv+wokKK6YkbR1DUiK6+uPGEbY/pO2E+rOoa887Vmxoy25bxRjOH7mTeidLebbmpb/uqJ0zPb/Ors9uOYdU5x+0JlU5aEn5GYUYJ5YtoY81kr+ltTZhSjVKe2r6vMbBfg/cBsd58ObE4yKeCngAvdfU/gj8ApdbVBREREImwa29xPg+qOyRkDbJWOpNoaWEUybPy76etRqdJFREREhqvOBJ0Ppakbfg/8GVhI8vXUOnd/Jt3sQWCXvPJK6yAiItKQAY3JqfPrqhcBc4HdgZ2BFwJHxJZ39/nuPtvdZ+sGR0RERIarzsDjVwH3u/sjAGZ2NUnSrXFmNiZ9mrMr8FAdlccElZWZHr9s0G5RYFnM1N8x+60qMDrmOKuqK2Y/MUG8MfsJt4k5x3l1h0GeE5ctK9xPKOYc1znde0zf6eV082GQbkygb1V1xYipu6rA7RgxfTA8zt1vuKGwTF6bw0Djz374t5ltzvzEHoX7LhITkJ7XvsemTm1b/sO0aZltpi5YMOz6y1z/y1yXekJPcobt98BBZrZ1mjZ9DnA38BPgmHSbqFTpIiIyclRxgyNShTpjcm41s+8CtwHPALeTZBVfAHzLzP41Xfe1utogIiIiEQb0SU7daR0+AnwkWP1b4MA66xURERHRjMciIiKj3YA+yTF373UbCs2ePb//GylSg6pmp25S2dmo66q7iuDRKtU1WKDfLF48r215/5ddmtmmimPPm626ztmxm7J48Txrsj678s7G/s76sfs2dmy1TgZoZmeY2V1mttzMvmlmW5rZFWZ2b7ruEjPTfNYiMiqMlun7wxucPIN6cyf9pRdpHa4A9gZmAFuR5LYSERGRXtk4trmfBtUdkzOU1uEvJGkdHnb3hUMvmtkikrlyRERERCpV25Mcd38IGErrsAp4PLjBeQFwPPCjvPJmNs/MFpvZ4kceuSlvExEREamCj23up0GNpnUws+NaNvkScJO7/yyvvNI6iIiIyPPRdFqHlwNfN7OPAOOBd8bsaMl5O7ctT/9se+R8VZH0MUGBMdOMV9WemP3WVbc8f0WjaKpKnRGznzpTIoSWXpRNLzDztBk5W3Z3/5w5bcsxKQjyhOcnHOkFzQXB5tXTy9FWZa95RWbPnp/Z99IL2jP4zDp9QuF+Yvp2XdfbvH0P6kguYGCHkNd5k/NsWgeSLORzgMVm9g7gb4E57r6pxvpFRKQHRssoMul/vUjr8CTwO+AXSUorrnb3j9XVDhERESmgJznD1yGtg2ZZFhERkdrphkNERGS0a3j+mqaMyLQOVQXLhQHNs857uLBMXgDnmPXruy5XJeaYygbGLTmzfQTbrM9WM2y/THvKfp9fJkg3pkzZ9zzc92NTp7Yt77BiRWHdZdMUFJXJK1dnwGtTddUVSDsSNRkkW+a8rzjqqMw2eyxc2Lac916tmdEexL7N6tWFddcVZB+jbBqRxtM6XLquubQOJ41r7NhqfZJjZmeQzGjswDLgJHdfn772BeBkd9+mzjaIiIhIgQGNyelFWgfMbDbworrqFhEREWk8rYOZbQ5cALwVeEPN9YuIiEiRhmcibkov0jq8F7jW3Vd1K6+0DiIiIvJ81PYkJ0jrsA74jpm9HXgTcFhReXefTzKvTibwWERERCq0cfNet6AWtY2uMrM3AUe4+ynp8tuBjwJbAUNDUXYDfuvue3bbV5mbnJho/3B695gRMlWpaxRI3oiK8Lhi9huOBIK40UChMiMLRsIImSrSaTw+eXJm3XYPPFC6Tf2iqve8TBqMsiNZ6hr9FZNSIuw7ZY+hqO6y+2nSh064s235U5fvW2o/4bE/sdNOmW2a+qyNmNFV/97cUGt/pzV2bLV9XUVLWgdLpjaeA3zO3Xdy9ynuPgV4qugGR0RERKSMXqR1EBERkT6y2aYmU0k299VYL9I6tL6uOXJERESkFkrrICIiMsrZxo0N1jYgT3LyZjwGngb+lWSU1Ubgy+7+harrjgnsCoMAywbllQlCrSoA8IFXvKJtefLNN5faTxgcN27lysw2TR3n+nHjMuvCadl7HVQZc+xhYHE4/XzecTYZeFxF8HSevODaovcm5r0ru034PuSd47r6Trjfqo5zJCgTzB0GGp/73+sy27zp0ve0Lc+44orC/Vb1uVo1a1Zm3aQlS7qWKRP4LtWpcwj50IzHL3b3P5vZlSQzHhswGdjb3TeZ2YS62iAiIs3TH+2Rp9knOc1pfMZjkqc4b3X3TQDuvrbmNoiIiMgo1IsZj/8a+Id0NuMfmll2QhY047GIiEhTNtu0qbGfRo+rrh0HMx7vDLzQzI4DtgDWu/ts4KvAJXnl3X2+u89299njxx9SVzNFRERkQNX5ddWrgPvd/REAM7saeDnwIHB1us33gEtrbIOIiIgUUEzO8D074zHwZ5IZjxcDfwJeCdwPHArcV2MbuqpqNEnMfsJAvHBkzZbrsqMIQnlR+jsvWtS1nk7lQuG05+FIICh3vsq0p+y5qEpVaSXCER3haKaJy5YNr2HDEDOypar+39R+y8rry/2sqpGDeWXqSl9RVYqSsH0fe1V2BOKV7zynbfnjvCSzTVUjZ0N56SHKqOt9kKxezHi8FXBFOrz8CZIh5iIiMiAGIQfbaKMnOSV0mPH4aeCoOusVERER0YzHIiIio1zTo56aUmcWchEREZGe6UVah1cAF5DcYD0BnOjuv34+9cQE6sUEk+ZNRx8GUYbBo3nbxAjTFORpctbQur5DLzOledlp0MsE78XsJ+89L9O/YvrJk+PHty3/Ydq0zDYxqTsUyPicfu+Ddewjdt9hf8uTNxAg3E9V14+Yz9XH/7090Pjjp92b2eacL02vpD2hqQsWZNYVffbz3s9+/HwOakxOnfPkDKV1mO3u00kycr0Z+DLwNnefCXwD+F91tUFERERGr7q/rhpK6zCG59I6OLBt+vp26ToRERERAMzsEjNba2bLW9Ztb2bXm9mK9N8XFe2nF2kd3gH8wMweBI4Hzs8rr7QOIiIizbCNGxv7iXQZcESw7izgBnefCtyQLnfVi7QOZwBHuvuuJLMdfy6vvNI6iIiIjE7ufhPwh2D1XODy9PfLgaOL9tN0WodXAPu5+63pNt8GflRjG0RERKRAk0PIzWweMK9l1Xx3nx9RdKK7r0p/Xw1MLCrQi7QObzKzae5+H/Bq4J7nW1FMpHrMNnmjq0LrpkzJrIuZnr+o/rxRDjEjsML9rpkxI7NN2L6qRohVJea9CdtcVXtj6s6rq4pp2fNGZYTveUwfKOuxqVPblndYsaK2uqpQ12imsvspUybms9fkyMEx69dntgnXxaSHyFNFm2Pq/peL9sps8x9faB/tdeRXX53ZJua6HdOefhwp1e/SG5qYm5pu+3Az86LtepHW4UHgKjPbBPwROLmuNoiISPP0h3/kGSFDyNeY2SR3X2Vmk4C1RQV6kdbhe+mPiIiISKxrgRNIBiydAHy/qIDSOoiIiIxy/fYkx8y+CRwG7JiOxv4Iyc3NlWZ2CvA74Nii/egmR0RERPqKu7+lw0tzhrOfutM6nAb8I2DAV93982a2PcmoqinASuBYd//jcPZbRdAnlAtmjQlWy1MUqBcT9Bxj3MqVmXV1Be02aSS2OVRVv63L45MnZ9bVle6jLnUGJ5ep64FXvKJteedFi2ppS568467qWhDue8mZ2Wk+Zn12+PObPbHTTm3Lef0vvFbmHefr/+3gtuW3XPDDzDY/PnrXwvbUlRKkHylB5zCZ2XSSG5wDgf2A15nZnpSYzEdEREaOMjc4InWo80nOPsCt7v4UgJn9FPh7ksl8Dku3uRy4EfhQje0QERGRLvotJqcqdeauWg4cbGY7pHPlHAlMJnIyH6V1EBERkeejznly7jGzTwELgSeBpcDGYJuOk/m0ThY0e/b8wgl/REREpJxBfZJj7s3cP5jZJ0kmAjwNOKxlMp8b3T07ZWWL8CYnDJAcacGR0NsAybx67nnjG9uWp117bWabsFzeMYRBgTGzBa8fN65tOW+W33Am57IB4EVtgfreh6pmmm4ykLxMUGVd57TfgorL7Keq8xcz43De7OxVzWpdV7Btk337pz/e0LZ86OFjM9v0Mqh48eJ51lhlwPgz7mnsYcIjF+7T2LHVPbpqgruvNbPdSOJxDiJJ2DmsyXxGg5EakS/lDcKIMREZDIM6uqrueXKuMrMdgL8A73H3dWY27Ml8RERERIar7rQOB+ese4xhTuYjIiIiMlya8VhERGSUG9TA4zqHkIuIiIj0TK2jqzqkdbgA+DtgA/Ab4CR3X9dtPxNPu6utkeHU6FUF7T45fnxm3Zbrujat0vrr0uQIgZE2xXmTI3b6qW7I9ve8UW1lPDZ1ambdi85tH6232fH7FO6nrpE2Sz6/NrNu1ukTKtl3Gb0cLZp3zauqH6yaNatteeu17ee9quOs6nNU57kokjfyctlP3tLo6KpJp97e2OiqVV/ev7Fj60Vah+uB6e6+L3AfcHZdbRARkeaFNzgivdJ4Wgd3/3TLNrcAx9TYBhERESkwqEPIe5HWodXJQDY9LO1pHf68/Ds1NlNEREQGUc/SOpjZh4FngCs6lH82rUMYkyMiIiLVGdTRVXXPk/M14GvQltYBMzsReB0wxyMinyfffHPbchiklRdkFm4TphsAGLN+fdflvH3HTKfeb2KOIdwmLxAuPId5QXnhNnnvTRXByTEpEmKOs5dB2GXrDgN786bqjznHYVB9XuBlUZm8feemDggCjcO68j57VQUah+ciL8i4qiDsovOe1yfLBODePyc71Vj43mx/332ZbcJzGqZUgeyx533WwvcrPM5JS5ZkysQoc23Iu7aX+WzlvedLzjykbXnWZ7PJoov6Tsy1SrOf16fxtA5mdgTwQeDQoXgdERER6R09ySknL63DF4EtgOvNDOAWd39Xze0QERGRUaYXaR32rLNOERERGR6NrhIREREZQZS7SkREZJQb1JicxtM6tLx2JvAZYLy7P9ptP/u/7NK2RlY1IqZMJH9VU4hXlf6g3/YTihlZIMMz0lJnjGZ6r6q3ZsaMtuWJy5Y1VveVlx2UWXfsibfUUtfixfMaTesw5bibGpuqZeXXD2ns2Gp7khOkddgA/MjM/svdf21mk4HXAL+vq34RERGJM6hPcuqMyXk2rYO7PwP8lGQYOcCFJMPINcmfiIiI1KLxtA5mNhd4yN3v6Fa4Na3Do2turLGZIiIiMoiaTuuwBXAOyVdVReWfTesQxuSIiIhIdQZ1CHnTaR3WAEcDd6QTAe4K3GZmB7r76k77qStYr8x+q2pLv+2njLwg7Ef33rttOSYoMCY4M9xm2dveltlm5mWXFdZVRtlg87qCTut6z/PSOoSpAvqtv/V7IG9MupQygfhLTzwxsy6m/z8+uT1HcpmUEk0K2wvZa0qT/SIvyPgX/397LMvLXr55LXVLOY2ndXD3i1peXwnMLhpdJSIiIvUZ1MDjxtM61FyfiIiICNCDtA7B61PqrF9ERESKDeqTHKV1EBERkYGktA4iIiKj3KCOrupJWgczex/wHmAjsMDdP9htP7Nnzx92I0frdOp5I2Re+MgjtdQVM6qhybQOMe952J7RkmJixVFHZdZNXbCgkn3v+92Vbcu3v2VqZpuiz19evx2zfn3bct579djU9rrCMtDbEUT91t/6PQ1MVal2QnW1L2/ff7psedvytidOL1VX02kd9pq7oLGpWu79/lGDm9YBmAzMBfZz96fNbEJdbRAREZFigxqTU+fXVc+mdQAws6G0DrOB8939aQB3X1tjG0RERGSUajytAzAtXX+rmf3UzA7IK9ya1uGRR26qsZkiIiKjm23c2NhPk2q7yXH3e4ChtA4/IknrsJHk6dH2wEHAB4ArLZ3+OCg/391nu/vs8eMPqauZIiIiMqCaTuvwILA3cLUnEc+LzGwTsCNQT3TsKBNOw1/WPW98Y2bdPldd1bYcE8zX60DLUJn29DKdwJoZMzLrYlJlhPKCjMNg37wA9ZhA0DuPmdK+DcVpOcL9lA2O32HFilLlisSkuHj4wAMz20y++ea25Sb7fxjknBeEXZWY9C1F73lMwHCMOj+LZVLOhIHGP/3xhkyZQw8fW0HrqjWoo6saT+sAbAJeCfzEzKYBYwGldRARGRBV3cCIPF+Np3Uws0uAS8xsOcmoqxO8znHsIiIi0pVGV5WQl9bB3TcAx9VZr4iIiIjSOoiIiMhAGhFpHYq+331ip50y68oE4JadXfOZLbdsW84LNgxnnK1qttlQ2WNYP25c23LeTMUxYoJZh7uP2P1UFYB4/5w5bcu73VRuCoPwPIfnGIqPK6Yf572f4Wcib9bfmHMa9u08VQSUxvS3JoN4Y85NGGScJ+azVtWM5E2enzDQOCY4v66ZifP6Thh0XfbaEHMMRfs+9PCxmXK/Peo1bct1/T0YjkH9uqrWJzlmdpqZLTezu8zs9HTdTDO7xcyWpvPgZIcoiIwCdaXbkP6Vd6M7iEZLGp0YCsLurV6kdfg08FF3/6GZHZkuH1ZXO0RERKQ7DSEfvk5pHRzYNt1mO+DhGtsgIiIio1SdNznLgU+kQ8j/TJLWYTFwOnCdmX2G5Ouyl+cVNrN5wDyAXXd/OztOPKzGpoqIiIxeiskZpi5pHU4FznD3ycAZpDMi55R/Nq2DbnBERERkuKypefha0jr8b2Ccu3uas+pxd9+2W9n9X3Zp10Y2GeSWF8lf16iGsK46R0+Umb687DZFx1V2BNtIU3ZEShi8mjd9f8z5ielfZUbElB0dV1R32ZGD4UizbVavzmxTxSgaKE6tEDPCLqbuvHNctN9Y4ejCnRctymzT1OevzpQqZfpXXfv5p4V/yKx76/ZnZXI61qno72yVbv/FSY0dW92jqyak/w6ldfgGSQzOoekmhwP1JJ8RERGRUa0XaR3+EbjIzMYA60njbkRERKQ3NLqqhA5pHX4OzKqzXhERERm5zOwI4CJgc+Bidz+/zH5GxIzHIiIiUp9+Gl1lZpsD/x/wapJY3l+a2bXufvdw96XcVSIiItJPDgR+7e6/TZN6fwuYW2pP7j5ifoB5TZVrqsyg1qX2jZy6+r19Ohc6F72uq9/bN9J+SGJxF7f8zAteP4bkK6qh5eOBL5aqq9cHO8wTs7ipck2VGdS61L6RU1e/t0/nQuei13X1e/sG7afKmxx9XSUiIiL95CFgcsvyrum6YdNNjoiIiPSTXwJTzWx3MxsLvBm4tsyORtroqvkNlmuqzKDWpfaNnLr6vX1N1tXv7Wuyrn5vX5N19Xv7Boq7P2Nm7wWuIxlCfom731VmX42ldRARERFpkr6uEhERkYGkmxwREREZTL0eKjaMIWVHAPcCvwbOith+S2ARcAdwF/DRYdQ1Dvgu8CvgHuBlEWVOA5andZ3eZbtLgLXA8pZ1F6R13Ql8jyRLe1GZ80iizZemP0dGlJkJ3JJuvxg4MCgzGfgJcHd6HKel69+ULm8CZuccU265ltfPBBzYMaKub7cc00pgacz7CuwO3Jr2j28DYyPKfC1dd2f6fm8TUcaATwD3pX3j/ZHtOxy4Le0jlwNjcs7j5sDtwH+ly1eQ9Pnl6fv5gogylwH3t5zDmR36YVhuTtq+pcDPgT2D7VcCy4b6Tky/6FSuW7/oUldRv8h8ZoHtgetJkgBfD7wo5rMOfDztE0uBhcDOMdcH4H3puruAT0fWtR/wi/R4/w+wbcv2e7Uc81LgT8DpdLledClzHt2vF53KFV0zzkiPdznwTZL+/16Sz2Hmve1WruW1LwBPxJQBftbS5oeBa4quy5H9Iq9cUb/I/RvQrV90qKdjn9DP8H963oCoRiYX5N8AewBjSf6AvLigjJH+0QJeQPIH8KDI+i4H3pH+PpbgpiNn++lpR92aJJj7vwn+SLRsewjwUtpvPl5D+kcP+BTwqYgy5wH/3KVNeWUWAq9Nfz8SuDEoMwl4afr7X5H8IX8xsA/JRfBG8m9ycsuly5NJgsd+R/tNTscyLdt8Fjg35n0FrgTenK7/CnBqRJnWPyifo+XmuUuZk4D/ADZLX5sQ0b6XAw8A09L1HwNOyTmP/wR8g+duPI5M92ckF/VTI8pcBhwT0cfDcvcB+6S/vxu4LNh+Jdmbka79olO5bv2iW5mCfpH5zAKfHnpPgbMIPlddyrX2i/cDX4ko80qSz/0Wef2iS7lfAoem604GPt7hmDcHVgP/DwXXiw5lzqPL9aJLuY7XDGAXkhvqrdLlK4ETgf2BKV3e+9xy6e+zgf8kuMnpVqZlm6uAt7cs516Xi/pFl3Id+0WXMh37RZcyUX1CP3E/I+XrqmFP8eyJJ9LFF6Q/XlSRmW1HcoPwtXQ/G9x9XUGxfYBb3f0pd38G+Cnw9x3adRPwh2DdwrQcJP9r2rWoTJEOZRzYNv19O5L/+bSWWeXut6W//w/J/zZ3cfd73P3eLnXllktfvhD4IMG5LyiDmRlwLMkf99Zynd7Xw0n+lwzJH5Oji8q4+59a6tqqtY1d6jkV+Ji7b0q3WxvRvo3ABne/L11/PfDG1nJmtitwFHBxy75+kO7PSZ4O7VpUJkaHcl37Rp6iflEgt18UyesXXT6zc0n6AgR9olu5oX6RemFrG7vUdSpwvrs/na5v6xddyk0Dbko3y/SLFnOA37j774quF3llOrzeSWu5on4xBtjKzMaQ/LF+2N1vd/eVBXVkyqX5ii4g6RdRZYZeMLNtSa4B17Rs3+m63LVfdCrXrV90qatbv+hUJrZPSISRcpOzC8n/hIc8SMsfxE7MbHMzW0rytc317n5rRF27A/VpcUsAAAplSURBVI8Al5rZ7WZ2sZm9sKDMcuBgM9vBzLYm+R/P5IIynZwM/DBy2/ea2Z1mdomZvShi+9OBC8zsAeAzwNmdNjSzKST/I4s5Z7nlzGwu8JC73xFbpmX1wcAad1+Rs33b+0rylG9dy4U/0z869QUzu5Tkf617A/8WUeavgX8ws8Vm9kMzmxrRvkXAGDObnW5yDNn+8XmSi/umnP29gGTGzx9FlvlE2i8uNLMtwv11KPcO4Adm9mBaV5jx14GFZrbEzObl7LOTTLmIftGtrrx+0ekzO9HdV6XbrAYmBvvq+Fk3s0+kn5O3AedGlJlGcg241cx+amYHRNZ1F8/9h+1NdL5uvJnghj/V7XoRlom9XrSW63jNcPeH0nW/B1YBj7v7wi77LSr3XuDalvcspsyQo4EbghuRTtflon7R8XrepV90KtOtX3QqE9snJMJIuckpxd03uvtMkv/pHGhm0yOKjSH5mufL7r4/8CTJI81u9dxD8th4IckfoqUk/3sfFjP7MPAMSSxGkS+T/MGdSfKh/2xEmVOBM9x9Msn321/r0I5tSB79nh5cNLpqLUdyHOfQfiEYTl1vIf+innlfSW5QuurUF9z9JGBnkidJ/xBRZgtgvbvPBr5KEitT1L6XkPzhuNDMFgH/Q0v/MLPXAWvdfUmH5n8JuMndfxZR5uz0fBxAEnvwodYXu5Q7gyROY1fgUpKv71r9jbu/FHgt8B4zO6RDW0N55Yr6Rbe68vpF4Wc2fRoWPjXqWM7dP5x+Tq4g+eNbVGYMyfk+CPgAcGX61Kmo3MnAu81sCcnXthvCk5FOhvZ64DvB+o7Xi5wyUdeLnHIdrxnpjdJckhu4nYEXmtlxefsN6sgr93aSP+j/NowyrXVl+kXMdTmvX3Qr16lfdCnTsV90KVPYJ2QYvA++Myv6IQnQu65l+Wzg7GHu41wivpMGdgJWtiwfDCwYZl2fBN7d5fUptMTKpOtOJAk22zq2TNFr4XrgcZ6bG8mAP+WUeQFJrMQ/5bx2I51jL9rKATNInmSsTH+eIflf2E5FdZFcGNYAu0a+rx8AHuW5OIW2/hLTF0i+SvivojIkAYS7t5zDx4fb70hiKq5sWf7fJE+fVpL8z/Ip4Ovpax8heQS/WbCPjmVatjksPKYO5RaQfD0xtM1uwN1djum81mPq1i9yyv1LUb/oVFenfkGHzyxJ0PakdN0k4N6YcsE2u9H+GepU14+AV7as/w0wfph1TQMW5ZyDucDCYN2JdL9eZMq0vDaFzteStnJ0uWaQ3JR8rWX57cCXWpZXkh+Tk1fu/rQ/DvWLTSQhCoV1ATsCj9ESvNzh2D5JEm/WtV90KtetX3Spq2u/iKgnt0/oJ/5npDzJGfYUz2Y23szGpb9vBbya5A9UV+6+GnjAzPZKV80hGQHUlZlNSP/djeR71W8UlWkpewTJ1wevd/enIstMall8A8mjzyIPA4emvx9OMrqgdZ9G8j+1e9w9/J98t7Zkyrn7Mnef4O5T3H0KyR/Wl6bnt6iuVwG/cvcHc+rKe1/vIRmpdUy62QnA9wvK3Gtme7a05fW09I8u/ecakmBCSM7lUJxN13It/WMLkqcrXxkq4+5nu/uu6Xl6M/Bjdz/OzN4B/C3wFk9jgCLKTGo5pqMJ+kVeOZI/bNuZ2bR0s6FzOnRMLzSzvxr6neQmrbC/dSj3y4J+0a2u3H7R5TN7LUlfgKBPdCtn7V9BzqWlX3Sp69l+kZ7HsSQ33kV1DfWLzYD/RUu/aNH2lCLyehGWib1ehE9Eul0zfg8cZGZbp/1tDi39pou8cp9z951a+sVT7r5nZF3HkNzMrw8r6nBd7tovOpXr1i+61NW1X3SoJ6ZPSKxe32XF/pB8X3kfyZ3whyO235dkiOydJB/oc4dR10yS4ZJ3knTSzBDDnDI/I7nY3QHM6bLdN0keF/+F5AJ/Cslwywd4bihkOJojr8x/kgwxvJPkQzsposzfAEvSNt4KzArK/A3Jo9s7W9pyJMlF8UHgaZL/SV8XUy7YZiXto6s6liEZIfSu4byvJCPvFqXn8jukoxk6lSH5qvbm9BwuJ3n8vG1EPeNI/ue+jOR/0vtFtu8CkovyvXSfYuAwnhvx9AxJfx86P7l9OCjz45Zj+jotw+ILyr0hLXcHyZOZPVq22yNdPzQs/sMtZbr1i9xyBf2iY5mCfpH5zAI7ADeQ/GH+b2D7yHJXpefvTpIhvLtElBmbnu/lJEPxD4+s6zSS69p9JHFQFpR5IclTiu1a1hVdL/LKdL1edClXdM34KMkf++VpHVuQjDx6kKT/PkxLNulu5YLX84aQ55Yh6a9HxF6XI/tFXrmifpFXpmu/6FCma5/Qz/B+lNZBREREBtJI+bpKREREZFh0kyMiIiIDSTc5IiIiMpB0kyMiIiIDSTc5IiIiMpB0kyPSR8xso5ktNbPlZvaddLr3svu6zMyOSX+/2Mxe3GXbw8zs5SXqWGlmO8auD7Z5otvrOdufZ2b/PNw2isjopZsckf7yZ3ef6e7TSaZzf1fri5YkJxw2d3+Hu3eb1PIwkmzpIiIDQzc5Iv3rZ8Ce6VOWn5nZtSSz5G5uZheY2S8tSbj4TkhmOTazL5rZvWb238CEoR2Z2Y2WJgg1syPM7DYzu8PMbrAkQeq7gDPSp0gHpzM3X5XW8Usze0VadgczW2hmd5nZxSRT/XdlZtdYkmzzLgsSblqSRPSutB3j03V/bWY/Ssv8zMwKc5OJiOQp9b9CEalX+sTmtTyXefylwHR3vz+9UXjc3Q9I00TcbGYLSTK57wW8mCSz8t0ECUTTG4mvAoek+9re3f9gZl8hmWX2M+l23wAudPefp1POXwfsQ5JL6+fu/jEzO4pkJu0iJ6d1bAX80syucvfHSGbYXezuZ5jZuem+3wvMJ5nZeIWZ/b8kCUoPL3EaRWSU002OSH/ZysyWpr//jCS/18tJkvTdn65/DbDvULwNsB0wlSTJ6DfdfSPwsJn9OGf/B5FkNL8fwN3/0KEdrwJebM8l0t7Wkozxh5Dk2MHdF5jZHyOO6f1m9ob098lpWx8jScL47XT914Gr0zpeDnynpe4tIuoQEcnQTY5If/mzu89sXZH+sX+ydRXwPne/LtjuyArbsRlwkAdJD1tuPKKY2WEkN0wvc/enzOxGYMsOm3ta77rwHIiIlKGYHJGR5zrgVDN7ASTZjdOM3TcB/5DG7EziuWzprW4BDjGz3dOy26fr/wf4q5btFgLvG1ows6GbjpuAt6brXkuSZLKb7YA/pjc4e5M8SRqyGc9ljn8ryddgfwLuN7M3pXWYme1XUIeISC7d5IiMPBeTxNvcZmbLgX8neSr7PZLMyncD/0GSJb2Nuz8CzCP5augOnvu66P8AbxgKPCbJJD07DWy+m+dGeX2U5CbpLpKvrX5f0NYfAWPM7B6SjMq3tLz2JHBgegyHAx9L178NOCVt313A3IhzIiKSoSzkIiIiMpD0JEdEREQGkm5yREREZCDpJkdEREQGkm5yREREZCDpJkdEREQGkm5yREREZCDpJkdEREQG0v8FRj4PlmKVABUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-o1n4skl-77"
      },
      "source": [
        "##CLOSED AND OPEN WORLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lUUjCewmNLo"
      },
      "source": [
        "#Dataset divided into 2 halves, 50 for closed 50 for open (choose five different random division)\n",
        "#1) closed world\n",
        "#  1.1)without rejection -> standard incremental scenario (train and test using selected 10 classes) but with 50 classes\n",
        "#      iter = 0 -> 10 or 20 (he does so in BDOC) classes ? ask Dario\n",
        "#      next iters -> add 10 until 50\n",
        "#      result expected -> equal to incremental \n",
        "#\n",
        "#  1.2)with rejection -> same procedure of above but we implement a rejection technique that \n",
        "#      classify as unknown an object that doesn't belong to the classes seen in the training (for the alg follow BDOC)\n",
        "#      result expected -> idealistic the model should not reject any of the object because we've tested the model with classes seen in the training\n",
        "#\n",
        "#2) open world\n",
        "#    at each step -> test the model only on unknown samples (the second half of dataset)\n",
        "#    \n",
        "#    result expected -> idealist the model should reject all of the test objects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFHiLXyLmaWu"
      },
      "source": [
        "###download and dividing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OJxXhGZmdDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e7a53d-7f83-4496-eb27-7035dc17f1f9"
      },
      "source": [
        "# Import dataset and apply transformations \n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# Check datasets length \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZFLVy0Tmt3G"
      },
      "source": [
        "#closed and open world\n",
        "splits_of_10 = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "#first 5 splits to closed world\n",
        "closed_data = {k:splits_of_10[k] for k in range(5)}\n",
        "\n",
        "#last 5 to open (removing the train val splits)\n",
        "open = []\n",
        "for k in range(5,10):\n",
        "  for j in[\"train\", \"val\"]:\n",
        "    open += splits_of_10[k][j]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQKnRMfjEaA-"
      },
      "source": [
        "### Modified iCaRL for closed/open\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mj0FEmsldUx"
      },
      "source": [
        "#### Update representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fIvHXxhzwNI"
      },
      "source": [
        "def updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot, old_net, rejection=False, closed=True):\n",
        "  #exemplars_set_tot contiene tutti gli exemplars set ottenuti fino ad ora\n",
        "  #train_iter contiene tutti i dati (immagini + labels) delle classi nuove (s, .., t)\n",
        "  exemplars_subset = []\n",
        "  exemplars_indices = []\n",
        "  total_exemplars = []\n",
        "  labels_tot = []\n",
        "\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "    if (exemplar_set_class_k != []):\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "\n",
        "  if group_id > 1:\n",
        "    train_subset_total = torch.utils.data.ConcatDataset([train_subset, total_exemplars])\n",
        "  else:\n",
        "    train_subset_total = train_subset\n",
        "    \n",
        "  print(\"Len TOTAL train susbset: \", len(train_subset_total))\n",
        "  train_loader = torch.utils.data.DataLoader(train_subset_total, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  # train_loader è la concatenazione delle nuove classi con gli exemplar_sets calcolati fino a questo punto \n",
        "  print(\"training\")\n",
        "  train(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net, rejection = rejection, closed = closed)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgFy_00olj3h"
      },
      "source": [
        "#### Incremental train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAJtrUjIzqz-"
      },
      "source": [
        "import copy\n",
        "def incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_until_now, rejection=False, closed=True):\n",
        "  print(\"Starting the update representation\")\n",
        "  exemplar_indices = None\n",
        "  num_classes = 10\n",
        "  new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "  print(\"NEW CLASSES: \", new_classes_examined)\n",
        "\n",
        "  updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, rejection=rejection, closed=closed)\n",
        "                \n",
        "  iteration = group_id - 1\n",
        "  t = (num_classes * iteration) + num_classes # num_classes ricevute fino a questo momento \n",
        "  m = int(K/t) #casto ad intero ? per difetto o eccesso?\n",
        "  #s = num_classes * iteration\n",
        "\n",
        "  # REDUCING EXEMPLAR SET FOR EXISTING CLASSES\n",
        "  print(\"reducing exemplars for each class\")\n",
        "  print(total_classes_until_now)\n",
        "  for y in total_classes_until_now: #ci serve un set con tutte le classi fino ad ora viste\n",
        "    exemplar_y_new = reduceExemplarSet(m, exemplars_set_tot[y]) # valore associato alla chiave y che rappresenta il label della classe \n",
        "    print(\"REDUCED EXEMPLAR: \", len(exemplar_y_new))\n",
        "    exemplars_set_tot[y] = exemplar_y_new\n",
        "\n",
        "  \n",
        "  # CONSTRUCTION EXEMPLAR SET FOR NEW CLASSES\n",
        "  for y in new_classes_examined: # nuovi classi in arrivo di cui vogliamo costruire il set rappresentativo\n",
        "    images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data']\n",
        "    imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "    class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "    print(\"class train: \", class_train_subset)\n",
        "    print(\"Constructing exemplars of class\", y)\n",
        "    exemplars_set = constructExemplarSet(net, class_train_subset, m) # exemplar set è un set di indici\n",
        "    #devo recuperare dal dataset iniziale l'indice delle immagini dell'exemplar set creato \n",
        "    #for image in exemplars_set:\n",
        "     # exemplars_set = train_dataset.df.index[train_dataset.df['data'] == image].tolist()\n",
        "    exemplars_set_tot[y] = exemplars_set\n",
        "    print(\"exemplar set: \", exemplars_set)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmQvKfw7lmIr"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkpzD3lazndM"
      },
      "source": [
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes_till_now, group_id, old_net, num_epochs=NUM_EPOCHS, rejection=False, closed=True, threshold=THRESHOLD):    \n",
        "\n",
        "      \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    print(\"num classes till now: \", num_classes_till_now)\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    print(num_epochs)\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        if rejection == True:\n",
        "          n_sample_known = 0\n",
        "          n_sample_unknown = 0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes_till_now, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            #if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "            if rejection == True:\n",
        "              prediction_batch = outputs.data.cpu().numpy()\n",
        "              for i in range(len(prediction_batch)):\n",
        "                current_softmax = softmax(prediction_batch[i])\n",
        "                if max(current_softmax)>THRESHOLD:\n",
        "                  n_sample_known += 1\n",
        "                else:\n",
        "                  n_sample_unknown += 1\n",
        "       \n",
        "\n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        if rejection == True:\n",
        "          if closed == True:\n",
        "            epoch_acc = n_sample_known / float(len(train_dataloader.dataset))\n",
        "            numb = n_sample_known\n",
        "          else:\n",
        "            epoch_acc = n_sample_unknown / float(len(train_dataloader.dataset))\n",
        "            numb = n_sample_unknown\n",
        "        else:\n",
        "          epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, numb))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1rVsryAln7d"
      },
      "source": [
        "#### validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0xrVtFyzj75"
      },
      "source": [
        "def validate(net, val_dataloader, criterion, num_classes, rejection = False, closed = False):\n",
        "    #counter for rejection part, known and unknown\n",
        "    if rejection==True:\n",
        "      n_sample_known = 0\n",
        "      n_sample_unknown = 0\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            \n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        if rejection == True:\n",
        "          prediction_batch = outputs.data.cpu().numpy()\n",
        "          print(len(prediction_batch))\n",
        "          for i in range(len(prediction_batch)):\n",
        "            current_softmax = softmax(prediction_batch[i])\n",
        "            if max(current_softmax)>THRESHOLD:\n",
        "              n_sample_known += 1\n",
        "            else:\n",
        "              n_sample_unknown += 1\n",
        "        else:\n",
        "          #_, preds = classify(images, )\n",
        "          running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "          all_preds_cm.extend(preds.tolist())\n",
        "          all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    if rejection == True:\n",
        "      if closed == True:\n",
        "        acc = n_sample_known / float(len(val_dataloader.dataset))\n",
        "      else:\n",
        "        acc = n_sample_unknown / float(len(val_dataloader.dataset))\n",
        "    else:\n",
        "      acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes, rejection = False, closed = True):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes, rejection, closed)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSgEkfTtlqMl"
      },
      "source": [
        "#### sequentialLearningiCaRL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcN9P3Gr1ICR"
      },
      "source": [
        "def sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets, rejection = False, closed = True):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 5\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      print(\"TEST SET LENGHT: \", len(test_set))\n",
        "      print(\"TEST CURRENT GROUP SET LENGHT: \", len(test_subset))\n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      test_classes = list(test_dataset.df.loc[test_set.indices, 'labels'].value_counts().index)\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TEST_SET CLASSES: \", test_classes)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen, rejection=rejection, closed=closed) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen, exemplars_set_tot, rejection=rejection, closed=closed)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen, exemplars_set_tot, rejection=rejection, closed=closed)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen, exemplars_set_tot)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trLViwIa2_bY"
      },
      "source": [
        "### Closed World"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyhNorNImn9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de21c30-d851-45e7-d057-b5bf4fa1455c"
      },
      "source": [
        "# Reverse indexing for closed and open world\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, splits_of_10)\n",
        "print(outputs_labels_mapping.getGroups())\n",
        "\n",
        "# TEST split\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)\n",
        "#above are 10 splits but I want 5 for closed and 5 for open\n",
        "test_splits_closed = {i:test_splits[i] for i in range(5)}\n",
        "test_splits_open = []\n",
        "for i in range(5,10):\n",
        "  test_splits_open += test_splits[i]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akWuYU2cB1j9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92629adf-9049-439d-f377-d823156eb15d"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "for i in outputs_labels_mapping.getGroups():\n",
        "  print(outputs_labels_mapping.getLabelsOfGroup(i))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    27\n",
            "1    86\n",
            "2    82\n",
            "3    78\n",
            "4    50\n",
            "5    30\n",
            "6    97\n",
            "7    69\n",
            "8    57\n",
            "9    25\n",
            "Name: labels, dtype: object\n",
            "10    95\n",
            "11    59\n",
            "12    58\n",
            "13    34\n",
            "14    81\n",
            "15    49\n",
            "16    13\n",
            "17    88\n",
            "18    68\n",
            "19    60\n",
            "Name: labels, dtype: object\n",
            "20    35\n",
            "21     3\n",
            "22    94\n",
            "23    10\n",
            "24    45\n",
            "25    37\n",
            "26    80\n",
            "27    72\n",
            "28    36\n",
            "29    24\n",
            "Name: labels, dtype: object\n",
            "30    31\n",
            "31    11\n",
            "32    98\n",
            "33    77\n",
            "34    53\n",
            "35    33\n",
            "36    96\n",
            "37    92\n",
            "38    52\n",
            "39    12\n",
            "Name: labels, dtype: object\n",
            "40    91\n",
            "41    67\n",
            "42     7\n",
            "43    62\n",
            "44    46\n",
            "45    65\n",
            "46    21\n",
            "47    32\n",
            "48    16\n",
            "49     0\n",
            "Name: labels, dtype: object\n",
            "50    79\n",
            "51    71\n",
            "52    63\n",
            "53    26\n",
            "54    14\n",
            "55     2\n",
            "56    73\n",
            "57    41\n",
            "58    56\n",
            "59    48\n",
            "Name: labels, dtype: object\n",
            "60    99\n",
            "61    23\n",
            "62    66\n",
            "63    22\n",
            "64     6\n",
            "65    93\n",
            "66    17\n",
            "67     5\n",
            "68     1\n",
            "69    28\n",
            "Name: labels, dtype: object\n",
            "70    87\n",
            "71    75\n",
            "72    55\n",
            "73    47\n",
            "74    39\n",
            "75    70\n",
            "76    42\n",
            "77    38\n",
            "78    29\n",
            "79     4\n",
            "Name: labels, dtype: object\n",
            "80    83\n",
            "81    51\n",
            "82    18\n",
            "83    89\n",
            "84    85\n",
            "85     9\n",
            "86    84\n",
            "87    64\n",
            "88    20\n",
            "89     8\n",
            "Name: labels, dtype: object\n",
            "90    43\n",
            "91    19\n",
            "92    15\n",
            "93    90\n",
            "94    74\n",
            "95    54\n",
            "96    61\n",
            "97    76\n",
            "98    44\n",
            "99    40\n",
            "Name: labels, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W6-U42kmn9F"
      },
      "source": [
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in closed_data.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,5):\n",
        "    v=test_splits_closed[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDPGgRJCtJe4"
      },
      "source": [
        "#for the test of open world\n",
        "open_test = Subset(test_dataset, test_splits_open)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgnJSvjtKYfS"
      },
      "source": [
        "targets_open = set()\n",
        "for i in range(len(open_test.indices)):\n",
        "  targets_open.add(test_dataset.__getitem__(open_test.indices[i])[2])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P76U7RxfLoxo"
      },
      "source": [
        "targets_closed = set()\n",
        "for k in test_splits_closed:\n",
        "  for j in range(len(test_splits_closed[k])):\n",
        "    targets_closed.add(test_dataset.__getitem__(test_splits_closed[k][j])[2])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gp8L6LONeqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f0c862-93bc-4ed1-9adf-c0420a727bc5"
      },
      "source": [
        "#verifing that there aren't objects of the same class\n",
        "list(targets_closed.intersection(targets_open))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMyXZhVemn8q"
      },
      "source": [
        "### Closed world without rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl8IKzFsocw5"
      },
      "source": [
        "#without rejection\n",
        "rejection = False\n",
        "closed = True\n",
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI36G5hjodD8"
      },
      "source": [
        "method = \"Closed world without Rejection\"\n",
        "print(\"metrics ClosedWord for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,5):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBFSAonO0mgQ"
      },
      "source": [
        "### Closed world with rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32Dv40KK1ICR",
        "outputId": "242f2b1a-5b74-4e1e-ccd1-9f08bb87fa2c"
      },
      "source": [
        "# train closed world with rejection\n",
        "rejection = True\n",
        "closed = True\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets, rejection=rejection, closed=closed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  1000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 86, 82, 78, 69, 57, 50, 30, 27, 25]\n",
            "TRAIN_SET CLASSES:  [27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "VALIDATION CLASSES:  [57, 50, 97, 30, 27, 25, 86, 82, 78, 69]\n",
            "GROUP:  1\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "Len TOTAL train susbset:  4950\n",
            "training\n",
            "num classes till now:  10\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.7325664758682251\n",
            "Train step - Step 10, Loss 0.2953501045703888\n",
            "Train step - Step 20, Loss 0.2760721743106842\n",
            "Train step - Step 30, Loss 0.24793823063373566\n",
            "Train epoch - Accuracy: 0.201010101010101 Loss: 0.3388997777242853 Corrects: 995\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.2608705461025238\n",
            "Train step - Step 50, Loss 0.22912640869617462\n",
            "Train step - Step 60, Loss 0.24861787259578705\n",
            "Train step - Step 70, Loss 0.20971480011940002\n",
            "Train epoch - Accuracy: 0.35555555555555557 Loss: 0.23450849609543578 Corrects: 1760\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.210896298289299\n",
            "Train step - Step 90, Loss 0.2240062803030014\n",
            "Train step - Step 100, Loss 0.2282872200012207\n",
            "Train step - Step 110, Loss 0.20540164411067963\n",
            "Train epoch - Accuracy: 0.46101010101010104 Loss: 0.21774203690615568 Corrects: 2282\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.21012310683727264\n",
            "Train step - Step 130, Loss 0.1885118931531906\n",
            "Train step - Step 140, Loss 0.2126309871673584\n",
            "Train step - Step 150, Loss 0.21149365603923798\n",
            "Train epoch - Accuracy: 0.5658585858585858 Loss: 0.20565660707878344 Corrects: 2801\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.1959778070449829\n",
            "Train step - Step 170, Loss 0.19284802675247192\n",
            "Train step - Step 180, Loss 0.19399695098400116\n",
            "Train step - Step 190, Loss 0.2225855439901352\n",
            "Train epoch - Accuracy: 0.6090909090909091 Loss: 0.19718997694025137 Corrects: 3015\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.17410019040107727\n",
            "Train step - Step 210, Loss 0.19064588844776154\n",
            "Train step - Step 220, Loss 0.1894211769104004\n",
            "Train step - Step 230, Loss 0.1798028200864792\n",
            "Train epoch - Accuracy: 0.6583838383838384 Loss: 0.18730989105171628 Corrects: 3259\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.17441710829734802\n",
            "Train step - Step 250, Loss 0.16493242979049683\n",
            "Train step - Step 260, Loss 0.18747077882289886\n",
            "Train step - Step 270, Loss 0.17483699321746826\n",
            "Train epoch - Accuracy: 0.693939393939394 Loss: 0.1808617340073441 Corrects: 3435\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1820220798254013\n",
            "Train step - Step 290, Loss 0.16364268958568573\n",
            "Train step - Step 300, Loss 0.179580956697464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KRvNYlfO1ICS",
        "outputId": "0227b47b-3ccc-4add-f379-05c74153e5c2"
      },
      "source": [
        "method = \"Closed world with Rejection\"\n",
        "print(\"metrics ClosedWord for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,5):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        " #writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics ClosedWord for seed 66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7Wc2tTMTlFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1c6484-fa39-4c2b-f675-aa6d6c5710ae"
      },
      "source": [
        "closed_with_rej_acc = new_accuracies\n",
        "print(closed_with_rej_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.508, 0.406, 0.3416666666666667, 0.29875, 0.2636]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pXLJaU3CtVQ"
      },
      "source": [
        "### open world with rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNW9TdvxHEl2"
      },
      "source": [
        "def sequentialLearningiCaRLOpen(train_subsets, val_subsets, open_dataset, rejection = False, closed = True):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 5\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset in zip(train_subsets, val_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      print(\"BATCH SIZE: \", BATCH_SIZE )\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      old_net = copy.deepcopy(net)\n",
        "      old_net.to(DEVICE)\n",
        "      addOutputs(net,10)\n",
        "    \n",
        "    \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen, rejection=rejection, closed=closed) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      \n",
        "      # val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      # acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen, rejection=rejection, closed=closed)\n",
        "      # print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      #if(group_id ==5):\n",
        "      \"\"\" modify batch size for the variation \"\"\"\n",
        "      #num_classes_seen=100\n",
        "      test_loader = DataLoader(open_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, 100, exemplars_set_tot, rejection, closed)\n",
        "      all_accuracies.append(acc_all)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, all_accuracies, all_preds_cm, all_labels_cm"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozKbRSusQ3am"
      },
      "source": [
        "def meanExemplars(exemplar_set, net):\n",
        "  exemplars_subset = []\n",
        "  exemplar_loader = []\n",
        "  toReturn = {}\n",
        "  for k, exemplar_set_class_k in exemplar_set.items():\n",
        "      distances = []\n",
        "      # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "      if (exemplar_set_class_k != []):\n",
        "        exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "        # print(exemplars_subset.__len__())\n",
        "        if exemplars_subset.__len__()==200:\n",
        "            exemplar_loader = torch.utils.data.DataLoader(exemplars_subset, shuffle = True, batch_size=128, num_workers=2, drop_last=True)\n",
        "        else:\n",
        "            exemplar_loader = torch.utils.data.DataLoader(exemplars_subset, shuffle = True, batch_size=exemplars_subset.__len__(), num_workers=2)\n",
        "            \n",
        "        for _, exemplars, labels  in exemplar_loader:\n",
        "          exemplars = exemplars.to(DEVICE)  \n",
        "          feature_exemplar = net.forward(exemplars).detach().cpu().numpy()\n",
        "          del exemplars #delete unnecessary variables \n",
        "          gc.collect()  \n",
        "          #print(feature_exemplar.shape) #len_exemplar_subset * 64\n",
        "          mean_feature_exemplar = feature_exemplar.mean(axis=0) # expected shape -> 1*64\n",
        "          #print(mean_feature_exemplar)\n",
        "          del feature_exemplar\n",
        "          gc.collect()\n",
        "          toReturn[k] = mean_feature_exemplar\n",
        "          # print(len(toReturn))\n",
        "  return toReturn      "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tY3UJjmjPji"
      },
      "source": [
        "import gc\n",
        "from scipy.special import softmax\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes, exemplar_set, rejection = False, closed = False):\n",
        "    #counter for rejection part, known and unknown\n",
        "    print(\"START VALIDATE!!!!!!!\")\n",
        "    rejection = True\n",
        "    if rejection==True:\n",
        "      n_sample_known = 0\n",
        "      n_sample_unknown = 0\n",
        "    net.eval()\n",
        "    exemplars_subset = []\n",
        "    exemplar_loader = [] \n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "    # print(len(val_dataloader))\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    # print(exemplar_set)\n",
        "\n",
        "    #mean of feature exemplars for each class of this iteration, is a dict class -> mean of features  (1,64)\n",
        "\n",
        "    print(\"I am calculating the mean of the exemplars...\")\n",
        "    mean_exemplars = meanExemplars(exemplar_set, net)\n",
        "    print(\"...Mean of the exemplars done!!\")\n",
        "\n",
        "\n",
        "    for _, image, labels in val_dataloader:\n",
        "        #print('ciclo immagini')\n",
        "        # Bring images and labels to GPU\n",
        "        image = image.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(image)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * image.size(0)\n",
        "            \n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        if rejection == True:\n",
        "          #passare exemplar_set_tot\n",
        "          reject = True\n",
        "          feature = net.forward(image)\n",
        "          # 128 * 64\n",
        "          #print(feature.shape)\n",
        "          mean_image = []\n",
        "          cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "          \n",
        "          distances = []\n",
        "          dist = 0\n",
        "          \n",
        "          for k, mean in mean_exemplars.items(): #k-> class\n",
        "            adjusted_mean = torch.tensor((np.ones(shape = (len(labels),1))*mean)).to(DEVICE)\n",
        "            output = (cos(adjusted_mean, feature)) # ex1 128distanze \n",
        "            output = output.detach().cpu().numpy() #128x1\n",
        "            distances.append(output) # 128xnum_classes\n",
        "          #print('DISTANZE: ', distances) # 128xnum_classes, distances of every image from each class\n",
        "            # 128 * num_classes\n",
        "\n",
        "          # normalized_vector = (distances- np.mean(distances, axis = 0)) / np.std(distances, axis = 0)\n",
        "          # print(\"DISTANCES: \", distances)\n",
        "          #softmax the normalized vector then take the maximum for each row, masking to true or false with the threshold\n",
        "          #if the sum of the vector is > 0 -> at least one value above 0.5 so image is known\n",
        "          threshold = 0.8\n",
        "          distances_np = np.array(distances)\n",
        "          flag_known = (distances_np.max(axis=0)>threshold).sum()\n",
        "         \n",
        "          n_sample_known += flag_known\n",
        "          n_sample_unknown += ( image.size(0) - flag_known)\n",
        "                    \n",
        "          #for values in normalized_vector:\n",
        "          #  for value in values:\n",
        "          #    if (value < 0.5):\n",
        "          #      reject = False\n",
        "          #  if (reject == False):\n",
        "          #    n_sample_known += 1\n",
        "          #  else:\n",
        "          #    n_sample_unknown += 1\n",
        "        # if rejection == True:\n",
        "        #   prediction_batch = outputs.data.cpu().numpy()\n",
        "        #   print(\"OUTPUTS shape: \", outputs.data.shape)\n",
        "        #   print(\"LEN PRED BATCH:\", len(prediction_batch))\n",
        "        #   for i in range(len(prediction_batch)):\n",
        "        #     current_softmax = softmax(prediction_batch[i])\n",
        "        #     #print(max(current_softmax))\n",
        "        #     if max(current_softmax)>THRESHOLD:\n",
        "        #       n_sample_known += 1\n",
        "        #     else:\n",
        "        #       n_sample_unknown += 1\n",
        "        else:\n",
        "          #_, preds = classify(images, )\n",
        "          running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "          all_preds_cm.extend(preds.tolist())\n",
        "          all_labels_cm.extend(labels.data.tolist())\n",
        "          \n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    if rejection == True:\n",
        "      if closed == True:\n",
        "        acc = n_sample_known / float(len(val_dataloader.dataset))\n",
        "      else:\n",
        "        acc = n_sample_unknown / float(len(val_dataloader.dataset))\n",
        "    else:\n",
        "      acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes, exemplar_set, rejection = False, closed = True):\n",
        "    # acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes, exemplar_set, rejection, closed)\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes, exemplar_set, rejection, closed)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs9uww-HEu08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ec436c-b54d-4b85-bff4-0af678cde2e2"
      },
      "source": [
        "#open with rejection\n",
        "rejection = True\n",
        "closed = False\n",
        "# train\n",
        "net, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRLOpen(train_subsets, val_subsets, open_test, rejection=rejection, closed=closed)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH SIZE:  128\n",
            "TRAIN:  4950\n",
            "TRAIN_SET CLASSES:  [27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "VALIDATION CLASSES:  [57, 50, 97, 30, 27, 25, 86, 82, 78, 69]\n",
            "GROUP:  1\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "Len TOTAL train susbset:  4950\n",
            "training\n",
            "num classes till now:  10\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.6837897300720215\n",
            "Train step - Step 10, Loss 0.3141202926635742\n",
            "Train step - Step 20, Loss 0.277765691280365\n",
            "Train step - Step 30, Loss 0.25492697954177856\n",
            "Train epoch - Accuracy: 0.8309090909090909 Loss: 0.331034652656979 Corrects: 4113\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.2501177489757538\n",
            "Train step - Step 50, Loss 0.21974359452724457\n",
            "Train step - Step 60, Loss 0.24579834938049316\n",
            "Train step - Step 70, Loss 0.24298842251300812\n",
            "Train epoch - Accuracy: 0.6527272727272727 Loss: 0.23977305104636182 Corrects: 3231\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.21840213239192963\n",
            "Train step - Step 90, Loss 0.24193596839904785\n",
            "Train step - Step 100, Loss 0.21606695652008057\n",
            "Train step - Step 110, Loss 0.22050617635250092\n",
            "Train epoch - Accuracy: 0.553939393939394 Loss: 0.22319429768456353 Corrects: 2742\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.24116496741771698\n",
            "Train step - Step 130, Loss 0.2171279937028885\n",
            "Train step - Step 140, Loss 0.21427154541015625\n",
            "Train step - Step 150, Loss 0.194271057844162\n",
            "Train epoch - Accuracy: 0.45454545454545453 Loss: 0.20713764766249995 Corrects: 2250\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.18951565027236938\n",
            "Train step - Step 170, Loss 0.2089429348707199\n",
            "Train step - Step 180, Loss 0.1777733415365219\n",
            "Train step - Step 190, Loss 0.19727246463298798\n",
            "Train epoch - Accuracy: 0.37414141414141416 Loss: 0.19946247587902377 Corrects: 1852\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.16896893084049225\n",
            "Train step - Step 210, Loss 0.19073636829853058\n",
            "Train step - Step 220, Loss 0.1932678073644638\n",
            "Train step - Step 230, Loss 0.17920982837677002\n",
            "Train epoch - Accuracy: 0.3422222222222222 Loss: 0.1905993842295926 Corrects: 1694\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.18369688093662262\n",
            "Train step - Step 250, Loss 0.18788458406925201\n",
            "Train step - Step 260, Loss 0.1724734753370285\n",
            "Train step - Step 270, Loss 0.21433864533901215\n",
            "Train epoch - Accuracy: 0.30606060606060603 Loss: 0.18716746228511888 Corrects: 1515\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.17998087406158447\n",
            "Train step - Step 290, Loss 0.16436338424682617\n",
            "Train step - Step 300, Loss 0.1764012724161148\n",
            "Train step - Step 310, Loss 0.17927969992160797\n",
            "Train epoch - Accuracy: 0.2919191919191919 Loss: 0.1786649798263203 Corrects: 1445\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.15712790191173553\n",
            "Train step - Step 330, Loss 0.16303622722625732\n",
            "Train step - Step 340, Loss 0.17671962082386017\n",
            "Train step - Step 350, Loss 0.18757250905036926\n",
            "Train epoch - Accuracy: 0.26606060606060605 Loss: 0.1736814823897198 Corrects: 1317\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.16624875366687775\n",
            "Train step - Step 370, Loss 0.16728390753269196\n",
            "Train step - Step 380, Loss 0.1570214480161667\n",
            "Train epoch - Accuracy: 0.24626262626262627 Loss: 0.16661293673996974 Corrects: 1219\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.16158698499202728\n",
            "Train step - Step 400, Loss 0.17030596733093262\n",
            "Train step - Step 410, Loss 0.15105518698692322\n",
            "Train step - Step 420, Loss 0.16654609143733978\n",
            "Train epoch - Accuracy: 0.22 Loss: 0.16118106376041066 Corrects: 1089\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.15510547161102295\n",
            "Train step - Step 440, Loss 0.16842104494571686\n",
            "Train step - Step 450, Loss 0.17388401925563812\n",
            "Train step - Step 460, Loss 0.15222494304180145\n",
            "Train epoch - Accuracy: 0.21252525252525253 Loss: 0.15710147082203565 Corrects: 1052\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.14798322319984436\n",
            "Train step - Step 480, Loss 0.14378459751605988\n",
            "Train step - Step 490, Loss 0.14807963371276855\n",
            "Train step - Step 500, Loss 0.13117997348308563\n",
            "Train epoch - Accuracy: 0.18828282828282827 Loss: 0.14952753070026936 Corrects: 932\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.1488223671913147\n",
            "Train step - Step 520, Loss 0.14625220000743866\n",
            "Train step - Step 530, Loss 0.13766808807849884\n",
            "Train step - Step 540, Loss 0.14066657423973083\n",
            "Train epoch - Accuracy: 0.18767676767676766 Loss: 0.15022770247676157 Corrects: 929\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.13973134756088257\n",
            "Train step - Step 560, Loss 0.14810924232006073\n",
            "Train step - Step 570, Loss 0.1290099024772644\n",
            "Train step - Step 580, Loss 0.14850755035877228\n",
            "Train epoch - Accuracy: 0.17454545454545456 Loss: 0.14322330991427104 Corrects: 864\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.1519296020269394\n",
            "Train step - Step 600, Loss 0.1610412299633026\n",
            "Train step - Step 610, Loss 0.13771386444568634\n",
            "Train step - Step 620, Loss 0.12690038979053497\n",
            "Train epoch - Accuracy: 0.16626262626262625 Loss: 0.14174081848426298 Corrects: 823\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.153360053896904\n",
            "Train step - Step 640, Loss 0.14016318321228027\n",
            "Train step - Step 650, Loss 0.13430099189281464\n",
            "Train step - Step 660, Loss 0.12937314808368683\n",
            "Train epoch - Accuracy: 0.1604040404040404 Loss: 0.13726175099310248 Corrects: 794\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.14968614280223846\n",
            "Train step - Step 680, Loss 0.1463429182767868\n",
            "Train step - Step 690, Loss 0.11246080696582794\n",
            "Train step - Step 700, Loss 0.15944281220436096\n",
            "Train epoch - Accuracy: 0.1387878787878788 Loss: 0.13391496002674103 Corrects: 687\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.15757037699222565\n",
            "Train step - Step 720, Loss 0.12156732380390167\n",
            "Train step - Step 730, Loss 0.15443895757198334\n",
            "Train step - Step 740, Loss 0.13346481323242188\n",
            "Train epoch - Accuracy: 0.141010101010101 Loss: 0.13301924426146228 Corrects: 698\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.1217350885272026\n",
            "Train step - Step 760, Loss 0.1518479883670807\n",
            "Train step - Step 770, Loss 0.10515031963586807\n",
            "Train epoch - Accuracy: 0.12828282828282828 Loss: 0.12704541442972242 Corrects: 635\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.12776553630828857\n",
            "Train step - Step 790, Loss 0.13979260623455048\n",
            "Train step - Step 800, Loss 0.10882239788770676\n",
            "Train step - Step 810, Loss 0.1284617930650711\n",
            "Train epoch - Accuracy: 0.1195959595959596 Loss: 0.12630902802703356 Corrects: 592\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.1152704507112503\n",
            "Train step - Step 830, Loss 0.11873650550842285\n",
            "Train step - Step 840, Loss 0.13175158202648163\n",
            "Train step - Step 850, Loss 0.11578869819641113\n",
            "Train epoch - Accuracy: 0.12404040404040403 Loss: 0.12167062637480823 Corrects: 614\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.11880171298980713\n",
            "Train step - Step 870, Loss 0.09974966943264008\n",
            "Train step - Step 880, Loss 0.11288034915924072\n",
            "Train step - Step 890, Loss 0.11823069304227829\n",
            "Train epoch - Accuracy: 0.1101010101010101 Loss: 0.11920854388463377 Corrects: 545\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.09628363698720932\n",
            "Train step - Step 910, Loss 0.10607631504535675\n",
            "Train step - Step 920, Loss 0.11431560665369034\n",
            "Train step - Step 930, Loss 0.10102877765893936\n",
            "Train epoch - Accuracy: 0.10505050505050505 Loss: 0.1137507814409757 Corrects: 520\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.11072079092264175\n",
            "Train step - Step 950, Loss 0.10683845728635788\n",
            "Train step - Step 960, Loss 0.10005854815244675\n",
            "Train step - Step 970, Loss 0.10257486253976822\n",
            "Train epoch - Accuracy: 0.09717171717171717 Loss: 0.11351151671674517 Corrects: 481\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.10449545830488205\n",
            "Train step - Step 990, Loss 0.1100781187415123\n",
            "Train step - Step 1000, Loss 0.09542632848024368\n",
            "Train step - Step 1010, Loss 0.12632670998573303\n",
            "Train epoch - Accuracy: 0.1004040404040404 Loss: 0.11018184935203706 Corrects: 497\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.11674833297729492\n",
            "Train step - Step 1030, Loss 0.08816324919462204\n",
            "Train step - Step 1040, Loss 0.12206254154443741\n",
            "Train step - Step 1050, Loss 0.12116419523954391\n",
            "Train epoch - Accuracy: 0.08646464646464647 Loss: 0.10622066175395792 Corrects: 428\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.10461067408323288\n",
            "Train step - Step 1070, Loss 0.12946106493473053\n",
            "Train step - Step 1080, Loss 0.09382108598947525\n",
            "Train step - Step 1090, Loss 0.10344637930393219\n",
            "Train epoch - Accuracy: 0.08626262626262626 Loss: 0.10293912602494461 Corrects: 427\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.09359616786241531\n",
            "Train step - Step 1110, Loss 0.0934613049030304\n",
            "Train step - Step 1120, Loss 0.10798316448926926\n",
            "Train step - Step 1130, Loss 0.0925338938832283\n",
            "Train epoch - Accuracy: 0.08121212121212121 Loss: 0.10092007271870218 Corrects: 402\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.09202896803617477\n",
            "Train step - Step 1150, Loss 0.0907445177435875\n",
            "Train step - Step 1160, Loss 0.11256852000951767\n",
            "Train epoch - Accuracy: 0.07898989898989899 Loss: 0.09945640265339553 Corrects: 391\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.09861242771148682\n",
            "Train step - Step 1180, Loss 0.09001315385103226\n",
            "Train step - Step 1190, Loss 0.06917943805456161\n",
            "Train step - Step 1200, Loss 0.10701646655797958\n",
            "Train epoch - Accuracy: 0.08181818181818182 Loss: 0.1017325884525222 Corrects: 405\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.08301647007465363\n",
            "Train step - Step 1220, Loss 0.08611270040273666\n",
            "Train step - Step 1230, Loss 0.09493411332368851\n",
            "Train step - Step 1240, Loss 0.10190903395414352\n",
            "Train epoch - Accuracy: 0.06787878787878789 Loss: 0.09634266418339026 Corrects: 336\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.10432322323322296\n",
            "Train step - Step 1260, Loss 0.09646522998809814\n",
            "Train step - Step 1270, Loss 0.08537101000547409\n",
            "Train step - Step 1280, Loss 0.10615979880094528\n",
            "Train epoch - Accuracy: 0.07272727272727272 Loss: 0.0932190161883229 Corrects: 360\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.11386966705322266\n",
            "Train step - Step 1300, Loss 0.07834278792142868\n",
            "Train step - Step 1310, Loss 0.09622915089130402\n",
            "Train step - Step 1320, Loss 0.09302370995283127\n",
            "Train epoch - Accuracy: 0.07151515151515152 Loss: 0.08996688063397552 Corrects: 354\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.07116967439651489\n",
            "Train step - Step 1340, Loss 0.07046862691640854\n",
            "Train step - Step 1350, Loss 0.07653766870498657\n",
            "Train step - Step 1360, Loss 0.0695321261882782\n",
            "Train epoch - Accuracy: 0.056767676767676765 Loss: 0.08508503690512494 Corrects: 281\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.08209352195262909\n",
            "Train step - Step 1380, Loss 0.06287204474210739\n",
            "Train step - Step 1390, Loss 0.10130748897790909\n",
            "Train step - Step 1400, Loss 0.07308483868837357\n",
            "Train epoch - Accuracy: 0.057777777777777775 Loss: 0.08447684346124379 Corrects: 286\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.0774591937661171\n",
            "Train step - Step 1420, Loss 0.09585094451904297\n",
            "Train step - Step 1430, Loss 0.09489816427230835\n",
            "Train step - Step 1440, Loss 0.07722415030002594\n",
            "Train epoch - Accuracy: 0.06383838383838383 Loss: 0.08341062451552864 Corrects: 316\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.08168671280145645\n",
            "Train step - Step 1460, Loss 0.05803297087550163\n",
            "Train step - Step 1470, Loss 0.06842096149921417\n",
            "Train step - Step 1480, Loss 0.09436415880918503\n",
            "Train epoch - Accuracy: 0.05696969696969697 Loss: 0.0838855214311619 Corrects: 282\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.07830671221017838\n",
            "Train step - Step 1500, Loss 0.06569874286651611\n",
            "Train step - Step 1510, Loss 0.08646146208047867\n",
            "Train step - Step 1520, Loss 0.07746928185224533\n",
            "Train epoch - Accuracy: 0.056767676767676765 Loss: 0.0841604240404235 Corrects: 281\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.08775024861097336\n",
            "Train step - Step 1540, Loss 0.08077960461378098\n",
            "Train step - Step 1550, Loss 0.10841517895460129\n",
            "Train epoch - Accuracy: 0.058787878787878785 Loss: 0.08400843490554828 Corrects: 291\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.07952550798654556\n",
            "Train step - Step 1570, Loss 0.0933036133646965\n",
            "Train step - Step 1580, Loss 0.0871395468711853\n",
            "Train step - Step 1590, Loss 0.07579254359006882\n",
            "Train epoch - Accuracy: 0.052525252525252523 Loss: 0.07829526134512642 Corrects: 260\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.07090076059103012\n",
            "Train step - Step 1610, Loss 0.07140715420246124\n",
            "Train step - Step 1620, Loss 0.07727028429508209\n",
            "Train step - Step 1630, Loss 0.07049161940813065\n",
            "Train epoch - Accuracy: 0.04525252525252525 Loss: 0.07258831990186614 Corrects: 224\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.0764925628900528\n",
            "Train step - Step 1650, Loss 0.05591621622443199\n",
            "Train step - Step 1660, Loss 0.06514749675989151\n",
            "Train step - Step 1670, Loss 0.09887474775314331\n",
            "Train epoch - Accuracy: 0.04929292929292929 Loss: 0.07403723022552451 Corrects: 244\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.07851114124059677\n",
            "Train step - Step 1690, Loss 0.06766657531261444\n",
            "Train step - Step 1700, Loss 0.06674168258905411\n",
            "Train step - Step 1710, Loss 0.07159712165594101\n",
            "Train epoch - Accuracy: 0.04464646464646465 Loss: 0.07503874334120991 Corrects: 221\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.06990887969732285\n",
            "Train step - Step 1730, Loss 0.06366319954395294\n",
            "Train step - Step 1740, Loss 0.0620388500392437\n",
            "Train step - Step 1750, Loss 0.05727158114314079\n",
            "Train epoch - Accuracy: 0.04626262626262626 Loss: 0.07260389473101106 Corrects: 229\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.06543148308992386\n",
            "Train step - Step 1770, Loss 0.07173190265893936\n",
            "Train step - Step 1780, Loss 0.06580225378274918\n",
            "Train step - Step 1790, Loss 0.05579913407564163\n",
            "Train epoch - Accuracy: 0.042222222222222223 Loss: 0.07050175762387237 Corrects: 209\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.05656943470239639\n",
            "Train step - Step 1810, Loss 0.06636583805084229\n",
            "Train step - Step 1820, Loss 0.06793685257434845\n",
            "Train step - Step 1830, Loss 0.10317721217870712\n",
            "Train epoch - Accuracy: 0.041212121212121214 Loss: 0.06811359135942026 Corrects: 204\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.06421041488647461\n",
            "Train step - Step 1850, Loss 0.07646031677722931\n",
            "Train step - Step 1860, Loss 0.07284475862979889\n",
            "Train step - Step 1870, Loss 0.06489349901676178\n",
            "Train epoch - Accuracy: 0.03777777777777778 Loss: 0.06658665239810943 Corrects: 187\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.04456094652414322\n",
            "Train step - Step 1890, Loss 0.05975514277815819\n",
            "Train step - Step 1900, Loss 0.08391682058572769\n",
            "Train step - Step 1910, Loss 0.05601263791322708\n",
            "Train epoch - Accuracy: 0.03696969696969697 Loss: 0.06665399448739158 Corrects: 183\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.06543685495853424\n",
            "Train step - Step 1930, Loss 0.03870566561818123\n",
            "Train step - Step 1940, Loss 0.04092228040099144\n",
            "Train epoch - Accuracy: 0.034545454545454546 Loss: 0.05357217245330714 Corrects: 171\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.03976206108927727\n",
            "Train step - Step 1960, Loss 0.06353416293859482\n",
            "Train step - Step 1970, Loss 0.03641944378614426\n",
            "Train step - Step 1980, Loss 0.04199148714542389\n",
            "Train epoch - Accuracy: 0.026868686868686868 Loss: 0.04656257033950151 Corrects: 133\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.02823796309530735\n",
            "Train step - Step 2000, Loss 0.04143645986914635\n",
            "Train step - Step 2010, Loss 0.05368497595191002\n",
            "Train step - Step 2020, Loss 0.05428265407681465\n",
            "Train epoch - Accuracy: 0.02585858585858586 Loss: 0.04481696644666219 Corrects: 128\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.045811355113983154\n",
            "Train step - Step 2040, Loss 0.05096247419714928\n",
            "Train step - Step 2050, Loss 0.035659436136484146\n",
            "Train step - Step 2060, Loss 0.04476494714617729\n",
            "Train epoch - Accuracy: 0.026060606060606062 Loss: 0.04363487798306677 Corrects: 129\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.05342499166727066\n",
            "Train step - Step 2080, Loss 0.05022219568490982\n",
            "Train step - Step 2090, Loss 0.054724592715501785\n",
            "Train step - Step 2100, Loss 0.04583101347088814\n",
            "Train epoch - Accuracy: 0.023636363636363636 Loss: 0.04392621132611024 Corrects: 117\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.038971733301877975\n",
            "Train step - Step 2120, Loss 0.05414038896560669\n",
            "Train step - Step 2130, Loss 0.04102277383208275\n",
            "Train step - Step 2140, Loss 0.041699644178152084\n",
            "Train epoch - Accuracy: 0.020404040404040404 Loss: 0.04257844822274314 Corrects: 101\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.03674907609820366\n",
            "Train step - Step 2160, Loss 0.03471401706337929\n",
            "Train step - Step 2170, Loss 0.03095760941505432\n",
            "Train step - Step 2180, Loss 0.029622023925185204\n",
            "Train epoch - Accuracy: 0.022424242424242423 Loss: 0.040107781412926584 Corrects: 111\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.04438936337828636\n",
            "Train step - Step 2200, Loss 0.039024412631988525\n",
            "Train step - Step 2210, Loss 0.04726988822221756\n",
            "Train step - Step 2220, Loss 0.046976201236248016\n",
            "Train epoch - Accuracy: 0.02383838383838384 Loss: 0.04015853103062119 Corrects: 118\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.03745381534099579\n",
            "Train step - Step 2240, Loss 0.03796623274683952\n",
            "Train step - Step 2250, Loss 0.032039571553468704\n",
            "Train step - Step 2260, Loss 0.04887056350708008\n",
            "Train epoch - Accuracy: 0.019797979797979797 Loss: 0.039833150877796035 Corrects: 98\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.0402330607175827\n",
            "Train step - Step 2280, Loss 0.0421830452978611\n",
            "Train step - Step 2290, Loss 0.04801061376929283\n",
            "Train step - Step 2300, Loss 0.044472530484199524\n",
            "Train epoch - Accuracy: 0.018383838383838384 Loss: 0.03993585128374774 Corrects: 91\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.04115667566657066\n",
            "Train step - Step 2320, Loss 0.04224433749914169\n",
            "Train step - Step 2330, Loss 0.04421649128198624\n",
            "Train epoch - Accuracy: 0.015151515151515152 Loss: 0.03931203313849189 Corrects: 75\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.03400440141558647\n",
            "Train step - Step 2350, Loss 0.03472079709172249\n",
            "Train step - Step 2360, Loss 0.03184395655989647\n",
            "Train step - Step 2370, Loss 0.05026155710220337\n",
            "Train epoch - Accuracy: 0.023232323232323233 Loss: 0.03803097804537927 Corrects: 115\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.04034413769841194\n",
            "Train step - Step 2390, Loss 0.031561531126499176\n",
            "Train step - Step 2400, Loss 0.03769353777170181\n",
            "Train step - Step 2410, Loss 0.03101908601820469\n",
            "Train epoch - Accuracy: 0.018585858585858588 Loss: 0.03803606994224317 Corrects: 92\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.03715848550200462\n",
            "Train step - Step 2430, Loss 0.03745555877685547\n",
            "Train step - Step 2440, Loss 0.022319413721561432\n",
            "Train step - Step 2450, Loss 0.056454312056303024\n",
            "Train epoch - Accuracy: 0.020404040404040404 Loss: 0.03577773661098697 Corrects: 101\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.027455875650048256\n",
            "Train step - Step 2470, Loss 0.033357568085193634\n",
            "Train step - Step 2480, Loss 0.02802593819797039\n",
            "Train step - Step 2490, Loss 0.04620874673128128\n",
            "Train epoch - Accuracy: 0.01797979797979798 Loss: 0.03433931142994852 Corrects: 89\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.04250755533576012\n",
            "Train step - Step 2510, Loss 0.021785540506243706\n",
            "Train step - Step 2520, Loss 0.028986025601625443\n",
            "Train step - Step 2530, Loss 0.043411996215581894\n",
            "Train epoch - Accuracy: 0.017575757575757574 Loss: 0.0346421163092659 Corrects: 87\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.035656627267599106\n",
            "Train step - Step 2550, Loss 0.033648669719696045\n",
            "Train step - Step 2560, Loss 0.02179626189172268\n",
            "Train step - Step 2570, Loss 0.02942291460931301\n",
            "Train epoch - Accuracy: 0.01797979797979798 Loss: 0.034172303666821634 Corrects: 89\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.0325840525329113\n",
            "Train step - Step 2590, Loss 0.032250821590423584\n",
            "Train step - Step 2600, Loss 0.04449748620390892\n",
            "Train step - Step 2610, Loss 0.028435377404093742\n",
            "Train epoch - Accuracy: 0.02 Loss: 0.03385683093709175 Corrects: 99\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.03394688293337822\n",
            "Train step - Step 2630, Loss 0.02772802673280239\n",
            "Train step - Step 2640, Loss 0.021864091977477074\n",
            "Train step - Step 2650, Loss 0.03908967971801758\n",
            "Train epoch - Accuracy: 0.01919191919191919 Loss: 0.03397319863239924 Corrects: 95\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.04343108460307121\n",
            "Train step - Step 2670, Loss 0.02443172223865986\n",
            "Train step - Step 2680, Loss 0.03068302385509014\n",
            "Train step - Step 2690, Loss 0.02431568317115307\n",
            "Train epoch - Accuracy: 0.015555555555555555 Loss: 0.032590960087348714 Corrects: 77\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.03307975456118584\n",
            "Train step - Step 2710, Loss 0.04039541259407997\n",
            "Train step - Step 2720, Loss 0.031868863850831985\n",
            "Train epoch - Accuracy: 0.01717171717171717 Loss: 0.03224738323929334 Corrects: 85\n",
            "Training finished in 213.87880682945251 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc6ce610>\n",
            "Constructing exemplars of class 27\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [23132, 19478, 35253, 26015, 36691, 33013, 29431, 7491, 2281, 35259, 7215, 25194, 34887, 25357, 40198, 68, 46338, 15494, 32071, 45785, 12851, 15502, 30875, 6628, 22940, 1538, 21583, 47928, 19642, 33232, 1782, 25662, 36691, 23508, 7491, 16743, 35192, 12953, 47595, 23684, 37456, 21211, 48181, 17563, 28959, 17200, 17655, 20987, 44487, 17695, 40768, 5228, 22683, 35259, 24841, 16125, 10410, 37704, 11025, 3356, 16566, 17566, 8693, 12620, 2583, 35632, 15502, 7593, 17963, 27039, 4533, 6861, 28815, 5972, 11184, 22532, 46516, 28727, 35362, 48999, 27239, 49551, 6703, 6296, 13165, 1771, 38805, 28763, 12015, 4296, 23356, 26815, 29158, 13943, 35632, 28379, 39385, 7769, 2519, 6837, 8143, 40198, 15066, 40955, 16632, 44812, 39659, 21527, 10719, 40330, 12953, 35192, 16451, 25662, 45754, 5599, 49853, 32474, 26690, 4296, 33737, 49948, 1782, 2231, 26815, 678, 36965, 24093, 45312, 33478, 40086, 27357, 9170, 7473, 22953, 5216, 1631, 49948, 23090, 8163, 41711, 33141, 28338, 48693, 8143, 3910, 17963, 6703, 49851, 43133, 17566, 29532, 5432, 37484, 11165, 29637, 19478, 36457, 25747, 11165, 16743, 14693, 49159, 20432, 5253, 26815, 16259, 17259, 40768, 33478, 41711, 11639, 16743, 19711, 41505, 21984, 8348, 35632, 15032, 1538, 42787, 19172, 11226, 31594, 39659, 48445, 37761, 17454, 49551, 12620, 2583, 23032, 39268, 19858, 48733, 23408, 27513, 35398, 13943, 1584]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc76c2d0>\n",
            "Constructing exemplars of class 86\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [42218, 41507, 198, 27109, 21679, 38089, 38987, 24875, 3334, 668, 32219, 47631, 22993, 47883, 32856, 15866, 8754, 22014, 14129, 21814, 3895, 36059, 5, 15548, 8251, 44460, 22170, 42466, 37836, 35457, 9159, 19443, 45871, 18516, 2378, 24114, 48900, 11369, 26375, 40963, 17668, 6638, 48302, 19443, 14000, 13121, 19017, 29633, 46429, 15501, 12375, 49171, 47494, 47969, 49405, 29326, 16522, 5186, 22924, 47493, 4512, 5186, 20196, 24906, 23598, 46275, 44113, 45861, 33095, 26719, 8251, 4392, 40334, 39912, 5645, 32576, 46659, 22288, 45988, 49008, 44794, 34345, 11892, 14170, 9982, 11534, 24831, 40963, 46162, 20828, 7646, 32495, 24760, 46670, 31612, 28175, 10467, 14129, 20778, 28160, 33205, 8251, 34458, 47631, 48884, 24367, 5629, 33083, 23829, 12066, 40886, 28787, 13053, 19138, 10660, 43821, 29370, 17668, 49008, 4329, 28822, 49425, 26097, 30757, 7858, 16134, 45871, 19017, 42936, 22547, 4512, 11093, 1809, 29520, 16487, 48884, 46967, 43084, 15858, 18093, 42707, 29520, 1997, 13053, 49880, 22287, 24538, 5251, 10744, 16522, 42707, 44794, 35599, 49345, 19624, 28497, 19443, 23829, 39129, 21598, 39912, 5767, 42707, 8251, 17322, 26907, 35424, 13228, 15858, 43084, 28787, 10837, 46264, 44949, 32665, 14000, 13121, 1513, 19053, 1083, 27109, 198, 29520, 5412, 8261, 35599, 43723, 48884, 28107, 21598, 24935, 26719, 21404, 42466, 37868, 20196, 17103, 24875, 44356, 36135]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc781410>\n",
            "Constructing exemplars of class 82\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [26440, 42626, 22190, 32376, 33298, 16437, 10757, 23483, 32944, 44697, 274, 22457, 49906, 42783, 22835, 36631, 49631, 12950, 23402, 23809, 40636, 18202, 49838, 44513, 20690, 10283, 21512, 42971, 22204, 26965, 26562, 24393, 31249, 6761, 42901, 32349, 28280, 13204, 625, 22970, 15317, 14057, 6578, 44523, 34305, 26562, 23955, 14131, 28347, 20690, 46394, 40636, 15113, 40462, 33477, 44513, 30911, 32602, 25139, 29650, 30049, 18917, 16897, 45189, 23402, 32644, 15403, 3569, 26471, 8580, 42519, 27487, 44513, 33687, 41719, 21973, 35285, 5904, 6227, 14162, 27707, 47124, 9700, 28488, 5275, 5257, 11084, 39637, 12380, 19839, 45076, 23431, 47862, 41778, 47534, 32644, 28404, 3486, 12950, 5656, 42541, 33931, 46147, 30450, 23431, 25817, 23955, 24024, 15141, 3555, 13318, 17244, 10901, 47737, 41778, 47862, 18702, 28888, 9700, 12801, 2542, 40656, 10283, 8070, 5586, 164, 36295, 49368, 36688, 16393, 45366, 48590, 29348, 44260, 24393, 34875, 5586, 12543, 26471, 43320, 37064, 13458, 29229, 10757, 36688, 16393, 40409, 48590, 22681, 23071, 32602, 33931, 14557, 6057, 34471, 39637, 29650, 38538, 11501, 8253, 32644, 22110, 25615, 871, 33382, 34612, 41547, 49838, 5454, 8257, 2542, 19595, 33729, 6057, 43081, 15141, 3555, 21973, 36301, 35336, 28265, 36690, 15939, 24387, 24714, 34883, 43249, 15939, 23681, 36705, 42971, 45656, 49906, 44909, 20690, 32368, 10727, 46768, 15939, 48703]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88cdbea950>\n",
            "Constructing exemplars of class 78\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [19985, 37043, 28749, 21194, 32455, 18075, 6630, 37680, 6907, 23152, 20551, 16955, 1880, 44594, 16383, 19702, 17236, 3002, 19381, 5168, 46710, 29063, 23175, 12285, 21283, 4536, 29538, 3219, 19728, 10771, 40018, 44199, 35594, 2916, 15742, 8066, 32793, 46141, 42037, 446, 46642, 45517, 3392, 9169, 47100, 2146, 26634, 4239, 5916, 23152, 16264, 905, 19643, 13832, 5916, 21291, 27624, 40382, 30868, 10900, 7570, 16792, 48865, 36503, 10053, 21129, 36435, 40087, 24529, 44962, 28872, 25903, 27875, 43789, 16179, 3984, 39018, 40539, 22426, 22456, 39558, 49, 19998, 34400, 12285, 35080, 16414, 42311, 34803, 1248, 6645, 13916, 14820, 23046, 20304, 4891, 20528, 49541, 5538, 19034, 47322, 7570, 6932, 28343, 14625, 18321, 28595, 36199, 37764, 446, 10900, 38877, 29736, 18228, 23152, 39208, 19381, 33353, 11895, 30871, 38866, 39558, 45244, 45034, 23172, 18075, 42782, 5569, 24660, 34400, 47918, 29518, 43064, 8158, 31093, 39142, 33525, 42663, 36854, 39920, 20474, 32455, 9387, 16792, 44507, 30370, 7382, 29518, 769, 7913, 42636, 22669, 12007, 1619, 10844, 15648, 43662, 15438, 19998, 28595, 33005, 25272, 19985, 1619, 2467, 10326, 43936, 7570, 47760, 38439, 34530, 36516, 37783, 38877, 35818, 39066, 23632, 8598, 48262, 22052, 25864, 29155, 6932, 1248, 36199, 8570, 47371, 37764, 44199, 42102, 47244, 45048, 5569, 48990, 30871, 37553, 44194, 11895, 42663, 33490]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc5a6d10>\n",
            "Constructing exemplars of class 50\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [44701, 17258, 1698, 14726, 42422, 10736, 10353, 47071, 28574, 40772, 41149, 13534, 30776, 12533, 46783, 25815, 9680, 26655, 4892, 47096, 44011, 18858, 30426, 840, 35011, 3398, 27271, 49134, 11906, 22847, 39006, 14133, 2504, 46037, 42268, 20840, 38516, 14070, 5923, 4622, 43215, 1655, 49408, 31637, 19475, 716, 28873, 38965, 38256, 25418, 15807, 1330, 21147, 2240, 16911, 44261, 12381, 47698, 8552, 41930, 43112, 30354, 12180, 42799, 4377, 9154, 15697, 19049, 37949, 17729, 32917, 24808, 20716, 2490, 39430, 5798, 2504, 19629, 44685, 23296, 23187, 21468, 48140, 38076, 16545, 49666, 4892, 5159, 23296, 71, 47346, 27277, 4236, 41149, 24682, 12381, 44685, 8030, 44063, 31545, 49134, 29452, 47427, 40642, 13799, 25815, 46783, 10316, 32640, 40388, 45107, 10353, 49134, 29494, 37949, 18282, 17694, 18858, 44011, 37242, 30312, 38965, 28621, 36477, 10169, 8609, 36768, 27271, 31637, 38965, 32841, 5805, 40020, 46783, 3945, 2240, 9210, 32586, 47346, 28574, 42268, 25946, 2490, 12419, 20959, 4440, 32640, 14258, 35844, 1698, 41252, 42051, 46037, 22674, 11475, 30545, 39031, 8644, 43558, 40546, 38903, 44744, 17925, 14133, 43272, 38839, 1337, 1545, 12736, 8607, 42845, 22479, 40388, 17729, 27060, 44063, 46034, 716, 16770, 9354, 43571, 28336, 46783, 3945, 33923, 1117, 32946, 29494, 32841, 14897, 2272, 25815, 22046, 10528, 16788, 34148, 30671, 32917, 24808, 28739]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc593510>\n",
            "Constructing exemplars of class 30\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [33061, 17424, 36716, 47155, 9585, 15436, 26220, 19212, 9799, 35421, 9692, 40217, 31200, 24992, 27182, 40798, 17433, 41960, 47038, 33698, 35592, 4101, 13224, 47679, 44756, 48707, 49334, 24298, 45646, 133, 44772, 23432, 39595, 41522, 17433, 10667, 609, 43838, 17463, 11873, 35567, 6215, 2569, 21880, 9692, 27625, 5291, 23642, 15610, 44772, 23581, 33600, 33053, 10403, 13137, 41024, 32680, 26846, 34222, 26243, 24104, 44236, 44401, 2041, 3568, 28039, 32680, 6694, 5370, 20823, 41685, 5484, 1711, 6103, 609, 21361, 48439, 18566, 3959, 48612, 17858, 10526, 19228, 21934, 22991, 23732, 3761, 47630, 18472, 20394, 48707, 45665, 15610, 23345, 5915, 32574, 43549, 32330, 31070, 39662, 48910, 12161, 31688, 48910, 38775, 21466, 33258, 15610, 3792, 42993, 9760, 44401, 45523, 28432, 2541, 44328, 3568, 4101, 22281, 44160, 6215, 42261, 19625, 44883, 9153, 13137, 20125, 12343, 12659, 14516, 34286, 37821, 19999, 32684, 43838, 34286, 25100, 7954, 7986, 42023, 36881, 10583, 19225, 11929, 35855, 20676, 35863, 21636, 7261, 34585, 22415, 15234, 20381, 41677, 35863, 5580, 20125, 41237, 322, 4101, 17504, 18103, 10690, 39595, 41675, 21880, 7846, 36170, 5840, 19672, 5095, 17918, 36564, 16223, 1214, 39346, 25227, 41472, 20045, 44842, 36358, 33600, 12463, 3794, 4093, 42208, 20627, 7846, 41247, 42369, 19672, 29049, 12463, 609, 19039, 14471, 4883, 41381, 5127, 23385]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc677a10>\n",
            "Constructing exemplars of class 97\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [39501, 3491, 39603, 44636, 48633, 41227, 49685, 8985, 5211, 3463, 17900, 33607, 41517, 20902, 20073, 36498, 13118, 11948, 28694, 37167, 17460, 5184, 43400, 15960, 2637, 20073, 47513, 30822, 10453, 13942, 49338, 34741, 13881, 40556, 36343, 21266, 25332, 22433, 19113, 22812, 28940, 43706, 20564, 37424, 15666, 45500, 20059, 22938, 4720, 19529, 13500, 16954, 20723, 20564, 48860, 4697, 27427, 3303, 29788, 34705, 27401, 25932, 45867, 43211, 1770, 2637, 38880, 8985, 19057, 14465, 19607, 46756, 35856, 5300, 43036, 5669, 31860, 26637, 1168, 40055, 47988, 1294, 25602, 33261, 43032, 2258, 26290, 13894, 45406, 45577, 13881, 40303, 36485, 40359, 43032, 32782, 8376, 45404, 28467, 45850, 20379, 19425, 17, 33838, 14302, 22091, 22369, 38377, 20723, 24167, 9207, 5768, 17021, 43596, 25535, 32811, 19829, 48808, 15960, 44012, 2258, 3614, 530, 12075, 6930, 9167, 19680, 13894, 9969, 22743, 42986, 34705, 5211, 40503, 20059, 26451, 31763, 43211, 20341, 12069, 48085, 27978, 7232, 16693, 18622, 19057, 23281, 23332, 43032, 17460, 5184, 46630, 32811, 10684, 41517, 49582, 9325, 33838, 16954, 26423, 41545, 11228, 24834, 29039, 1294, 45867, 10823, 12863, 36498, 17, 17021, 5204, 11933, 22433, 8376, 47988, 45577, 18412, 2709, 29788, 38282, 8551, 8317, 40964, 44677, 39885, 37409, 7884, 36485, 5142, 10684, 44601, 38222, 45227, 17360, 5768, 39785, 9377, 42282, 47033]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc58b610>\n",
            "Constructing exemplars of class 69\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [12976, 40324, 41963, 7510, 46543, 20142, 33019, 31419, 49568, 6131, 6572, 10737, 9148, 6007, 49598, 30173, 47270, 13875, 48993, 2005, 14354, 25544, 29900, 39750, 24263, 20696, 27838, 46543, 41261, 42617, 5048, 9865, 19706, 13946, 18152, 1465, 45811, 14140, 29833, 10653, 21181, 40738, 20291, 3958, 8901, 8821, 20302, 10852, 35985, 12861, 31446, 14140, 11567, 23336, 16030, 10614, 32083, 31989, 23973, 48594, 7510, 13167, 29900, 11143, 6693, 1405, 1465, 29508, 47270, 49128, 34929, 12365, 45550, 28883, 16852, 29773, 16030, 4201, 14775, 11387, 26766, 20249, 29773, 19961, 21550, 25881, 24210, 9577, 19706, 34161, 26754, 22418, 13875, 14178, 4217, 45417, 17010, 14081, 20055, 40929, 6007, 12976, 49181, 7165, 40846, 49700, 35437, 40469, 43285, 39450, 5048, 6461, 27611, 17311, 34161, 19731, 17783, 10828, 24618, 45680, 32083, 43010, 17909, 14186, 29599, 9595, 12404, 23336, 41143, 45113, 19961, 6840, 10888, 38145, 13167, 31042, 47800, 33593, 24528, 10772, 39962, 851, 1206, 13927, 44855, 43010, 9148, 6693, 18254, 48993, 2005, 2152, 37672, 10614, 46042, 45550, 2628, 40639, 23902, 29823, 20801, 36067, 9592, 25360, 33514, 11143, 19180, 34161, 6634, 14178, 29606, 35666, 12602, 5048, 33667, 47800, 28883, 8737, 46927, 42026, 31273, 93, 29866, 26754, 12844, 34161, 36449, 25117, 17447, 7662, 33593, 10888, 43010, 32437, 25664, 23730, 13535, 21927, 41127, 3819]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc53e990>\n",
            "Constructing exemplars of class 57\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [41608, 30979, 44222, 1618, 40553, 1501, 7269, 43367, 592, 41412, 9720, 47470, 38333, 7855, 6668, 22472, 2768, 27491, 32832, 48777, 18595, 11243, 49750, 13012, 4879, 16512, 11076, 29882, 671, 18962, 47929, 25804, 23240, 33148, 24962, 41313, 28484, 8366, 41313, 17215, 2087, 6723, 32304, 6981, 15353, 12757, 21487, 18595, 24962, 22510, 45213, 11996, 23264, 44495, 14135, 31623, 41412, 32133, 24125, 41810, 19777, 22974, 16557, 26603, 9237, 5693, 7855, 47420, 43877, 39551, 270, 40553, 283, 32456, 3586, 5418, 24623, 39722, 16219, 32221, 12427, 43622, 44222, 25710, 29542, 36415, 11032, 23240, 46465, 31946, 18100, 40950, 45213, 4453, 7340, 34473, 27428, 43314, 5512, 38939, 34294, 8753, 49914, 43707, 4705, 44930, 46645, 47215, 7578, 513, 38372, 18495, 40996, 44632, 32133, 41412, 23915, 6986, 7414, 38693, 6441, 19851, 49911, 26481, 16557, 20026, 8753, 29054, 22219, 45213, 21462, 13112, 21441, 48650, 27433, 18952, 11474, 6986, 36321, 5753, 10117, 17861, 45281, 18952, 19198, 48650, 36405, 9990, 37504, 38762, 46558, 2768, 13112, 43707, 44647, 38626, 49561, 26755, 5131, 11133, 39466, 37902, 17215, 1495, 16597, 5941, 6066, 2661, 3683, 48650, 3399, 18100, 16219, 31810, 7340, 10117, 4830, 48653, 49750, 46797, 2023, 34969, 15171, 41952, 4705, 31623, 14135, 2205, 14490, 43067, 46323, 1501, 47859, 11243, 8270, 25429, 48650, 13012, 12430, 2320]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc593590>\n",
            "Constructing exemplars of class 25\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [47981, 37800, 15226, 22854, 8619, 45949, 40871, 37590, 33703, 35704, 42308, 10224, 37072, 33804, 17821, 20170, 41658, 40472, 36148, 44003, 35027, 32311, 47197, 13040, 35964, 21149, 19860, 3852, 29159, 4482, 11978, 18992, 48474, 11331, 19623, 4486, 45949, 15226, 42030, 4565, 36823, 25546, 20616, 6268, 24982, 37633, 1257, 15242, 35704, 48335, 4482, 24019, 45402, 38896, 47942, 3665, 40247, 9458, 39777, 11878, 28453, 24244, 24982, 37633, 20462, 20099, 33303, 46113, 39156, 31943, 9495, 43798, 17937, 12415, 31983, 19878, 21005, 23243, 32116, 32700, 15281, 22907, 39438, 45458, 3852, 27232, 29942, 47736, 3669, 32202, 49824, 31983, 8478, 18489, 32700, 15242, 33466, 703, 21005, 33254, 41659, 4771, 241, 36148, 39652, 16690, 43563, 24019, 44689, 40316, 35866, 49575, 41249, 39267, 34334, 4663, 24982, 15226, 22846, 32116, 44456, 23686, 9729, 22666, 33303, 35704, 15242, 1257, 20170, 7243, 20200, 21192, 40554, 41983, 44757, 25546, 19623, 41659, 4469, 42829, 15289, 15632, 33443, 38788, 46144, 34147, 9975, 1669, 45402, 1521, 6589, 36454, 47163, 38817, 22202, 16485, 16563, 29467, 34698, 18489, 20099, 35225, 37072, 33804, 18374, 42308, 2690, 23031, 40247, 15377, 17873, 41249, 33866, 5664, 34978, 47197, 9558, 44609, 13175, 17937, 8619, 31220, 39156, 31965, 33604, 49266, 16922, 15289, 27058, 49718, 45622, 39302, 43577, 29942, 48086, 36148, 5664, 20179, 24244, 4834]\n",
            "I am calculating the mean of the exemplars...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...Mean of the exemplars done!!\n",
            "TEST ALL:  0.6108\n",
            "BATCH SIZE:  128\n",
            "TRAIN:  4950\n",
            "TRAIN_SET CLASSES:  [95, 59, 58, 34, 81, 49, 13, 88, 68, 60]\n",
            "VALIDATION CLASSES:  [60, 59, 58, 49, 34, 95, 88, 81, 13, 68]\n",
            "GROUP:  2\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [95, 59, 58, 34, 81, 49, 13, 88, 68, 60]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  20\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.408018559217453\n",
            "Train step - Step 10, Loss 0.1748546063899994\n",
            "Train step - Step 20, Loss 0.15310008823871613\n",
            "Train step - Step 30, Loss 0.1482415646314621\n",
            "Train step - Step 40, Loss 0.1407526731491089\n",
            "Train step - Step 50, Loss 0.14017365872859955\n",
            "Train epoch - Accuracy: 0.2706474820143885 Loss: 0.16617167992128742 Corrects: 1881\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.1339963674545288\n",
            "Train step - Step 70, Loss 0.1403830498456955\n",
            "Train step - Step 80, Loss 0.12755922973155975\n",
            "Train step - Step 90, Loss 0.13530749082565308\n",
            "Train step - Step 100, Loss 0.13424520194530487\n",
            "Train epoch - Accuracy: 0.24661870503597122 Loss: 0.12835007987434058 Corrects: 1714\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.12332966178655624\n",
            "Train step - Step 120, Loss 0.11680948734283447\n",
            "Train step - Step 130, Loss 0.1294112205505371\n",
            "Train step - Step 140, Loss 0.11977863311767578\n",
            "Train step - Step 150, Loss 0.12390263378620148\n",
            "Train step - Step 160, Loss 0.11521925777196884\n",
            "Train epoch - Accuracy: 0.23194244604316547 Loss: 0.12206985386584303 Corrects: 1612\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.11651100963354111\n",
            "Train step - Step 180, Loss 0.12706288695335388\n",
            "Train step - Step 190, Loss 0.10887930542230606\n",
            "Train step - Step 200, Loss 0.12153340876102448\n",
            "Train step - Step 210, Loss 0.11772871017456055\n",
            "Train epoch - Accuracy: 0.22244604316546762 Loss: 0.11813553248163608 Corrects: 1546\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11579853296279907\n",
            "Train step - Step 230, Loss 0.11846932023763657\n",
            "Train step - Step 240, Loss 0.10961053520441055\n",
            "Train step - Step 250, Loss 0.11523979902267456\n",
            "Train step - Step 260, Loss 0.10555998235940933\n",
            "Train step - Step 270, Loss 0.11645140498876572\n",
            "Train epoch - Accuracy: 0.20863309352517986 Loss: 0.11525602384865713 Corrects: 1450\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1107063814997673\n",
            "Train step - Step 290, Loss 0.12679100036621094\n",
            "Train step - Step 300, Loss 0.10713372379541397\n",
            "Train step - Step 310, Loss 0.10913471132516861\n",
            "Train step - Step 320, Loss 0.11359312385320663\n",
            "Train epoch - Accuracy: 0.20388489208633093 Loss: 0.11202796826259695 Corrects: 1417\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11444091796875\n",
            "Train step - Step 340, Loss 0.10274302959442139\n",
            "Train step - Step 350, Loss 0.11004145443439484\n",
            "Train step - Step 360, Loss 0.10984747856855392\n",
            "Train step - Step 370, Loss 0.1085568442940712\n",
            "Train step - Step 380, Loss 0.11084902286529541\n",
            "Train epoch - Accuracy: 0.1949640287769784 Loss: 0.10958685013673289 Corrects: 1355\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.11172187328338623\n",
            "Train step - Step 400, Loss 0.11038744449615479\n",
            "Train step - Step 410, Loss 0.09754004329442978\n",
            "Train step - Step 420, Loss 0.11527438461780548\n",
            "Train step - Step 430, Loss 0.10262705385684967\n",
            "Train epoch - Accuracy: 0.19007194244604317 Loss: 0.10754105576079527 Corrects: 1321\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10840242356061935\n",
            "Train step - Step 450, Loss 0.11947720497846603\n",
            "Train step - Step 460, Loss 0.10200943797826767\n",
            "Train step - Step 470, Loss 0.1066422238945961\n",
            "Train step - Step 480, Loss 0.10518479347229004\n",
            "Train step - Step 490, Loss 0.12090146541595459\n",
            "Train epoch - Accuracy: 0.17971223021582733 Loss: 0.10508862538946619 Corrects: 1249\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10044194757938385\n",
            "Train step - Step 510, Loss 0.10047644376754761\n",
            "Train step - Step 520, Loss 0.10145125538110733\n",
            "Train step - Step 530, Loss 0.10289683192968369\n",
            "Train step - Step 540, Loss 0.11367239058017731\n",
            "Train epoch - Accuracy: 0.17798561151079137 Loss: 0.10446879405983918 Corrects: 1237\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.09322669357061386\n",
            "Train step - Step 560, Loss 0.09813129156827927\n",
            "Train step - Step 570, Loss 0.10331355780363083\n",
            "Train step - Step 580, Loss 0.11254524439573288\n",
            "Train step - Step 590, Loss 0.11061793565750122\n",
            "Train step - Step 600, Loss 0.10129266232252121\n",
            "Train epoch - Accuracy: 0.16805755395683453 Loss: 0.10348120389439219 Corrects: 1168\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.10685711354017258\n",
            "Train step - Step 620, Loss 0.10594576597213745\n",
            "Train step - Step 630, Loss 0.10409128665924072\n",
            "Train step - Step 640, Loss 0.09830976277589798\n",
            "Train step - Step 650, Loss 0.09450546652078629\n",
            "Train epoch - Accuracy: 0.16345323741007195 Loss: 0.10108275379851568 Corrects: 1136\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.09540905803442001\n",
            "Train step - Step 670, Loss 0.09843706339597702\n",
            "Train step - Step 680, Loss 0.10066991299390793\n",
            "Train step - Step 690, Loss 0.09897082298994064\n",
            "Train step - Step 700, Loss 0.10101791471242905\n",
            "Train step - Step 710, Loss 0.11401046812534332\n",
            "Train epoch - Accuracy: 0.1562589928057554 Loss: 0.09932495675284228 Corrects: 1086\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.09697902202606201\n",
            "Train step - Step 730, Loss 0.0953553318977356\n",
            "Train step - Step 740, Loss 0.09269474446773529\n",
            "Train step - Step 750, Loss 0.10027801990509033\n",
            "Train step - Step 760, Loss 0.09993638843297958\n",
            "Train epoch - Accuracy: 0.15050359712230216 Loss: 0.09977485375224257 Corrects: 1046\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.09596029669046402\n",
            "Train step - Step 780, Loss 0.10000105202198029\n",
            "Train step - Step 790, Loss 0.09289643913507462\n",
            "Train step - Step 800, Loss 0.10011526197195053\n",
            "Train step - Step 810, Loss 0.09098278731107712\n",
            "Train step - Step 820, Loss 0.09890832006931305\n",
            "Train epoch - Accuracy: 0.1441726618705036 Loss: 0.09801769751010181 Corrects: 1002\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.09815105050802231\n",
            "Train step - Step 840, Loss 0.09320550411939621\n",
            "Train step - Step 850, Loss 0.09313815087080002\n",
            "Train step - Step 860, Loss 0.09819133579730988\n",
            "Train step - Step 870, Loss 0.0948130264878273\n",
            "Train epoch - Accuracy: 0.14690647482014388 Loss: 0.0969371822304863 Corrects: 1021\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.09385611861944199\n",
            "Train step - Step 890, Loss 0.10084547847509384\n",
            "Train step - Step 900, Loss 0.09850654751062393\n",
            "Train step - Step 910, Loss 0.10209263861179352\n",
            "Train step - Step 920, Loss 0.0948142409324646\n",
            "Train step - Step 930, Loss 0.09599971026182175\n",
            "Train epoch - Accuracy: 0.14302158273381296 Loss: 0.09744947813612094 Corrects: 994\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.09863540530204773\n",
            "Train step - Step 950, Loss 0.08829391747713089\n",
            "Train step - Step 960, Loss 0.09496282041072845\n",
            "Train step - Step 970, Loss 0.09637006372213364\n",
            "Train step - Step 980, Loss 0.09790189564228058\n",
            "Train epoch - Accuracy: 0.13755395683453236 Loss: 0.09559623459474646 Corrects: 956\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.08877179771661758\n",
            "Train step - Step 1000, Loss 0.09366672486066818\n",
            "Train step - Step 1010, Loss 0.08999550342559814\n",
            "Train step - Step 1020, Loss 0.10400135815143585\n",
            "Train step - Step 1030, Loss 0.10146354883909225\n",
            "Train step - Step 1040, Loss 0.1058306023478508\n",
            "Train epoch - Accuracy: 0.13525179856115108 Loss: 0.0945904184073853 Corrects: 940\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.08870619535446167\n",
            "Train step - Step 1060, Loss 0.0990152657032013\n",
            "Train step - Step 1070, Loss 0.11129093170166016\n",
            "Train step - Step 1080, Loss 0.09085696190595627\n",
            "Train step - Step 1090, Loss 0.10208107531070709\n",
            "Train epoch - Accuracy: 0.13410071942446042 Loss: 0.09440237129120518 Corrects: 932\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.09139923006296158\n",
            "Train step - Step 1110, Loss 0.10123225301504135\n",
            "Train step - Step 1120, Loss 0.10023291409015656\n",
            "Train step - Step 1130, Loss 0.10551059246063232\n",
            "Train step - Step 1140, Loss 0.09715080261230469\n",
            "Train step - Step 1150, Loss 0.10088073462247849\n",
            "Train epoch - Accuracy: 0.13007194244604317 Loss: 0.09366308815187688 Corrects: 904\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.09210860729217529\n",
            "Train step - Step 1170, Loss 0.09037639945745468\n",
            "Train step - Step 1180, Loss 0.09449610114097595\n",
            "Train step - Step 1190, Loss 0.09806318581104279\n",
            "Train step - Step 1200, Loss 0.08728716522455215\n",
            "Train epoch - Accuracy: 0.12431654676258992 Loss: 0.09321539027013367 Corrects: 864\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.08771058171987534\n",
            "Train step - Step 1220, Loss 0.08450651913881302\n",
            "Train step - Step 1230, Loss 0.09632044285535812\n",
            "Train step - Step 1240, Loss 0.09230320900678635\n",
            "Train step - Step 1250, Loss 0.08876601606607437\n",
            "Train step - Step 1260, Loss 0.08082731068134308\n",
            "Train epoch - Accuracy: 0.12719424460431655 Loss: 0.09147238519980753 Corrects: 884\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.0833083763718605\n",
            "Train step - Step 1280, Loss 0.09442098438739777\n",
            "Train step - Step 1290, Loss 0.08892475813627243\n",
            "Train step - Step 1300, Loss 0.0898909941315651\n",
            "Train step - Step 1310, Loss 0.09131741523742676\n",
            "Train epoch - Accuracy: 0.1162589928057554 Loss: 0.09116808511799188 Corrects: 808\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.08522345870733261\n",
            "Train step - Step 1330, Loss 0.097189761698246\n",
            "Train step - Step 1340, Loss 0.09446090459823608\n",
            "Train step - Step 1350, Loss 0.091971255838871\n",
            "Train step - Step 1360, Loss 0.08692287653684616\n",
            "Train step - Step 1370, Loss 0.08213623613119125\n",
            "Train epoch - Accuracy: 0.1214388489208633 Loss: 0.0895421378968431 Corrects: 844\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.08378643542528152\n",
            "Train step - Step 1390, Loss 0.08975743502378464\n",
            "Train step - Step 1400, Loss 0.09430892020463943\n",
            "Train step - Step 1410, Loss 0.08578101545572281\n",
            "Train step - Step 1420, Loss 0.08063112944364548\n",
            "Train epoch - Accuracy: 0.11654676258992806 Loss: 0.08921530932188033 Corrects: 810\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.09337284415960312\n",
            "Train step - Step 1440, Loss 0.08827357739210129\n",
            "Train step - Step 1450, Loss 0.0828581377863884\n",
            "Train step - Step 1460, Loss 0.08677710592746735\n",
            "Train step - Step 1470, Loss 0.08805621415376663\n",
            "Train step - Step 1480, Loss 0.09134350717067719\n",
            "Train epoch - Accuracy: 0.11856115107913669 Loss: 0.0891381714155348 Corrects: 824\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.08637233823537827\n",
            "Train step - Step 1500, Loss 0.08997038751840591\n",
            "Train step - Step 1510, Loss 0.08001833409070969\n",
            "Train step - Step 1520, Loss 0.09963173419237137\n",
            "Train step - Step 1530, Loss 0.0832161083817482\n",
            "Train epoch - Accuracy: 0.11122302158273381 Loss: 0.08853223699031117 Corrects: 773\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.08346010744571686\n",
            "Train step - Step 1550, Loss 0.08766347169876099\n",
            "Train step - Step 1560, Loss 0.07645589113235474\n",
            "Train step - Step 1570, Loss 0.0824388861656189\n",
            "Train step - Step 1580, Loss 0.07897544652223587\n",
            "Train step - Step 1590, Loss 0.09311312437057495\n",
            "Train epoch - Accuracy: 0.1033093525179856 Loss: 0.08695788818512032 Corrects: 718\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.08901022374629974\n",
            "Train step - Step 1610, Loss 0.10127241909503937\n",
            "Train step - Step 1620, Loss 0.09114392101764679\n",
            "Train step - Step 1630, Loss 0.08908343315124512\n",
            "Train step - Step 1640, Loss 0.07526013255119324\n",
            "Train epoch - Accuracy: 0.1041726618705036 Loss: 0.0869470272518748 Corrects: 724\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.08950883150100708\n",
            "Train step - Step 1660, Loss 0.08012431114912033\n",
            "Train step - Step 1670, Loss 0.08602288365364075\n",
            "Train step - Step 1680, Loss 0.08442901074886322\n",
            "Train step - Step 1690, Loss 0.07894429564476013\n",
            "Train step - Step 1700, Loss 0.07658462226390839\n",
            "Train epoch - Accuracy: 0.10733812949640288 Loss: 0.08558584940090454 Corrects: 746\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.07948996871709824\n",
            "Train step - Step 1720, Loss 0.09790041297674179\n",
            "Train step - Step 1730, Loss 0.08680249750614166\n",
            "Train step - Step 1740, Loss 0.10015882551670074\n",
            "Train step - Step 1750, Loss 0.08721178025007248\n",
            "Train epoch - Accuracy: 0.10388489208633093 Loss: 0.08647845255599605 Corrects: 722\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.08501651883125305\n",
            "Train step - Step 1770, Loss 0.09069668501615524\n",
            "Train step - Step 1780, Loss 0.08129934221506119\n",
            "Train step - Step 1790, Loss 0.08566602319478989\n",
            "Train step - Step 1800, Loss 0.0895932987332344\n",
            "Train step - Step 1810, Loss 0.08043727278709412\n",
            "Train epoch - Accuracy: 0.10561151079136691 Loss: 0.0854892438478607 Corrects: 734\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.09049166738986969\n",
            "Train step - Step 1830, Loss 0.08745481818914413\n",
            "Train step - Step 1840, Loss 0.07960666716098785\n",
            "Train step - Step 1850, Loss 0.08761715888977051\n",
            "Train step - Step 1860, Loss 0.09772630780935287\n",
            "Train epoch - Accuracy: 0.09640287769784173 Loss: 0.08453646244762612 Corrects: 670\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.08209558576345444\n",
            "Train step - Step 1880, Loss 0.08152269572019577\n",
            "Train step - Step 1890, Loss 0.09111493825912476\n",
            "Train step - Step 1900, Loss 0.08109475672245026\n",
            "Train step - Step 1910, Loss 0.09352164715528488\n",
            "Train step - Step 1920, Loss 0.08354905992746353\n",
            "Train epoch - Accuracy: 0.09136690647482014 Loss: 0.08476306539216487 Corrects: 635\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.08416789025068283\n",
            "Train step - Step 1940, Loss 0.08087541908025742\n",
            "Train step - Step 1950, Loss 0.08437170833349228\n",
            "Train step - Step 1960, Loss 0.0931834951043129\n",
            "Train step - Step 1970, Loss 0.07655953615903854\n",
            "Train epoch - Accuracy: 0.09079136690647482 Loss: 0.08515542785040767 Corrects: 631\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.08683355897665024\n",
            "Train step - Step 1990, Loss 0.0869472548365593\n",
            "Train step - Step 2000, Loss 0.08255957812070847\n",
            "Train step - Step 2010, Loss 0.08904997259378433\n",
            "Train step - Step 2020, Loss 0.08613765984773636\n",
            "Train step - Step 2030, Loss 0.08257216215133667\n",
            "Train epoch - Accuracy: 0.09467625899280575 Loss: 0.08375955970167256 Corrects: 658\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.08665625751018524\n",
            "Train step - Step 2050, Loss 0.07776140421628952\n",
            "Train step - Step 2060, Loss 0.07937397807836533\n",
            "Train step - Step 2070, Loss 0.08457290381193161\n",
            "Train step - Step 2080, Loss 0.07651373744010925\n",
            "Train epoch - Accuracy: 0.09122302158273381 Loss: 0.08183455474299492 Corrects: 634\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.07916378974914551\n",
            "Train step - Step 2100, Loss 0.07644643634557724\n",
            "Train step - Step 2110, Loss 0.08432942628860474\n",
            "Train step - Step 2120, Loss 0.09046812355518341\n",
            "Train step - Step 2130, Loss 0.08426777273416519\n",
            "Train step - Step 2140, Loss 0.08811252564191818\n",
            "Train epoch - Accuracy: 0.08935251798561152 Loss: 0.08260960528104426 Corrects: 621\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.07559423893690109\n",
            "Train step - Step 2160, Loss 0.08087769895792007\n",
            "Train step - Step 2170, Loss 0.08155893534421921\n",
            "Train step - Step 2180, Loss 0.08202361315488815\n",
            "Train step - Step 2190, Loss 0.07773032039403915\n",
            "Train epoch - Accuracy: 0.08302158273381295 Loss: 0.08133357218701205 Corrects: 577\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.0853615328669548\n",
            "Train step - Step 2210, Loss 0.07040562480688095\n",
            "Train step - Step 2220, Loss 0.07760518044233322\n",
            "Train step - Step 2230, Loss 0.0820571556687355\n",
            "Train step - Step 2240, Loss 0.08108535408973694\n",
            "Train step - Step 2250, Loss 0.07989195734262466\n",
            "Train epoch - Accuracy: 0.08302158273381295 Loss: 0.08124575019311561 Corrects: 577\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.07989206165075302\n",
            "Train step - Step 2270, Loss 0.07613680511713028\n",
            "Train step - Step 2280, Loss 0.08020605891942978\n",
            "Train step - Step 2290, Loss 0.0787060558795929\n",
            "Train step - Step 2300, Loss 0.08568520843982697\n",
            "Train epoch - Accuracy: 0.08028776978417267 Loss: 0.08216817848973995 Corrects: 558\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.07229223102331161\n",
            "Train step - Step 2320, Loss 0.08307013660669327\n",
            "Train step - Step 2330, Loss 0.08621642738580704\n",
            "Train step - Step 2340, Loss 0.08044089376926422\n",
            "Train step - Step 2350, Loss 0.08869515359401703\n",
            "Train step - Step 2360, Loss 0.07545378059148788\n",
            "Train epoch - Accuracy: 0.08330935251798562 Loss: 0.08121066852653627 Corrects: 579\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.08335918188095093\n",
            "Train step - Step 2380, Loss 0.07915156334638596\n",
            "Train step - Step 2390, Loss 0.08004799485206604\n",
            "Train step - Step 2400, Loss 0.07376836985349655\n",
            "Train step - Step 2410, Loss 0.0777120292186737\n",
            "Train epoch - Accuracy: 0.07913669064748201 Loss: 0.07984986463896662 Corrects: 550\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.07105624675750732\n",
            "Train step - Step 2430, Loss 0.08255846798419952\n",
            "Train step - Step 2440, Loss 0.08133333176374435\n",
            "Train step - Step 2450, Loss 0.08926435559988022\n",
            "Train step - Step 2460, Loss 0.07844360917806625\n",
            "Train step - Step 2470, Loss 0.07278401404619217\n",
            "Train epoch - Accuracy: 0.08345323741007195 Loss: 0.0805960906109364 Corrects: 580\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.0780801773071289\n",
            "Train step - Step 2490, Loss 0.08693039417266846\n",
            "Train step - Step 2500, Loss 0.08262502402067184\n",
            "Train step - Step 2510, Loss 0.08588912338018417\n",
            "Train step - Step 2520, Loss 0.08153282850980759\n",
            "Train epoch - Accuracy: 0.07985611510791367 Loss: 0.07884726013854253 Corrects: 555\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.08121230453252792\n",
            "Train step - Step 2540, Loss 0.08458530157804489\n",
            "Train step - Step 2550, Loss 0.07573889940977097\n",
            "Train step - Step 2560, Loss 0.07843776047229767\n",
            "Train step - Step 2570, Loss 0.0879194363951683\n",
            "Train step - Step 2580, Loss 0.07869799435138702\n",
            "Train epoch - Accuracy: 0.07424460431654677 Loss: 0.07872768564618748 Corrects: 516\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.07786618918180466\n",
            "Train step - Step 2600, Loss 0.07562066614627838\n",
            "Train step - Step 2610, Loss 0.06952526420354843\n",
            "Train step - Step 2620, Loss 0.07433200627565384\n",
            "Train step - Step 2630, Loss 0.08287020027637482\n",
            "Train epoch - Accuracy: 0.07482014388489208 Loss: 0.07786712627616718 Corrects: 520\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.07800843566656113\n",
            "Train step - Step 2650, Loss 0.08063507080078125\n",
            "Train step - Step 2660, Loss 0.07676832377910614\n",
            "Train step - Step 2670, Loss 0.08439933508634567\n",
            "Train step - Step 2680, Loss 0.07449948042631149\n",
            "Train step - Step 2690, Loss 0.07434217631816864\n",
            "Train epoch - Accuracy: 0.07107913669064748 Loss: 0.07858883979938013 Corrects: 494\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.08167760074138641\n",
            "Train step - Step 2710, Loss 0.07432916015386581\n",
            "Train step - Step 2720, Loss 0.07298380136489868\n",
            "Train step - Step 2730, Loss 0.07349362224340439\n",
            "Train step - Step 2740, Loss 0.07426787167787552\n",
            "Train epoch - Accuracy: 0.06906474820143885 Loss: 0.07327823296939727 Corrects: 480\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.07042781263589859\n",
            "Train step - Step 2760, Loss 0.07123947143554688\n",
            "Train step - Step 2770, Loss 0.06918125599622726\n",
            "Train step - Step 2780, Loss 0.07094621658325195\n",
            "Train step - Step 2790, Loss 0.0734776183962822\n",
            "Train step - Step 2800, Loss 0.06637505441904068\n",
            "Train epoch - Accuracy: 0.0674820143884892 Loss: 0.07068434266520919 Corrects: 469\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.06236977502703667\n",
            "Train step - Step 2820, Loss 0.07980191707611084\n",
            "Train step - Step 2830, Loss 0.0688777044415474\n",
            "Train step - Step 2840, Loss 0.06877347081899643\n",
            "Train step - Step 2850, Loss 0.07189345359802246\n",
            "Train epoch - Accuracy: 0.06388489208633094 Loss: 0.07007772391434196 Corrects: 444\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.07077982276678085\n",
            "Train step - Step 2870, Loss 0.0720185786485672\n",
            "Train step - Step 2880, Loss 0.06416627019643784\n",
            "Train step - Step 2890, Loss 0.07500889152288437\n",
            "Train step - Step 2900, Loss 0.0692976787686348\n",
            "Train step - Step 2910, Loss 0.06521888077259064\n",
            "Train epoch - Accuracy: 0.06057553956834533 Loss: 0.07002901021953967 Corrects: 421\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.06524138897657394\n",
            "Train step - Step 2930, Loss 0.07059460133314133\n",
            "Train step - Step 2940, Loss 0.07606025785207748\n",
            "Train step - Step 2950, Loss 0.06540783494710922\n",
            "Train step - Step 2960, Loss 0.07250770181417465\n",
            "Train epoch - Accuracy: 0.05841726618705036 Loss: 0.06943812268886634 Corrects: 406\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.06217154487967491\n",
            "Train step - Step 2980, Loss 0.07514785975217819\n",
            "Train step - Step 2990, Loss 0.06935923546552658\n",
            "Train step - Step 3000, Loss 0.06234191730618477\n",
            "Train step - Step 3010, Loss 0.07048584520816803\n",
            "Train step - Step 3020, Loss 0.06982212513685226\n",
            "Train epoch - Accuracy: 0.06230215827338129 Loss: 0.06915804292872656 Corrects: 433\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.06533172726631165\n",
            "Train step - Step 3040, Loss 0.07068520784378052\n",
            "Train step - Step 3050, Loss 0.07557079195976257\n",
            "Train step - Step 3060, Loss 0.07013387233018875\n",
            "Train step - Step 3070, Loss 0.06653086841106415\n",
            "Train epoch - Accuracy: 0.062014388489208636 Loss: 0.06926800935174064 Corrects: 431\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.07702109962701797\n",
            "Train step - Step 3090, Loss 0.06536432355642319\n",
            "Train step - Step 3100, Loss 0.08014696091413498\n",
            "Train step - Step 3110, Loss 0.06840705126523972\n",
            "Train step - Step 3120, Loss 0.07319719344377518\n",
            "Train step - Step 3130, Loss 0.07075151056051254\n",
            "Train epoch - Accuracy: 0.05928057553956834 Loss: 0.0693462798492514 Corrects: 412\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.06937667727470398\n",
            "Train step - Step 3150, Loss 0.07644230127334595\n",
            "Train step - Step 3160, Loss 0.06288135796785355\n",
            "Train step - Step 3170, Loss 0.07041189819574356\n",
            "Train step - Step 3180, Loss 0.06547906249761581\n",
            "Train epoch - Accuracy: 0.057841726618705035 Loss: 0.06886828671578023 Corrects: 402\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.06669806689023972\n",
            "Train step - Step 3200, Loss 0.06558044999837875\n",
            "Train step - Step 3210, Loss 0.06010475382208824\n",
            "Train step - Step 3220, Loss 0.0803924947977066\n",
            "Train step - Step 3230, Loss 0.0620279423892498\n",
            "Train step - Step 3240, Loss 0.054949354380369186\n",
            "Train epoch - Accuracy: 0.05841726618705036 Loss: 0.0688176801929371 Corrects: 406\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.06338858604431152\n",
            "Train step - Step 3260, Loss 0.06732281297445297\n",
            "Train step - Step 3270, Loss 0.061291445046663284\n",
            "Train step - Step 3280, Loss 0.065299853682518\n",
            "Train step - Step 3290, Loss 0.05984313040971756\n",
            "Train epoch - Accuracy: 0.05913669064748201 Loss: 0.06887272483582119 Corrects: 411\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.06445121020078659\n",
            "Train step - Step 3310, Loss 0.07110800594091415\n",
            "Train step - Step 3320, Loss 0.07154450565576553\n",
            "Train step - Step 3330, Loss 0.06818778067827225\n",
            "Train step - Step 3340, Loss 0.06630431860685349\n",
            "Train step - Step 3350, Loss 0.07055938243865967\n",
            "Train epoch - Accuracy: 0.05669064748201439 Loss: 0.06906498652996777 Corrects: 394\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.06161739304661751\n",
            "Train step - Step 3370, Loss 0.06135237216949463\n",
            "Train step - Step 3380, Loss 0.06598859280347824\n",
            "Train step - Step 3390, Loss 0.06597007811069489\n",
            "Train step - Step 3400, Loss 0.07238065451383591\n",
            "Train epoch - Accuracy: 0.056402877697841726 Loss: 0.06902026830174082 Corrects: 392\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.0676368772983551\n",
            "Train step - Step 3420, Loss 0.06806155294179916\n",
            "Train step - Step 3430, Loss 0.06685974448919296\n",
            "Train step - Step 3440, Loss 0.0683865174651146\n",
            "Train step - Step 3450, Loss 0.07424590736627579\n",
            "Train step - Step 3460, Loss 0.06717989593744278\n",
            "Train epoch - Accuracy: 0.05467625899280575 Loss: 0.06874157121713213 Corrects: 380\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.06014648824930191\n",
            "Train step - Step 3480, Loss 0.06141442060470581\n",
            "Train step - Step 3490, Loss 0.06450241804122925\n",
            "Train step - Step 3500, Loss 0.07008238881826401\n",
            "Train step - Step 3510, Loss 0.0726952776312828\n",
            "Train epoch - Accuracy: 0.0539568345323741 Loss: 0.06719922497118119 Corrects: 375\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.06869059801101685\n",
            "Train step - Step 3530, Loss 0.07066457718610764\n",
            "Train step - Step 3540, Loss 0.06304589658975601\n",
            "Train step - Step 3550, Loss 0.06683149188756943\n",
            "Train step - Step 3560, Loss 0.06612303107976913\n",
            "Train step - Step 3570, Loss 0.06477531045675278\n",
            "Train epoch - Accuracy: 0.05611510791366906 Loss: 0.06731612066999614 Corrects: 390\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.063105009496212\n",
            "Train step - Step 3590, Loss 0.061484504491090775\n",
            "Train step - Step 3600, Loss 0.06252986937761307\n",
            "Train step - Step 3610, Loss 0.06704675406217575\n",
            "Train step - Step 3620, Loss 0.07088349014520645\n",
            "Train epoch - Accuracy: 0.05798561151079137 Loss: 0.06729984346482394 Corrects: 403\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.07000337541103363\n",
            "Train step - Step 3640, Loss 0.06516509503126144\n",
            "Train step - Step 3650, Loss 0.06510155647993088\n",
            "Train step - Step 3660, Loss 0.06452416628599167\n",
            "Train step - Step 3670, Loss 0.06756407767534256\n",
            "Train step - Step 3680, Loss 0.06278258562088013\n",
            "Train epoch - Accuracy: 0.05424460431654676 Loss: 0.06729721357925332 Corrects: 377\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.060778435319662094\n",
            "Train step - Step 3700, Loss 0.07168501615524292\n",
            "Train step - Step 3710, Loss 0.06963943690061569\n",
            "Train step - Step 3720, Loss 0.06974587589502335\n",
            "Train step - Step 3730, Loss 0.05571490526199341\n",
            "Train epoch - Accuracy: 0.058992805755395686 Loss: 0.0677278984579251 Corrects: 410\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.06324651837348938\n",
            "Train step - Step 3750, Loss 0.0656876340508461\n",
            "Train step - Step 3760, Loss 0.06670496612787247\n",
            "Train step - Step 3770, Loss 0.06438849866390228\n",
            "Train step - Step 3780, Loss 0.0677122101187706\n",
            "Train step - Step 3790, Loss 0.06834430992603302\n",
            "Train epoch - Accuracy: 0.05223021582733813 Loss: 0.06671964162330833 Corrects: 363\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.058628346771001816\n",
            "Train step - Step 3810, Loss 0.06935882568359375\n",
            "Train step - Step 3820, Loss 0.06375371664762497\n",
            "Train step - Step 3830, Loss 0.06359322369098663\n",
            "Train step - Step 3840, Loss 0.06789512932300568\n",
            "Train epoch - Accuracy: 0.05697841726618705 Loss: 0.06682276617923229 Corrects: 396\n",
            "Training finished in 469.7959942817688 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60]\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88ceec8250>\n",
            "Constructing exemplars of class 95\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [13842, 18669, 13780, 46579, 17482, 23800, 37254, 33121, 25200, 15216, 14838, 44800, 5708, 18309, 7020, 39105, 20164, 15016, 42997, 48729, 30445, 5606, 45532, 42728, 41274, 37579, 12686, 5579, 19816, 7217, 19820, 11471, 18987, 17792, 29681, 30156, 4485, 48204, 38855, 30239, 48839, 46078, 13798, 42486, 32897, 35851, 18669, 31227, 36621, 18784, 14838, 12433, 801, 4466, 48603, 8550, 47606, 24573, 42997, 25442, 23800, 18669, 29012, 18052, 39209, 11454, 27842, 4692, 46855, 25996, 14251, 25587, 42567, 38072, 4736, 17231, 34804, 7551, 6646, 11454, 49018, 16364, 38447, 46276, 8166, 1246, 36757, 10223, 5788, 14767, 13640, 3547, 47545, 40988, 37094, 47381, 22863, 25232, 49037, 46111]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880a041fd0>\n",
            "Constructing exemplars of class 59\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [4649, 25774, 49165, 47618, 24236, 47967, 15950, 43182, 6065, 43511, 12113, 5774, 18865, 39641, 42060, 4560, 16650, 34455, 14956, 35676, 43463, 30328, 25142, 10103, 44581, 2970, 24236, 4113, 44095, 16001, 34779, 10092, 13465, 687, 29968, 7037, 27541, 49401, 22642, 28590, 34129, 45838, 973, 17090, 7736, 39034, 31607, 26688, 31150, 30615, 46102, 22866, 31674, 18828, 45468, 40320, 37074, 11260, 41436, 8271, 2834, 36637, 10111, 7065, 22797, 22751, 1844, 29872, 49931, 31685, 1378, 31150, 4936, 1270, 8578, 40985, 6571, 517, 19261, 13249, 17507, 27489, 34504, 24658, 14052, 29872, 9981, 36701, 19431, 32077, 28806, 27858, 3500, 47683, 36387, 26379, 42060, 14358, 31388, 40723]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc678690>\n",
            "Constructing exemplars of class 58\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [37983, 38296, 35227, 33651, 1740, 25074, 42298, 17597, 26227, 43961, 42359, 34033, 29759, 25751, 22489, 31977, 30591, 9988, 44047, 44848, 17357, 32383, 1179, 16795, 32364, 16226, 25156, 7222, 12895, 41457, 27316, 34808, 27132, 19360, 42790, 28571, 25374, 15237, 49301, 26235, 37320, 11641, 37653, 41048, 27939, 45460, 30648, 21969, 7843, 18284, 6213, 25751, 31306, 4204, 34033, 40233, 35816, 2750, 27347, 38948, 22849, 25123, 19360, 39162, 33063, 32772, 18524, 35941, 4347, 34611, 35551, 15921, 44047, 25074, 1364, 15184, 23098, 12760, 1363, 44223, 47840, 29649, 3337, 14623, 15070, 44705, 32617, 16529, 32288, 48123, 49424, 46535, 7326, 5051, 6780, 25735, 20677, 27865, 38641, 41401]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88ce009a10>\n",
            "Constructing exemplars of class 34\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [19826, 44020, 4681, 15359, 48505, 11424, 27241, 41528, 32901, 439, 43487, 10940, 10016, 5124, 13038, 17954, 34964, 17613, 28935, 25992, 7399, 19206, 43775, 21142, 42020, 40428, 43805, 21858, 28292, 21112, 49111, 120, 19009, 12039, 31527, 17105, 32593, 15165, 3499, 25251, 35397, 28672, 17613, 9812, 20103, 13748, 11646, 25116, 24559, 41170, 35462, 14378, 9845, 12459, 20892, 43297, 12666, 28935, 36335, 38325, 24224, 5287, 15291, 20103, 18397, 35104, 18257, 32733, 40597, 35762, 13335, 10770, 42158, 18632, 25110, 1969, 39647, 21205, 31783, 10940, 45766, 15179, 3515, 37632, 27238, 14198, 22548, 20461, 39101, 49721, 10238, 5064, 42093, 1125, 45291, 14824, 11594, 24224, 27238, 8112]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88cdf2ad10>\n",
            "Constructing exemplars of class 81\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [5539, 34666, 26745, 26308, 14660, 28000, 21463, 32821, 14858, 30677, 37864, 46266, 2596, 13510, 25963, 32625, 39868, 2377, 16778, 47085, 7533, 33016, 34261, 47272, 24801, 673, 11347, 6023, 40958, 13060, 29499, 49179, 8668, 28607, 30362, 26045, 8321, 18735, 21157, 22076, 36359, 35026, 31364, 45491, 12539, 22889, 20392, 7428, 38090, 25308, 47817, 23159, 16857, 196, 14144, 7373, 49473, 29415, 13014, 48066, 36046, 47568, 654, 46366, 42926, 3401, 21151, 28774, 18708, 9789, 19095, 39569, 18666, 25857, 7023, 21003, 18265, 40958, 9275, 41058, 30437, 1481, 19993, 35102, 17715, 12588, 37599, 28913, 46002, 15252, 25308, 49102, 41192, 45143, 28607, 30409, 33573, 36296, 8699, 40177]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc635dd0>\n",
            "Constructing exemplars of class 49\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [42904, 25906, 9142, 7209, 6094, 24785, 14406, 34801, 42803, 10758, 35400, 4869, 27122, 43337, 40191, 20833, 23913, 28384, 35767, 47029, 18001, 7773, 49525, 8721, 13215, 49610, 46508, 9078, 8987, 19957, 2844, 25656, 33565, 11652, 14611, 5061, 41320, 26063, 43049, 23188, 37957, 13062, 34940, 40974, 33332, 15491, 46508, 33409, 41037, 32352, 27143, 29567, 35550, 25822, 20211, 5908, 18138, 48776, 20183, 30596, 12722, 42740, 6144, 12555, 29567, 33106, 9463, 43337, 20833, 43524, 44525, 46569, 16676, 21962, 35671, 12713, 33271, 34446, 19075, 38731, 47253, 208, 34940, 44670, 8997, 8515, 7209, 34963, 479, 33332, 20833, 7587, 12189, 49966, 30596, 15401, 34224, 27098, 11736, 617]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880b1b8a90>\n",
            "Constructing exemplars of class 13\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [45007, 28971, 12685, 10751, 29621, 5108, 21049, 15785, 10130, 47791, 12466, 45834, 12655, 27155, 40043, 45608, 35179, 29766, 24353, 39903, 13306, 44490, 29418, 45203, 46647, 30495, 44566, 1287, 39773, 3096, 41856, 39277, 36018, 7422, 36655, 46081, 48614, 42532, 24649, 33026, 27567, 1746, 40595, 20671, 26515, 16607, 27521, 43195, 41744, 12809, 10242, 28213, 4359, 20536, 25382, 7397, 27128, 25181, 40645, 27991, 39186, 39026, 20536, 43520, 11870, 44668, 3803, 47791, 5108, 44668, 9427, 47911, 32008, 12810, 26522, 37107, 2691, 35365, 20998, 23978, 24313, 38777, 27525, 10751, 49162, 27555, 45604, 5058, 32649, 29219, 19445, 1287, 6529, 723, 1176, 27236, 25846, 43567, 11834, 48616]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880ab932d0>\n",
            "Constructing exemplars of class 88\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [24266, 7300, 44382, 20222, 21473, 32367, 25616, 40591, 31263, 24375, 49717, 9262, 38955, 483, 49131, 8284, 42133, 40743, 2680, 48442, 38821, 31676, 18661, 3992, 28490, 6567, 31379, 17239, 44913, 18631, 15656, 37233, 3903, 25796, 8284, 14433, 17295, 41471, 46045, 29577, 46863, 45689, 39605, 34430, 48758, 29762, 10507, 16957, 30562, 8310, 27300, 969, 32105, 11826, 11631, 531, 5092, 5821, 19590, 13179, 28487, 23704, 46863, 42247, 27258, 44099, 24340, 23995, 49844, 37231, 11746, 11930, 33576, 39306, 18258, 1119, 31667, 9747, 28487, 25659, 22568, 47743, 5593, 25195, 7594, 18661, 45844, 10507, 30366, 32448, 39883, 16754, 12173, 12584, 47318, 28329, 7139, 30280, 9698, 30652]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88cec535d0>\n",
            "Constructing exemplars of class 68\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [20363, 37232, 36856, 35394, 13299, 9871, 5589, 2740, 9030, 4616, 14793, 41709, 23037, 25189, 38643, 10028, 21628, 13880, 8412, 36824, 44287, 2956, 38822, 8137, 27410, 33333, 47868, 40887, 10601, 12280, 39158, 25899, 3918, 45044, 33189, 40887, 14706, 34184, 12920, 11147, 49464, 41388, 36567, 30297, 6929, 21535, 40926, 42007, 25189, 9030, 21628, 23007, 11854, 22188, 2330, 26732, 20363, 27702, 21731, 18331, 3845, 15384, 28636, 41640, 30403, 39615, 40674, 37274, 20325, 26658, 1185, 5680, 33189, 39158, 38968, 8137, 32693, 5090, 25904, 41709, 18947, 3845, 14817, 25271, 46610, 33270, 41892, 4616, 4267, 36169, 3533, 18662, 5486, 43778, 4454, 2670, 22454, 19950, 30834, 21535]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88d002aa50>\n",
            "Constructing exemplars of class 60\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [43918, 49427, 41703, 37184, 47765, 24171, 5715, 24523, 39802, 40682, 8295, 46432, 5247, 33903, 5062, 6475, 42187, 26691, 22117, 29668, 7127, 36949, 11840, 3872, 6772, 653, 31284, 36760, 20247, 42187, 27928, 24169, 20843, 18047, 35237, 38494, 35036, 462, 4815, 38276, 20898, 27635, 33807, 5667, 25434, 39135, 23813, 36052, 18379, 46986, 48864, 16305, 40548, 17201, 20850, 35234, 48216, 11691, 28035, 32966, 7035, 24109, 29304, 6280, 8520, 23698, 5795, 7814, 17568, 38277, 7379, 17327, 4259, 10801, 26295, 11340, 39802, 49956, 21831, 25736, 20729, 34684, 24109, 28169, 39402, 1516, 46285, 23813, 39339, 34008, 43918, 11399, 28818, 7127, 15908, 34954, 25524, 34587, 29146, 42187]\n",
            "I am calculating the mean of the exemplars...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...Mean of the exemplars done!!\n",
            "TEST ALL:  0.616\n",
            "BATCH SIZE:  128\n",
            "TRAIN:  4950\n",
            "TRAIN_SET CLASSES:  [35, 3, 94, 10, 45, 37, 80, 72, 36, 24]\n",
            "VALIDATION CLASSES:  [45, 37, 36, 35, 94, 24, 80, 10, 72, 3]\n",
            "GROUP:  3\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [35, 3, 94, 10, 45, 37, 80, 72, 36, 24]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  30\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.39127880334854126\n",
            "Train step - Step 10, Loss 0.1588975340127945\n",
            "Train step - Step 20, Loss 0.15261223912239075\n",
            "Train step - Step 30, Loss 0.1311495304107666\n",
            "Train step - Step 40, Loss 0.1287691593170166\n",
            "Train step - Step 50, Loss 0.13443627953529358\n",
            "Train epoch - Accuracy: 0.3375539568345324 Loss: 0.15879740954303057 Corrects: 2346\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12388765066862106\n",
            "Train step - Step 70, Loss 0.11988617479801178\n",
            "Train step - Step 80, Loss 0.11909811198711395\n",
            "Train step - Step 90, Loss 0.12249014526605606\n",
            "Train step - Step 100, Loss 0.13479270040988922\n",
            "Train epoch - Accuracy: 0.33223021582733814 Loss: 0.12177895502220812 Corrects: 2309\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.12176526337862015\n",
            "Train step - Step 120, Loss 0.12207755446434021\n",
            "Train step - Step 130, Loss 0.11556089669466019\n",
            "Train step - Step 140, Loss 0.12274934351444244\n",
            "Train step - Step 150, Loss 0.12046191096305847\n",
            "Train step - Step 160, Loss 0.11703434586524963\n",
            "Train epoch - Accuracy: 0.3194244604316547 Loss: 0.11792976167991007 Corrects: 2220\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.1159830093383789\n",
            "Train step - Step 180, Loss 0.11314176768064499\n",
            "Train step - Step 190, Loss 0.1102595180273056\n",
            "Train step - Step 200, Loss 0.12517473101615906\n",
            "Train step - Step 210, Loss 0.12196777760982513\n",
            "Train epoch - Accuracy: 0.3148201438848921 Loss: 0.1152048262739353 Corrects: 2188\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11574357748031616\n",
            "Train step - Step 230, Loss 0.1181333139538765\n",
            "Train step - Step 240, Loss 0.11452478170394897\n",
            "Train step - Step 250, Loss 0.11878011375665665\n",
            "Train step - Step 260, Loss 0.11429101973772049\n",
            "Train step - Step 270, Loss 0.11012447625398636\n",
            "Train epoch - Accuracy: 0.31107913669064746 Loss: 0.11358887528772835 Corrects: 2162\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1134812980890274\n",
            "Train step - Step 290, Loss 0.12103777378797531\n",
            "Train step - Step 300, Loss 0.11067289113998413\n",
            "Train step - Step 310, Loss 0.10493945330381393\n",
            "Train step - Step 320, Loss 0.11408913135528564\n",
            "Train epoch - Accuracy: 0.3018705035971223 Loss: 0.1118095849250718 Corrects: 2098\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.10773137956857681\n",
            "Train step - Step 340, Loss 0.11011810600757599\n",
            "Train step - Step 350, Loss 0.1154598817229271\n",
            "Train step - Step 360, Loss 0.1095353364944458\n",
            "Train step - Step 370, Loss 0.10760559141635895\n",
            "Train step - Step 380, Loss 0.11353730410337448\n",
            "Train epoch - Accuracy: 0.3172661870503597 Loss: 0.11079741623761842 Corrects: 2205\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.11195792257785797\n",
            "Train step - Step 400, Loss 0.1061079204082489\n",
            "Train step - Step 410, Loss 0.10659998655319214\n",
            "Train step - Step 420, Loss 0.11375391483306885\n",
            "Train step - Step 430, Loss 0.11839145421981812\n",
            "Train epoch - Accuracy: 0.29467625899280575 Loss: 0.10972664026905307 Corrects: 2048\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10927334427833557\n",
            "Train step - Step 450, Loss 0.10371247678995132\n",
            "Train step - Step 460, Loss 0.10797270387411118\n",
            "Train step - Step 470, Loss 0.10655693709850311\n",
            "Train step - Step 480, Loss 0.10599758476018906\n",
            "Train step - Step 490, Loss 0.10612992197275162\n",
            "Train epoch - Accuracy: 0.2953956834532374 Loss: 0.10879068290586952 Corrects: 2053\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.11394038796424866\n",
            "Train step - Step 510, Loss 0.10327529162168503\n",
            "Train step - Step 520, Loss 0.10880976170301437\n",
            "Train step - Step 530, Loss 0.10600556433200836\n",
            "Train step - Step 540, Loss 0.11346250772476196\n",
            "Train epoch - Accuracy: 0.2896402877697842 Loss: 0.10819187894570742 Corrects: 2013\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.10783232003450394\n",
            "Train step - Step 560, Loss 0.11073645204305649\n",
            "Train step - Step 570, Loss 0.11535802483558655\n",
            "Train step - Step 580, Loss 0.11540606617927551\n",
            "Train step - Step 590, Loss 0.10725794732570648\n",
            "Train step - Step 600, Loss 0.10261283814907074\n",
            "Train epoch - Accuracy: 0.2789928057553957 Loss: 0.10759406983852386 Corrects: 1939\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.11418288946151733\n",
            "Train step - Step 620, Loss 0.11262155324220657\n",
            "Train step - Step 630, Loss 0.0978255346417427\n",
            "Train step - Step 640, Loss 0.11916312575340271\n",
            "Train step - Step 650, Loss 0.10055433958768845\n",
            "Train epoch - Accuracy: 0.28086330935251796 Loss: 0.10676988273215808 Corrects: 1952\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.09862251579761505\n",
            "Train step - Step 670, Loss 0.10293220728635788\n",
            "Train step - Step 680, Loss 0.1061631441116333\n",
            "Train step - Step 690, Loss 0.10734278708696365\n",
            "Train step - Step 700, Loss 0.10137703269720078\n",
            "Train step - Step 710, Loss 0.09892457723617554\n",
            "Train epoch - Accuracy: 0.2766906474820144 Loss: 0.10598072457656586 Corrects: 1923\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.11166371405124664\n",
            "Train step - Step 730, Loss 0.10140246152877808\n",
            "Train step - Step 740, Loss 0.10244832932949066\n",
            "Train step - Step 750, Loss 0.10457617044448853\n",
            "Train step - Step 760, Loss 0.10933797061443329\n",
            "Train epoch - Accuracy: 0.2776978417266187 Loss: 0.10612935600520895 Corrects: 1930\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.11280534416437149\n",
            "Train step - Step 780, Loss 0.1087181568145752\n",
            "Train step - Step 790, Loss 0.09839215874671936\n",
            "Train step - Step 800, Loss 0.11339609324932098\n",
            "Train step - Step 810, Loss 0.10132470726966858\n",
            "Train step - Step 820, Loss 0.1082737073302269\n",
            "Train epoch - Accuracy: 0.26863309352517983 Loss: 0.10464820479317535 Corrects: 1867\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.10834876447916031\n",
            "Train step - Step 840, Loss 0.10350314527750015\n",
            "Train step - Step 850, Loss 0.10621678084135056\n",
            "Train step - Step 860, Loss 0.09909292310476303\n",
            "Train step - Step 870, Loss 0.09620889276266098\n",
            "Train epoch - Accuracy: 0.26762589928057556 Loss: 0.10460268003691872 Corrects: 1860\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.10627227276563644\n",
            "Train step - Step 890, Loss 0.10293184220790863\n",
            "Train step - Step 900, Loss 0.11093343049287796\n",
            "Train step - Step 910, Loss 0.10613948851823807\n",
            "Train step - Step 920, Loss 0.10616752505302429\n",
            "Train step - Step 930, Loss 0.09613311290740967\n",
            "Train epoch - Accuracy: 0.2693525179856115 Loss: 0.10406966597056218 Corrects: 1872\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.1114605963230133\n",
            "Train step - Step 950, Loss 0.10215921700000763\n",
            "Train step - Step 960, Loss 0.10927510261535645\n",
            "Train step - Step 970, Loss 0.10453179478645325\n",
            "Train step - Step 980, Loss 0.10553235560655594\n",
            "Train epoch - Accuracy: 0.26258992805755393 Loss: 0.103607977427167 Corrects: 1825\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.10502316802740097\n",
            "Train step - Step 1000, Loss 0.1113705262541771\n",
            "Train step - Step 1010, Loss 0.09758469462394714\n",
            "Train step - Step 1020, Loss 0.10893942415714264\n",
            "Train step - Step 1030, Loss 0.10516441613435745\n",
            "Train step - Step 1040, Loss 0.09569200128316879\n",
            "Train epoch - Accuracy: 0.2539568345323741 Loss: 0.10290854741129087 Corrects: 1765\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.0995703861117363\n",
            "Train step - Step 1060, Loss 0.10212995857000351\n",
            "Train step - Step 1070, Loss 0.11196546256542206\n",
            "Train step - Step 1080, Loss 0.10313928872346878\n",
            "Train step - Step 1090, Loss 0.10207220166921616\n",
            "Train epoch - Accuracy: 0.25669064748201437 Loss: 0.10301289464072358 Corrects: 1784\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10645610839128494\n",
            "Train step - Step 1110, Loss 0.0995396077632904\n",
            "Train step - Step 1120, Loss 0.10030494630336761\n",
            "Train step - Step 1130, Loss 0.09833777695894241\n",
            "Train step - Step 1140, Loss 0.1093604639172554\n",
            "Train step - Step 1150, Loss 0.1001729667186737\n",
            "Train epoch - Accuracy: 0.2564028776978417 Loss: 0.10255383523248082 Corrects: 1782\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.08887413144111633\n",
            "Train step - Step 1170, Loss 0.09572362154722214\n",
            "Train step - Step 1180, Loss 0.1004401296377182\n",
            "Train step - Step 1190, Loss 0.10364698618650436\n",
            "Train step - Step 1200, Loss 0.10506810247898102\n",
            "Train epoch - Accuracy: 0.2548201438848921 Loss: 0.10199666386885609 Corrects: 1771\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.0926402136683464\n",
            "Train step - Step 1220, Loss 0.10357847064733505\n",
            "Train step - Step 1230, Loss 0.10955674201250076\n",
            "Train step - Step 1240, Loss 0.09420282393693924\n",
            "Train step - Step 1250, Loss 0.09870510548353195\n",
            "Train step - Step 1260, Loss 0.1026935800909996\n",
            "Train epoch - Accuracy: 0.24733812949640288 Loss: 0.1011485600900307 Corrects: 1719\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.09914934635162354\n",
            "Train step - Step 1280, Loss 0.10829460620880127\n",
            "Train step - Step 1290, Loss 0.1022978201508522\n",
            "Train step - Step 1300, Loss 0.09941946715116501\n",
            "Train step - Step 1310, Loss 0.09853038936853409\n",
            "Train epoch - Accuracy: 0.25007194244604314 Loss: 0.10082747690754829 Corrects: 1738\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.09026147425174713\n",
            "Train step - Step 1330, Loss 0.10857667773962021\n",
            "Train step - Step 1340, Loss 0.09554881602525711\n",
            "Train step - Step 1350, Loss 0.09618866443634033\n",
            "Train step - Step 1360, Loss 0.10779370367527008\n",
            "Train step - Step 1370, Loss 0.09564932435750961\n",
            "Train epoch - Accuracy: 0.2441726618705036 Loss: 0.1005393845014435 Corrects: 1697\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.095924973487854\n",
            "Train step - Step 1390, Loss 0.0998259037733078\n",
            "Train step - Step 1400, Loss 0.08537312597036362\n",
            "Train step - Step 1410, Loss 0.10696426033973694\n",
            "Train step - Step 1420, Loss 0.09910310059785843\n",
            "Train epoch - Accuracy: 0.24431654676258993 Loss: 0.10101445363151083 Corrects: 1698\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.0999901294708252\n",
            "Train step - Step 1440, Loss 0.09649913012981415\n",
            "Train step - Step 1450, Loss 0.09776929765939713\n",
            "Train step - Step 1460, Loss 0.09843914210796356\n",
            "Train step - Step 1470, Loss 0.10042654722929001\n",
            "Train step - Step 1480, Loss 0.1015469878911972\n",
            "Train epoch - Accuracy: 0.24244604316546764 Loss: 0.09990054677073046 Corrects: 1685\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.09277214854955673\n",
            "Train step - Step 1500, Loss 0.10534541308879852\n",
            "Train step - Step 1510, Loss 0.09729699045419693\n",
            "Train step - Step 1520, Loss 0.09661427140235901\n",
            "Train step - Step 1530, Loss 0.09005104005336761\n",
            "Train epoch - Accuracy: 0.24302158273381294 Loss: 0.09979952581494832 Corrects: 1689\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.09084787964820862\n",
            "Train step - Step 1550, Loss 0.10147731751203537\n",
            "Train step - Step 1560, Loss 0.08885334432125092\n",
            "Train step - Step 1570, Loss 0.09407209604978561\n",
            "Train step - Step 1580, Loss 0.10621587932109833\n",
            "Train step - Step 1590, Loss 0.09030909091234207\n",
            "Train epoch - Accuracy: 0.23424460431654676 Loss: 0.09912707193935517 Corrects: 1628\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.09687261283397675\n",
            "Train step - Step 1610, Loss 0.08894126117229462\n",
            "Train step - Step 1620, Loss 0.10202592611312866\n",
            "Train step - Step 1630, Loss 0.10320842266082764\n",
            "Train step - Step 1640, Loss 0.09090094268321991\n",
            "Train epoch - Accuracy: 0.23223021582733813 Loss: 0.09876734784395574 Corrects: 1614\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.0943586528301239\n",
            "Train step - Step 1660, Loss 0.10353027284145355\n",
            "Train step - Step 1670, Loss 0.10228791832923889\n",
            "Train step - Step 1680, Loss 0.10175646096467972\n",
            "Train step - Step 1690, Loss 0.10027824342250824\n",
            "Train step - Step 1700, Loss 0.1086660772562027\n",
            "Train epoch - Accuracy: 0.23597122302158274 Loss: 0.0989666526587747 Corrects: 1640\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.09369758516550064\n",
            "Train step - Step 1720, Loss 0.09572000056505203\n",
            "Train step - Step 1730, Loss 0.09797455370426178\n",
            "Train step - Step 1740, Loss 0.10637728124856949\n",
            "Train step - Step 1750, Loss 0.08984430879354477\n",
            "Train epoch - Accuracy: 0.22848920863309352 Loss: 0.09841818286360597 Corrects: 1588\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.10365457832813263\n",
            "Train step - Step 1770, Loss 0.10326310992240906\n",
            "Train step - Step 1780, Loss 0.0965663269162178\n",
            "Train step - Step 1790, Loss 0.0999024510383606\n",
            "Train step - Step 1800, Loss 0.10833129286766052\n",
            "Train step - Step 1810, Loss 0.10209479928016663\n",
            "Train epoch - Accuracy: 0.2273381294964029 Loss: 0.09860253483271428 Corrects: 1580\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.08949071168899536\n",
            "Train step - Step 1830, Loss 0.10572391003370285\n",
            "Train step - Step 1840, Loss 0.10146339982748032\n",
            "Train step - Step 1850, Loss 0.10216381400823593\n",
            "Train step - Step 1860, Loss 0.09907783567905426\n",
            "Train epoch - Accuracy: 0.22460431654676258 Loss: 0.09775715921636965 Corrects: 1561\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.08948130905628204\n",
            "Train step - Step 1880, Loss 0.0935334712266922\n",
            "Train step - Step 1890, Loss 0.09976837038993835\n",
            "Train step - Step 1900, Loss 0.10063958913087845\n",
            "Train step - Step 1910, Loss 0.09903652220964432\n",
            "Train step - Step 1920, Loss 0.09732069820165634\n",
            "Train epoch - Accuracy: 0.22964028776978418 Loss: 0.09738050958449893 Corrects: 1596\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.09269695729017258\n",
            "Train step - Step 1940, Loss 0.10067228227853775\n",
            "Train step - Step 1950, Loss 0.08946079015731812\n",
            "Train step - Step 1960, Loss 0.09078869968652725\n",
            "Train step - Step 1970, Loss 0.0941392257809639\n",
            "Train epoch - Accuracy: 0.21784172661870504 Loss: 0.0974023516379672 Corrects: 1514\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.09778133034706116\n",
            "Train step - Step 1990, Loss 0.09923213720321655\n",
            "Train step - Step 2000, Loss 0.08900704979896545\n",
            "Train step - Step 2010, Loss 0.0956878587603569\n",
            "Train step - Step 2020, Loss 0.09205569326877594\n",
            "Train step - Step 2030, Loss 0.09753022342920303\n",
            "Train epoch - Accuracy: 0.21784172661870504 Loss: 0.09685107863206657 Corrects: 1514\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.09121458232402802\n",
            "Train step - Step 2050, Loss 0.10252487659454346\n",
            "Train step - Step 2060, Loss 0.09551248699426651\n",
            "Train step - Step 2070, Loss 0.09381480515003204\n",
            "Train step - Step 2080, Loss 0.09429372847080231\n",
            "Train epoch - Accuracy: 0.21223021582733814 Loss: 0.09680040486639352 Corrects: 1475\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.09899862855672836\n",
            "Train step - Step 2100, Loss 0.09842327982187271\n",
            "Train step - Step 2110, Loss 0.09629660099744797\n",
            "Train step - Step 2120, Loss 0.10639688372612\n",
            "Train step - Step 2130, Loss 0.10016631335020065\n",
            "Train step - Step 2140, Loss 0.10639894753694534\n",
            "Train epoch - Accuracy: 0.21827338129496404 Loss: 0.09614528944595255 Corrects: 1517\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.09266749769449234\n",
            "Train step - Step 2160, Loss 0.10050179809331894\n",
            "Train step - Step 2170, Loss 0.10035251080989838\n",
            "Train step - Step 2180, Loss 0.1003020703792572\n",
            "Train step - Step 2190, Loss 0.08529861271381378\n",
            "Train epoch - Accuracy: 0.21237410071942445 Loss: 0.09607495776612124 Corrects: 1476\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.09239662438631058\n",
            "Train step - Step 2210, Loss 0.09153106808662415\n",
            "Train step - Step 2220, Loss 0.09092817455530167\n",
            "Train step - Step 2230, Loss 0.0980185717344284\n",
            "Train step - Step 2240, Loss 0.0926927924156189\n",
            "Train step - Step 2250, Loss 0.08859850466251373\n",
            "Train epoch - Accuracy: 0.20575539568345325 Loss: 0.0958017874128527 Corrects: 1430\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.10112234950065613\n",
            "Train step - Step 2270, Loss 0.10162048786878586\n",
            "Train step - Step 2280, Loss 0.09039144963026047\n",
            "Train step - Step 2290, Loss 0.09179695695638657\n",
            "Train step - Step 2300, Loss 0.09821432828903198\n",
            "Train epoch - Accuracy: 0.2041726618705036 Loss: 0.09547050280751085 Corrects: 1419\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.093880295753479\n",
            "Train step - Step 2320, Loss 0.09161191433668137\n",
            "Train step - Step 2330, Loss 0.09570540487766266\n",
            "Train step - Step 2340, Loss 0.09305030107498169\n",
            "Train step - Step 2350, Loss 0.09748534858226776\n",
            "Train step - Step 2360, Loss 0.10405217111110687\n",
            "Train epoch - Accuracy: 0.20618705035971224 Loss: 0.09509584307884998 Corrects: 1433\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.099260114133358\n",
            "Train step - Step 2380, Loss 0.08822603523731232\n",
            "Train step - Step 2390, Loss 0.10304979234933853\n",
            "Train step - Step 2400, Loss 0.08468028903007507\n",
            "Train step - Step 2410, Loss 0.08845002204179764\n",
            "Train epoch - Accuracy: 0.20589928057553958 Loss: 0.09492198794865779 Corrects: 1431\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.09900085628032684\n",
            "Train step - Step 2430, Loss 0.10029469430446625\n",
            "Train step - Step 2440, Loss 0.09385388344526291\n",
            "Train step - Step 2450, Loss 0.09926742315292358\n",
            "Train step - Step 2460, Loss 0.08701401948928833\n",
            "Train step - Step 2470, Loss 0.09070897847414017\n",
            "Train epoch - Accuracy: 0.19899280575539569 Loss: 0.09432767713670251 Corrects: 1383\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.09645789116621017\n",
            "Train step - Step 2490, Loss 0.08884701132774353\n",
            "Train step - Step 2500, Loss 0.09706106781959534\n",
            "Train step - Step 2510, Loss 0.08702066540718079\n",
            "Train step - Step 2520, Loss 0.09270066022872925\n",
            "Train epoch - Accuracy: 0.19985611510791368 Loss: 0.09375064282108554 Corrects: 1389\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.08655805885791779\n",
            "Train step - Step 2540, Loss 0.09060461074113846\n",
            "Train step - Step 2550, Loss 0.08877449482679367\n",
            "Train step - Step 2560, Loss 0.09368035197257996\n",
            "Train step - Step 2570, Loss 0.09698939323425293\n",
            "Train step - Step 2580, Loss 0.09851948171854019\n",
            "Train epoch - Accuracy: 0.19438848920863308 Loss: 0.09329917255494234 Corrects: 1351\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.09619546681642532\n",
            "Train step - Step 2600, Loss 0.08467856794595718\n",
            "Train step - Step 2610, Loss 0.09432374686002731\n",
            "Train step - Step 2620, Loss 0.0928054079413414\n",
            "Train step - Step 2630, Loss 0.09432100504636765\n",
            "Train epoch - Accuracy: 0.19611510791366907 Loss: 0.0933947192336158 Corrects: 1363\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.09126213192939758\n",
            "Train step - Step 2650, Loss 0.08890993148088455\n",
            "Train step - Step 2660, Loss 0.08513578772544861\n",
            "Train step - Step 2670, Loss 0.09189298748970032\n",
            "Train step - Step 2680, Loss 0.10199549049139023\n",
            "Train step - Step 2690, Loss 0.0858738049864769\n",
            "Train epoch - Accuracy: 0.19784172661870503 Loss: 0.09401753439963292 Corrects: 1375\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.09148646146059036\n",
            "Train step - Step 2710, Loss 0.09003469347953796\n",
            "Train step - Step 2720, Loss 0.08948604017496109\n",
            "Train step - Step 2730, Loss 0.09003811329603195\n",
            "Train step - Step 2740, Loss 0.09160411357879639\n",
            "Train epoch - Accuracy: 0.18359712230215827 Loss: 0.09047398501806123 Corrects: 1276\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.09248104691505432\n",
            "Train step - Step 2760, Loss 0.08335929363965988\n",
            "Train step - Step 2770, Loss 0.09476566314697266\n",
            "Train step - Step 2780, Loss 0.0961899533867836\n",
            "Train step - Step 2790, Loss 0.08901842683553696\n",
            "Train step - Step 2800, Loss 0.0859113410115242\n",
            "Train epoch - Accuracy: 0.1869064748201439 Loss: 0.0896765155157597 Corrects: 1299\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.09245578199625015\n",
            "Train step - Step 2820, Loss 0.09035624563694\n",
            "Train step - Step 2830, Loss 0.08593239635229111\n",
            "Train step - Step 2840, Loss 0.0880715548992157\n",
            "Train step - Step 2850, Loss 0.08872519433498383\n",
            "Train epoch - Accuracy: 0.18618705035971223 Loss: 0.0896722090651663 Corrects: 1294\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.09607786685228348\n",
            "Train step - Step 2870, Loss 0.08698438107967377\n",
            "Train step - Step 2880, Loss 0.0798674002289772\n",
            "Train step - Step 2890, Loss 0.08156266808509827\n",
            "Train step - Step 2900, Loss 0.09044269472360611\n",
            "Train step - Step 2910, Loss 0.08779783546924591\n",
            "Train epoch - Accuracy: 0.18273381294964028 Loss: 0.08928695024560682 Corrects: 1270\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.10396689176559448\n",
            "Train step - Step 2930, Loss 0.0847979411482811\n",
            "Train step - Step 2940, Loss 0.09146983176469803\n",
            "Train step - Step 2950, Loss 0.08941622078418732\n",
            "Train step - Step 2960, Loss 0.09274360537528992\n",
            "Train epoch - Accuracy: 0.18676258992805755 Loss: 0.08923848080334904 Corrects: 1298\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.08649291098117828\n",
            "Train step - Step 2980, Loss 0.09061036258935928\n",
            "Train step - Step 2990, Loss 0.08129464089870453\n",
            "Train step - Step 3000, Loss 0.08680228143930435\n",
            "Train step - Step 3010, Loss 0.087477907538414\n",
            "Train step - Step 3020, Loss 0.09655490517616272\n",
            "Train epoch - Accuracy: 0.18129496402877698 Loss: 0.08890735533597657 Corrects: 1260\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.08964407444000244\n",
            "Train step - Step 3040, Loss 0.08922364562749863\n",
            "Train step - Step 3050, Loss 0.0956825241446495\n",
            "Train step - Step 3060, Loss 0.08476810157299042\n",
            "Train step - Step 3070, Loss 0.08594207465648651\n",
            "Train epoch - Accuracy: 0.18618705035971223 Loss: 0.08902215826640027 Corrects: 1294\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.09119217097759247\n",
            "Train step - Step 3090, Loss 0.0899701938033104\n",
            "Train step - Step 3100, Loss 0.09229762107133865\n",
            "Train step - Step 3110, Loss 0.08179572969675064\n",
            "Train step - Step 3120, Loss 0.08593014627695084\n",
            "Train step - Step 3130, Loss 0.0858621895313263\n",
            "Train epoch - Accuracy: 0.17237410071942447 Loss: 0.08875658988952637 Corrects: 1198\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.0919451117515564\n",
            "Train step - Step 3150, Loss 0.09370597451925278\n",
            "Train step - Step 3160, Loss 0.08436192572116852\n",
            "Train step - Step 3170, Loss 0.0871206745505333\n",
            "Train step - Step 3180, Loss 0.08346562087535858\n",
            "Train epoch - Accuracy: 0.18172661870503598 Loss: 0.08879945832405159 Corrects: 1263\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.09020254015922546\n",
            "Train step - Step 3200, Loss 0.0956386923789978\n",
            "Train step - Step 3210, Loss 0.08644028007984161\n",
            "Train step - Step 3220, Loss 0.07891092449426651\n",
            "Train step - Step 3230, Loss 0.08604947477579117\n",
            "Train step - Step 3240, Loss 0.08897756040096283\n",
            "Train epoch - Accuracy: 0.17194244604316547 Loss: 0.08844449298630515 Corrects: 1195\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.08588654547929764\n",
            "Train step - Step 3260, Loss 0.08802572637796402\n",
            "Train step - Step 3270, Loss 0.09160412847995758\n",
            "Train step - Step 3280, Loss 0.08476155996322632\n",
            "Train step - Step 3290, Loss 0.08886080235242844\n",
            "Train epoch - Accuracy: 0.18129496402877698 Loss: 0.08896851221005693 Corrects: 1260\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.08553721755743027\n",
            "Train step - Step 3310, Loss 0.09112570434808731\n",
            "Train step - Step 3320, Loss 0.09218893200159073\n",
            "Train step - Step 3330, Loss 0.09091617912054062\n",
            "Train step - Step 3340, Loss 0.0918843150138855\n",
            "Train step - Step 3350, Loss 0.07954905927181244\n",
            "Train epoch - Accuracy: 0.18143884892086332 Loss: 0.08876602382968656 Corrects: 1261\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.08253076672554016\n",
            "Train step - Step 3370, Loss 0.08514927327632904\n",
            "Train step - Step 3380, Loss 0.0888473317027092\n",
            "Train step - Step 3390, Loss 0.08746460825204849\n",
            "Train step - Step 3400, Loss 0.08033333718776703\n",
            "Train epoch - Accuracy: 0.179568345323741 Loss: 0.08767988480681138 Corrects: 1248\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.09763135015964508\n",
            "Train step - Step 3420, Loss 0.08938527852296829\n",
            "Train step - Step 3430, Loss 0.08960827440023422\n",
            "Train step - Step 3440, Loss 0.08332696557044983\n",
            "Train step - Step 3450, Loss 0.08434619009494781\n",
            "Train step - Step 3460, Loss 0.08923725038766861\n",
            "Train epoch - Accuracy: 0.17021582733812948 Loss: 0.08860410924009282 Corrects: 1183\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.0931721031665802\n",
            "Train step - Step 3480, Loss 0.09000499546527863\n",
            "Train step - Step 3490, Loss 0.08601801842451096\n",
            "Train step - Step 3500, Loss 0.09040460735559464\n",
            "Train step - Step 3510, Loss 0.08172409236431122\n",
            "Train epoch - Accuracy: 0.17712230215827338 Loss: 0.08861238901134876 Corrects: 1231\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.0872693881392479\n",
            "Train step - Step 3530, Loss 0.08701704442501068\n",
            "Train step - Step 3540, Loss 0.08914840221405029\n",
            "Train step - Step 3550, Loss 0.0888419896364212\n",
            "Train step - Step 3560, Loss 0.09506890177726746\n",
            "Train step - Step 3570, Loss 0.09296700358390808\n",
            "Train epoch - Accuracy: 0.17280575539568346 Loss: 0.08737492801045342 Corrects: 1201\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.08419563621282578\n",
            "Train step - Step 3590, Loss 0.10200420022010803\n",
            "Train step - Step 3600, Loss 0.08608198910951614\n",
            "Train step - Step 3610, Loss 0.08692038059234619\n",
            "Train step - Step 3620, Loss 0.08311805874109268\n",
            "Train epoch - Accuracy: 0.17884892086330936 Loss: 0.08780315650667218 Corrects: 1243\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.0902489572763443\n",
            "Train step - Step 3640, Loss 0.08600766211748123\n",
            "Train step - Step 3650, Loss 0.08298788219690323\n",
            "Train step - Step 3660, Loss 0.09545447677373886\n",
            "Train step - Step 3670, Loss 0.08651015162467957\n",
            "Train step - Step 3680, Loss 0.0919831171631813\n",
            "Train epoch - Accuracy: 0.16546762589928057 Loss: 0.08757522568213853 Corrects: 1150\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.08713686466217041\n",
            "Train step - Step 3700, Loss 0.0886414647102356\n",
            "Train step - Step 3710, Loss 0.08177126944065094\n",
            "Train step - Step 3720, Loss 0.09444885700941086\n",
            "Train step - Step 3730, Loss 0.09185051172971725\n",
            "Train epoch - Accuracy: 0.17237410071942447 Loss: 0.0876271467071643 Corrects: 1198\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.08854305744171143\n",
            "Train step - Step 3750, Loss 0.08011778444051743\n",
            "Train step - Step 3760, Loss 0.08662596344947815\n",
            "Train step - Step 3770, Loss 0.08470718562602997\n",
            "Train step - Step 3780, Loss 0.09123566746711731\n",
            "Train step - Step 3790, Loss 0.0824221596121788\n",
            "Train epoch - Accuracy: 0.17553956834532375 Loss: 0.08745771445601964 Corrects: 1220\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.08159387856721878\n",
            "Train step - Step 3810, Loss 0.08547045290470123\n",
            "Train step - Step 3820, Loss 0.08558149635791779\n",
            "Train step - Step 3830, Loss 0.08403658866882324\n",
            "Train step - Step 3840, Loss 0.09031858295202255\n",
            "Train epoch - Accuracy: 0.17093525179856114 Loss: 0.0873210746178524 Corrects: 1188\n",
            "Training finished in 472.2565562725067 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24]\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc5b6790>\n",
            "Constructing exemplars of class 35\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [37573, 41733, 30167, 33720, 36480, 47650, 20477, 45626, 13414, 43100, 24333, 7218, 13641, 42998, 29325, 7113, 11508, 18371, 41660, 33372, 3465, 27228, 24700, 25605, 13156, 6216, 24005, 45626, 18145, 49625, 34610, 13774, 19766, 49543, 29894, 9160, 27456, 37955, 14294, 13294, 27490, 45701, 3456, 20765, 36741, 26480, 2585, 14797, 29607, 23869, 17775, 3131, 38499, 24710, 30459, 31374, 3465, 27629, 46064, 29177, 43820, 65, 22864, 34306, 28580, 11638]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880aa93a50>\n",
            "Constructing exemplars of class 3\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [26033, 7088, 11629, 22606, 12671, 16773, 3083, 44826, 42975, 37641, 26708, 24568, 6896, 23456, 45601, 25179, 47026, 10100, 7088, 41989, 15625, 23083, 22502, 33956, 16113, 6415, 3074, 27383, 36556, 1350, 1324, 34283, 43162, 5964, 33347, 26818, 13581, 48182, 33347, 9826, 40237, 612, 19323, 41083, 5646, 26014, 24486, 42005, 30893, 13146, 48197, 38723, 3145, 5973, 30279, 360, 3835, 20189, 3193, 24976, 21232, 13302, 15293, 42954, 15558, 34671]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88ce0090d0>\n",
            "Constructing exemplars of class 94\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [30813, 44337, 35686, 1387, 15538, 20877, 30002, 9304, 24209, 22009, 9157, 14943, 14379, 17962, 45320, 45652, 538, 27975, 40568, 19361, 48660, 2562, 44145, 31429, 40927, 9714, 32710, 21313, 17891, 2298, 9917, 28179, 12591, 35663, 44593, 18882, 43291, 48187, 15435, 11670, 4910, 18076, 13815, 9736, 8493, 29470, 15930, 14944, 10574, 10808, 31825, 9917, 14734, 47770, 1615, 10640, 40568, 44016, 41485, 2298, 23058, 35248, 14379, 13250, 45683, 9917]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88ce7cbfd0>\n",
            "Constructing exemplars of class 10\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [28091, 49103, 37077, 18361, 9703, 6642, 38253, 48264, 44562, 11486, 17875, 8458, 22697, 7006, 46846, 40997, 20186, 14384, 39956, 28233, 37823, 23512, 28098, 34081, 43754, 19477, 6426, 39752, 38253, 41600, 46530, 28166, 48258, 13554, 17980, 9386, 46846, 46245, 25426, 10822, 48922, 35787, 42166, 48217, 12816, 4776, 37647, 48942, 11486, 29819, 18383, 23922, 39934, 17256, 44165, 23317, 30127, 966, 28084, 46201, 28748, 14149, 4908, 4972, 33399, 36051]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc76ca90>\n",
            "Constructing exemplars of class 45\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [6309, 13973, 22352, 12287, 35953, 447, 27769, 185, 32160, 48033, 34202, 552, 6760, 20732, 1050, 9570, 39322, 8916, 43581, 24522, 20962, 34770, 17934, 40236, 48288, 46572, 13527, 16254, 3021, 44981, 40942, 23244, 26383, 33695, 34834, 41950, 41411, 40317, 14590, 31056, 6235, 6537, 45863, 3021, 46572, 9150, 18818, 13776, 49183, 11230, 27482, 21933, 12914, 19300, 22157, 17957, 19831, 6804, 29678, 21933, 13812, 21678, 27683, 35992, 40942, 41807]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc7d58d0>\n",
            "Constructing exemplars of class 37\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [28426, 49775, 39124, 28419, 34925, 40713, 4701, 6396, 45280, 32315, 30340, 43410, 29842, 44229, 43914, 25559, 34643, 39760, 1153, 29506, 45120, 25428, 16012, 12465, 27746, 42151, 21356, 29530, 23027, 11145, 3629, 12157, 2948, 47950, 10289, 1463, 31396, 2035, 45120, 11973, 36816, 44120, 44280, 40865, 36043, 46601, 12416, 5227, 40667, 39212, 4163, 9279, 1018, 30067, 19295, 18972, 36916, 9775, 566, 43474, 23123, 217, 7948, 24219, 31423, 37067]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880ab98ed0>\n",
            "Constructing exemplars of class 80\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [684, 30148, 5735, 13303, 22486, 45959, 16443, 42990, 20105, 19901, 28987, 48318, 1404, 34060, 30112, 1081, 46381, 11991, 7929, 11461, 48143, 16315, 37053, 24195, 23042, 47220, 3026, 37393, 48735, 48667, 42828, 42906, 11704, 30237, 34262, 35419, 6706, 45678, 28734, 32319, 1081, 8777, 31010, 7929, 15025, 46700, 22265, 2322, 31929, 49861, 1157, 4862, 16519, 28767, 4349, 17990, 38972, 15339, 30177, 1157, 40423, 192, 35513, 33348, 40550, 42500]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880abfc3d0>\n",
            "Constructing exemplars of class 72\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [40471, 31552, 9419, 27946, 42849, 1347, 44687, 41216, 7805, 44554, 33944, 20113, 35693, 30465, 22707, 25788, 5278, 13975, 45538, 1700, 38628, 1678, 35003, 43912, 40163, 26512, 3644, 3329, 13974, 33127, 7687, 37914, 6358, 29205, 18003, 10847, 12623, 10182, 14080, 38107, 38358, 17916, 13974, 8134, 35068, 7909, 38704, 8584, 13975, 21482, 15309, 25619, 39035, 44206, 37914, 23013, 10564, 2782, 8662, 10199, 40163, 15650, 3564, 7962, 41575, 19653]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880ac69cd0>\n",
            "Constructing exemplars of class 36\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [49751, 39473, 47978, 12823, 6345, 25209, 16395, 38349, 21682, 10903, 47960, 14303, 46597, 26003, 34405, 45414, 23583, 48720, 10803, 16090, 22306, 35913, 48923, 18648, 5170, 28688, 42804, 42299, 25165, 881, 43661, 43855, 24886, 12063, 2318, 15277, 26798, 30902, 41845, 46609, 33779, 3180, 16354, 29101, 39190, 47960, 9415, 9740, 27535, 5143, 44158, 48029, 2886, 32214, 19104, 45414, 44172, 41223, 9307, 15266, 16354, 10803, 24152, 5822, 36365, 4967]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f893de20b50>\n",
            "Constructing exemplars of class 24\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [12384, 5295, 24107, 24232, 29993, 47067, 28985, 30090, 32618, 41755, 38344, 25306, 13361, 7034, 26151, 31264, 49477, 35490, 49290, 38, 14741, 21849, 35409, 24366, 48635, 18040, 8565, 26938, 27871, 43335, 48163, 49379, 14400, 4279, 34279, 48515, 43138, 48844, 15821, 44519, 9064, 34001, 28721, 5501, 1458, 35673, 27816, 46174, 13216, 38564, 1959, 17149, 27621, 5955, 36888, 46385, 41776, 23249, 7288, 1506, 44835, 16361, 35673, 46596, 26117, 27621]\n",
            "I am calculating the mean of the exemplars...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...Mean of the exemplars done!!\n",
            "TEST ALL:  0.7126\n",
            "BATCH SIZE:  128\n",
            "TRAIN:  4950\n",
            "TRAIN_SET CLASSES:  [31, 11, 98, 77, 53, 33, 96, 92, 52, 12]\n",
            "VALIDATION CLASSES:  [53, 52, 98, 33, 96, 31, 92, 77, 12, 11]\n",
            "GROUP:  4\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [31, 11, 98, 77, 53, 33, 96, 92, 52, 12]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "num classes till now:  40\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.3093474805355072\n",
            "Train step - Step 10, Loss 0.1432926505804062\n",
            "Train step - Step 20, Loss 0.13307015597820282\n",
            "Train step - Step 30, Loss 0.13052375614643097\n",
            "Train step - Step 40, Loss 0.12178993225097656\n",
            "Train step - Step 50, Loss 0.11950962990522385\n",
            "Train epoch - Accuracy: 0.366955266955267 Loss: 0.14125408243393314 Corrects: 2543\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.11498947441577911\n",
            "Train step - Step 70, Loss 0.10375595092773438\n",
            "Train step - Step 80, Loss 0.11707358807325363\n",
            "Train step - Step 90, Loss 0.12091328948736191\n",
            "Train step - Step 100, Loss 0.10713545233011246\n",
            "Train epoch - Accuracy: 0.3424242424242424 Loss: 0.11284475112159187 Corrects: 2373\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.10186871141195297\n",
            "Train step - Step 120, Loss 0.11402306705713272\n",
            "Train step - Step 130, Loss 0.11146736145019531\n",
            "Train step - Step 140, Loss 0.1056932583451271\n",
            "Train step - Step 150, Loss 0.11127535253763199\n",
            "Train step - Step 160, Loss 0.11288688331842422\n",
            "Train epoch - Accuracy: 0.33910533910533913 Loss: 0.10977269843500242 Corrects: 2350\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.10954411327838898\n",
            "Train step - Step 180, Loss 0.11432462930679321\n",
            "Train step - Step 190, Loss 0.0997234582901001\n",
            "Train step - Step 200, Loss 0.11747422069311142\n",
            "Train step - Step 210, Loss 0.11304227262735367\n",
            "Train epoch - Accuracy: 0.33564213564213563 Loss: 0.10872749353384042 Corrects: 2326\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11637359112501144\n",
            "Train step - Step 230, Loss 0.10316860675811768\n",
            "Train step - Step 240, Loss 0.11095364391803741\n",
            "Train step - Step 250, Loss 0.11419868469238281\n",
            "Train step - Step 260, Loss 0.09873675554990768\n",
            "Train step - Step 270, Loss 0.10904451459646225\n",
            "Train epoch - Accuracy: 0.3331890331890332 Loss: 0.1072779901400961 Corrects: 2309\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.10993116348981857\n",
            "Train step - Step 290, Loss 0.10431639105081558\n",
            "Train step - Step 300, Loss 0.10888423770666122\n",
            "Train step - Step 310, Loss 0.09956353157758713\n",
            "Train step - Step 320, Loss 0.10688090324401855\n",
            "Train epoch - Accuracy: 0.33650793650793653 Loss: 0.10673776666538368 Corrects: 2332\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.10366251319646835\n",
            "Train step - Step 340, Loss 0.11617516726255417\n",
            "Train step - Step 350, Loss 0.11133332550525665\n",
            "Train step - Step 360, Loss 0.10170936584472656\n",
            "Train step - Step 370, Loss 0.10504027456045151\n",
            "Train step - Step 380, Loss 0.10881612449884415\n",
            "Train epoch - Accuracy: 0.332034632034632 Loss: 0.1062807347652372 Corrects: 2301\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.1049257293343544\n",
            "Train step - Step 400, Loss 0.10250749439001083\n",
            "Train step - Step 410, Loss 0.10461469739675522\n",
            "Train step - Step 420, Loss 0.10941066592931747\n",
            "Train step - Step 430, Loss 0.10283973067998886\n",
            "Train epoch - Accuracy: 0.3357864357864358 Loss: 0.10597555973872134 Corrects: 2327\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10466951131820679\n",
            "Train step - Step 450, Loss 0.10211775451898575\n",
            "Train step - Step 460, Loss 0.10946781933307648\n",
            "Train step - Step 470, Loss 0.10436496883630753\n",
            "Train step - Step 480, Loss 0.09629243612289429\n",
            "Train step - Step 490, Loss 0.10412458330392838\n",
            "Train epoch - Accuracy: 0.33304473304473303 Loss: 0.10497214693957765 Corrects: 2308\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10749931633472443\n",
            "Train step - Step 510, Loss 0.11390324681997299\n",
            "Train step - Step 520, Loss 0.1019366979598999\n",
            "Train step - Step 530, Loss 0.10119378566741943\n",
            "Train step - Step 540, Loss 0.09578566998243332\n",
            "Train epoch - Accuracy: 0.32323232323232326 Loss: 0.10514685676942484 Corrects: 2240\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.10024239867925644\n",
            "Train step - Step 560, Loss 0.09965423494577408\n",
            "Train step - Step 570, Loss 0.10226009041070938\n",
            "Train step - Step 580, Loss 0.10420971363782883\n",
            "Train step - Step 590, Loss 0.09817775338888168\n",
            "Train step - Step 600, Loss 0.10474736988544464\n",
            "Train epoch - Accuracy: 0.3207792207792208 Loss: 0.10419945426790574 Corrects: 2223\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.09800147265195847\n",
            "Train step - Step 620, Loss 0.10172348469495773\n",
            "Train step - Step 630, Loss 0.10459490120410919\n",
            "Train step - Step 640, Loss 0.10573265701532364\n",
            "Train step - Step 650, Loss 0.10199401527643204\n",
            "Train epoch - Accuracy: 0.31875901875901874 Loss: 0.10381718623474735 Corrects: 2209\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.09703846275806427\n",
            "Train step - Step 670, Loss 0.10233879089355469\n",
            "Train step - Step 680, Loss 0.10179629176855087\n",
            "Train step - Step 690, Loss 0.10908310860395432\n",
            "Train step - Step 700, Loss 0.10566776245832443\n",
            "Train step - Step 710, Loss 0.09901305288076401\n",
            "Train epoch - Accuracy: 0.312987012987013 Loss: 0.10294346618445921 Corrects: 2169\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10211586952209473\n",
            "Train step - Step 730, Loss 0.1015763208270073\n",
            "Train step - Step 740, Loss 0.10345842689275742\n",
            "Train step - Step 750, Loss 0.09952118247747421\n",
            "Train step - Step 760, Loss 0.10469797998666763\n",
            "Train epoch - Accuracy: 0.31544011544011546 Loss: 0.10359231303264568 Corrects: 2186\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.1050259992480278\n",
            "Train step - Step 780, Loss 0.10259093344211578\n",
            "Train step - Step 790, Loss 0.10094356536865234\n",
            "Train step - Step 800, Loss 0.10093388706445694\n",
            "Train step - Step 810, Loss 0.09676789492368698\n",
            "Train step - Step 820, Loss 0.10270263999700546\n",
            "Train epoch - Accuracy: 0.3164502164502164 Loss: 0.10303536473113566 Corrects: 2193\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.1078762635588646\n",
            "Train step - Step 840, Loss 0.10618889331817627\n",
            "Train step - Step 850, Loss 0.1006186231970787\n",
            "Train step - Step 860, Loss 0.10161562263965607\n",
            "Train step - Step 870, Loss 0.09748678654432297\n",
            "Train epoch - Accuracy: 0.31313131313131315 Loss: 0.1028416995964353 Corrects: 2170\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.09755352139472961\n",
            "Train step - Step 890, Loss 0.1076030507683754\n",
            "Train step - Step 900, Loss 0.1073077917098999\n",
            "Train step - Step 910, Loss 0.10010545700788498\n",
            "Train step - Step 920, Loss 0.10040853172540665\n",
            "Train step - Step 930, Loss 0.10172553360462189\n",
            "Train epoch - Accuracy: 0.30966810966810965 Loss: 0.10197416036934047 Corrects: 2146\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.10622983425855637\n",
            "Train step - Step 950, Loss 0.10602591186761856\n",
            "Train step - Step 960, Loss 0.1128833070397377\n",
            "Train step - Step 970, Loss 0.10885091125965118\n",
            "Train step - Step 980, Loss 0.10270865261554718\n",
            "Train epoch - Accuracy: 0.30432900432900434 Loss: 0.1019590022588017 Corrects: 2109\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.09858294576406479\n",
            "Train step - Step 1000, Loss 0.10853099077939987\n",
            "Train step - Step 1010, Loss 0.10396987199783325\n",
            "Train step - Step 1020, Loss 0.0949568822979927\n",
            "Train step - Step 1030, Loss 0.10194816440343857\n",
            "Train step - Step 1040, Loss 0.1063532754778862\n",
            "Train epoch - Accuracy: 0.31341991341991343 Loss: 0.10237573343954046 Corrects: 2172\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.10597751289606094\n",
            "Train step - Step 1060, Loss 0.0983988419175148\n",
            "Train step - Step 1070, Loss 0.10294574499130249\n",
            "Train step - Step 1080, Loss 0.10327208042144775\n",
            "Train step - Step 1090, Loss 0.09180496633052826\n",
            "Train epoch - Accuracy: 0.30028860028860027 Loss: 0.10130939465150517 Corrects: 2081\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10948564857244492\n",
            "Train step - Step 1110, Loss 0.10087182372808456\n",
            "Train step - Step 1120, Loss 0.10133292526006699\n",
            "Train step - Step 1130, Loss 0.10419269651174545\n",
            "Train step - Step 1140, Loss 0.10095597803592682\n",
            "Train step - Step 1150, Loss 0.10089542716741562\n",
            "Train epoch - Accuracy: 0.3072150072150072 Loss: 0.10106614624267017 Corrects: 2129\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.1000911220908165\n",
            "Train step - Step 1170, Loss 0.10285034030675888\n",
            "Train step - Step 1180, Loss 0.09620722383260727\n",
            "Train step - Step 1190, Loss 0.105915866792202\n",
            "Train step - Step 1200, Loss 0.09844017773866653\n",
            "Train epoch - Accuracy: 0.30432900432900434 Loss: 0.10116072412564125 Corrects: 2109\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.09940579533576965\n",
            "Train step - Step 1220, Loss 0.09992586821317673\n",
            "Train step - Step 1230, Loss 0.10432354360818863\n",
            "Train step - Step 1240, Loss 0.10729324072599411\n",
            "Train step - Step 1250, Loss 0.09561987966299057\n",
            "Train step - Step 1260, Loss 0.11022968590259552\n",
            "Train epoch - Accuracy: 0.3046176046176046 Loss: 0.1009870898585987 Corrects: 2111\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.09421185404062271\n",
            "Train step - Step 1280, Loss 0.10152976959943771\n",
            "Train step - Step 1290, Loss 0.10144726186990738\n",
            "Train step - Step 1300, Loss 0.10206595808267593\n",
            "Train step - Step 1310, Loss 0.10314315557479858\n",
            "Train epoch - Accuracy: 0.2976911976911977 Loss: 0.10077551149401658 Corrects: 2063\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.09781967848539352\n",
            "Train step - Step 1330, Loss 0.10117175430059433\n",
            "Train step - Step 1340, Loss 0.10591033846139908\n",
            "Train step - Step 1350, Loss 0.09635453671216965\n",
            "Train step - Step 1360, Loss 0.10397106409072876\n",
            "Train step - Step 1370, Loss 0.0970751941204071\n",
            "Train epoch - Accuracy: 0.3007215007215007 Loss: 0.10055556600916093 Corrects: 2084\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.1016133576631546\n",
            "Train step - Step 1390, Loss 0.09901722520589828\n",
            "Train step - Step 1400, Loss 0.1045135036110878\n",
            "Train step - Step 1410, Loss 0.09528299421072006\n",
            "Train step - Step 1420, Loss 0.09974300116300583\n",
            "Train epoch - Accuracy: 0.2933621933621934 Loss: 0.1001045698232809 Corrects: 2033\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.09437042474746704\n",
            "Train step - Step 1440, Loss 0.10290490835905075\n",
            "Train step - Step 1450, Loss 0.10024292767047882\n",
            "Train step - Step 1460, Loss 0.09506673365831375\n",
            "Train step - Step 1470, Loss 0.10249835252761841\n",
            "Train step - Step 1480, Loss 0.10575144737958908\n",
            "Train epoch - Accuracy: 0.2984126984126984 Loss: 0.10036200381933696 Corrects: 2068\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.10626917332410812\n",
            "Train step - Step 1500, Loss 0.10075516998767853\n",
            "Train step - Step 1510, Loss 0.10015558451414108\n",
            "Train step - Step 1520, Loss 0.10412521660327911\n",
            "Train step - Step 1530, Loss 0.09456726163625717\n",
            "Train epoch - Accuracy: 0.2904761904761905 Loss: 0.09994389665075194 Corrects: 2013\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.10038094967603683\n",
            "Train step - Step 1550, Loss 0.10184147208929062\n",
            "Train step - Step 1560, Loss 0.09253039211034775\n",
            "Train step - Step 1570, Loss 0.09781699627637863\n",
            "Train step - Step 1580, Loss 0.09610136598348618\n",
            "Train step - Step 1590, Loss 0.09033234417438507\n",
            "Train epoch - Accuracy: 0.2943722943722944 Loss: 0.09926763539448445 Corrects: 2040\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.10108902305364609\n",
            "Train step - Step 1610, Loss 0.10558267682790756\n",
            "Train step - Step 1620, Loss 0.10597570985555649\n",
            "Train step - Step 1630, Loss 0.10046186298131943\n",
            "Train step - Step 1640, Loss 0.09435097128152847\n",
            "Train epoch - Accuracy: 0.2955266955266955 Loss: 0.09993218438105837 Corrects: 2048\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.1025991439819336\n",
            "Train step - Step 1660, Loss 0.09759049862623215\n",
            "Train step - Step 1670, Loss 0.09330020099878311\n",
            "Train step - Step 1680, Loss 0.0966736301779747\n",
            "Train step - Step 1690, Loss 0.09529244154691696\n",
            "Train step - Step 1700, Loss 0.10379739105701447\n",
            "Train epoch - Accuracy: 0.2894660894660895 Loss: 0.09933855969238418 Corrects: 2006\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.10209961235523224\n",
            "Train step - Step 1720, Loss 0.09450738877058029\n",
            "Train step - Step 1730, Loss 0.10355416685342789\n",
            "Train step - Step 1740, Loss 0.10149881988763809\n",
            "Train step - Step 1750, Loss 0.10029248148202896\n",
            "Train epoch - Accuracy: 0.2896103896103896 Loss: 0.09923328661815428 Corrects: 2007\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.10360151529312134\n",
            "Train step - Step 1770, Loss 0.1009141206741333\n",
            "Train step - Step 1780, Loss 0.09461810439825058\n",
            "Train step - Step 1790, Loss 0.09883411228656769\n",
            "Train step - Step 1800, Loss 0.09413855522871017\n",
            "Train step - Step 1810, Loss 0.09987395256757736\n",
            "Train epoch - Accuracy: 0.2841269841269841 Loss: 0.09869453524915343 Corrects: 1969\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.09183412790298462\n",
            "Train step - Step 1830, Loss 0.09136207401752472\n",
            "Train step - Step 1840, Loss 0.09122077375650406\n",
            "Train step - Step 1850, Loss 0.10015889257192612\n",
            "Train step - Step 1860, Loss 0.09552782773971558\n",
            "Train epoch - Accuracy: 0.2831168831168831 Loss: 0.09864981543450128 Corrects: 1962\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.10035961121320724\n",
            "Train step - Step 1880, Loss 0.09901684522628784\n",
            "Train step - Step 1890, Loss 0.0974712148308754\n",
            "Train step - Step 1900, Loss 0.09530743211507797\n",
            "Train step - Step 1910, Loss 0.10193260759115219\n",
            "Train step - Step 1920, Loss 0.09534835070371628\n",
            "Train epoch - Accuracy: 0.2821067821067821 Loss: 0.09858241402483606 Corrects: 1955\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.09687405079603195\n",
            "Train step - Step 1940, Loss 0.0982411727309227\n",
            "Train step - Step 1950, Loss 0.10360155254602432\n",
            "Train step - Step 1960, Loss 0.09895067662000656\n",
            "Train step - Step 1970, Loss 0.10172679275274277\n",
            "Train epoch - Accuracy: 0.27215007215007214 Loss: 0.09803863161312037 Corrects: 1886\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.09422288835048676\n",
            "Train step - Step 1990, Loss 0.09538610279560089\n",
            "Train step - Step 2000, Loss 0.09497180581092834\n",
            "Train step - Step 2010, Loss 0.09282772988080978\n",
            "Train step - Step 2020, Loss 0.09495528787374496\n",
            "Train step - Step 2030, Loss 0.09656652808189392\n",
            "Train epoch - Accuracy: 0.2803751803751804 Loss: 0.09807123018033577 Corrects: 1943\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.1075037270784378\n",
            "Train step - Step 2050, Loss 0.09778472036123276\n",
            "Train step - Step 2060, Loss 0.09654394537210464\n",
            "Train step - Step 2070, Loss 0.10077283531427383\n",
            "Train step - Step 2080, Loss 0.09794559329748154\n",
            "Train epoch - Accuracy: 0.2792207792207792 Loss: 0.09837393033685106 Corrects: 1935\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.09606502205133438\n",
            "Train step - Step 2100, Loss 0.10578807443380356\n",
            "Train step - Step 2110, Loss 0.10200128704309464\n",
            "Train step - Step 2120, Loss 0.09505166858434677\n",
            "Train step - Step 2130, Loss 0.09560582041740417\n",
            "Train step - Step 2140, Loss 0.09927528351545334\n",
            "Train epoch - Accuracy: 0.2792207792207792 Loss: 0.0975519104566409 Corrects: 1935\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.10114121437072754\n",
            "Train step - Step 2160, Loss 0.09865983575582504\n",
            "Train step - Step 2170, Loss 0.09849971532821655\n",
            "Train step - Step 2180, Loss 0.1023804098367691\n",
            "Train step - Step 2190, Loss 0.10196550190448761\n",
            "Train epoch - Accuracy: 0.2728715728715729 Loss: 0.09779856942733817 Corrects: 1891\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.09606634825468063\n",
            "Train step - Step 2210, Loss 0.09674253314733505\n",
            "Train step - Step 2220, Loss 0.09450674057006836\n",
            "Train step - Step 2230, Loss 0.08926290273666382\n",
            "Train step - Step 2240, Loss 0.09519251435995102\n",
            "Train step - Step 2250, Loss 0.08880432695150375\n",
            "Train epoch - Accuracy: 0.2686868686868687 Loss: 0.09865971311690315 Corrects: 1862\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.10459978878498077\n",
            "Train step - Step 2270, Loss 0.1007935181260109\n",
            "Train step - Step 2280, Loss 0.0995316132903099\n",
            "Train step - Step 2290, Loss 0.09518292546272278\n",
            "Train step - Step 2300, Loss 0.09382643550634384\n",
            "Train epoch - Accuracy: 0.2772005772005772 Loss: 0.09787806312286149 Corrects: 1921\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.09449682384729385\n",
            "Train step - Step 2320, Loss 0.0981193333864212\n",
            "Train step - Step 2330, Loss 0.09111278504133224\n",
            "Train step - Step 2340, Loss 0.09836355596780777\n",
            "Train step - Step 2350, Loss 0.0954660102725029\n",
            "Train step - Step 2360, Loss 0.10173570364713669\n",
            "Train epoch - Accuracy: 0.2650793650793651 Loss: 0.09727069704348562 Corrects: 1837\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.09149511903524399\n",
            "Train step - Step 2380, Loss 0.09362372010946274\n",
            "Train step - Step 2390, Loss 0.09907238930463791\n",
            "Train step - Step 2400, Loss 0.10296054184436798\n",
            "Train step - Step 2410, Loss 0.10055569559335709\n",
            "Train epoch - Accuracy: 0.25656565656565655 Loss: 0.09693106135936698 Corrects: 1778\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.09897466748952866\n",
            "Train step - Step 2430, Loss 0.10194776207208633\n",
            "Train step - Step 2440, Loss 0.09545712918043137\n",
            "Train step - Step 2450, Loss 0.09990396350622177\n",
            "Train step - Step 2460, Loss 0.09791449457406998\n",
            "Train step - Step 2470, Loss 0.09626656770706177\n",
            "Train epoch - Accuracy: 0.2608946608946609 Loss: 0.09725919938276685 Corrects: 1808\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.09975146502256393\n",
            "Train step - Step 2490, Loss 0.10073786973953247\n",
            "Train step - Step 2500, Loss 0.09442217648029327\n",
            "Train step - Step 2510, Loss 0.09622187912464142\n",
            "Train step - Step 2520, Loss 0.0981878787279129\n",
            "Train epoch - Accuracy: 0.2636363636363636 Loss: 0.09655423042879133 Corrects: 1827\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.09848690032958984\n",
            "Train step - Step 2540, Loss 0.09981245547533035\n",
            "Train step - Step 2550, Loss 0.09002702683210373\n",
            "Train step - Step 2560, Loss 0.08821051567792892\n",
            "Train step - Step 2570, Loss 0.09708473831415176\n",
            "Train step - Step 2580, Loss 0.10595323890447617\n",
            "Train epoch - Accuracy: 0.26262626262626265 Loss: 0.0967780020802912 Corrects: 1820\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.0927472934126854\n",
            "Train step - Step 2600, Loss 0.09657973051071167\n",
            "Train step - Step 2610, Loss 0.09820657223463058\n",
            "Train step - Step 2620, Loss 0.10008064657449722\n",
            "Train step - Step 2630, Loss 0.09480714052915573\n",
            "Train epoch - Accuracy: 0.2571428571428571 Loss: 0.09651011442295229 Corrects: 1782\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.09500699490308762\n",
            "Train step - Step 2650, Loss 0.09670138359069824\n",
            "Train step - Step 2660, Loss 0.09840667247772217\n",
            "Train step - Step 2670, Loss 0.09408524632453918\n",
            "Train step - Step 2680, Loss 0.09502837806940079\n",
            "Train step - Step 2690, Loss 0.09175064414739609\n",
            "Train epoch - Accuracy: 0.2572871572871573 Loss: 0.09584730668329401 Corrects: 1783\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.1019272580742836\n",
            "Train step - Step 2710, Loss 0.097930409014225\n",
            "Train step - Step 2720, Loss 0.09183944761753082\n",
            "Train step - Step 2730, Loss 0.08703632652759552\n",
            "Train step - Step 2740, Loss 0.0894932970404625\n",
            "Train epoch - Accuracy: 0.25093795093795096 Loss: 0.09456546278441967 Corrects: 1739\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.0905531793832779\n",
            "Train step - Step 2760, Loss 0.09443455934524536\n",
            "Train step - Step 2770, Loss 0.09452994167804718\n",
            "Train step - Step 2780, Loss 0.10141640156507492\n",
            "Train step - Step 2790, Loss 0.08867199718952179\n",
            "Train step - Step 2800, Loss 0.09052454680204391\n",
            "Train epoch - Accuracy: 0.2474747474747475 Loss: 0.09375084815130261 Corrects: 1715\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.10172714293003082\n",
            "Train step - Step 2820, Loss 0.09082906693220139\n",
            "Train step - Step 2830, Loss 0.0996444970369339\n",
            "Train step - Step 2840, Loss 0.09257643669843674\n",
            "Train step - Step 2850, Loss 0.08754640817642212\n",
            "Train epoch - Accuracy: 0.2523809523809524 Loss: 0.09393213861346417 Corrects: 1749\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.10308754444122314\n",
            "Train step - Step 2870, Loss 0.09837666153907776\n",
            "Train step - Step 2880, Loss 0.10174589604139328\n",
            "Train step - Step 2890, Loss 0.09683582186698914\n",
            "Train step - Step 2900, Loss 0.09710165113210678\n",
            "Train step - Step 2910, Loss 0.09542306512594223\n",
            "Train epoch - Accuracy: 0.24112554112554113 Loss: 0.094062275737309 Corrects: 1671\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.09019612520933151\n",
            "Train step - Step 2930, Loss 0.09470114856958389\n",
            "Train step - Step 2940, Loss 0.0953059270977974\n",
            "Train step - Step 2950, Loss 0.09496825933456421\n",
            "Train step - Step 2960, Loss 0.09098745882511139\n",
            "Train epoch - Accuracy: 0.2481962481962482 Loss: 0.09396203062450043 Corrects: 1720\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.10168298333883286\n",
            "Train step - Step 2980, Loss 0.09953520447015762\n",
            "Train step - Step 2990, Loss 0.09467747062444687\n",
            "Train step - Step 3000, Loss 0.09160948544740677\n",
            "Train step - Step 3010, Loss 0.0969696193933487\n",
            "Train step - Step 3020, Loss 0.09161112457513809\n",
            "Train epoch - Accuracy: 0.24314574314574314 Loss: 0.09356414570350839 Corrects: 1685\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.08932068943977356\n",
            "Train step - Step 3040, Loss 0.0935785323381424\n",
            "Train step - Step 3050, Loss 0.09356403350830078\n",
            "Train step - Step 3060, Loss 0.0983395129442215\n",
            "Train step - Step 3070, Loss 0.10077722370624542\n",
            "Train epoch - Accuracy: 0.2546897546897547 Loss: 0.09327464606209751 Corrects: 1765\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.0848858579993248\n",
            "Train step - Step 3090, Loss 0.09168680757284164\n",
            "Train step - Step 3100, Loss 0.09817369282245636\n",
            "Train step - Step 3110, Loss 0.09198042005300522\n",
            "Train step - Step 3120, Loss 0.09700673818588257\n",
            "Train step - Step 3130, Loss 0.09016934782266617\n",
            "Train epoch - Accuracy: 0.25252525252525254 Loss: 0.0942395806957633 Corrects: 1750\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.08904214948415756\n",
            "Train step - Step 3150, Loss 0.09526246041059494\n",
            "Train step - Step 3160, Loss 0.08978959172964096\n",
            "Train step - Step 3170, Loss 0.0923708975315094\n",
            "Train step - Step 3180, Loss 0.10424697399139404\n",
            "Train epoch - Accuracy: 0.2502164502164502 Loss: 0.09363901251322264 Corrects: 1734\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.08649783581495285\n",
            "Train step - Step 3200, Loss 0.09143856912851334\n",
            "Train step - Step 3210, Loss 0.09868857264518738\n",
            "Train step - Step 3220, Loss 0.09537309408187866\n",
            "Train step - Step 3230, Loss 0.0940883606672287\n",
            "Train step - Step 3240, Loss 0.09304340928792953\n",
            "Train epoch - Accuracy: 0.25036075036075034 Loss: 0.09363312217666302 Corrects: 1735\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.09082552790641785\n",
            "Train step - Step 3260, Loss 0.09323592483997345\n",
            "Train step - Step 3270, Loss 0.09657955169677734\n",
            "Train step - Step 3280, Loss 0.09614690393209457\n",
            "Train step - Step 3290, Loss 0.09341403096914291\n",
            "Train epoch - Accuracy: 0.24834054834054833 Loss: 0.09371682908842918 Corrects: 1721\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.09091755002737045\n",
            "Train step - Step 3310, Loss 0.09699153155088425\n",
            "Train step - Step 3320, Loss 0.09624385088682175\n",
            "Train step - Step 3330, Loss 0.09141425043344498\n",
            "Train step - Step 3340, Loss 0.0925273522734642\n",
            "Train step - Step 3350, Loss 0.0936562716960907\n",
            "Train epoch - Accuracy: 0.2448773448773449 Loss: 0.09339959737972435 Corrects: 1697\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.09642606973648071\n",
            "Train step - Step 3370, Loss 0.09846674650907516\n",
            "Train step - Step 3380, Loss 0.0937049612402916\n",
            "Train step - Step 3390, Loss 0.09243811666965485\n",
            "Train step - Step 3400, Loss 0.09093727916479111\n",
            "Train epoch - Accuracy: 0.2528138528138528 Loss: 0.09374235710196337 Corrects: 1752\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.08477436751127243\n",
            "Train step - Step 3420, Loss 0.09006654471158981\n",
            "Train step - Step 3430, Loss 0.096705362200737\n",
            "Train step - Step 3440, Loss 0.0984780341386795\n",
            "Train step - Step 3450, Loss 0.09336739033460617\n",
            "Train step - Step 3460, Loss 0.0969758853316307\n",
            "Train epoch - Accuracy: 0.24992784992784992 Loss: 0.09363037321130607 Corrects: 1732\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.09368415176868439\n",
            "Train step - Step 3480, Loss 0.08874022215604782\n",
            "Train step - Step 3490, Loss 0.09609157592058182\n",
            "Train step - Step 3500, Loss 0.08579733222723007\n",
            "Train step - Step 3510, Loss 0.08694207668304443\n",
            "Train epoch - Accuracy: 0.2505050505050505 Loss: 0.09367824444523105 Corrects: 1736\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.08923226594924927\n",
            "Train step - Step 3530, Loss 0.0974816083908081\n",
            "Train step - Step 3540, Loss 0.09240411967039108\n",
            "Train step - Step 3550, Loss 0.09293758869171143\n",
            "Train step - Step 3560, Loss 0.09459233283996582\n",
            "Train step - Step 3570, Loss 0.09602170437574387\n",
            "Train epoch - Accuracy: 0.2494949494949495 Loss: 0.09295603338559125 Corrects: 1729\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.0888388380408287\n",
            "Train step - Step 3590, Loss 0.09339480847120285\n",
            "Train step - Step 3600, Loss 0.09034454077482224\n",
            "Train step - Step 3610, Loss 0.08910391479730606\n",
            "Train step - Step 3620, Loss 0.09300430864095688\n",
            "Train epoch - Accuracy: 0.24891774891774893 Loss: 0.09319152102884964 Corrects: 1725\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.09163154661655426\n",
            "Train step - Step 3640, Loss 0.09370823949575424\n",
            "Train step - Step 3650, Loss 0.08810345828533173\n",
            "Train step - Step 3660, Loss 0.08452059328556061\n",
            "Train step - Step 3670, Loss 0.0951361283659935\n",
            "Train step - Step 3680, Loss 0.09345381706953049\n",
            "Train epoch - Accuracy: 0.24213564213564215 Loss: 0.09278998575341306 Corrects: 1678\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.08732195943593979\n",
            "Train step - Step 3700, Loss 0.08671197295188904\n",
            "Train step - Step 3710, Loss 0.09273864328861237\n",
            "Train step - Step 3720, Loss 0.09073039144277573\n",
            "Train step - Step 3730, Loss 0.09570685774087906\n",
            "Train epoch - Accuracy: 0.24444444444444444 Loss: 0.09299553327402167 Corrects: 1694\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.09190520644187927\n",
            "Train step - Step 3750, Loss 0.08894076943397522\n",
            "Train step - Step 3760, Loss 0.0952315479516983\n",
            "Train step - Step 3770, Loss 0.09359093755483627\n",
            "Train step - Step 3780, Loss 0.09036589413881302\n",
            "Train step - Step 3790, Loss 0.09772851318120956\n",
            "Train epoch - Accuracy: 0.2404040404040404 Loss: 0.09326985757932346 Corrects: 1666\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.09226081520318985\n",
            "Train step - Step 3810, Loss 0.08761707693338394\n",
            "Train step - Step 3820, Loss 0.09451448172330856\n",
            "Train step - Step 3830, Loss 0.09409945458173752\n",
            "Train step - Step 3840, Loss 0.09707999974489212\n",
            "Train epoch - Accuracy: 0.244011544011544 Loss: 0.09299695573069833 Corrects: 1691\n",
            "Training finished in 470.47372913360596 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12]\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc575f90>\n",
            "Constructing exemplars of class 31\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [2187, 33892, 1315, 25592, 2066, 48403, 35039, 15819, 45651, 26286, 25141, 28948, 36062, 11468, 26118, 14420, 29386, 35474, 49438, 4506, 12022, 27126, 43437, 30565, 24718, 4549, 24474, 32514, 31798, 27652, 28294, 29560, 14138, 33584, 33430, 14204, 27076, 24128, 20000, 29212, 33089, 30179, 3427, 19082, 8486, 35687, 40691, 8308, 31798, 5836]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88cec20a90>\n",
            "Constructing exemplars of class 11\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [6841, 36685, 27864, 23173, 25486, 5246, 44945, 49530, 9903, 4443, 11325, 39916, 24227, 7840, 15806, 8359, 4040, 32003, 33257, 25567, 45524, 8142, 5246, 26093, 30603, 42551, 10207, 30744, 17876, 8719, 24361, 4890, 21793, 27413, 4276, 10710, 20273, 49803, 32087, 49514, 8593, 28418, 8690, 33743, 44058, 16218, 24840, 3, 27864, 19768]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880aa99b90>\n",
            "Constructing exemplars of class 98\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [8208, 39564, 20294, 40263, 26862, 42283, 34528, 6319, 2317, 38341, 46965, 15835, 45591, 27795, 31348, 31312, 33834, 39452, 8375, 14056, 29088, 3766, 9966, 21443, 48221, 12567, 22963, 13666, 18539, 29028, 7050, 11907, 21782, 48054, 5913, 23349, 8118, 23584, 25449, 9334, 28425, 25848, 6657, 49670, 37063, 25961, 24518, 4271, 35906, 24952]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880aa70990>\n",
            "Constructing exemplars of class 77\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [23288, 38179, 10001, 24059, 19254, 16925, 28232, 4282, 32590, 3358, 2988, 7833, 30387, 6676, 2665, 26181, 21383, 216, 5699, 25295, 3299, 12598, 43358, 30357, 622, 16925, 35588, 45918, 15388, 30628, 7833, 17804, 16310, 32721, 43358, 29624, 48578, 44877, 20968, 7987, 42687, 46282, 31643, 21876, 17030, 17644, 19378, 38618, 18490, 34289]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880aa75290>\n",
            "Constructing exemplars of class 53\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [25985, 20991, 30854, 44174, 26769, 15813, 19337, 40970, 31218, 10593, 7756, 38994, 21739, 40352, 5022, 4727, 46029, 19374, 41365, 18054, 6955, 752, 22903, 6179, 26610, 39199, 26894, 27141, 15315, 6955, 27441, 3732, 15240, 43503, 37223, 42372, 2350, 47235, 1427, 10566, 43020, 26054, 20264, 1059, 31517, 18645, 3573, 3209, 36342, 19255]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc565710>\n",
            "Constructing exemplars of class 33\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [14851, 10967, 16418, 32429, 36861, 13478, 3122, 26783, 33502, 49607, 17706, 15881, 22739, 19975, 36840, 30496, 33370, 7248, 49854, 19081, 512, 38873, 4234, 32429, 6128, 23864, 40592, 4557, 45067, 3097, 9339, 27225, 34173, 6437, 42811, 45972, 36738, 13345, 33614, 17171, 39497, 7996, 29719, 9323, 22509, 47001, 21861, 7924, 43132, 31029]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88ceea7f10>\n",
            "Constructing exemplars of class 96\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [23566, 23118, 1823, 46076, 41214, 21700, 16409, 24890, 18212, 15849, 23542, 40509, 49878, 49038, 45012, 44895, 16426, 25808, 22207, 37342, 17640, 9244, 10669, 43807, 19208, 35790, 35249, 6170, 45594, 7301, 17725, 29757, 45779, 40383, 17587, 27244, 43528, 46418, 22236, 27987, 41879, 12677, 13450, 7765, 6412, 16789, 11, 38757, 11674, 2285]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88ceec8250>\n",
            "Constructing exemplars of class 92\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [40697, 11845, 27057, 16423, 34854, 19798, 23359, 38755, 70, 13140, 42542, 48773, 43910, 9117, 7538, 42364, 3279, 24269, 23865, 37903, 36725, 10602, 10600, 8027, 38550, 24476, 31176, 43797, 35966, 12038, 35699, 45371, 20922, 8195, 5448, 41056, 9688, 21544, 26547, 31635, 12128, 17050, 8430, 46891, 42271, 8502, 10945, 16683, 27224, 25226]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880ab98510>\n",
            "Constructing exemplars of class 52\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [9970, 5210, 4927, 23954, 47824, 29086, 11503, 6346, 47781, 48936, 18287, 37928, 35974, 1567, 1431, 42513, 17855, 1807, 17290, 8927, 25264, 47023, 2978, 49093, 35232, 39369, 26067, 7905, 14030, 42571, 12092, 3909, 6943, 48739, 30574, 11922, 26359, 35263, 22997, 16658, 7900, 602, 11503, 14705, 33200, 19090, 31237, 29729, 15520, 45647]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88ce3abd90>\n",
            "Constructing exemplars of class 12\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [17359, 23154, 24312, 49138, 12123, 46930, 42152, 43929, 30392, 21355, 13408, 8400, 20119, 42691, 11590, 22039, 13033, 49078, 34027, 24388, 20958, 42798, 24250, 48740, 43012, 23918, 11022, 42440, 44036, 47276, 10307, 20193, 48423, 29854, 47605, 1084, 35827, 11891, 35168, 16869, 30987, 43048, 37904, 29337, 38044, 16898, 16677, 37904, 7825, 25148]\n",
            "I am calculating the mean of the exemplars...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...Mean of the exemplars done!!\n",
            "TEST ALL:  0.6984\n",
            "BATCH SIZE:  128\n",
            "TRAIN:  4950\n",
            "TRAIN_SET CLASSES:  [91, 67, 7, 62, 46, 65, 21, 32, 16, 0]\n",
            "VALIDATION CLASSES:  [62, 46, 32, 91, 21, 16, 7, 67, 65, 0]\n",
            "GROUP:  5\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [91, 67, 7, 62, 46, 65, 21, 32, 16, 0]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  50\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.29321783781051636\n",
            "Train step - Step 10, Loss 0.14722761511802673\n",
            "Train step - Step 20, Loss 0.13995756208896637\n",
            "Train step - Step 30, Loss 0.13692544400691986\n",
            "Train step - Step 40, Loss 0.12730672955513\n",
            "Train step - Step 50, Loss 0.12666916847229004\n",
            "Train epoch - Accuracy: 0.47784172661870505 Loss: 0.14437141668667897 Corrects: 3321\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12012088298797607\n",
            "Train step - Step 70, Loss 0.11550794541835785\n",
            "Train step - Step 80, Loss 0.11194560676813126\n",
            "Train step - Step 90, Loss 0.11637410521507263\n",
            "Train step - Step 100, Loss 0.12011498957872391\n",
            "Train epoch - Accuracy: 0.46014388489208635 Loss: 0.11681348353409939 Corrects: 3198\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11357762664556503\n",
            "Train step - Step 120, Loss 0.11658252775669098\n",
            "Train step - Step 130, Loss 0.11574000120162964\n",
            "Train step - Step 140, Loss 0.1192956492304802\n",
            "Train step - Step 150, Loss 0.1161159873008728\n",
            "Train step - Step 160, Loss 0.11694017052650452\n",
            "Train epoch - Accuracy: 0.4627338129496403 Loss: 0.11433014752839109 Corrects: 3216\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.11409803479909897\n",
            "Train step - Step 180, Loss 0.11724135279655457\n",
            "Train step - Step 190, Loss 0.11106698960065842\n",
            "Train step - Step 200, Loss 0.11017268896102905\n",
            "Train step - Step 210, Loss 0.11228079348802567\n",
            "Train epoch - Accuracy: 0.4618705035971223 Loss: 0.11326217081906984 Corrects: 3210\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.10826194286346436\n",
            "Train step - Step 230, Loss 0.10670861601829529\n",
            "Train step - Step 240, Loss 0.10636802762746811\n",
            "Train step - Step 250, Loss 0.1132647842168808\n",
            "Train step - Step 260, Loss 0.11333154141902924\n",
            "Train step - Step 270, Loss 0.11121076345443726\n",
            "Train epoch - Accuracy: 0.45453237410071945 Loss: 0.11192885760351909 Corrects: 3159\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.11564746499061584\n",
            "Train step - Step 290, Loss 0.1107669398188591\n",
            "Train step - Step 300, Loss 0.1118127629160881\n",
            "Train step - Step 310, Loss 0.11242210119962692\n",
            "Train step - Step 320, Loss 0.11465413123369217\n",
            "Train epoch - Accuracy: 0.44848920863309355 Loss: 0.11153919303803135 Corrects: 3117\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11759310215711594\n",
            "Train step - Step 340, Loss 0.11338792741298676\n",
            "Train step - Step 350, Loss 0.11397823691368103\n",
            "Train step - Step 360, Loss 0.1158013716340065\n",
            "Train step - Step 370, Loss 0.11416316777467728\n",
            "Train step - Step 380, Loss 0.10992663353681564\n",
            "Train epoch - Accuracy: 0.4503597122302158 Loss: 0.1111452223049651 Corrects: 3130\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.10100787878036499\n",
            "Train step - Step 400, Loss 0.11441825330257416\n",
            "Train step - Step 410, Loss 0.11216909438371658\n",
            "Train step - Step 420, Loss 0.10975725203752518\n",
            "Train step - Step 430, Loss 0.1105191558599472\n",
            "Train epoch - Accuracy: 0.44719424460431656 Loss: 0.11061247399385027 Corrects: 3108\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10962586104869843\n",
            "Train step - Step 450, Loss 0.11238709837198257\n",
            "Train step - Step 460, Loss 0.10975729674100876\n",
            "Train step - Step 470, Loss 0.1062125563621521\n",
            "Train step - Step 480, Loss 0.1150420755147934\n",
            "Train step - Step 490, Loss 0.11272352188825607\n",
            "Train epoch - Accuracy: 0.4427338129496403 Loss: 0.11015944758765131 Corrects: 3077\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10096052289009094\n",
            "Train step - Step 510, Loss 0.11430062353610992\n",
            "Train step - Step 520, Loss 0.11116444319486618\n",
            "Train step - Step 530, Loss 0.10966209322214127\n",
            "Train step - Step 540, Loss 0.11410915106534958\n",
            "Train epoch - Accuracy: 0.4365467625899281 Loss: 0.11000771390877181 Corrects: 3034\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.11593911796808243\n",
            "Train step - Step 560, Loss 0.10299767553806305\n",
            "Train step - Step 570, Loss 0.1092522144317627\n",
            "Train step - Step 580, Loss 0.10809797793626785\n",
            "Train step - Step 590, Loss 0.112185038626194\n",
            "Train step - Step 600, Loss 0.10770914703607559\n",
            "Train epoch - Accuracy: 0.43553956834532376 Loss: 0.10943715452290267 Corrects: 3027\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.10201489925384521\n",
            "Train step - Step 620, Loss 0.11144424229860306\n",
            "Train step - Step 630, Loss 0.10753181576728821\n",
            "Train step - Step 640, Loss 0.10227477550506592\n",
            "Train step - Step 650, Loss 0.10889769345521927\n",
            "Train epoch - Accuracy: 0.4469064748201439 Loss: 0.10919410541975241 Corrects: 3106\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.11318809539079666\n",
            "Train step - Step 670, Loss 0.10951140522956848\n",
            "Train step - Step 680, Loss 0.10378359258174896\n",
            "Train step - Step 690, Loss 0.1125468835234642\n",
            "Train step - Step 700, Loss 0.11145200580358505\n",
            "Train step - Step 710, Loss 0.11400005966424942\n",
            "Train epoch - Accuracy: 0.440863309352518 Loss: 0.10908121783527538 Corrects: 3064\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10861865431070328\n",
            "Train step - Step 730, Loss 0.10429611057043076\n",
            "Train step - Step 740, Loss 0.10778281837701797\n",
            "Train step - Step 750, Loss 0.1087384968996048\n",
            "Train step - Step 760, Loss 0.10175859928131104\n",
            "Train epoch - Accuracy: 0.43151079136690645 Loss: 0.10859671947338598 Corrects: 2999\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.11095933616161346\n",
            "Train step - Step 780, Loss 0.11331164836883545\n",
            "Train step - Step 790, Loss 0.1133711040019989\n",
            "Train step - Step 800, Loss 0.10470806062221527\n",
            "Train step - Step 810, Loss 0.11585325747728348\n",
            "Train step - Step 820, Loss 0.10768706351518631\n",
            "Train epoch - Accuracy: 0.43510791366906476 Loss: 0.10838595615445282 Corrects: 3024\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.1053091287612915\n",
            "Train step - Step 840, Loss 0.10630462318658829\n",
            "Train step - Step 850, Loss 0.11216407269239426\n",
            "Train step - Step 860, Loss 0.11464973539113998\n",
            "Train step - Step 870, Loss 0.11196206510066986\n",
            "Train epoch - Accuracy: 0.43640287769784175 Loss: 0.10835228432425492 Corrects: 3033\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.10478373616933823\n",
            "Train step - Step 890, Loss 0.10521598905324936\n",
            "Train step - Step 900, Loss 0.10096197575330734\n",
            "Train step - Step 910, Loss 0.10053423047065735\n",
            "Train step - Step 920, Loss 0.10602700710296631\n",
            "Train step - Step 930, Loss 0.11032002419233322\n",
            "Train epoch - Accuracy: 0.4307913669064748 Loss: 0.10739902618977663 Corrects: 2994\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.10498140007257462\n",
            "Train step - Step 950, Loss 0.10428130626678467\n",
            "Train step - Step 960, Loss 0.10906088352203369\n",
            "Train step - Step 970, Loss 0.10252698510885239\n",
            "Train step - Step 980, Loss 0.09817056357860565\n",
            "Train epoch - Accuracy: 0.4352517985611511 Loss: 0.10780169548748209 Corrects: 3025\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.10568792372941971\n",
            "Train step - Step 1000, Loss 0.10831578820943832\n",
            "Train step - Step 1010, Loss 0.10536058247089386\n",
            "Train step - Step 1020, Loss 0.10654719918966293\n",
            "Train step - Step 1030, Loss 0.1073390394449234\n",
            "Train step - Step 1040, Loss 0.10489877313375473\n",
            "Train epoch - Accuracy: 0.4307913669064748 Loss: 0.10745917981477092 Corrects: 2994\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.10892423987388611\n",
            "Train step - Step 1060, Loss 0.11042612046003342\n",
            "Train step - Step 1070, Loss 0.10051605105400085\n",
            "Train step - Step 1080, Loss 0.1082938015460968\n",
            "Train step - Step 1090, Loss 0.10555236786603928\n",
            "Train epoch - Accuracy: 0.4305035971223022 Loss: 0.10722455716604809 Corrects: 2992\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10691031813621521\n",
            "Train step - Step 1110, Loss 0.10662808269262314\n",
            "Train step - Step 1120, Loss 0.10401979088783264\n",
            "Train step - Step 1130, Loss 0.10640513896942139\n",
            "Train step - Step 1140, Loss 0.10560418665409088\n",
            "Train step - Step 1150, Loss 0.1110016405582428\n",
            "Train epoch - Accuracy: 0.4273381294964029 Loss: 0.10717639740851286 Corrects: 2970\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10083888471126556\n",
            "Train step - Step 1170, Loss 0.11110639572143555\n",
            "Train step - Step 1180, Loss 0.10456991195678711\n",
            "Train step - Step 1190, Loss 0.10572884976863861\n",
            "Train step - Step 1200, Loss 0.11284734308719635\n",
            "Train epoch - Accuracy: 0.4279136690647482 Loss: 0.10713678643643428 Corrects: 2974\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.1003461703658104\n",
            "Train step - Step 1220, Loss 0.10331392288208008\n",
            "Train step - Step 1230, Loss 0.10745557397603989\n",
            "Train step - Step 1240, Loss 0.10236785560846329\n",
            "Train step - Step 1250, Loss 0.10580775886774063\n",
            "Train step - Step 1260, Loss 0.10808251798152924\n",
            "Train epoch - Accuracy: 0.41553956834532374 Loss: 0.10648140128996732 Corrects: 2888\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.10879409313201904\n",
            "Train step - Step 1280, Loss 0.10614071786403656\n",
            "Train step - Step 1290, Loss 0.10238458216190338\n",
            "Train step - Step 1300, Loss 0.10319638252258301\n",
            "Train step - Step 1310, Loss 0.10427743941545486\n",
            "Train epoch - Accuracy: 0.42115107913669064 Loss: 0.10637368652889197 Corrects: 2927\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.10756279528141022\n",
            "Train step - Step 1330, Loss 0.10731559246778488\n",
            "Train step - Step 1340, Loss 0.10337124764919281\n",
            "Train step - Step 1350, Loss 0.10092094540596008\n",
            "Train step - Step 1360, Loss 0.10879695415496826\n",
            "Train step - Step 1370, Loss 0.0990876629948616\n",
            "Train epoch - Accuracy: 0.4115107913669065 Loss: 0.1063294007666677 Corrects: 2860\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.10301393270492554\n",
            "Train step - Step 1390, Loss 0.11098174750804901\n",
            "Train step - Step 1400, Loss 0.10620510578155518\n",
            "Train step - Step 1410, Loss 0.10337719321250916\n",
            "Train step - Step 1420, Loss 0.11008475720882416\n",
            "Train epoch - Accuracy: 0.4132374100719424 Loss: 0.10589289812518539 Corrects: 2872\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.10230834782123566\n",
            "Train step - Step 1440, Loss 0.10896564275026321\n",
            "Train step - Step 1450, Loss 0.10422670096158981\n",
            "Train step - Step 1460, Loss 0.10258901119232178\n",
            "Train step - Step 1470, Loss 0.10252704471349716\n",
            "Train step - Step 1480, Loss 0.11487940698862076\n",
            "Train epoch - Accuracy: 0.40992805755395684 Loss: 0.10596082775069655 Corrects: 2849\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.10579773038625717\n",
            "Train step - Step 1500, Loss 0.10222190618515015\n",
            "Train step - Step 1510, Loss 0.11005605757236481\n",
            "Train step - Step 1520, Loss 0.10140205174684525\n",
            "Train step - Step 1530, Loss 0.10800012946128845\n",
            "Train epoch - Accuracy: 0.41007194244604317 Loss: 0.10591866782457708 Corrects: 2850\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.10495683550834656\n",
            "Train step - Step 1550, Loss 0.10762310773134232\n",
            "Train step - Step 1560, Loss 0.11040544509887695\n",
            "Train step - Step 1570, Loss 0.10075099766254425\n",
            "Train step - Step 1580, Loss 0.1032942533493042\n",
            "Train step - Step 1590, Loss 0.10893592238426208\n",
            "Train epoch - Accuracy: 0.4050359712230216 Loss: 0.10580345998350665 Corrects: 2815\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.10568097978830338\n",
            "Train step - Step 1610, Loss 0.10308524966239929\n",
            "Train step - Step 1620, Loss 0.10388194769620895\n",
            "Train step - Step 1630, Loss 0.09799381345510483\n",
            "Train step - Step 1640, Loss 0.11093610525131226\n",
            "Train epoch - Accuracy: 0.40820143884892085 Loss: 0.10549652889263715 Corrects: 2837\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.10722614079713821\n",
            "Train step - Step 1660, Loss 0.10227153450250626\n",
            "Train step - Step 1670, Loss 0.10193075239658356\n",
            "Train step - Step 1680, Loss 0.10670915246009827\n",
            "Train step - Step 1690, Loss 0.10838260501623154\n",
            "Train step - Step 1700, Loss 0.10541386157274246\n",
            "Train epoch - Accuracy: 0.4064748201438849 Loss: 0.105239485995375 Corrects: 2825\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.10405906289815903\n",
            "Train step - Step 1720, Loss 0.1044885590672493\n",
            "Train step - Step 1730, Loss 0.10181877762079239\n",
            "Train step - Step 1740, Loss 0.09709766507148743\n",
            "Train step - Step 1750, Loss 0.10276798158884048\n",
            "Train epoch - Accuracy: 0.39568345323741005 Loss: 0.10504204994054149 Corrects: 2750\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.1150527074933052\n",
            "Train step - Step 1770, Loss 0.10338941216468811\n",
            "Train step - Step 1780, Loss 0.10217110067605972\n",
            "Train step - Step 1790, Loss 0.11287166178226471\n",
            "Train step - Step 1800, Loss 0.099840447306633\n",
            "Train step - Step 1810, Loss 0.10403977334499359\n",
            "Train epoch - Accuracy: 0.4060431654676259 Loss: 0.10546612983770508 Corrects: 2822\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.10095879435539246\n",
            "Train step - Step 1830, Loss 0.11143476516008377\n",
            "Train step - Step 1840, Loss 0.10266704112291336\n",
            "Train step - Step 1850, Loss 0.11155952513217926\n",
            "Train step - Step 1860, Loss 0.10531720519065857\n",
            "Train epoch - Accuracy: 0.4050359712230216 Loss: 0.1049865833546618 Corrects: 2815\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.10022182017564774\n",
            "Train step - Step 1880, Loss 0.0995958149433136\n",
            "Train step - Step 1890, Loss 0.1089031770825386\n",
            "Train step - Step 1900, Loss 0.10610202699899673\n",
            "Train step - Step 1910, Loss 0.10386257618665695\n",
            "Train step - Step 1920, Loss 0.10764709115028381\n",
            "Train epoch - Accuracy: 0.39309352517985613 Loss: 0.10498068549864584 Corrects: 2732\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.10071855038404465\n",
            "Train step - Step 1940, Loss 0.10816016793251038\n",
            "Train step - Step 1950, Loss 0.11116927862167358\n",
            "Train step - Step 1960, Loss 0.10280518978834152\n",
            "Train step - Step 1970, Loss 0.10625888407230377\n",
            "Train epoch - Accuracy: 0.4002877697841727 Loss: 0.10492566763497085 Corrects: 2782\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.09738535434007645\n",
            "Train step - Step 1990, Loss 0.103914774954319\n",
            "Train step - Step 2000, Loss 0.09977493435144424\n",
            "Train step - Step 2010, Loss 0.10435613989830017\n",
            "Train step - Step 2020, Loss 0.10003206133842468\n",
            "Train step - Step 2030, Loss 0.10352759063243866\n",
            "Train epoch - Accuracy: 0.38848920863309355 Loss: 0.10434623763072405 Corrects: 2700\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.10508286207914352\n",
            "Train step - Step 2050, Loss 0.10529998689889908\n",
            "Train step - Step 2060, Loss 0.10450775921344757\n",
            "Train step - Step 2070, Loss 0.1090548112988472\n",
            "Train step - Step 2080, Loss 0.10380636155605316\n",
            "Train epoch - Accuracy: 0.3964028776978417 Loss: 0.10403129662112366 Corrects: 2755\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.10119310766458511\n",
            "Train step - Step 2100, Loss 0.10294687002897263\n",
            "Train step - Step 2110, Loss 0.10796166956424713\n",
            "Train step - Step 2120, Loss 0.10837345570325851\n",
            "Train step - Step 2130, Loss 0.09719782322645187\n",
            "Train step - Step 2140, Loss 0.10198206454515457\n",
            "Train epoch - Accuracy: 0.39223021582733814 Loss: 0.10461244963484702 Corrects: 2726\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.10585853457450867\n",
            "Train step - Step 2160, Loss 0.10146982222795486\n",
            "Train step - Step 2170, Loss 0.10178029537200928\n",
            "Train step - Step 2180, Loss 0.09966513514518738\n",
            "Train step - Step 2190, Loss 0.1044754683971405\n",
            "Train epoch - Accuracy: 0.38676258992805757 Loss: 0.10424443811392613 Corrects: 2688\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.10092021524906158\n",
            "Train step - Step 2210, Loss 0.10740983486175537\n",
            "Train step - Step 2220, Loss 0.1036796048283577\n",
            "Train step - Step 2230, Loss 0.10126907378435135\n",
            "Train step - Step 2240, Loss 0.1085081398487091\n",
            "Train step - Step 2250, Loss 0.10480540990829468\n",
            "Train epoch - Accuracy: 0.3938129496402878 Loss: 0.10424201264870253 Corrects: 2737\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.1082892194390297\n",
            "Train step - Step 2270, Loss 0.10092690587043762\n",
            "Train step - Step 2280, Loss 0.10320670902729034\n",
            "Train step - Step 2290, Loss 0.10201987624168396\n",
            "Train step - Step 2300, Loss 0.09904546290636063\n",
            "Train epoch - Accuracy: 0.38661870503597123 Loss: 0.1039785065282163 Corrects: 2687\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.10787799954414368\n",
            "Train step - Step 2320, Loss 0.10093158483505249\n",
            "Train step - Step 2330, Loss 0.10622529685497284\n",
            "Train step - Step 2340, Loss 0.10826217383146286\n",
            "Train step - Step 2350, Loss 0.10457073152065277\n",
            "Train step - Step 2360, Loss 0.10397500544786453\n",
            "Train epoch - Accuracy: 0.38115107913669066 Loss: 0.10397665715689282 Corrects: 2649\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.10114865750074387\n",
            "Train step - Step 2380, Loss 0.10628478974103928\n",
            "Train step - Step 2390, Loss 0.10370337218046188\n",
            "Train step - Step 2400, Loss 0.09989968687295914\n",
            "Train step - Step 2410, Loss 0.10573408007621765\n",
            "Train epoch - Accuracy: 0.37366906474820144 Loss: 0.10337326082823088 Corrects: 2597\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.10442307591438293\n",
            "Train step - Step 2430, Loss 0.09995362907648087\n",
            "Train step - Step 2440, Loss 0.10236835479736328\n",
            "Train step - Step 2450, Loss 0.10493167489767075\n",
            "Train step - Step 2460, Loss 0.09989480674266815\n",
            "Train step - Step 2470, Loss 0.09960201382637024\n",
            "Train epoch - Accuracy: 0.3794244604316547 Loss: 0.10350102146752446 Corrects: 2637\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.10237215459346771\n",
            "Train step - Step 2490, Loss 0.10251940786838531\n",
            "Train step - Step 2500, Loss 0.0977090373635292\n",
            "Train step - Step 2510, Loss 0.10285266488790512\n",
            "Train step - Step 2520, Loss 0.10471370816230774\n",
            "Train epoch - Accuracy: 0.37640287769784175 Loss: 0.10352388735083368 Corrects: 2616\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.11409300565719604\n",
            "Train step - Step 2540, Loss 0.10050887614488602\n",
            "Train step - Step 2550, Loss 0.10597065091133118\n",
            "Train step - Step 2560, Loss 0.10668271780014038\n",
            "Train step - Step 2570, Loss 0.10299848020076752\n",
            "Train step - Step 2580, Loss 0.09955428540706635\n",
            "Train epoch - Accuracy: 0.36633093525179855 Loss: 0.10365920535094446 Corrects: 2546\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.09540347009897232\n",
            "Train step - Step 2600, Loss 0.10076741129159927\n",
            "Train step - Step 2610, Loss 0.10300777852535248\n",
            "Train step - Step 2620, Loss 0.10934313386678696\n",
            "Train step - Step 2630, Loss 0.10143035650253296\n",
            "Train epoch - Accuracy: 0.377841726618705 Loss: 0.10330165399278668 Corrects: 2626\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.10806284844875336\n",
            "Train step - Step 2650, Loss 0.10624749958515167\n",
            "Train step - Step 2660, Loss 0.10160592943429947\n",
            "Train step - Step 2670, Loss 0.10304498672485352\n",
            "Train step - Step 2680, Loss 0.11243775486946106\n",
            "Train step - Step 2690, Loss 0.10412882268428802\n",
            "Train epoch - Accuracy: 0.3713669064748201 Loss: 0.10314350330143524 Corrects: 2581\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.10410993546247482\n",
            "Train step - Step 2710, Loss 0.0951065644621849\n",
            "Train step - Step 2720, Loss 0.10173171013593674\n",
            "Train step - Step 2730, Loss 0.10650123655796051\n",
            "Train step - Step 2740, Loss 0.10105327516794205\n",
            "Train epoch - Accuracy: 0.3713669064748201 Loss: 0.10268767111593013 Corrects: 2581\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.10064273327589035\n",
            "Train step - Step 2760, Loss 0.11405732482671738\n",
            "Train step - Step 2770, Loss 0.09787321090698242\n",
            "Train step - Step 2780, Loss 0.0976269394159317\n",
            "Train step - Step 2790, Loss 0.10086289793252945\n",
            "Train step - Step 2800, Loss 0.09834166616201401\n",
            "Train epoch - Accuracy: 0.36633093525179855 Loss: 0.10169643299185115 Corrects: 2546\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.10800626873970032\n",
            "Train step - Step 2820, Loss 0.09908002614974976\n",
            "Train step - Step 2830, Loss 0.10341683775186539\n",
            "Train step - Step 2840, Loss 0.10827035456895828\n",
            "Train step - Step 2850, Loss 0.10500019788742065\n",
            "Train epoch - Accuracy: 0.3677697841726619 Loss: 0.10161279916334495 Corrects: 2556\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.10457088053226471\n",
            "Train step - Step 2870, Loss 0.10205113887786865\n",
            "Train step - Step 2880, Loss 0.10334748774766922\n",
            "Train step - Step 2890, Loss 0.09874509274959564\n",
            "Train step - Step 2900, Loss 0.10292085260152817\n",
            "Train step - Step 2910, Loss 0.10105995088815689\n",
            "Train epoch - Accuracy: 0.3735251798561151 Loss: 0.10202787073396093 Corrects: 2596\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.09775694459676743\n",
            "Train step - Step 2930, Loss 0.10007531940937042\n",
            "Train step - Step 2940, Loss 0.10038876533508301\n",
            "Train step - Step 2950, Loss 0.1133565902709961\n",
            "Train step - Step 2960, Loss 0.10293117165565491\n",
            "Train epoch - Accuracy: 0.36949640287769786 Loss: 0.1019590073009189 Corrects: 2568\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.10389451682567596\n",
            "Train step - Step 2980, Loss 0.09642454236745834\n",
            "Train step - Step 2990, Loss 0.10143774002790451\n",
            "Train step - Step 3000, Loss 0.10160073637962341\n",
            "Train step - Step 3010, Loss 0.10316383093595505\n",
            "Train step - Step 3020, Loss 0.10317070037126541\n",
            "Train epoch - Accuracy: 0.3640287769784173 Loss: 0.10190404075298377 Corrects: 2530\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.10044983774423599\n",
            "Train step - Step 3040, Loss 0.10318149626255035\n",
            "Train step - Step 3050, Loss 0.10281839966773987\n",
            "Train step - Step 3060, Loss 0.10409954190254211\n",
            "Train step - Step 3070, Loss 0.10263378173112869\n",
            "Train epoch - Accuracy: 0.36345323741007196 Loss: 0.10166693172866492 Corrects: 2526\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.10200110822916031\n",
            "Train step - Step 3090, Loss 0.09974018484354019\n",
            "Train step - Step 3100, Loss 0.10324843972921371\n",
            "Train step - Step 3110, Loss 0.10280807316303253\n",
            "Train step - Step 3120, Loss 0.09548719227313995\n",
            "Train step - Step 3130, Loss 0.10439270734786987\n",
            "Train epoch - Accuracy: 0.36201438848920864 Loss: 0.10151307401468428 Corrects: 2516\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.099611297249794\n",
            "Train step - Step 3150, Loss 0.09864446520805359\n",
            "Train step - Step 3160, Loss 0.09772355109453201\n",
            "Train step - Step 3170, Loss 0.1042770966887474\n",
            "Train step - Step 3180, Loss 0.10746061056852341\n",
            "Train epoch - Accuracy: 0.3674820143884892 Loss: 0.10141104000935451 Corrects: 2554\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.10017991811037064\n",
            "Train step - Step 3200, Loss 0.10147448629140854\n",
            "Train step - Step 3210, Loss 0.10289636254310608\n",
            "Train step - Step 3220, Loss 0.0995347797870636\n",
            "Train step - Step 3230, Loss 0.10942097753286362\n",
            "Train step - Step 3240, Loss 0.10289052128791809\n",
            "Train epoch - Accuracy: 0.36489208633093523 Loss: 0.10111664633099124 Corrects: 2536\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.09997104108333588\n",
            "Train step - Step 3260, Loss 0.10396639257669449\n",
            "Train step - Step 3270, Loss 0.10734333097934723\n",
            "Train step - Step 3280, Loss 0.10098032653331757\n",
            "Train step - Step 3290, Loss 0.10467696934938431\n",
            "Train epoch - Accuracy: 0.3687769784172662 Loss: 0.10170877761763635 Corrects: 2563\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.10317199677228928\n",
            "Train step - Step 3310, Loss 0.10881240665912628\n",
            "Train step - Step 3320, Loss 0.10185832530260086\n",
            "Train step - Step 3330, Loss 0.0965321734547615\n",
            "Train step - Step 3340, Loss 0.1033906564116478\n",
            "Train step - Step 3350, Loss 0.10099254548549652\n",
            "Train epoch - Accuracy: 0.3651798561151079 Loss: 0.10155955002462264 Corrects: 2538\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.10184215009212494\n",
            "Train step - Step 3370, Loss 0.10330560803413391\n",
            "Train step - Step 3380, Loss 0.10546568036079407\n",
            "Train step - Step 3390, Loss 0.09738834202289581\n",
            "Train step - Step 3400, Loss 0.09696803987026215\n",
            "Train epoch - Accuracy: 0.35697841726618706 Loss: 0.10165656559973311 Corrects: 2481\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.10238277167081833\n",
            "Train step - Step 3420, Loss 0.1022505834698677\n",
            "Train step - Step 3430, Loss 0.10198219120502472\n",
            "Train step - Step 3440, Loss 0.1042444109916687\n",
            "Train step - Step 3450, Loss 0.09986983984708786\n",
            "Train step - Step 3460, Loss 0.10694192349910736\n",
            "Train epoch - Accuracy: 0.3523741007194245 Loss: 0.10122117265522909 Corrects: 2449\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.09597840160131454\n",
            "Train step - Step 3480, Loss 0.10070549696683884\n",
            "Train step - Step 3490, Loss 0.09784393012523651\n",
            "Train step - Step 3500, Loss 0.09980464726686478\n",
            "Train step - Step 3510, Loss 0.09362360090017319\n",
            "Train epoch - Accuracy: 0.3635971223021583 Loss: 0.10112364354322283 Corrects: 2527\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.10171833634376526\n",
            "Train step - Step 3530, Loss 0.0956970602273941\n",
            "Train step - Step 3540, Loss 0.10261636227369308\n",
            "Train step - Step 3550, Loss 0.0987907275557518\n",
            "Train step - Step 3560, Loss 0.10462301969528198\n",
            "Train step - Step 3570, Loss 0.10537015646696091\n",
            "Train epoch - Accuracy: 0.3605755395683453 Loss: 0.10085388354474692 Corrects: 2506\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.10403852164745331\n",
            "Train step - Step 3590, Loss 0.10574913024902344\n",
            "Train step - Step 3600, Loss 0.10381964594125748\n",
            "Train step - Step 3610, Loss 0.10369117558002472\n",
            "Train step - Step 3620, Loss 0.10682029277086258\n",
            "Train epoch - Accuracy: 0.3576978417266187 Loss: 0.10113621693077705 Corrects: 2486\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.10040953755378723\n",
            "Train step - Step 3640, Loss 0.10664420574903488\n",
            "Train step - Step 3650, Loss 0.10592447221279144\n",
            "Train step - Step 3660, Loss 0.10514011979103088\n",
            "Train step - Step 3670, Loss 0.10173141211271286\n",
            "Train step - Step 3680, Loss 0.09971132129430771\n",
            "Train epoch - Accuracy: 0.36633093525179855 Loss: 0.10128293473514721 Corrects: 2546\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.10357748717069626\n",
            "Train step - Step 3700, Loss 0.09839950501918793\n",
            "Train step - Step 3710, Loss 0.09956952929496765\n",
            "Train step - Step 3720, Loss 0.10630588233470917\n",
            "Train step - Step 3730, Loss 0.10464639216661453\n",
            "Train epoch - Accuracy: 0.3610071942446043 Loss: 0.10117869195749433 Corrects: 2509\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.107760950922966\n",
            "Train step - Step 3750, Loss 0.10700076818466187\n",
            "Train step - Step 3760, Loss 0.09867376834154129\n",
            "Train step - Step 3770, Loss 0.10430959612131119\n",
            "Train step - Step 3780, Loss 0.09868764877319336\n",
            "Train step - Step 3790, Loss 0.10707937926054001\n",
            "Train epoch - Accuracy: 0.36071942446043165 Loss: 0.10117569399394577 Corrects: 2507\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.10138639807701111\n",
            "Train step - Step 3810, Loss 0.1030670776963234\n",
            "Train step - Step 3820, Loss 0.0967685878276825\n",
            "Train step - Step 3830, Loss 0.1047043576836586\n",
            "Train step - Step 3840, Loss 0.09596696496009827\n",
            "Train epoch - Accuracy: 0.3530935251798561 Loss: 0.10085562635025531 Corrects: 2454\n",
            "Training finished in 470.3347842693329 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0]\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880a9da650>\n",
            "Constructing exemplars of class 91\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [30634, 11193, 35469, 23489, 14266, 1725, 15896, 5265, 47118, 47570, 41226, 46481, 12842, 14613, 6321, 47892, 8059, 17063, 20203, 24736, 869, 45820, 32041, 25884, 14077, 21362, 48503, 4100, 12583, 22534, 13793, 13028, 29604, 26279, 12850, 41629, 19557, 43601, 26279, 28498]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc6cfa50>\n",
            "Constructing exemplars of class 67\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [11761, 29655, 26856, 14591, 30658, 4226, 10380, 37012, 18577, 14604, 49153, 49244, 35785, 22434, 30023, 8825, 29480, 47266, 32224, 4609, 39931, 41760, 17396, 6815, 30890, 26856, 16820, 49153, 1101, 7961, 30412, 40875, 18314, 46619, 18830, 36708, 44094, 27223, 3540, 191]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880ac85bd0>\n",
            "Constructing exemplars of class 7\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [16378, 6138, 37477, 15699, 35405, 43273, 29449, 9816, 37445, 13962, 1321, 1035, 46307, 21476, 18920, 4085, 11628, 805, 29867, 28351, 48696, 32730, 31658, 22695, 30784, 18878, 19419, 13162, 3846, 23862, 3285, 2161, 35399, 25968, 6616, 377, 49164, 8368, 40919, 3476]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880a027f10>\n",
            "Constructing exemplars of class 62\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [44885, 33993, 12936, 16432, 26184, 46557, 29571, 25212, 17626, 27557, 48609, 33765, 7981, 43187, 41225, 524, 49731, 18693, 12882, 9808, 1647, 33679, 42711, 38442, 34449, 19192, 25291, 27240, 47702, 6436, 43418, 5094, 8075, 4780, 44774, 19422, 18436, 24256, 24500, 49603]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880ab70cd0>\n",
            "Constructing exemplars of class 46\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [14055, 39429, 31484, 11848, 26804, 42304, 9219, 5049, 31645, 27444, 14069, 35828, 90, 259, 24307, 18303, 28795, 29118, 17528, 2706, 18667, 39429, 19586, 39513, 34957, 12552, 23615, 5028, 25070, 14069, 49229, 43353, 34176, 49177, 45702, 25070, 17967, 18273, 36325, 21657]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880aa72250>\n",
            "Constructing exemplars of class 65\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [44923, 33050, 848, 15020, 42242, 40984, 26586, 31718, 21873, 20321, 1934, 36020, 32045, 30351, 13399, 2972, 13187, 19912, 8531, 33472, 13533, 48625, 17959, 31883, 1814, 16099, 12304, 39188, 4755, 42353, 24847, 1314, 3838, 22729, 30020, 3602, 44708, 3734, 33229, 16237]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f88cdeef250>\n",
            "Constructing exemplars of class 21\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [45054, 29589, 43986, 43491, 19147, 48016, 7680, 32923, 18764, 3082, 35934, 11125, 19147, 43407, 7501, 30927, 13806, 35040, 9043, 7134, 45710, 34947, 24348, 35819, 45457, 38935, 48598, 48058, 17263, 36661, 21611, 18761, 46074, 26773, 24902, 17787, 16105, 31387, 6046, 32240]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880aa7f4d0>\n",
            "Constructing exemplars of class 32\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [2892, 12725, 27477, 16491, 19390, 2222, 8480, 24010, 23661, 33040, 23193, 29881, 26386, 36416, 11183, 24675, 33467, 1010, 20519, 23957, 48637, 7249, 48742, 22098, 44203, 19262, 19236, 38371, 9818, 19992, 23957, 2230, 40878, 20292, 43613, 15973, 26772, 37500, 2855, 33717]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f880ac85bd0>\n",
            "Constructing exemplars of class 16\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [43035, 47194, 6305, 15273, 16145, 37833, 12488, 5004, 24132, 41823, 22464, 41281, 21274, 2868, 35111, 34895, 38519, 8120, 12340, 16732, 2991, 8157, 22825, 36032, 38148, 33920, 42335, 23899, 14309, 28120, 23203, 48125, 8633, 30215, 31620, 27470, 23067, 33489, 34546, 43404]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f87dc6b4c10>\n",
            "Constructing exemplars of class 0\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [4142, 21210, 9324, 33145, 19037, 39318, 6247, 41248, 24889, 24458, 48823, 10382, 14344, 48751, 45375, 16520, 27272, 5129, 48469, 3520, 8957, 45317, 36562, 304, 10958, 36391, 31066, 35963, 26819, 7143, 317, 6607, 39303, 5914, 35959, 41259, 19002, 37365, 9404, 19745]\n",
            "I am calculating the mean of the exemplars...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...Mean of the exemplars done!!\n",
            "TEST ALL:  0.714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeUV-v0GW7yA"
      },
      "source": [
        "## PLOT \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be-n6SM6uV13"
      },
      "source": [
        "import numpy as np\n",
        "run1 = np.array([0.823,\t0.447,\t0.286,\t0.1985,\t0.163,\t0.1428,\t0.126,\t0.098,\t0.0953, 0.086])\n",
        "run2 = np.array([0.75, 0.438, 0.285, 0.218, 0.17, 0.14, 0.12, 0.1, 0.09, 0.08])\n",
        "run3 = np.array([0.817, 0.44, 0.27, 0.206, 0.168, 0.1442, 0.125, 0.101, 0.0912, 0.0831])\n",
        "\n",
        "finetuning = np.array([run1, run2, run3])\n",
        "mean = np.mean(finetuning, axis = 0)\n",
        "std = np.std(finetuning, axis = 0)\n",
        "\n",
        "run1l = np.array([0.816,\t0.683,\t0.6006,\t0.52175,\t0.4666,\t0.4365,\t0.4108,\t0.378,\t0.3486, 0.3226])\n",
        "run2l = np.array([0.769, 0.6445, 0.5563, 0.46675, 0.4362, 0.4105, 0.3772, 0.35075, 0.3262, 0.3104])\n",
        "run3l = np.array([0.825, 0.6625, 0.572, 0.47125, 0.4132, 0.3883, 0.369, 0.3485, 0.3325, 0.3149])\n",
        "LWF = np.array([run1l, run2l, run3l])\n",
        "meanLWF = np.mean(LWF, axis = 0)\n",
        "stdLWF = np.std(LWF, axis = 0)\n",
        "\n",
        "run1c = np.array([0.812,\t0.7385,\t0.6893,\t0.62025,\t0.582,\t0.5628,\t0.538,\t0.5166,\t0.4948, 0.482])\n",
        "run2c = np.array([0.754, 0.7185, 0.662, 0.608, 0.5796, 0.5598, 0.5278, 0.5005, 0.4985, 0.4707])\n",
        "run3c = np.array([0.833, 0.759, 0.695, 0.6367, 0.5966, 0.5823, 0.545, 0.52125, 0.495, 0.4947])\n",
        "iCaRL = np.array([run1c, run2c, run3c])\n",
        "meaniCaRL = np.mean(iCaRL, axis = 0)\n",
        "stdiCaRL = np.std(iCaRL, axis = 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PVNKrSpx9bk",
        "outputId": "2dba2c1f-7fcc-4e39-e3a5-0fd5ba2b1c4c"
      },
      "source": [
        "stdLWF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0245538 , 0.0157286 , 0.01833921, 0.02493436, 0.02187012,\n",
              "       0.01969794, 0.01808449, 0.01340761, 0.00943198, 0.00503742])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "eX9I7PBPvhuv",
        "outputId": "ffc04d86-6ce7-4e6c-c0da-fe5fbdd41dc4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = np.linspace(10,100, num = 10)\n",
        "plt.show()\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.errorbar(x, mean,yerr=std, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='blue', color = 'tab:blue')\n",
        "plt.errorbar(x, meanLWF, yerr=stdLWF, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='tab:purple', color = 'm')\n",
        "plt.errorbar(x, meaniCaRL, yerr=stdiCaRL, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='r', color = 'tab:orange')\n",
        "plt.legend(['Finetuning', 'LWF', 'iCaRL'])\n",
        "#plt.show()\n",
        "plt.savefig(\"plot.pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHUEaGUhweV_"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR-BSgUeMn5x"
      },
      "source": [
        "run1CEL1 = np.array([0.751,0.713,0.607,0.5175,0.5032,0.48233333333333334,0.475228571428571426,0.463625,0.4451,0.435])\n",
        "run2CEL1 = np.array([0.804, 0.701, 0.632, 0.5235, 0.498, 0.4877, 0.47025, 0.46806, 0.4591244, 0.4431])\n",
        "run3CEL1 = np.array([0.829, 0.747, 0.669, 0.56475, 0.5342, 0.495, 0.487571428571429, 0.471225, 0.46175, 0.4501])\n",
        "CEl1 = np.array([run1CEL1, run2CEL1, run3CEL1])\n",
        "meanCEl1 = np.mean(CEl1, axis = 0)\n",
        "stdCEl1 = np.std(CEl1, axis = 0)\n",
        "\n",
        "run1c = np.array([0.812,\t0.7385,\t0.6893,\t0.62025,\t0.582,\t0.5628,\t0.538,\t0.5166,\t0.4948, 0.4862])\n",
        "run2c = np.array([0.754, 0.7185, 0.662, 0.608, 0.5796, 0.5598, 0.5278, 0.5005, 0.4985, 0.4707])\n",
        "run3c = np.array([0.833, 0.759, 0.695, 0.6367, 0.5966, 0.5823, 0.545, 0.52125, 0.495, 0.4847])\n",
        "iCaRL = np.array([run1c, run2c, run3c])\n",
        "meaniCaRL = np.mean(iCaRL, axis = 0)\n",
        "stdiCaRL = np.std(iCaRL, axis = 0)\n",
        "\n",
        "runCEBCE = np.array([0.84, 0.785, 0.7133333333333334, 0.664, 0.5748, 0.541, 0.4888571428571429, 0.45875, 0.43922222222222224, 0.4009])\n",
        "\n",
        "runCEKlDiv = np.array([0.832, 0.7945, 0.685, 0.6205, 0.5372, 0.49583333333333335, 0.4667142857142857, 0.419125, 0.39222222222222225, 0.3614])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "teuEpnpQNNA0",
        "outputId": "05922855-0dd5-4478-dae4-0277a9204513"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = np.linspace(10,100, num = 10)\n",
        "plt.show()\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.errorbar(x, meanCEl1, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:pink')\n",
        "plt.errorbar(x, meaniCaRL, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:orange')\n",
        "plt.errorbar(x, runCEKlDiv, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:cyan')\n",
        "plt.errorbar(x, runCEBCE, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:purple')\n",
        "plt.axis([0, 105, 0, 1])\n",
        "plt.legend(['CE + L1', 'BCE + BCE', 'CE + KLDiv', 'CE + BCE'])\n",
        "#plt.show()\n",
        "plt.savefig(\"losses.pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2adzRbEPLbH"
      },
      "source": [
        "runSVM = np.array([0.808, 0.7615, 0.6686666666666666,0.60725,0.5356,0.499,0.47,0.441875,0.4142222222222222,0.3876])\n",
        "\n",
        "runKNN = np.array([0.853, 0.797, 0.68, 0.632, 0.547, 0.5197, 0.503, 0.489, 0.475, 0.455])\n",
        "\n",
        "\n",
        "run1c = np.array([0.812,\t0.7385,\t0.6893,\t0.62025,\t0.582,\t0.5628,\t0.538,\t0.5166,\t0.4948, 0.4862])\n",
        "run2c = np.array([0.754, 0.7185, 0.662, 0.608, 0.5796, 0.5598, 0.5278, 0.5005, 0.4985, 0.4707])\n",
        "run3c = np.array([0.833, 0.759, 0.695, 0.6367, 0.5966, 0.5823, 0.545, 0.52125, 0.495, 0.4847])\n",
        "iCaRL = np.array([run1c, run2c, run3c])\n",
        "meaniCaRL = np.mean(iCaRL, axis = 0)\n",
        "stdiCaRL = np.std(iCaRL, axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "92lLwTKaunVv",
        "outputId": "1b3c50ee-9a74-4118-b17c-923bea2750f2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = np.linspace(10,100, num = 10)\n",
        "plt.show()\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.errorbar(x, runSVM, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:green')\n",
        "plt.errorbar(x, runKNN, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:orange')\n",
        "plt.errorbar(x, meaniCaRL, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'm')\n",
        "plt.axis([0, 105, 0, 1])\n",
        "plt.legend(['SVM', 'KNN', 'NME'])\n",
        "#plt.show()\n",
        "plt.savefig(\"classifiers.pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "oWXkGcIhu4P2",
        "outputId": "8da1ce74-0d8e-4807-f743-fd2a23c3bda6"
      },
      "source": [
        "import numpy as np\n",
        "runBF = np.array([0.784,\t0.705,\t0.6433,\t0.563,\t0.5128,\t0.4801,\t0.4497,\t0.405,\t0.3926, 0.3875])\n",
        "\n",
        "run1c = np.array([0.812,\t0.7385,\t0.6893,\t0.62025,\t0.582,\t0.5628,\t0.538,\t0.5166,\t0.4948, 0.4862])\n",
        "run2c = np.array([0.754, 0.7185, 0.662, 0.608, 0.5796, 0.5598, 0.5278, 0.5005, 0.4985, 0.4707])\n",
        "run3c = np.array([0.833, 0.759, 0.695, 0.6367, 0.5966, 0.5823, 0.545, 0.52125, 0.495, 0.4847])\n",
        "iCaRL = np.array([run1c, run2c, run3c])\n",
        "meaniCaRL = np.mean(iCaRL, axis = 0)\n",
        "stdiCaRL = np.std(iCaRL, axis = 0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "x = np.linspace(10,100, num = 10)\n",
        "plt.show()\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.errorbar(x, runBF, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:green')\n",
        "plt.errorbar(x, meaniCaRL, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:blue')\n",
        "plt.axis([0, 105, 0, 1])\n",
        "plt.legend(['Balanced Finetune', 'iCaRL'])\n",
        "#plt.show()\n",
        "plt.savefig(\"variation.pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PObBr0MPCTFk",
        "outputId": "1a5e2c8b-4fcb-4d01-decf-b46835386f4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "distances = [[2,3],[3,6]]\n",
        "distancesNp = np.array(distances)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}