{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di Copia di LWF funzionante.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-OWtqytyzmMF",
        "KpWv5ZkhxTPJ",
        "qJ-J8aC_xx-c",
        "9nX10znUi6qd",
        "TKBdrJ8Csy2a",
        "F5HRagcujHgM",
        "gy_HVL-tjZcb",
        "DFHiLXyLmaWu",
        "FQKnRMfjEaA-",
        "trLViwIa2_bY",
        "CMyXZhVemn8q",
        "SBFSAonO0mgQ"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoManca-FM/Project_MLDL/blob/main/Copia_di_Copia_di_LWF_funzionante.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAfp19qRVa7l"
      },
      "source": [
        "### GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag2iq0PaVWC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2985fc08-3eb7-4f07-beb6-0b258481a970"
      },
      "source": [
        "# Check GPU assigned\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jul  9 15:36:30 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OWtqytyzmMF"
      },
      "source": [
        "## Network, dataset, functions and parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itMjhanGVTUp"
      },
      "source": [
        "### LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6tCA_s2rru"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nHVORYbiZRx"
      },
      "source": [
        "### Resnet32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIdSy6PnibZA"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\"\"\"\n",
        "Credits to @hshustc\n",
        "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from torch.nn import init\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_relu=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.stride = stride\n",
        "        self.use_relu = use_relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "        if self.use_relu:\n",
        "            out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        assert stride == 2\n",
        "        self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat((x, x.mul(0)), 1)\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block=BasicBlock, depth=32, num_classes=0, channels=3):\n",
        "\n",
        "        super(CifarResNet, self).__init__()\n",
        "\n",
        "        # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
        "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
        "        layer_blocks = (depth - 2) // 6\n",
        "        bn = nn.BatchNorm2d\n",
        "        self.inplanes = 16\n",
        "\n",
        "        self.conv_1_3x3 = nn.Conv2d(channels, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn_1 = bn(self.inplanes)\n",
        "\n",
        "        self.stage_1 = self._make_layer(block, 16, layer_blocks, 1)\n",
        "        self.stage_2 = self._make_layer(block, 32, layer_blocks, 2)\n",
        "        self.stage_3 = self._make_layer(block, 64, layer_blocks, 2, last=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.relu = nn.ReLU()\n",
        "        #self.linear = nn.Linear(64, num_classes)\n",
        "        self.fcs = nn.ModuleList([nn.Linear(64, num_classes)])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, math.sqrt(1. / 64.))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, last=False):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = Downsample(self.\n",
        "inplanes, planes * block.expansion, stride)\n",
        "\n",
        "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        if last:\n",
        "            for i in range(1, blocks - 1):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            layers.append(block(self.inplanes, planes, use_relu=False))\n",
        "\n",
        "        else:\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv_1_3x3(x)\n",
        "\n",
        "        x = self.bn_1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.stage_1(x)\n",
        "        x = self.stage_2(x)\n",
        "        x = self.stage_3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        out = []\n",
        "        for fc in self.fcs:\n",
        "            out.append(fc(x))\n",
        "        out = torch.cat(out, dim=1)\n",
        "        return out\n",
        "\n",
        "    def addOutputNodes(self, num_classes):\n",
        "        self.fcs.append(nn.Linear(64, num_classes))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ga-SaCsjkYV"
      },
      "source": [
        "### Cifar100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHAyljuejmkd"
      },
      "source": [
        "# ref:\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py\n",
        "# https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR10\n",
        "# homework2 (caltech)\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# This is an handler class for the Cifar dataset\n",
        "class CIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    `CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "    This is a subclass of the `CIFAR100` Dataset.\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar-100-python'\n",
        "    train_file = 'train'\n",
        "    test_file = 'test'\n",
        "    meta_file = 'meta'\n",
        "\n",
        "    def __init__(self, root, split = 'train', transform = None):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              root (string): Root directory of the dataset where directory\n",
        "                  cifar-100-python exists.\n",
        "              split (string, optional): If 'train', creates dataset from training\n",
        "                  set, otherwise creates from test set.\n",
        "              transform (callable, optional): A function/transform that takes in a\n",
        "                  PIL image and returns a transformed version.\n",
        "        \"\"\"\n",
        "        super(CIFAR100, self).__init__(root, transform=transform)\n",
        "\n",
        "        self.split = split\n",
        "        if split == 'train':\n",
        "            filename = self.train_file\n",
        "        else:\n",
        "            filename = self.test_file\n",
        "\n",
        "        data_path = os.path.join(self.root, self.base_folder, filename)\n",
        "        data = None\n",
        "        labels = None\n",
        "\n",
        "        with open(data_path, 'rb') as f:\n",
        "            entry = pickle.load(f, encoding='latin1')\n",
        "            data = entry['data']\n",
        "            labels = entry['fine_labels']\n",
        "        \n",
        "        data = np.vstack(data).reshape(-1, 3, 32, 32)\n",
        "        data = data.transpose((0, 2, 3, 1))  # Convert to HWC\n",
        "        \n",
        "        labels = np.array(labels)\n",
        "\n",
        "        self.df = pd.DataFrame()\n",
        "        self.df['data'] = pd.Series(list(data))\n",
        "        self.df['labels'] = labels\n",
        "\n",
        "        self.data = self.df['data']\n",
        "        self.labels = self.df['labels']\n",
        "\n",
        "        self._load_meta()\n",
        "\n",
        "    def _load_meta(self):\n",
        "        meta_path = os.path.join(self.root, self.base_folder, self.meta_file)\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f, encoding='latin1')\n",
        "            self.label_names = meta['fine_label_names']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.df.loc[index, 'data'], self.df.loc[index, 'labels']\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img) # Return a PIL image\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return index, img, target\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def getTargets(self):\n",
        "        return set(self.labels)\n",
        "     \n",
        "    # test\n",
        "    def get_indices(self, labels):\n",
        "        return list(self.df[self.df['labels'].isin(labels)].index)\n",
        "\n",
        "    def split_classes(self, n_splits=10, seed=None, dictionary_of='dataframes'):\n",
        "        if dictionary_of not in ['dataframes','indices']:\n",
        "            raise ValueError(\"'dictionary_of' must be equal to 'dataframes' or 'indices'\")\n",
        "\n",
        "        all_classes = list(self.df['labels'].value_counts().index)\n",
        "        dictionary = {}\n",
        "        random.seed(seed)\n",
        "        random.shuffle(all_classes)\n",
        "        split_size = int(len(all_classes)/n_splits)\n",
        "        for j in range(n_splits):\n",
        "            if ((j+1)*split_size < len(all_classes)):\n",
        "                split_end = (j+1)*split_size\n",
        "            else:\n",
        "                split_end = None\n",
        "            subgroup = all_classes[j*split_size:split_end]\n",
        "            if dictionary_of == 'dataframes':\n",
        "                dictionary[j] = self.df[self.df['labels'].isin(subgroup)]\n",
        "            elif dictionary_of == 'indices':\n",
        "                dictionary[j] = list(self.df[self.df['labels'].isin(subgroup)].index)\n",
        "        return dictionary\n",
        "\n",
        "    def split_groups_in_train_validation(self, groups, ratio=0.5, seed=None):\n",
        "        groups_train_val = dict()\n",
        "        for k, subdf in groups.items():\n",
        "            train_indexes = []\n",
        "            val_indexes = []\n",
        "            split_labels = list(subdf['labels'].value_counts().index)\n",
        "            for l in split_labels:\n",
        "                indexes_to_sample = list(subdf[subdf['labels'] == l].index)\n",
        "                random.seed(seed)\n",
        "                train_samples = random.sample(indexes_to_sample, int(len(indexes_to_sample)*ratio))\n",
        "                train_indexes = train_indexes + train_samples\n",
        "                val_indexes = val_indexes + list(set(indexes_to_sample).difference(set(train_samples)))\n",
        "            groups_train_val[k] = {\n",
        "                'train': train_indexes,\n",
        "                'val': val_indexes\n",
        "            }\n",
        "        return groups_train_val\n",
        "    \n",
        "    def split_in_train_val_groups(self, n_splits=10, ratio=0.5, seed=None):\n",
        "        groups = self.split_classes(n_splits=n_splits, seed=seed)\n",
        "        return self.split_groups_in_train_validation(groups, ratio=ratio, seed=seed)\n",
        "\n",
        "    # given a tensors returns an image (used in exemplars)\n",
        "    #def tensorToImg(self, tensor):\n",
        "    #   return Variable(transform(Image.fromarray(img)), volatile=True)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_M1anVYkRnF"
      },
      "source": [
        "### Reverse Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j66mUOXekTlM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "__all__ = ['ReverseIndex']\n",
        "\n",
        "class ReverseIndex():\n",
        "\n",
        "    def __init__(self, dataset, splits, device='cuda'):\n",
        "\n",
        "        self.df = pd.DataFrame(columns=['group', 'labels'])\n",
        "\n",
        "        for k in splits.keys():\n",
        "            labels = list(dataset.df.loc[splits[k]['train'],'labels'].value_counts().index)\n",
        "            group = [k for i in range(len(labels))]\n",
        "            data = pd.DataFrame(np.array([group, labels]).T, columns=['group', 'labels'])\n",
        "            self.df = self.df.append(data, ignore_index=True)\n",
        "\n",
        "        self.df['nodes'] = self.df.index\n",
        "        self.device = device\n",
        "    \n",
        "    def _changeIndex(self, reverse_index, column):\n",
        "        reverse_index = reverse_index.set_index(column)\n",
        "        reverse_index[column] = reverse_index.index\n",
        "        return reverse_index\n",
        "\n",
        "    def getLabels(self, outputs):\n",
        "        outs = outputs.cpu().numpy()\n",
        "        reverse_index = self._changeIndex(self.df, 'nodes')\n",
        "        labels = reverse_index.loc[outs, 'labels']\n",
        "\n",
        "        labels = torch.tensor(list(labels))\n",
        "        return labels.to(self.device)\n",
        "\n",
        "    def getNodes(self, labels):\n",
        "        labels = labels.cpu().numpy()\n",
        "       \n",
        "        reverse_index = self._changeIndex(self.df, 'labels')\n",
        "\n",
        "        nodes = reverse_index.loc[labels, 'nodes']\n",
        "\n",
        "        nodes = torch.tensor(list(nodes))\n",
        "        return nodes.to(self.device)\n",
        "\n",
        "    def getGroups(self, distinct=True):\n",
        "        return self.df['group'].value_counts().index.sort_values()\n",
        "    \n",
        "    def getLabelsOfGroup(self, group):\n",
        "        return self.df.loc[self.df['group'] == group, 'labels']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVfOeOCuknB8"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QpAHzFkoW0"
      },
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Module\n",
        "\n",
        "# These are the default iCaRL hyper-parameters\n",
        "def getHyperparams():\n",
        "\tdictHyperparams = {\n",
        "\t\t\"LR\": 2,\n",
        "\t\t\"MOMENTUM\": 0.9,\n",
        "\t\t\"WEIGHT_DECAY\": 1e-5,\n",
        "\t\t\"NUM_EPOCHS\": 70,\n",
        "\t\t\"MILESTONES\": [49, 63],\n",
        "\t\t\"BATCH_SIZE\": 128,\n",
        "\t\t\"DEVICE\": 'cuda',\n",
        "\t\t\"GAMMA\": 0.2,\n",
        "\t\t\"SEED\": 66, #use 30, 42, 16\n",
        "\t\t\"LOG_FREQUENCY\": 10,\n",
        "\t\t\"NUM_CLASSES\": 100\n",
        "\t}\n",
        "\treturn dictHyperparams\n",
        "\n",
        "def getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize):\n",
        "\toptimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\tscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA, last_epoch=-1) \n",
        "\treturn optimizer, scheduler\n",
        "\n",
        "# the mean and the std have been found on the web as mean and std of cifar100\n",
        "# alternative (realistic): compute mean and std for the dataset\n",
        "def getTransformations():\n",
        "\t# Define transforms for training phase\n",
        "\ttrain_transform = transforms.Compose([transforms.RandomHorizontalFlip(), # Randomly flip the image with probability of 0.5\n",
        "\t                                      transforms.Pad(4), # Add padding\n",
        "\t                                      transforms.RandomCrop(32),# Crops a random squares of the image\n",
        "\t                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\t# Define transforms for the evaluation phase\n",
        "\teval_transform = transforms.Compose([\n",
        "\t                                      transforms.ToTensor(),\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\treturn train_transform, eval_transform\n",
        "\n",
        "# BCEWithLogits = Sigmoid + BCE, is the loss used in iCaRL\n",
        "def getLossCriterion():\n",
        "\tcriterion = nn.BCEWithLogitsLoss(reduction = 'mean') \n",
        "\treturn criterion\n",
        "\n",
        "# CrossEntropyLoss \n",
        "def computeLoss(criterion, outputs, labels):\n",
        "\treturn criterion(outputs, labels)\n",
        " \n",
        "# Loss L2\n",
        "def l2Loss (outputs, labels):\n",
        "  criterion = nn.MSELoss()\n",
        "  return criterion(outputs, labels)\n",
        "\n",
        "# Loss L1\n",
        "def l1Loss(outputs, labels):\n",
        "  criterion = nn.L1Loss()\n",
        "  return criterion(outputs, labels)\n",
        "\n",
        "# support BCE\n",
        "def _one_hot_encode(labels, n_classes, reverse_index, dtype=None, device='cuda'):\n",
        "\tbatch_size = len(labels)\n",
        "\tenconded = torch.zeros(batch_size, n_classes, dtype=dtype, device=device)\n",
        "\tlabels=map_to_outputs(labels, reverse_index)\n",
        "\tfor i, l in enumerate(labels):\n",
        "\t  enconded[i, l] = 1\n",
        "\treturn enconded\n",
        "\n",
        "def map_to_outputs(labels, reverse_index):\n",
        "\tif reverse_index is None:\n",
        "\t  return labels\n",
        "\tif type(labels) == int:\n",
        "\t  return int(reverse_index.getNodes(torch.tensor([labels])))\n",
        "\telif type(labels) == torch.Tensor:\n",
        "\t\treturn reverse_index.getNodes(labels)\n",
        "\n",
        "\n",
        "def plotAccuracyTrend(method, data_plot_line, seed):\n",
        "\tplt.figure(figsize=(20,7))\n",
        "\taccuracyDF=pd.DataFrame(data_plot_line, columns = ['Classes','Accuracy'])\n",
        "\tax = sns.lineplot(x=\"Classes\", y=\"Accuracy\",data=accuracyDF, marker = 'o')\n",
        "\tax.minorticks_on()\n",
        "\tax.set_xticks(np.arange(10,110,10))\n",
        "\tax.set_xlim(xmin=9, xmax=101)\n",
        "\tax.set_ylim(ymin=0, ymax=1)\n",
        "\tplt.legend(['Accuracy {}'.format(method)])\n",
        "\tax.grid(axis='y')\n",
        "\tplt.title(\"Accuracies against seen classes {} - seed: {}\".format(method, seed))\n",
        "\t\n",
        "\tfilename = \"acc_{}_{}.jpg\".format(method, seed) # ex. acc_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "def plotConfusionMatrix(method, confusionMatrixData, seed):\n",
        "\tfig,ax=plt.subplots(figsize=(10,10))\n",
        "\tsns.heatmap(confusionMatrixData,cmap='terrain',ax=ax)\n",
        "\tplt.ylabel('True label')\n",
        "\tplt.xlabel('Predicted label')\n",
        "\tplt.title(\"Confusion Matrix {} - seed: {}\".format(method, seed))\n",
        "\n",
        "\tfilename = \"cm_{}_{}.jpg\".format(method, seed) # ex. cm_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "# Write down the metrics (accuracy trand and confusion matrix)\n",
        "# this method is a shortcut when perfoming multiple tests with different splits (random_seed)\n",
        "# and allow us to plot on the same graph the data from multiple models (accuracy)\n",
        "def writeMetrics(method, seed, accuracies, confusionMatrixData):\n",
        "  data = {}\n",
        "  data['accuracies'] = []\n",
        "  data['cm'] = [] #cm line\n",
        "  i = 0\n",
        "  for classes_seen in range(10, 110, 10): #x axis on the plot\n",
        "    data['accuracies'].append({classes_seen : accuracies[i]}) \n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  for class_num in range(0,len(confusionMatrixData)): #rows of the cm\n",
        "    data['cm'].append({class_num : confusionMatrixData[i].tolist()}) \n",
        "    i += 1\n",
        "  \n",
        "  # dump to file\n",
        "  aus = method + '_' + str(seed)\n",
        "  filename = 'data_{}.json'.format(aus)\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "\n",
        "# Functions\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getResNet32():\n",
        "    net = CifarResNet()\n",
        "    # net.fc = nn.Linear(net.fc.in_features, output_size) # embedded in the class\n",
        "\n",
        "    criterion = getLossCriterion()\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return net, criterion, optimizer, scheduler\n",
        "\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getNet():\n",
        "    return getResNet32()\n",
        "\n",
        "def getSchedulerOptimizer(net):\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return optimizer, scheduler\n",
        "def softmax(x):\n",
        "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
        "    return f_x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfJj1z2LSxp0"
      },
      "source": [
        "### DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysghtAWOPYZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52062b1-3bcb-423c-e366-c6ac89a12779"
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "if not os.path.isdir('./{}'.format(\"$DATA_DIR/cifar-100-python\")):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-09 15:36:34--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  49.4MB/s    in 3.6s    \n",
            "\n",
            "2021-07-09 15:36:38 (45.0 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n",
            "mv: cannot move 'cifar-100-python' to 'DATA/cifar-100-python/cifar-100-python': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY10HfpTS1lq"
      },
      "source": [
        "### HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-pqSNg4_Ris"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100\n",
        "BATCH_SIZE = 128 \n",
        "LR = 0.1    \n",
        "MOMENTUM = 0.9     \n",
        "WEIGHT_DECAY = 1e-05\n",
        "NUM_EPOCHS = 70\n",
        "NUM_EPOCHS_FINETUNE = 10  \n",
        "GAMMA = 0.2\n",
        "LOG_FREQUENCY = 10\n",
        "MILESTONES = [49,63]\n",
        "RANDOM_SEED = 31\n",
        "THRESHOLD = 0.5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW7WxV_QS4re"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_jHycn_1kk"
      },
      "source": [
        "train_transform, eval_transform = getTransformations()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPrwhTcIqUbA"
      },
      "source": [
        "### train & test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJKwvGljJj2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf8cccf5-e81f-404a-f786-958c818ab46d"
      },
      "source": [
        "# Import dataset and apply transformations \n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# Check datasets length \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnooWwcyS_YN"
      },
      "source": [
        "### SPLIT DATA IN CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckn3H69iJj2X"
      },
      "source": [
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpcJvhxhJOLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe30ade4-c08f-4dc2-8153-367b42a7d2ca"
      },
      "source": [
        "# TRAIN / VAL split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "print(outputs_labels_mapping.getGroups())\n",
        "\n",
        "# TEST split\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MSItI0QVpn"
      },
      "source": [
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10):\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpWv5ZkhxTPJ"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4NQa-wNxWL1"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pisadUTPxcRP"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            # if (group_id > 1):\n",
        "            #   old_features = old_net.forward(images)\n",
        "            #   old_outputs = old_net.predict(old_features)\n",
        "            #   labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX_61nSuxh3v"
      },
      "source": [
        "### sequential learning fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM0GysHZxk1B"
      },
      "source": [
        "### Fine tuning\n",
        "def sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    old_net = None\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    #confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2X2IVZ-xrVT"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DGpd8lslxtUW",
        "outputId": "2a7a5171-8688-4b1f-fe52-f88f4b532a6a"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.7173749208450317\n",
            "Train step - Step 10, Loss 0.31387636065483093\n",
            "Train step - Step 20, Loss 0.2874338626861572\n",
            "Train step - Step 30, Loss 0.2654590606689453\n",
            "Train epoch - Accuracy: 0.318989898989899 Loss: 0.34560690457772725 Corrects: 1579\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.26789265871047974\n",
            "Train step - Step 50, Loss 0.258944034576416\n",
            "Train step - Step 60, Loss 0.2350231260061264\n",
            "Train step - Step 70, Loss 0.23865528404712677\n",
            "Train epoch - Accuracy: 0.44363636363636366 Loss: 0.24301170718790305 Corrects: 2196\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.23784291744232178\n",
            "Train step - Step 90, Loss 0.22167158126831055\n",
            "Train step - Step 100, Loss 0.22836807370185852\n",
            "Train step - Step 110, Loss 0.21694694459438324\n",
            "Train epoch - Accuracy: 0.48707070707070704 Loss: 0.22610946089330344 Corrects: 2411\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.21196965873241425\n",
            "Train step - Step 130, Loss 0.20575585961341858\n",
            "Train step - Step 140, Loss 0.22211872041225433\n",
            "Train step - Step 150, Loss 0.20619133114814758\n",
            "Train epoch - Accuracy: 0.5016161616161616 Loss: 0.21912972303954037 Corrects: 2483\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.2055768072605133\n",
            "Train step - Step 170, Loss 0.20553207397460938\n",
            "Train step - Step 180, Loss 0.22383907437324524\n",
            "Train step - Step 190, Loss 0.19269728660583496\n",
            "Train epoch - Accuracy: 0.5418181818181819 Loss: 0.20608459960932685 Corrects: 2682\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.19962547719478607\n",
            "Train step - Step 210, Loss 0.1956397444009781\n",
            "Train step - Step 220, Loss 0.23317204415798187\n",
            "Train step - Step 230, Loss 0.1870502084493637\n",
            "Train epoch - Accuracy: 0.5523232323232323 Loss: 0.19966931800047558 Corrects: 2734\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.19728612899780273\n",
            "Train step - Step 250, Loss 0.17967957258224487\n",
            "Train step - Step 260, Loss 0.1929226964712143\n",
            "Train step - Step 270, Loss 0.19188766181468964\n",
            "Train epoch - Accuracy: 0.5834343434343434 Loss: 0.19277908541939476 Corrects: 2888\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1998911201953888\n",
            "Train step - Step 290, Loss 0.18329913914203644\n",
            "Train step - Step 300, Loss 0.16586630046367645\n",
            "Train step - Step 310, Loss 0.17675089836120605\n",
            "Train epoch - Accuracy: 0.598989898989899 Loss: 0.1864176897809963 Corrects: 2965\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.17738604545593262\n",
            "Train step - Step 330, Loss 0.18055176734924316\n",
            "Train step - Step 340, Loss 0.17641305923461914\n",
            "Train step - Step 350, Loss 0.14969894289970398\n",
            "Train epoch - Accuracy: 0.6155555555555555 Loss: 0.17903075204955207 Corrects: 3047\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.1671830266714096\n",
            "Train step - Step 370, Loss 0.1687958985567093\n",
            "Train step - Step 380, Loss 0.159011110663414\n",
            "Train epoch - Accuracy: 0.6309090909090909 Loss: 0.1738420210462628 Corrects: 3123\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.15419088304042816\n",
            "Train step - Step 400, Loss 0.14775967597961426\n",
            "Train step - Step 410, Loss 0.16979841887950897\n",
            "Train step - Step 420, Loss 0.1525963395833969\n",
            "Train epoch - Accuracy: 0.6539393939393939 Loss: 0.1639658623993999 Corrects: 3237\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.19196446239948273\n",
            "Train step - Step 440, Loss 0.1466265171766281\n",
            "Train step - Step 450, Loss 0.17056730389595032\n",
            "Train step - Step 460, Loss 0.18163080513477325\n",
            "Train epoch - Accuracy: 0.6608080808080808 Loss: 0.16202940798167026 Corrects: 3271\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.176211878657341\n",
            "Train step - Step 480, Loss 0.1588747799396515\n",
            "Train step - Step 490, Loss 0.1342204362154007\n",
            "Train step - Step 500, Loss 0.14058950543403625\n",
            "Train epoch - Accuracy: 0.6767676767676768 Loss: 0.15383696049752862 Corrects: 3350\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.1555892676115036\n",
            "Train step - Step 520, Loss 0.13571906089782715\n",
            "Train step - Step 530, Loss 0.1515876054763794\n",
            "Train step - Step 540, Loss 0.1359780877828598\n",
            "Train epoch - Accuracy: 0.702020202020202 Loss: 0.14594818502363532 Corrects: 3475\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.15486030280590057\n",
            "Train step - Step 560, Loss 0.13412602245807648\n",
            "Train step - Step 570, Loss 0.15869688987731934\n",
            "Train step - Step 580, Loss 0.13754160702228546\n",
            "Train epoch - Accuracy: 0.7034343434343434 Loss: 0.14330182038172326 Corrects: 3482\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.13797789812088013\n",
            "Train step - Step 600, Loss 0.14471013844013214\n",
            "Train step - Step 610, Loss 0.1659015417098999\n",
            "Train step - Step 620, Loss 0.13035106658935547\n",
            "Train epoch - Accuracy: 0.712929292929293 Loss: 0.1402043603164981 Corrects: 3529\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.10688119381666183\n",
            "Train step - Step 640, Loss 0.10551120340824127\n",
            "Train step - Step 650, Loss 0.11231940239667892\n",
            "Train step - Step 660, Loss 0.12159333378076553\n",
            "Train epoch - Accuracy: 0.7434343434343434 Loss: 0.1296956703397963 Corrects: 3680\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.13172531127929688\n",
            "Train step - Step 680, Loss 0.142404243350029\n",
            "Train step - Step 690, Loss 0.12234494835138321\n",
            "Train step - Step 700, Loss 0.14414963126182556\n",
            "Train epoch - Accuracy: 0.756969696969697 Loss: 0.12469814313180519 Corrects: 3747\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.1333446353673935\n",
            "Train step - Step 720, Loss 0.15927483141422272\n",
            "Train step - Step 730, Loss 0.11001887172460556\n",
            "Train step - Step 740, Loss 0.12100914865732193\n",
            "Train epoch - Accuracy: 0.7450505050505051 Loss: 0.12714890332836093 Corrects: 3688\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.11506552994251251\n",
            "Train step - Step 760, Loss 0.10524898767471313\n",
            "Train step - Step 770, Loss 0.0984029471874237\n",
            "Train epoch - Accuracy: 0.7668686868686869 Loss: 0.11733168877435453 Corrects: 3796\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.12441835552453995\n",
            "Train step - Step 790, Loss 0.11353017389774323\n",
            "Train step - Step 800, Loss 0.13449843227863312\n",
            "Train step - Step 810, Loss 0.12670955061912537\n",
            "Train epoch - Accuracy: 0.7739393939393939 Loss: 0.11266988289175611 Corrects: 3831\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.10420570522546768\n",
            "Train step - Step 830, Loss 0.10050588846206665\n",
            "Train step - Step 840, Loss 0.09052333980798721\n",
            "Train step - Step 850, Loss 0.07840898633003235\n",
            "Train epoch - Accuracy: 0.7808080808080808 Loss: 0.11037116950509523 Corrects: 3865\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.13141851127147675\n",
            "Train step - Step 870, Loss 0.11617157608270645\n",
            "Train step - Step 880, Loss 0.11001942306756973\n",
            "Train step - Step 890, Loss 0.08741968870162964\n",
            "Train epoch - Accuracy: 0.7844444444444445 Loss: 0.10794938291564132 Corrects: 3883\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.11431687325239182\n",
            "Train step - Step 910, Loss 0.0919078066945076\n",
            "Train step - Step 920, Loss 0.1131000742316246\n",
            "Train step - Step 930, Loss 0.12487182766199112\n",
            "Train epoch - Accuracy: 0.7935353535353535 Loss: 0.10410453104310566 Corrects: 3928\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.0851389467716217\n",
            "Train step - Step 950, Loss 0.10327544063329697\n",
            "Train step - Step 960, Loss 0.09313042461872101\n",
            "Train step - Step 970, Loss 0.12224413454532623\n",
            "Train epoch - Accuracy: 0.8014141414141415 Loss: 0.10208582940125706 Corrects: 3967\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.10216508060693741\n",
            "Train step - Step 990, Loss 0.10625173896551132\n",
            "Train step - Step 1000, Loss 0.12439626455307007\n",
            "Train step - Step 1010, Loss 0.09492490440607071\n",
            "Train epoch - Accuracy: 0.805050505050505 Loss: 0.10121105768764863 Corrects: 3985\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.1075296625494957\n",
            "Train step - Step 1030, Loss 0.08558668196201324\n",
            "Train step - Step 1040, Loss 0.10448060184717178\n",
            "Train step - Step 1050, Loss 0.0987623855471611\n",
            "Train epoch - Accuracy: 0.8115151515151515 Loss: 0.09598487866647316 Corrects: 4017\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.11706822365522385\n",
            "Train step - Step 1070, Loss 0.08957835286855698\n",
            "Train step - Step 1080, Loss 0.1004997119307518\n",
            "Train step - Step 1090, Loss 0.10269360989332199\n",
            "Train epoch - Accuracy: 0.8151515151515152 Loss: 0.093790872918837 Corrects: 4035\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10780717432498932\n",
            "Train step - Step 1110, Loss 0.0829559937119484\n",
            "Train step - Step 1120, Loss 0.06946247071027756\n",
            "Train step - Step 1130, Loss 0.08679612725973129\n",
            "Train epoch - Accuracy: 0.8232323232323232 Loss: 0.09224864899811118 Corrects: 4075\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.0877915769815445\n",
            "Train step - Step 1150, Loss 0.07081689685583115\n",
            "Train step - Step 1160, Loss 0.09684779495000839\n",
            "Train epoch - Accuracy: 0.8305050505050505 Loss: 0.08737428996298048 Corrects: 4111\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.08674424141645432\n",
            "Train step - Step 1180, Loss 0.08422601222991943\n",
            "Train step - Step 1190, Loss 0.09191498905420303\n",
            "Train step - Step 1200, Loss 0.09191594272851944\n",
            "Train epoch - Accuracy: 0.834949494949495 Loss: 0.08547621402174535 Corrects: 4133\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.07304417341947556\n",
            "Train step - Step 1220, Loss 0.09637052565813065\n",
            "Train step - Step 1230, Loss 0.08423800766468048\n",
            "Train step - Step 1240, Loss 0.09754743427038193\n",
            "Train epoch - Accuracy: 0.8418181818181818 Loss: 0.0811596822106477 Corrects: 4167\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.08752617239952087\n",
            "Train step - Step 1260, Loss 0.08612791448831558\n",
            "Train step - Step 1270, Loss 0.09345199167728424\n",
            "Train step - Step 1280, Loss 0.06988094002008438\n",
            "Train epoch - Accuracy: 0.8432323232323232 Loss: 0.08195995536717501 Corrects: 4174\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.063926100730896\n",
            "Train step - Step 1300, Loss 0.09519979357719421\n",
            "Train step - Step 1310, Loss 0.0794561430811882\n",
            "Train step - Step 1320, Loss 0.09330239146947861\n",
            "Train epoch - Accuracy: 0.8434343434343434 Loss: 0.08210123597973525 Corrects: 4175\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.06706669181585312\n",
            "Train step - Step 1340, Loss 0.07936250418424606\n",
            "Train step - Step 1350, Loss 0.08014490455389023\n",
            "Train step - Step 1360, Loss 0.08957503736019135\n",
            "Train epoch - Accuracy: 0.8488888888888889 Loss: 0.07934777613541093 Corrects: 4202\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.08020991086959839\n",
            "Train step - Step 1380, Loss 0.09724964946508408\n",
            "Train step - Step 1390, Loss 0.06381719559431076\n",
            "Train step - Step 1400, Loss 0.06528539955615997\n",
            "Train epoch - Accuracy: 0.8549494949494949 Loss: 0.07585879642855037 Corrects: 4232\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.07746966183185577\n",
            "Train step - Step 1420, Loss 0.08617367595434189\n",
            "Train step - Step 1430, Loss 0.06970410794019699\n",
            "Train step - Step 1440, Loss 0.08160582929849625\n",
            "Train epoch - Accuracy: 0.8527272727272728 Loss: 0.07876499318715298 Corrects: 4221\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.06916437298059464\n",
            "Train step - Step 1460, Loss 0.06720501184463501\n",
            "Train step - Step 1470, Loss 0.06595558673143387\n",
            "Train step - Step 1480, Loss 0.0707656592130661\n",
            "Train epoch - Accuracy: 0.8654545454545455 Loss: 0.07132082007449082 Corrects: 4284\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.05659304931759834\n",
            "Train step - Step 1500, Loss 0.06974741071462631\n",
            "Train step - Step 1510, Loss 0.0784800574183464\n",
            "Train step - Step 1520, Loss 0.06901217252016068\n",
            "Train epoch - Accuracy: 0.8672727272727273 Loss: 0.07315080337753199 Corrects: 4293\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.07148046046495438\n",
            "Train step - Step 1540, Loss 0.08015324175357819\n",
            "Train step - Step 1550, Loss 0.07519340515136719\n",
            "Train epoch - Accuracy: 0.8682828282828283 Loss: 0.06976768631826748 Corrects: 4298\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.061872996389865875\n",
            "Train step - Step 1570, Loss 0.08952672779560089\n",
            "Train step - Step 1580, Loss 0.0770217701792717\n",
            "Train step - Step 1590, Loss 0.06781128793954849\n",
            "Train epoch - Accuracy: 0.8719191919191919 Loss: 0.06902587638659911 Corrects: 4316\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.0734851062297821\n",
            "Train step - Step 1610, Loss 0.055662430822849274\n",
            "Train step - Step 1620, Loss 0.08112241327762604\n",
            "Train step - Step 1630, Loss 0.07834818959236145\n",
            "Train epoch - Accuracy: 0.8739393939393939 Loss: 0.06590250587222551 Corrects: 4326\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.05307411774992943\n",
            "Train step - Step 1650, Loss 0.04475534334778786\n",
            "Train step - Step 1660, Loss 0.06450628489255905\n",
            "Train step - Step 1670, Loss 0.06469791382551193\n",
            "Train epoch - Accuracy: 0.8838383838383839 Loss: 0.06405000432874217 Corrects: 4375\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.055307772010564804\n",
            "Train step - Step 1690, Loss 0.06018352508544922\n",
            "Train step - Step 1700, Loss 0.05738571286201477\n",
            "Train step - Step 1710, Loss 0.07564640045166016\n",
            "Train epoch - Accuracy: 0.883030303030303 Loss: 0.06291912011124871 Corrects: 4371\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.0579797700047493\n",
            "Train step - Step 1730, Loss 0.05599530413746834\n",
            "Train step - Step 1740, Loss 0.07791357487440109\n",
            "Train step - Step 1750, Loss 0.07570113986730576\n",
            "Train epoch - Accuracy: 0.8860606060606061 Loss: 0.06122048602411241 Corrects: 4386\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.046970587223768234\n",
            "Train step - Step 1770, Loss 0.05448814854025841\n",
            "Train step - Step 1780, Loss 0.0570836178958416\n",
            "Train step - Step 1790, Loss 0.052846431732177734\n",
            "Train epoch - Accuracy: 0.8921212121212121 Loss: 0.05948092536342264 Corrects: 4416\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.0464363619685173\n",
            "Train step - Step 1810, Loss 0.056273628026247025\n",
            "Train step - Step 1820, Loss 0.0658160001039505\n",
            "Train step - Step 1830, Loss 0.0690997764468193\n",
            "Train epoch - Accuracy: 0.8955555555555555 Loss: 0.05688923456151076 Corrects: 4433\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.059149112552404404\n",
            "Train step - Step 1850, Loss 0.05253974348306656\n",
            "Train step - Step 1860, Loss 0.07070215791463852\n",
            "Train step - Step 1870, Loss 0.05269395932555199\n",
            "Train epoch - Accuracy: 0.8927272727272727 Loss: 0.05812157820571553 Corrects: 4419\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.04611649736762047\n",
            "Train step - Step 1890, Loss 0.06521129608154297\n",
            "Train step - Step 1900, Loss 0.0597667470574379\n",
            "Train step - Step 1910, Loss 0.07057886570692062\n",
            "Train epoch - Accuracy: 0.9002020202020202 Loss: 0.05608734115506663 Corrects: 4456\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.042010191828012466\n",
            "Train step - Step 1930, Loss 0.043184537440538406\n",
            "Train step - Step 1940, Loss 0.0397011898458004\n",
            "Train epoch - Accuracy: 0.9242424242424242 Loss: 0.04325935698518849 Corrects: 4575\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.039822064340114594\n",
            "Train step - Step 1960, Loss 0.036546554416418076\n",
            "Train step - Step 1970, Loss 0.04635452851653099\n",
            "Train step - Step 1980, Loss 0.033635109663009644\n",
            "Train epoch - Accuracy: 0.9432323232323232 Loss: 0.037418713830035144 Corrects: 4669\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.03440694510936737\n",
            "Train step - Step 2000, Loss 0.04546314477920532\n",
            "Train step - Step 2010, Loss 0.03715860843658447\n",
            "Train step - Step 2020, Loss 0.050867773592472076\n",
            "Train epoch - Accuracy: 0.9412121212121212 Loss: 0.03600090804876703 Corrects: 4659\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.02928098477423191\n",
            "Train step - Step 2040, Loss 0.03901729732751846\n",
            "Train step - Step 2050, Loss 0.037106212228536606\n",
            "Train step - Step 2060, Loss 0.03669671341776848\n",
            "Train epoch - Accuracy: 0.9452525252525252 Loss: 0.03400145272564406 Corrects: 4679\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.0314776748418808\n",
            "Train step - Step 2080, Loss 0.031674575060606\n",
            "Train step - Step 2090, Loss 0.018039535731077194\n",
            "Train step - Step 2100, Loss 0.0348820686340332\n",
            "Train epoch - Accuracy: 0.9468686868686869 Loss: 0.033380321872354755 Corrects: 4687\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.02144073322415352\n",
            "Train step - Step 2120, Loss 0.029142146930098534\n",
            "Train step - Step 2130, Loss 0.03788841888308525\n",
            "Train step - Step 2140, Loss 0.032097745686769485\n",
            "Train epoch - Accuracy: 0.9496969696969697 Loss: 0.03260654991022264 Corrects: 4701\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.027982238680124283\n",
            "Train step - Step 2160, Loss 0.020777106285095215\n",
            "Train step - Step 2170, Loss 0.030742371454834938\n",
            "Train step - Step 2180, Loss 0.0357779823243618\n",
            "Train epoch - Accuracy: 0.9543434343434344 Loss: 0.03025592699345916 Corrects: 4724\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.031895961612463\n",
            "Train step - Step 2200, Loss 0.025660132989287376\n",
            "Train step - Step 2210, Loss 0.038197994232177734\n",
            "Train step - Step 2220, Loss 0.025359375402331352\n",
            "Train epoch - Accuracy: 0.9525252525252526 Loss: 0.03145596713730783 Corrects: 4715\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.03523494675755501\n",
            "Train step - Step 2240, Loss 0.02871292270720005\n",
            "Train step - Step 2250, Loss 0.028518855571746826\n",
            "Train step - Step 2260, Loss 0.028394445776939392\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.031200148419599342 Corrects: 4703\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.026044586673378944\n",
            "Train step - Step 2280, Loss 0.03078872337937355\n",
            "Train step - Step 2290, Loss 0.03193800523877144\n",
            "Train step - Step 2300, Loss 0.05611391365528107\n",
            "Train epoch - Accuracy: 0.9555555555555556 Loss: 0.030103796654277377 Corrects: 4730\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.02573627233505249\n",
            "Train step - Step 2320, Loss 0.026890838518738747\n",
            "Train step - Step 2330, Loss 0.03133704885840416\n",
            "Train epoch - Accuracy: 0.953939393939394 Loss: 0.0305143504032884 Corrects: 4722\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.029058242216706276\n",
            "Train step - Step 2350, Loss 0.02426040545105934\n",
            "Train step - Step 2360, Loss 0.03160322457551956\n",
            "Train step - Step 2370, Loss 0.04511835798621178\n",
            "Train epoch - Accuracy: 0.9563636363636364 Loss: 0.029337084354324774 Corrects: 4734\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.031107379123568535\n",
            "Train step - Step 2390, Loss 0.029924381524324417\n",
            "Train step - Step 2400, Loss 0.024590641260147095\n",
            "Train step - Step 2410, Loss 0.020830800756812096\n",
            "Train epoch - Accuracy: 0.961010101010101 Loss: 0.02768125159346094 Corrects: 4757\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.02306819148361683\n",
            "Train step - Step 2430, Loss 0.05020679160952568\n",
            "Train step - Step 2440, Loss 0.032019324600696564\n",
            "Train step - Step 2450, Loss 0.035698115825653076\n",
            "Train epoch - Accuracy: 0.9587878787878787 Loss: 0.028257839812172784 Corrects: 4746\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.03360297158360481\n",
            "Train step - Step 2470, Loss 0.02808537892997265\n",
            "Train step - Step 2480, Loss 0.01882505789399147\n",
            "Train step - Step 2490, Loss 0.01958288811147213\n",
            "Train epoch - Accuracy: 0.96 Loss: 0.026811572033347504 Corrects: 4752\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.024240998551249504\n",
            "Train step - Step 2510, Loss 0.015550089068710804\n",
            "Train step - Step 2520, Loss 0.01995779201388359\n",
            "Train step - Step 2530, Loss 0.02513287402689457\n",
            "Train epoch - Accuracy: 0.962020202020202 Loss: 0.025361865840174934 Corrects: 4762\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.021166345104575157\n",
            "Train step - Step 2550, Loss 0.018641367554664612\n",
            "Train step - Step 2560, Loss 0.021410910412669182\n",
            "Train step - Step 2570, Loss 0.03057834319770336\n",
            "Train epoch - Accuracy: 0.9646464646464646 Loss: 0.023534335130167127 Corrects: 4775\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.029833734035491943\n",
            "Train step - Step 2590, Loss 0.018363453447818756\n",
            "Train step - Step 2600, Loss 0.026480138301849365\n",
            "Train step - Step 2610, Loss 0.028178563341498375\n",
            "Train epoch - Accuracy: 0.9660606060606061 Loss: 0.024363452725187695 Corrects: 4782\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.0342327281832695\n",
            "Train step - Step 2630, Loss 0.02055048942565918\n",
            "Train step - Step 2640, Loss 0.020426509901881218\n",
            "Train step - Step 2650, Loss 0.021581510081887245\n",
            "Train epoch - Accuracy: 0.9648484848484848 Loss: 0.025252315486320343 Corrects: 4776\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.03008689358830452\n",
            "Train step - Step 2670, Loss 0.0230241846293211\n",
            "Train step - Step 2680, Loss 0.019735518842935562\n",
            "Train step - Step 2690, Loss 0.01372586376965046\n",
            "Train epoch - Accuracy: 0.9644444444444444 Loss: 0.024579011685769966 Corrects: 4774\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.02005911059677601\n",
            "Train step - Step 2710, Loss 0.023675382137298584\n",
            "Train step - Step 2720, Loss 0.02118927799165249\n",
            "Train epoch - Accuracy: 0.9634343434343434 Loss: 0.024567253339772274 Corrects: 4769\n",
            "Training finished in 203.4930591583252 seconds\n",
            "EVALUATION:  0.78 0.12797264754772186\n",
            "TEST GROUP:  0.836\n",
            "TEST ALL:  0.836\n",
            "GROUP:  2\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.5198947787284851\n",
            "Train step - Step 10, Loss 0.18119554221630096\n",
            "Train step - Step 20, Loss 0.1347174048423767\n",
            "Train step - Step 30, Loss 0.10982789844274521\n",
            "Train epoch - Accuracy: 0.36808080808080806 Loss: 0.17154767145111102 Corrects: 1822\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.09604242444038391\n",
            "Train step - Step 50, Loss 0.09236812591552734\n",
            "Train step - Step 60, Loss 0.08287163078784943\n",
            "Train step - Step 70, Loss 0.08627394586801529\n",
            "Train epoch - Accuracy: 0.6482828282828282 Loss: 0.08684955637563359 Corrects: 3209\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.08100271224975586\n",
            "Train step - Step 90, Loss 0.07603686302900314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-754cea9f07ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequentialLearningFineTuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_subsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-31e6b3e7dcf7>\u001b[0m in \u001b[0;36msequentialLearningFineTuning\u001b[0;34m(train_subsets, val_subsets, test_subsets)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSchedulerOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset learning rate and step_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes_seen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# Validate on current group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-20cff53f0192>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Bring images and labels to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aakVopSdyeGr"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ0VbtGQygRH"
      },
      "source": [
        "method = \"finetuning\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ-J8aC_xx-c"
      },
      "source": [
        "## LWF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZBNivOix1FB"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdfIllqBx59v"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkzFiDVDyAm_"
      },
      "source": [
        "### sequential learning LWF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf7REFkIyEiQ"
      },
      "source": [
        "### LWF\n",
        "def sequentialLearningLWF(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZTwTS_4TcES"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkrMQy2TuUAb"
      },
      "source": [
        "# train\n",
        "# net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningLWF(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2qUxVyMTfB4"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_4xLfwcpDz"
      },
      "source": [
        "method = \"Learning Without Forgetting\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGHBWaLNXGeI"
      },
      "source": [
        "\"\"\"num_classes_seen = 100\n",
        "dif_accuracies=printAccuracyDifference(net,old_accuracies, num_classes_seen)\n",
        "dif_accuracies\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjGRIqDtNQP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nX10znUi6qd"
      },
      "source": [
        "## iCaRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKBdrJ8Csy2a"
      },
      "source": [
        "### classify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAPTOU9bszTY"
      },
      "source": [
        "def classify(batch_img, net, exemplar_sets):\n",
        "  \"\"\" classify images by nearest mean-of-exemplars \"\"\" \n",
        "  \"\"\" \n",
        "  class1: list_of_indices for images that represent class1\n",
        "  class2: list_of_indices for images that represent class2\n",
        "  ...\n",
        "  class100: list_of_indices for images that represent class 100\n",
        "  with Subset I retrieve the images from the train_dataset\n",
        "  iterate over it to calculate mean for each class\n",
        "  \"\"\"\n",
        "  net.eval()\n",
        "  classes_mean = []\n",
        "  for k, exemplars_indices in exemplar_sets.items():\n",
        "    features = []\n",
        "    class_images_set = Subset(train_dataset, exemplars_indices)\n",
        "    class_images = DataLoader(class_images_set, batch_size=BATCH_SIZE, num_workers=2)\n",
        "    for _, images, labels in class_images:\n",
        "      # for each class (paper from y=1...t) calculate features and then mean\n",
        "      feature = net.forward(images)\n",
        "      features.append(feature)\n",
        "    features_s = torch.cat(features)\n",
        "    class_mean = features_s.mean(0)\n",
        "    classes_mean.append(class_mean)\n",
        "    means_exemplars = torch.cat(classes_mean, dim=0)\n",
        "    means_exemplars = torch.stack([means_exemplars] * BATCH_SIZE)\n",
        "    means_exemplars = means_exemplars.transpose(1,2)\n",
        "  feature_images_to_classify = net.forward(batch_img)\n",
        "  # sono da normalizzare?\n",
        "  feature_images_to_classify = feature_images_to_classify.unsqueeze(2)\n",
        "  feature_images_to_classify = feature_images_to_classify.expand_as(means_exemplars) # expand_as to get the same dimension\n",
        "  preds = torch.argmin((feature_images_to_classify - means_exemplars).pow(2).sum(1), dim=1)\n",
        "  return preds"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5HRagcujHgM"
      },
      "source": [
        "### construct exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blwBqG6BjNsM"
      },
      "source": [
        "import sys\n",
        "\n",
        "def constructExemplarSet(net, Xclass, m):\n",
        "#Xclass contiene immagini e label della classe X\n",
        "  exemplars_set = []\n",
        "  feature_exemplars = []\n",
        "  indexes = []\n",
        "  features = [] \n",
        "  class_images = []\n",
        "  net.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    XtrainLoader = torch.utils.data.DataLoader(Xclass, shuffle = True, batch_size=1, num_workers=2)\n",
        "    for _, image, labels in XtrainLoader:\n",
        "      image = image.to(DEVICE)\n",
        "      class_images.append(image)\n",
        "      # per ogni immagine della classe x, prendiamo le rispettive feature e le uniamo nel vettore features, che contiene tutte quelle delle immagini della classe x\n",
        "      feature = net.forward(image)\n",
        "      #feature = net.predict(featuretmp)\n",
        "      feature = feature/np.linalg.norm(feature.cpu())\n",
        "      features.append(feature)\n",
        "\n",
        "    features = torch.cat(features, dim=0) #cat solve the problem of inequal size of tensors \n",
        "    current_class_mean = features.mean(0) # mu = media delle features delle immagini della classe\n",
        "\n",
        "    for k in range(1, m+1):\n",
        "      min = 100000\n",
        "      sum = 0\n",
        "      for j in range(k-1):\n",
        "        sum += feature_exemplars[j]\n",
        "      for x in range(len(Xclass)): \n",
        "        if (x not in indexes):\n",
        "          phiX = features[x]\n",
        "          val = current_class_mean - ((phiX + sum)/k)\n",
        "          val = np.linalg.norm(val.cpu().numpy()) ## NORMA \n",
        "          if (val < min):\n",
        "            min = val\n",
        "            feature_min = phiX\n",
        "            index_min = x\n",
        "      feature_exemplars.append(feature_min)\n",
        "      exemplars_set.append(Xclass[index_min][0])\n",
        "\n",
        "    print(\"lunghezza exemplar set: \", len(exemplars_set))\n",
        "    return exemplars_set"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy_HVL-tjZcb"
      },
      "source": [
        "### reduce exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7kuQ5DnjcCP"
      },
      "source": [
        "# DA CHIAMARE PER OGNI CLASSE k: riduce il numero di exemplars all'interno dell'exemplar_set della classe k \n",
        "# EXEMPLAR SET è una lista di indici che rappresentano la posizione dell'immagine selezionata per l'exemplar della classe corrente nel dataset di partenza\n",
        "def reduceExemplarSet(m, exemplars_set):\n",
        "  exemplars_new = []\n",
        "  for i in range(m):\n",
        "    if (exemplars_set != []):\n",
        "      exemplars_new.append(exemplars_set[i])\n",
        "\n",
        "  return exemplars_new"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMMlEpLsjfaj"
      },
      "source": [
        "### update representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRdA9RtDjhak"
      },
      "source": [
        "def updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot, old_net):\n",
        "  exemplars_subset = []\n",
        "  exemplars_indices = []\n",
        "  total_exemplars = []\n",
        "  labels_tot = []\n",
        "\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "    if (exemplar_set_class_k != []):\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "\n",
        "  if group_id > 1:\n",
        "    train_subset_total = torch.utils.data.ConcatDataset([train_subset, total_exemplars])\n",
        "  else:\n",
        "    train_subset_total = train_subset\n",
        "    \n",
        "  print(\"Len TOTAL train susbset: \", len(train_subset_total))\n",
        "  # train_loader is the concatenation of new images and exemplars of old classes\n",
        "  train_loader = torch.utils.data.DataLoader(train_subset_total, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  print(\"training\")\n",
        "  train(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)\n",
        "  #trainWithOtherLosses(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)\n",
        "  #trainCEandL1(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac65-6QGjm9O"
      },
      "source": [
        "### incremental train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPU_32ArjpPH"
      },
      "source": [
        "import copy\n",
        "def incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_until_now):\n",
        "  print(\"Starting the update representation\")\n",
        "  exemplar_indices = None\n",
        "  num_classes = 10\n",
        "  new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "  print(\"NEW CLASSES: \", new_classes_examined)\n",
        "\n",
        "  updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net)\n",
        "\n",
        "  iteration = group_id - 1\n",
        "  # num_classes received till now\n",
        "  t = (num_classes * iteration) + num_classes  \n",
        "  m = int(K/t)\n",
        "\n",
        "  # REDUCING EXEMPLAR SET FOR EXISTING CLASSES\n",
        "  print(\"reducing exemplars for each class\")\n",
        "  print(total_classes_until_now)\n",
        "  for y in total_classes_until_now: \n",
        "    exemplar_y_new = reduceExemplarSet(m, exemplars_set_tot[y])\n",
        "    print(\"REDUCED EXEMPLAR: \", len(exemplar_y_new))\n",
        "    exemplars_set_tot[y] = exemplar_y_new\n",
        "\n",
        "  \n",
        "  # CONSTRUCTION EXEMPLAR SET FOR NEW CLASSES\n",
        "  for y in new_classes_examined:\n",
        "    images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data']\n",
        "    imgs_idxs = images_current_class.index \n",
        "    class_train_subset = Subset(train_dataset, imgs_idxs)\n",
        "    print(\"class train: \", class_train_subset)\n",
        "    print(\"Constructing exemplars of class\", y)\n",
        "    # exemplar set is a set of indexes \n",
        "    exemplars_set = constructExemplarSet(net, class_train_subset, m) \n",
        "    exemplars_set_tot[y] = exemplars_set\n",
        "    print(\"exemplar set: \", exemplars_set)\n",
        "\n",
        "  # IMPLEMENTATION 'END-to-END Incremental Learning' PAPER\n",
        "  #balancedFinetune(net, group_id, exemplars_set_tot, NUM_EPOCHS_FINETUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDxmsQlWjp5D"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dQznGoEju9J"
      },
      "source": [
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes_till_now, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    print(\"num classes till now: \", num_classes_till_now)\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes_till_now, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            #if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcZUqVCjeMG4"
      },
      "source": [
        "### Train con CE + L1Loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl4zFMYGtAEJ"
      },
      "source": [
        "import copy\n",
        "def trainCEandL1(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    distillation_loss = 0\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "            # Classification LOSS\n",
        "            lr = 0.01\n",
        "            classification_loss = nn.CrossEntropyLoss()(outputs, labels) # BCE\n",
        "\n",
        "            # Distillation LOSS \n",
        "            if (group_id > 1):\n",
        "               old_features = old_net.forward(images)\n",
        "               old_outputs = old_net.predict(old_features)\n",
        "               #labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "               new_labels = torch.sigmoid(old_outputs)\n",
        "               new_outputs = outputs[:, 0:num_classes_till_previous_step]\n",
        "               distillation_loss = l1Loss(new_outputs, new_labels) # L1\n",
        "               print(distillation_loss)\n",
        "\n",
        "            loss = classification_loss + distillation_loss\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrbPsdT2eRho"
      },
      "source": [
        "### Train con CE + KLDiv Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzTrBE-Dea1S"
      },
      "source": [
        "import copy\n",
        "def trainWithOtherLosses(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    distillation_loss = 0\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "            # Classification LOSS\n",
        "            classification_loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "            # Distillation LOSS \n",
        "            if (group_id > 1):\n",
        "               old_features = old_net.forward(images)\n",
        "               old_outputs = old_net.predict(old_features)\n",
        "               T = 2\n",
        "               beta = 0.25\n",
        "               distillation_loss = nn.KLDivLoss()(F.log_softmax(outputs[:, 0:num_classes_till_previous_step]/T, dim = 1), F.softmax(old_outputs.detach()/T, dim = 1)) * T * T * beta * num_classes_till_previous_step\n",
        "\n",
        "            loss = classification_loss + distillation_loss\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHpi410ljzWD"
      },
      "source": [
        "### test and validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQYB0-cLj0sP"
      },
      "source": [
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        #_, preds = classify(images, )\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "246M7j-s4Chq"
      },
      "source": [
        "### Balanced Finetune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtFJY_fB5ymS"
      },
      "source": [
        "def data_augmentation_e2e(img, lab):\n",
        "    \"\"\"\n",
        "        Realize the data augmentation in End-to-End paper\n",
        "        Parameters\n",
        "        ----------\n",
        "        img: the original images, size = (n, c, w, h)\n",
        "        lab: the original labels, size = (n)\n",
        "        Returns\n",
        "        ----------\n",
        "        img_aug: the original images, size = (n * 12, c, w, h)\n",
        "        lab_aug: the original labels, size = (n * 12)\n",
        "    \"\"\"\n",
        "    \n",
        "    shape = np.shape(img)\n",
        "    # print(\"IMG is: \",img)\n",
        "    # print(shape[0], 1, shape[1], shape[2], shape[3])\n",
        "    img_aug = np.zeros((shape[0], 6, shape[1], shape[2], shape[3]))\n",
        "    img_aug[:, 0, :, :, :] = img\n",
        "    lab_aug = np.zeros((shape[0], 6))\n",
        "    # print(\"IMG_AUG is: \", img_aug)\n",
        "\n",
        "    for i in range(shape[0]):\n",
        "        # np.random.seed(int(time.time()) % 1000)\n",
        "\n",
        "        # convert image from tensor to numpy\n",
        "        image=img.numpy()\n",
        "        im = image[i]\n",
        "      \n",
        "        # # brightness\n",
        "        brightness = (np.random.rand(1)-0.5)*2*63\n",
        "        im_temp = im + brightness\n",
        "\n",
        "        img_aug[i, 1] = im_temp\n",
        "\n",
        "\n",
        "        # constrast\n",
        "        constrast = (np.random.rand(1)-0.5)*2*0.8+1\n",
        "        m0 = np.mean(im[0])\n",
        "        m1 = np.mean(im[1])\n",
        "        m2 = np.mean(im[2])\n",
        "        im_temp = im\n",
        "        im_temp[0] = (im_temp[0]-m0)*constrast + m0\n",
        "        im_temp[1] = (im_temp[1]-m1)*constrast + m1\n",
        "        im_temp[2] = (im_temp[2]-m2)*constrast + m2\n",
        "        img_aug[i, 2] = im_temp\n",
        "\n",
        "        # crop\n",
        "        im_temp = img_aug[i, :3]\n",
        "        for j in range(3):\n",
        "            x_ = int(np.random.rand(1)*1000)%8\n",
        "            y_ = int(np.random.rand(1)*1000)%8\n",
        "            im_temp = np.zeros(shape=(shape[1], shape[2]+8, shape[3]+8))\n",
        "            im_temp[:, 4:-4, 4:-4] = img_aug[i, j]\n",
        "            img_aug[i, 3+j] = im_temp[:, x_:x_+shape[2], y_:y_+shape[3]]\n",
        "\n",
        "\n",
        "\n",
        "        # mirror\n",
        "        # for j in range(6):\n",
        "        #     im_temp = img_aug[i, j]\n",
        "        #     img_aug[i, 6 + j] = im_temp[:,-1::-1,:]\n",
        "\n",
        "        lab_aug[i, :] = lab[i]\n",
        "\n",
        "    # idx = np.where(img_aug>255)\n",
        "    # img_aug[idx] = 255\n",
        "    # idx = np.where(img_aug<0)\n",
        "    # img_aug[idx] = 0\n",
        "\n",
        "    img_aug = np.reshape(img_aug, newshape=(shape[0]*6, shape[1], shape[2], shape[3]))\n",
        "    img_aug = np.array(img_aug, dtype=np.float64)\n",
        "    lab_aug = np.reshape(lab_aug, newshape=(shape[0]*6))\n",
        "    lab_aug = np.array(lab_aug, dtype=np.float64)\n",
        "    return img_aug, lab_aug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC_zGBOB4B4J"
      },
      "source": [
        "def balancedFinetune(net, iteration, exemplars_set_tot, NUM_EPOCHS):\n",
        "  num_classes_till_previous_step = iteration * 10 - 10\n",
        "  num_classes = iteration * 10\n",
        "  total_exemplars = []\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  old_net = copy.deepcopy(net)\n",
        "  old_net.eval()\n",
        "\n",
        "  #if iteration > 1:\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    if (exemplar_set_class_k != []):\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "    \n",
        "  reduced_train_loader = torch.utils.data.DataLoader(total_exemplars, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  # reduced train loader is the sets of all exemplars of new and old classes\n",
        "\n",
        "  # finetune\n",
        "  lrc = LR *0.1 # small learning rate for finetune\n",
        "  print('current lr = %f' % (lrc))\n",
        "  softmax = nn.Softmax(dim=-1).cuda()\n",
        "  current_step = 0\n",
        "  acc_finetune_train = []\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS_FINETUNE):\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for _, images, labels in reduced_train_loader:\n",
        "      images, labels = data_augmentation_e2e(images,labels) \n",
        "      images = torch.from_numpy(images) \n",
        "      labels = torch.from_numpy(labels) \n",
        "    # reduced_train_loader contains the same number of images for both new classes and old classes\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "      optimizer = torch.optim.SGD(net.parameters(), lr=lrc, momentum=MOMENTUM,\n",
        "                                    weight_decay= WEIGHT_DECAY, nesterov=True)\n",
        "\n",
        "\n",
        "      # print(\"Outside: input size\", img.size(), \"output_size\", lab.size())\n",
        "      features = net.forward(images)\n",
        "      outputs = net.predict(features)\n",
        "\n",
        "      # classification loss\n",
        "      prob_cls = softmax(outputs)\n",
        "      labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "      labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "    # distillation loss for all classes (maybe the author only distillates for novel classes)\n",
        "      if iteration > 1:\n",
        "        old_features = old_net.forward(images)\n",
        "        old_outputs = old_net.predict(old_features)\n",
        "        labels_enc[:, 0:num_classes_till_previous_step] = torch.sigmoid(old_outputs[:, 0:num_classes_till_previous_step])\n",
        "\n",
        "\n",
        "      loss = computeLoss(criterion, outputs, labels_enc)\n",
        "\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "          \n",
        "    # Update Corrects & Loss\n",
        "      running_loss += loss.item() * images.size(0)\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Log loss\n",
        "      if current_step % LOG_FREQUENCY == 0:\n",
        "        print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss = running_loss / float(len(reduced_train_loader.dataset)*6)\n",
        "      epoch_acc = running_corrects / float(len(reduced_train_loader.dataset)*6)\n",
        "      \n",
        "    print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T96WcHWlIYGR"
      },
      "source": [
        "### CLASSIFIERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZTGxaotIXBI"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def classifierTrain(net, exemplars_set_tot):\n",
        "  torch.no_grad()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  X_train, y_train = [], []\n",
        "  total_exemplars = []\n",
        "  counter = 0\n",
        "\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "    if (exemplar_set_class_k != []):\n",
        "      counter += 1\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "\n",
        "\n",
        "  exemplar_sets = torch.utils.data.DataLoader(total_exemplars, shuffle = True, batch_size=1, num_workers=2)\n",
        "  for _, exemplar, label in exemplar_sets:\n",
        "    exemplar = exemplar.to(DEVICE)\n",
        "    features = net.forward(exemplar)\n",
        "    outputs = net.predict(features)\n",
        "    outputs = outputs.squeeze()\n",
        "    outputs.data = outputs.data / outputs.data.norm()\n",
        "    X_train.append(outputs.cpu().detach().numpy())\n",
        "    y_train.append(label)\n",
        "\n",
        "  K_nn = math.ceil(2000/counter)\n",
        "  model1 = LinearSVC()\n",
        "  model2 = KNeighborsClassifier(n_neighbors = K_nn)\n",
        "  print(\"x train: \", X_train[0])\n",
        "  print(\"y_train: \", y_train)\n",
        "  model = model1.fit(X_train, y_train)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYs6xKy1J-pQ"
      },
      "source": [
        "def classifySVM_KNN(net, images, model):\n",
        "  torch.no_grad()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  X_pred = []\n",
        "  images = images.to(DEVICE)\n",
        "  features = net.forward(images)\n",
        "  outputs = net.predict(features)\n",
        "\n",
        "  for feature in outputs:\n",
        "    feature = feature.squeeze()\n",
        "    feature.data = feature.data / feature.data.norm()\n",
        "    X_pred.append(feature.cpu().detach().numpy())\n",
        "\n",
        "  preds = model.predict(X_pred)\n",
        "  return torch.tensor(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7WwJLGnJcXC"
      },
      "source": [
        "def validateSVM_KNN(net, val_dataloader, criterion, num_classes, model):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        #images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        preds = classifySVM_KNN(net, images, model)\n",
        "        preds = preds.to(DEVICE)\n",
        "\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def testSVM_KNN(net, test_dataloader, num_classes, model):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validateSVM_KNN(net, test_dataloader, None, num_classes, model)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTv6zBetj3Cb"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_cnELYdkap4"
      },
      "source": [
        "def sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 10\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      print(\"TEST SET LENGHT: \", len(test_set))\n",
        "      print(\"TEST CURRENT GROUP SET LENGHT: \", len(test_subset))\n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      test_classes = list(test_dataset.df.loc[test_set.indices, 'labels'].value_counts().index)\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TEST_SET CLASSES: \", test_classes)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen) # Train the network with 10 classes at a time\n",
        "      model = classifierTrain(net, exemplars_set_tot)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      #acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      acc, loss, _, _ = validateSVM_KNN(net, val_loader, criterion, num_classes_seen, model)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      #acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      acc_group, _, _ = testSVM_KNN(net, test_group_loader, num_classes_seen, model)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      #acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      acc_all, all_preds_cm, all_labels_cm = testSVM_KNN(net, test_loader, num_classes_seen, model)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTV1Gxq6yF5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f667f4f-d602-4022-e835-150e18af6cfd"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  1000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 86, 82, 78, 69, 57, 50, 30, 27, 25]\n",
            "TRAIN_SET CLASSES:  [27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "VALIDATION CLASSES:  [57, 50, 97, 30, 27, 25, 86, 82, 78, 69]\n",
            "GROUP:  1\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "Len TOTAL train susbset:  4950\n",
            "training\n",
            "num classes till now:  10\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.6801885962486267\n",
            "Train step - Step 10, Loss 0.31596842408180237\n",
            "Train step - Step 20, Loss 0.2731570303440094\n",
            "Train step - Step 30, Loss 0.2711406648159027\n",
            "Train epoch - Accuracy: 0.32606060606060605 Loss: 0.329973349992675 Corrects: 1614\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.2539721429347992\n",
            "Train step - Step 50, Loss 0.2279220074415207\n",
            "Train step - Step 60, Loss 0.23143987357616425\n",
            "Train step - Step 70, Loss 0.22050456702709198\n",
            "Train epoch - Accuracy: 0.4628282828282828 Loss: 0.23180835402975178 Corrects: 2291\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.21377062797546387\n",
            "Train step - Step 90, Loss 0.20685572922229767\n",
            "Train step - Step 100, Loss 0.21114130318164825\n",
            "Train step - Step 110, Loss 0.218997523188591\n",
            "Train epoch - Accuracy: 0.5226262626262627 Loss: 0.2140810085667504 Corrects: 2587\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.21719515323638916\n",
            "Train step - Step 130, Loss 0.18628831207752228\n",
            "Train step - Step 140, Loss 0.20993471145629883\n",
            "Train step - Step 150, Loss 0.18042322993278503\n",
            "Train epoch - Accuracy: 0.561010101010101 Loss: 0.20115475563689916 Corrects: 2777\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.22087962925434113\n",
            "Train step - Step 170, Loss 0.18273575603961945\n",
            "Train step - Step 180, Loss 0.20491765439510345\n",
            "Train step - Step 190, Loss 0.1794435828924179\n",
            "Train epoch - Accuracy: 0.5747474747474748 Loss: 0.19426715459486452 Corrects: 2845\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.19612903892993927\n",
            "Train step - Step 210, Loss 0.21881070733070374\n",
            "Train step - Step 220, Loss 0.1859501451253891\n",
            "Train step - Step 230, Loss 0.1871321201324463\n",
            "Train epoch - Accuracy: 0.5961616161616161 Loss: 0.18830902715524037 Corrects: 2951\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.18181344866752625\n",
            "Train step - Step 250, Loss 0.17936241626739502\n",
            "Train step - Step 260, Loss 0.19067373871803284\n",
            "Train step - Step 270, Loss 0.2004012167453766\n",
            "Train epoch - Accuracy: 0.6070707070707071 Loss: 0.18266964852809905 Corrects: 3005\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.177969828248024\n",
            "Train step - Step 290, Loss 0.14841704070568085\n",
            "Train step - Step 300, Loss 0.19109229743480682\n",
            "Train step - Step 310, Loss 0.18652692437171936\n",
            "Train epoch - Accuracy: 0.6218181818181818 Loss: 0.17710750207756504 Corrects: 3078\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.1752675622701645\n",
            "Train step - Step 330, Loss 0.1941029131412506\n",
            "Train step - Step 340, Loss 0.17527244985103607\n",
            "Train step - Step 350, Loss 0.14842455089092255\n",
            "Train epoch - Accuracy: 0.6363636363636364 Loss: 0.17002986759248406 Corrects: 3150\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.16549070179462433\n",
            "Train step - Step 370, Loss 0.17320922017097473\n",
            "Train step - Step 380, Loss 0.1689566820859909\n",
            "Train epoch - Accuracy: 0.6531313131313131 Loss: 0.16659907465029244 Corrects: 3233\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.14423613250255585\n",
            "Train step - Step 400, Loss 0.134902685880661\n",
            "Train step - Step 410, Loss 0.15653492510318756\n",
            "Train step - Step 420, Loss 0.1572187840938568\n",
            "Train epoch - Accuracy: 0.656969696969697 Loss: 0.16027003822302577 Corrects: 3252\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.15987712144851685\n",
            "Train step - Step 440, Loss 0.17122653126716614\n",
            "Train step - Step 450, Loss 0.15857088565826416\n",
            "Train step - Step 460, Loss 0.13228221237659454\n",
            "Train epoch - Accuracy: 0.6705050505050505 Loss: 0.15851967622535398 Corrects: 3319\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.15999342501163483\n",
            "Train step - Step 480, Loss 0.11872871965169907\n",
            "Train step - Step 490, Loss 0.16088657081127167\n",
            "Train step - Step 500, Loss 0.157939612865448\n",
            "Train epoch - Accuracy: 0.6882828282828283 Loss: 0.15269361883702903 Corrects: 3407\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.1439782828092575\n",
            "Train step - Step 520, Loss 0.15573200583457947\n",
            "Train step - Step 530, Loss 0.1700887233018875\n",
            "Train step - Step 540, Loss 0.14948436617851257\n",
            "Train epoch - Accuracy: 0.6903030303030303 Loss: 0.1483452897722071 Corrects: 3417\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.15274588763713837\n",
            "Train step - Step 560, Loss 0.14973624050617218\n",
            "Train step - Step 570, Loss 0.1405818909406662\n",
            "Train step - Step 580, Loss 0.15431532263755798\n",
            "Train epoch - Accuracy: 0.7014141414141414 Loss: 0.14633188253701335 Corrects: 3472\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.1661142259836197\n",
            "Train step - Step 600, Loss 0.12461123615503311\n",
            "Train step - Step 610, Loss 0.1441589593887329\n",
            "Train step - Step 620, Loss 0.1398155838251114\n",
            "Train epoch - Accuracy: 0.7143434343434344 Loss: 0.1423917437743659 Corrects: 3536\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.16444554924964905\n",
            "Train step - Step 640, Loss 0.14363978803157806\n",
            "Train step - Step 650, Loss 0.1277310848236084\n",
            "Train step - Step 660, Loss 0.1338883340358734\n",
            "Train epoch - Accuracy: 0.7157575757575757 Loss: 0.1380305762905063 Corrects: 3543\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.13400594890117645\n",
            "Train step - Step 680, Loss 0.1329967975616455\n",
            "Train step - Step 690, Loss 0.15841792523860931\n",
            "Train step - Step 700, Loss 0.13776440918445587\n",
            "Train epoch - Accuracy: 0.7165656565656565 Loss: 0.13903051299278182 Corrects: 3547\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.12796147167682648\n",
            "Train step - Step 720, Loss 0.1258554309606552\n",
            "Train step - Step 730, Loss 0.16898487508296967\n",
            "Train step - Step 740, Loss 0.1332443803548813\n",
            "Train epoch - Accuracy: 0.7341414141414141 Loss: 0.1324704607387986 Corrects: 3634\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.11676754802465439\n",
            "Train step - Step 760, Loss 0.1355283111333847\n",
            "Train step - Step 770, Loss 0.15644250810146332\n",
            "Train epoch - Accuracy: 0.743030303030303 Loss: 0.1291443628012532 Corrects: 3678\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.12238788604736328\n",
            "Train step - Step 790, Loss 0.13207300007343292\n",
            "Train step - Step 800, Loss 0.10969586670398712\n",
            "Train step - Step 810, Loss 0.1393928825855255\n",
            "Train epoch - Accuracy: 0.7496969696969698 Loss: 0.12452204399337673 Corrects: 3711\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.12576986849308014\n",
            "Train step - Step 830, Loss 0.12391433864831924\n",
            "Train step - Step 840, Loss 0.10678648948669434\n",
            "Train step - Step 850, Loss 0.131744846701622\n",
            "Train epoch - Accuracy: 0.7507070707070707 Loss: 0.1241225717404876 Corrects: 3716\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.1098310723900795\n",
            "Train step - Step 870, Loss 0.14549683034420013\n",
            "Train step - Step 880, Loss 0.12707987427711487\n",
            "Train step - Step 890, Loss 0.13417749106884003\n",
            "Train epoch - Accuracy: 0.7616161616161616 Loss: 0.12072531962635541 Corrects: 3770\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.11482100933790207\n",
            "Train step - Step 910, Loss 0.14926491677761078\n",
            "Train step - Step 920, Loss 0.11061177402734756\n",
            "Train step - Step 930, Loss 0.10653563588857651\n",
            "Train epoch - Accuracy: 0.7717171717171717 Loss: 0.11582399561549678 Corrects: 3820\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.09030082076787949\n",
            "Train step - Step 950, Loss 0.10036599636077881\n",
            "Train step - Step 960, Loss 0.11583433300256729\n",
            "Train step - Step 970, Loss 0.1142544150352478\n",
            "Train epoch - Accuracy: 0.7709090909090909 Loss: 0.11373101772081973 Corrects: 3816\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.11963369697332382\n",
            "Train step - Step 990, Loss 0.11331479996442795\n",
            "Train step - Step 1000, Loss 0.10716702789068222\n",
            "Train step - Step 1010, Loss 0.10718295723199844\n",
            "Train epoch - Accuracy: 0.7822222222222223 Loss: 0.11119616711380506 Corrects: 3872\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.12028782814741135\n",
            "Train step - Step 1030, Loss 0.11975691467523575\n",
            "Train step - Step 1040, Loss 0.12254001945257187\n",
            "Train step - Step 1050, Loss 0.11107625812292099\n",
            "Train epoch - Accuracy: 0.7840404040404041 Loss: 0.11038017975260513 Corrects: 3881\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.09505651146173477\n",
            "Train step - Step 1070, Loss 0.10531965643167496\n",
            "Train step - Step 1080, Loss 0.10866502672433853\n",
            "Train step - Step 1090, Loss 0.11696965992450714\n",
            "Train epoch - Accuracy: 0.7903030303030303 Loss: 0.10805643119595268 Corrects: 3912\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.09771980345249176\n",
            "Train step - Step 1110, Loss 0.12008290737867355\n",
            "Train step - Step 1120, Loss 0.11248107254505157\n",
            "Train step - Step 1130, Loss 0.09199295938014984\n",
            "Train epoch - Accuracy: 0.7872727272727272 Loss: 0.10709001317770794 Corrects: 3897\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.10482148081064224\n",
            "Train step - Step 1150, Loss 0.08934979885816574\n",
            "Train step - Step 1160, Loss 0.09189499914646149\n",
            "Train epoch - Accuracy: 0.8054545454545454 Loss: 0.10113225839053741 Corrects: 3987\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.08463390171527863\n",
            "Train step - Step 1180, Loss 0.10870154201984406\n",
            "Train step - Step 1190, Loss 0.09779161214828491\n",
            "Train step - Step 1200, Loss 0.11580055207014084\n",
            "Train epoch - Accuracy: 0.8036363636363636 Loss: 0.09974132014043403 Corrects: 3978\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.08904778957366943\n",
            "Train step - Step 1220, Loss 0.10474862158298492\n",
            "Train step - Step 1230, Loss 0.08565967530012131\n",
            "Train step - Step 1240, Loss 0.09429839998483658\n",
            "Train epoch - Accuracy: 0.8121212121212121 Loss: 0.09718191007171015 Corrects: 4020\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.07452838867902756\n",
            "Train step - Step 1260, Loss 0.08387517184019089\n",
            "Train step - Step 1270, Loss 0.10280704498291016\n",
            "Train step - Step 1280, Loss 0.09020688384771347\n",
            "Train epoch - Accuracy: 0.817979797979798 Loss: 0.09505710119550878 Corrects: 4049\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.09348670393228531\n",
            "Train step - Step 1300, Loss 0.0966816172003746\n",
            "Train step - Step 1310, Loss 0.09426916390657425\n",
            "Train step - Step 1320, Loss 0.07322331517934799\n",
            "Train epoch - Accuracy: 0.8224242424242424 Loss: 0.09241034232004725 Corrects: 4071\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.09675595909357071\n",
            "Train step - Step 1340, Loss 0.10219261795282364\n",
            "Train step - Step 1350, Loss 0.08873657137155533\n",
            "Train step - Step 1360, Loss 0.09818710386753082\n",
            "Train epoch - Accuracy: 0.826060606060606 Loss: 0.09074502785699536 Corrects: 4089\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.10465703159570694\n",
            "Train step - Step 1380, Loss 0.0732247605919838\n",
            "Train step - Step 1390, Loss 0.08496981114149094\n",
            "Train step - Step 1400, Loss 0.08410823345184326\n",
            "Train epoch - Accuracy: 0.8331313131313132 Loss: 0.08770237013848141 Corrects: 4124\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.09651598334312439\n",
            "Train step - Step 1420, Loss 0.09794630110263824\n",
            "Train step - Step 1430, Loss 0.08567681908607483\n",
            "Train step - Step 1440, Loss 0.11767764389514923\n",
            "Train epoch - Accuracy: 0.8347474747474748 Loss: 0.08656945762008128 Corrects: 4132\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.06170625612139702\n",
            "Train step - Step 1460, Loss 0.0803455039858818\n",
            "Train step - Step 1470, Loss 0.11235398054122925\n",
            "Train step - Step 1480, Loss 0.1000329777598381\n",
            "Train epoch - Accuracy: 0.8385858585858585 Loss: 0.0848502096262845 Corrects: 4151\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.09291418641805649\n",
            "Train step - Step 1500, Loss 0.08967498689889908\n",
            "Train step - Step 1510, Loss 0.10292232036590576\n",
            "Train step - Step 1520, Loss 0.08340246230363846\n",
            "Train epoch - Accuracy: 0.8446464646464646 Loss: 0.08444140704894307 Corrects: 4181\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.07348300516605377\n",
            "Train step - Step 1540, Loss 0.09858930110931396\n",
            "Train step - Step 1550, Loss 0.07685663551092148\n",
            "Train epoch - Accuracy: 0.8381818181818181 Loss: 0.08384637111666227 Corrects: 4149\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.06541422754526138\n",
            "Train step - Step 1570, Loss 0.0901155099272728\n",
            "Train step - Step 1580, Loss 0.0884426087141037\n",
            "Train step - Step 1590, Loss 0.07603722810745239\n",
            "Train epoch - Accuracy: 0.8494949494949495 Loss: 0.0779907303777608 Corrects: 4205\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.07098301500082016\n",
            "Train step - Step 1610, Loss 0.08132288604974747\n",
            "Train step - Step 1620, Loss 0.07077030092477798\n",
            "Train step - Step 1630, Loss 0.06654886901378632\n",
            "Train epoch - Accuracy: 0.8478787878787879 Loss: 0.07805610999314472 Corrects: 4197\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.08779653161764145\n",
            "Train step - Step 1650, Loss 0.08308717608451843\n",
            "Train step - Step 1660, Loss 0.09248536825180054\n",
            "Train step - Step 1670, Loss 0.08948539942502975\n",
            "Train epoch - Accuracy: 0.857979797979798 Loss: 0.07542225102583568 Corrects: 4247\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.06140752509236336\n",
            "Train step - Step 1690, Loss 0.0744214877486229\n",
            "Train step - Step 1700, Loss 0.0788266733288765\n",
            "Train step - Step 1710, Loss 0.07667946070432663\n",
            "Train epoch - Accuracy: 0.8591919191919192 Loss: 0.07288990187524545 Corrects: 4253\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.0615917444229126\n",
            "Train step - Step 1730, Loss 0.07789044827222824\n",
            "Train step - Step 1740, Loss 0.055745627731084824\n",
            "Train step - Step 1750, Loss 0.06391704082489014\n",
            "Train epoch - Accuracy: 0.8626262626262626 Loss: 0.07362030220152152 Corrects: 4270\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.06825561076402664\n",
            "Train step - Step 1770, Loss 0.046947479248046875\n",
            "Train step - Step 1780, Loss 0.09384144842624664\n",
            "Train step - Step 1790, Loss 0.06786467134952545\n",
            "Train epoch - Accuracy: 0.8703030303030304 Loss: 0.07108488432686738 Corrects: 4308\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.0781230702996254\n",
            "Train step - Step 1810, Loss 0.07139875739812851\n",
            "Train step - Step 1820, Loss 0.06408446282148361\n",
            "Train step - Step 1830, Loss 0.08346384018659592\n",
            "Train epoch - Accuracy: 0.8684848484848485 Loss: 0.07167251124526515 Corrects: 4299\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.06950754672288895\n",
            "Train step - Step 1850, Loss 0.09237580001354218\n",
            "Train step - Step 1860, Loss 0.07034152001142502\n",
            "Train step - Step 1870, Loss 0.06451306492090225\n",
            "Train epoch - Accuracy: 0.8765656565656565 Loss: 0.06696742118910105 Corrects: 4339\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.05532893165946007\n",
            "Train step - Step 1890, Loss 0.06110068038105965\n",
            "Train step - Step 1900, Loss 0.05307018384337425\n",
            "Train step - Step 1910, Loss 0.06369353830814362\n",
            "Train epoch - Accuracy: 0.8852525252525253 Loss: 0.06329396832470942 Corrects: 4382\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.05373300239443779\n",
            "Train step - Step 1930, Loss 0.0678364634513855\n",
            "Train step - Step 1940, Loss 0.06437301635742188\n",
            "Train epoch - Accuracy: 0.9056565656565656 Loss: 0.053406230236845785 Corrects: 4483\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.05193695053458214\n",
            "Train step - Step 1960, Loss 0.03764570876955986\n",
            "Train step - Step 1970, Loss 0.049049653112888336\n",
            "Train step - Step 1980, Loss 0.06239850074052811\n",
            "Train epoch - Accuracy: 0.9204040404040404 Loss: 0.04694935512512621 Corrects: 4556\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.02669139765202999\n",
            "Train step - Step 2000, Loss 0.047059182077646255\n",
            "Train step - Step 2010, Loss 0.03976645693182945\n",
            "Train step - Step 2020, Loss 0.0529276505112648\n",
            "Train epoch - Accuracy: 0.9272727272727272 Loss: 0.043594326899208204 Corrects: 4590\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.04145532473921776\n",
            "Train step - Step 2040, Loss 0.03855226933956146\n",
            "Train step - Step 2050, Loss 0.03469913825392723\n",
            "Train step - Step 2060, Loss 0.05890791490674019\n",
            "Train epoch - Accuracy: 0.9272727272727272 Loss: 0.042767652399612194 Corrects: 4590\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.034095119684934616\n",
            "Train step - Step 2080, Loss 0.04942398890852928\n",
            "Train step - Step 2090, Loss 0.03676915541291237\n",
            "Train step - Step 2100, Loss 0.04602118581533432\n",
            "Train epoch - Accuracy: 0.9286868686868687 Loss: 0.041698502272066444 Corrects: 4597\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.0503293052315712\n",
            "Train step - Step 2120, Loss 0.0491487942636013\n",
            "Train step - Step 2130, Loss 0.043297238647937775\n",
            "Train step - Step 2140, Loss 0.03433180972933769\n",
            "Train epoch - Accuracy: 0.9323232323232323 Loss: 0.04082575900386078 Corrects: 4615\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.03644726797938347\n",
            "Train step - Step 2160, Loss 0.04270150884985924\n",
            "Train step - Step 2170, Loss 0.03621853515505791\n",
            "Train step - Step 2180, Loss 0.04602242261171341\n",
            "Train epoch - Accuracy: 0.9345454545454546 Loss: 0.039539663969266295 Corrects: 4626\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.052423980087041855\n",
            "Train step - Step 2200, Loss 0.04297227784991264\n",
            "Train step - Step 2210, Loss 0.03800292685627937\n",
            "Train step - Step 2220, Loss 0.04821097478270531\n",
            "Train epoch - Accuracy: 0.9343434343434344 Loss: 0.03884252425697115 Corrects: 4625\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.033987388014793396\n",
            "Train step - Step 2240, Loss 0.03644297644495964\n",
            "Train step - Step 2250, Loss 0.043219175189733505\n",
            "Train step - Step 2260, Loss 0.04017328470945358\n",
            "Train epoch - Accuracy: 0.9345454545454546 Loss: 0.03834717375612018 Corrects: 4626\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.04067723825573921\n",
            "Train step - Step 2280, Loss 0.03248061612248421\n",
            "Train step - Step 2290, Loss 0.03619120642542839\n",
            "Train step - Step 2300, Loss 0.037205666303634644\n",
            "Train epoch - Accuracy: 0.9371717171717172 Loss: 0.038058251383328676 Corrects: 4639\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.0396200567483902\n",
            "Train step - Step 2320, Loss 0.03423497825860977\n",
            "Train step - Step 2330, Loss 0.03277399763464928\n",
            "Train epoch - Accuracy: 0.938989898989899 Loss: 0.03763099920418527 Corrects: 4648\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.04044891521334648\n",
            "Train step - Step 2350, Loss 0.024458957836031914\n",
            "Train step - Step 2360, Loss 0.0401582308113575\n",
            "Train step - Step 2370, Loss 0.03321150690317154\n",
            "Train epoch - Accuracy: 0.9422222222222222 Loss: 0.035753154780226525 Corrects: 4664\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.025066232308745384\n",
            "Train step - Step 2390, Loss 0.030449002981185913\n",
            "Train step - Step 2400, Loss 0.03524748608469963\n",
            "Train step - Step 2410, Loss 0.028080839663743973\n",
            "Train epoch - Accuracy: 0.9414141414141414 Loss: 0.035972266811313054 Corrects: 4660\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.042317841202020645\n",
            "Train step - Step 2430, Loss 0.03648374602198601\n",
            "Train step - Step 2440, Loss 0.031631745398044586\n",
            "Train step - Step 2450, Loss 0.04098673537373543\n",
            "Train epoch - Accuracy: 0.9418181818181818 Loss: 0.03539517698414398 Corrects: 4662\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.02989429607987404\n",
            "Train step - Step 2470, Loss 0.04218396171927452\n",
            "Train step - Step 2480, Loss 0.03715358301997185\n",
            "Train step - Step 2490, Loss 0.031822916120290756\n",
            "Train epoch - Accuracy: 0.9490909090909091 Loss: 0.03330550387727492 Corrects: 4698\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.028939152136445045\n",
            "Train step - Step 2510, Loss 0.02005034312605858\n",
            "Train step - Step 2520, Loss 0.03604603558778763\n",
            "Train step - Step 2530, Loss 0.03163815662264824\n",
            "Train epoch - Accuracy: 0.9482828282828283 Loss: 0.03211410686072677 Corrects: 4694\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.025999635457992554\n",
            "Train step - Step 2550, Loss 0.028922153636813164\n",
            "Train step - Step 2560, Loss 0.03385809436440468\n",
            "Train step - Step 2570, Loss 0.029063407331705093\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.031800762718976146 Corrects: 4703\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.023937581107020378\n",
            "Train step - Step 2590, Loss 0.041477106511592865\n",
            "Train step - Step 2600, Loss 0.03649153187870979\n",
            "Train step - Step 2610, Loss 0.026939142495393753\n",
            "Train epoch - Accuracy: 0.9446464646464646 Loss: 0.03349831054156477 Corrects: 4676\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.03261159360408783\n",
            "Train step - Step 2630, Loss 0.019862627610564232\n",
            "Train step - Step 2640, Loss 0.027196794748306274\n",
            "Train step - Step 2650, Loss 0.03332221135497093\n",
            "Train epoch - Accuracy: 0.9509090909090909 Loss: 0.03196508015647079 Corrects: 4707\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.02246338315308094\n",
            "Train step - Step 2670, Loss 0.035455893725156784\n",
            "Train step - Step 2680, Loss 0.025931669399142265\n",
            "Train step - Step 2690, Loss 0.03257901966571808\n",
            "Train epoch - Accuracy: 0.9551515151515152 Loss: 0.03069974917956073 Corrects: 4728\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.038380254060029984\n",
            "Train step - Step 2710, Loss 0.03411146625876427\n",
            "Train step - Step 2720, Loss 0.02543148212134838\n",
            "Train epoch - Accuracy: 0.9494949494949495 Loss: 0.032284092195708346 Corrects: 4700\n",
            "Training finished in 182.40718293190002 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238249150>\n",
            "Constructing exemplars of class 27\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [499, 47137, 25357, 6296, 36691, 32071, 46338, 32808, 25699, 46334, 2901, 4790, 25823, 36457, 21305, 30375, 47378, 2579, 17563, 38941, 2526, 8348, 13226, 29745, 1631, 17200, 25194, 48775, 15494, 41738, 22683, 27286, 5972, 23090, 8348, 33447, 2519, 37513, 5216, 17655, 22953, 10862, 47901, 21370, 43791, 34577, 35259, 33343, 23880, 37704, 17530, 40413, 28959, 19478, 9170, 12072, 11536, 13843, 5972, 17728, 48257, 38306, 40198, 34390, 15441, 1055, 26692, 34823, 30875, 12730, 47177, 16451, 20510, 15502, 20476, 34807, 13955, 33830, 17728, 44619, 9190, 27513, 30751, 46334, 8693, 13570, 3240, 34084, 40086, 40638, 48296, 48999, 42787, 38941, 32098, 30271, 40198, 26690, 5396, 39439, 49948, 19172, 28763, 17563, 33447, 13843, 39265, 42220, 35737, 23880, 10551, 4949, 13856, 8384, 37456, 20314, 33447, 27775, 12197, 19711, 19172, 22940, 1782, 6640, 40216, 21305, 4296, 19527, 41788, 7170, 47669, 47985, 5790, 15469, 8143, 27239, 8268, 42787, 40513, 32808, 11177, 15502, 21305, 17703, 42910, 20987, 2810, 19771, 29745, 10410, 41932, 15228, 25049, 10875, 1239, 34313, 32691, 30800, 39385, 11353, 37183, 25662, 46211, 45785, 28781, 45258, 13712, 4296, 23132, 33737, 10410, 7167, 8693, 9467, 10875, 39325, 17698, 13712, 38146, 32808, 7215, 48257, 42910, 40513, 1598, 5432, 13570, 16652, 46544, 31719, 36691, 34823, 35737, 26442, 17703, 45681, 37704, 23400, 9910, 32289]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22380f9e90>\n",
            "Constructing exemplars of class 86\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [7345, 31202, 22232, 45299, 42913, 35599, 33691, 5, 10837, 22949, 30570, 6638, 48900, 16815, 8251, 31357, 9113, 45536, 40526, 47631, 33691, 42820, 8802, 31538, 15672, 39043, 27703, 22317, 10299, 42218, 3318, 28461, 21816, 4512, 20852, 23586, 23177, 26401, 33691, 47214, 19624, 45187, 10837, 17533, 39129, 33083, 23177, 21404, 16815, 25436, 32601, 27908, 1342, 23108, 45686, 45263, 22547, 36947, 21762, 23586, 13053, 6649, 4434, 17306, 12635, 23177, 23034, 10176, 23280, 26719, 34863, 5645, 45951, 34052, 39718, 1186, 3334, 15688, 38753, 263, 16656, 23108, 18310, 5629, 45915, 24114, 43938, 21404, 42290, 3045, 18516, 32495, 46650, 39129, 10915, 19443, 31612, 8086, 35599, 32129, 5251, 1186, 1941, 49008, 30757, 32121, 47610, 47855, 496, 37678, 26304, 22033, 3895, 30586, 40963, 37014, 2171, 48978, 11035, 3318, 46357, 29520, 47855, 32129, 34586, 3045, 39718, 20828, 36755, 4651, 10991, 12561, 12375, 34591, 23600, 21679, 14994, 31896, 12755, 32483, 37678, 29326, 37836, 2378, 35457, 46375, 4392, 35090, 48978, 23808, 32483, 41129, 22229, 43676, 40492, 37391, 46559, 16656, 45141, 11892, 19219, 7646, 44794, 2235, 15688, 42707, 39739, 34543, 46967, 40526, 8251, 42913, 5595, 21691, 41129, 18105, 25287, 24004, 16134, 21404, 28461, 21600, 8303, 39699, 2636, 44113, 1342, 8379, 48884, 39576, 11892, 32605, 43948, 2353, 1439, 21600, 23394, 21519, 16656, 39718]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2244394290>\n",
            "Constructing exemplars of class 82\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [41968, 32028, 10309, 18917, 6761, 35967, 40462, 5450, 2753, 31249, 7027, 48289, 43830, 26921, 5656, 24024, 13546, 44260, 34608, 16838, 30468, 1296, 40408, 19060, 10727, 22681, 11501, 12252, 30203, 41895, 49858, 47957, 164, 33382, 2542, 30203, 41895, 47516, 26976, 30450, 1854, 48025, 33709, 49631, 8581, 39637, 15317, 35369, 32847, 8331, 27063, 5244, 43453, 20690, 15113, 36437, 11901, 20686, 32417, 6173, 16467, 40408, 28096, 1136, 2753, 18626, 49708, 36261, 6173, 44928, 5257, 34875, 36799, 274, 164, 30639, 46394, 38486, 32175, 30450, 46951, 39262, 46832, 16897, 2542, 22110, 14057, 47319, 36727, 33729, 25615, 21590, 28101, 32449, 18644, 24387, 14557, 46908, 15358, 15304, 13318, 18644, 35336, 36261, 30639, 3040, 164, 20690, 49832, 46550, 16897, 47737, 25724, 1402, 3876, 32602, 5244, 45064, 39114, 14398, 33382, 8518, 35336, 36799, 10923, 11832, 44523, 47124, 47516, 13318, 43320, 44417, 36492, 44909, 12, 19817, 47319, 1143, 39114, 5454, 28783, 39637, 20686, 47675, 38171, 32858, 21695, 31759, 40952, 5073, 3507, 10381, 6961, 46951, 36194, 27709, 25557, 39637, 164, 45656, 40367, 13318, 21512, 40409, 3438, 23130, 28404, 31775, 23402, 46147, 27053, 15403, 1136, 5586, 11600, 21991, 36705, 44030, 4877, 3221, 35369, 32847, 29478, 34267, 15304, 324, 9512, 21695, 25615, 36301, 28893, 12, 25817, 26921, 30450, 48289, 7471, 14557, 6173, 37064]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238254350>\n",
            "Constructing exemplars of class 78\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [12515, 15944, 22314, 35456, 39142, 42102, 42980, 46141, 13733, 19888, 10326, 44594, 10582, 4495, 31093, 19728, 7351, 10900, 28343, 24529, 165, 29518, 29063, 3984, 21245, 18321, 18228, 47100, 3392, 31473, 14886, 27875, 42102, 15532, 39693, 39066, 19489, 17956, 43936, 5221, 33082, 48964, 47371, 47715, 45048, 37783, 21721, 23172, 46710, 17712, 32597, 12285, 46923, 42875, 2895, 6759, 15532, 33671, 2916, 32972, 18091, 43662, 46292, 36435, 43662, 33321, 38949, 25317, 23046, 18741, 34349, 49541, 37043, 31093, 27624, 30370, 14886, 27925, 29612, 27624, 28872, 30077, 16414, 48341, 23632, 47100, 2630, 33353, 26686, 42439, 1539, 29043, 38585, 15438, 29063, 9258, 23945, 38042, 45244, 16845, 2058, 3976, 6932, 165, 14820, 13147, 33157, 26752, 40090, 46734, 15718, 43557, 29063, 49946, 16367, 14895, 2355, 19381, 33789, 21245, 42980, 31346, 20404, 36516, 29063, 37783, 7022, 16792, 27737, 12615, 24843, 36210, 37001, 47603, 22314, 30672, 49, 24781, 44594, 10326, 3764, 41759, 31346, 22426, 24664, 26752, 22083, 3984, 33322, 5214, 19436, 42281, 28020, 35818, 30868, 47715, 34157, 21721, 3219, 34089, 16801, 17292, 15648, 40087, 12344, 21721, 25646, 13916, 3984, 10053, 19489, 35818, 29612, 1619, 22799, 5916, 7889, 12268, 41337, 44594, 28595, 38439, 13392, 7624, 13925, 16009, 9946, 37998, 16009, 32793, 27624, 24549, 5348, 37998, 37680, 34324, 3422, 19436, 31346, 47760]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a91610>\n",
            "Constructing exemplars of class 50\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [32282, 16788, 9154, 13011, 10353, 22023, 11062, 20483, 37242, 40724, 18525, 2433, 42845, 18858, 8552, 5182, 47346, 25946, 38226, 31166, 716, 4682, 30545, 16229, 49801, 17551, 11964, 10750, 24682, 35011, 14448, 30671, 7152, 29494, 30312, 28002, 6677, 42051, 49701, 23294, 3376, 29452, 3603, 9081, 23187, 29076, 14061, 26655, 17694, 26647, 32947, 44685, 37359, 38927, 1655, 16550, 31448, 29184, 15697, 17925, 14133, 31000, 45042, 49701, 9210, 30956, 12736, 30882, 30312, 43261, 36768, 35530, 9154, 18402, 40724, 35844, 38839, 37711, 35427, 49666, 13686, 39006, 30776, 8640, 28739, 48966, 44661, 16788, 44701, 37563, 19649, 16646, 6379, 40299, 22674, 47377, 9354, 42699, 49701, 42824, 13799, 43122, 24044, 27918, 41252, 34444, 26647, 18119, 19049, 35600, 35085, 42528, 14897, 26647, 9147, 22479, 37713, 24899, 42799, 48077, 1545, 17258, 42268, 10078, 16940, 13534, 10169, 20129, 35011, 19514, 14258, 39986, 49408, 38173, 25622, 20483, 42051, 21468, 10811, 3376, 24044, 28574, 10544, 26624, 11609, 18119, 47071, 44063, 30882, 19649, 19204, 3945, 10413, 35844, 18153, 2070, 41934, 35983, 13677, 11475, 4368, 12607, 37711, 43094, 854, 25649, 36255, 29076, 41217, 19611, 42057, 12563, 14070, 30426, 16940, 39248, 19204, 12663, 10528, 31081, 49913, 6435, 25692, 16911, 39364, 27060, 26723, 19649, 42752, 30882, 10528, 37713, 35530, 20840, 30956, 35085, 42057, 10533, 42528, 748]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a91350>\n",
            "Constructing exemplars of class 30\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [2413, 9913, 12797, 17555, 27245, 48748, 5484, 40798, 20271, 4669, 5737, 41101, 31622, 19509, 30490, 39595, 13670, 37151, 41010, 8243, 26915, 44583, 17858, 47679, 505, 6936, 17424, 42267, 47703, 41685, 2879, 3792, 20885, 18698, 37648, 5737, 29426, 28897, 41381, 6215, 3959, 8216, 19999, 44883, 20823, 8877, 43518, 505, 18447, 48707, 14466, 32520, 20279, 48982, 18584, 20125, 48233, 7413, 26915, 2511, 1012, 36170, 44328, 23465, 35421, 20318, 33698, 42312, 3959, 44772, 8877, 23881, 29360, 36587, 48707, 12161, 34222, 17504, 25585, 6576, 41297, 34418, 46352, 21934, 22281, 24257, 8601, 48982, 2511, 10403, 3864, 36587, 24678, 10526, 34693, 32330, 19039, 5127, 10118, 5460, 49334, 39411, 24104, 12797, 16860, 27269, 21345, 23314, 39595, 6922, 42993, 19225, 15018, 18447, 2041, 33698, 38545, 1522, 35567, 30650, 33600, 45523, 20323, 16088, 29579, 8726, 20318, 25227, 17424, 42267, 2165, 9928, 4352, 41247, 10526, 3394, 3409, 28897, 44755, 28039, 23642, 9760, 15610, 12696, 6103, 36882, 13868, 23385, 24104, 44467, 24298, 43702, 32830, 4883, 8459, 2879, 49334, 5460, 29949, 6343, 13845, 13529, 43838, 21636, 42731, 12929, 24753, 1012, 21484, 4766, 29722, 12768, 9527, 20125, 36716, 37821, 20071, 10667, 46746, 44219, 47703, 41685, 16634, 9760, 3959, 18486, 6568, 44842, 34585, 41381, 10403, 44756, 41635, 20676, 36293, 6215, 32684, 13224, 46184, 20627]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238246350>\n",
            "Constructing exemplars of class 97\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [36343, 44473, 12941, 41517, 8376, 886, 40168, 10935, 26479, 10823, 48808, 38282, 13319, 44415, 29017, 46311, 26147, 25932, 23332, 8417, 2662, 46009, 41545, 42894, 12109, 35940, 903, 45858, 16559, 18971, 34203, 46801, 29077, 43211, 17021, 4720, 37464, 42067, 28256, 36439, 13865, 17460, 27998, 47396, 27738, 15643, 23104, 33838, 4086, 21924, 46225, 14228, 3015, 43767, 48042, 26771, 33305, 45500, 12927, 428, 22851, 33838, 7433, 8317, 45354, 9712, 45500, 21266, 42986, 10935, 12927, 46822, 5211, 9756, 22433, 30754, 9167, 49451, 15542, 40829, 40556, 7629, 23008, 49396, 733, 6826, 9584, 12798, 20592, 22938, 7782, 41638, 10510, 35010, 1720, 40327, 30057, 30709, 42958, 28974, 18032, 3498, 33893, 14484, 21751, 46009, 49451, 28610, 41773, 47513, 44473, 26746, 15643, 11371, 7250, 33058, 40303, 44601, 48426, 41368, 5768, 4086, 11228, 10128, 18032, 5786, 8630, 9756, 39561, 10275, 32465, 5184, 39603, 20662, 15414, 13500, 48673, 17320, 33893, 36270, 3791, 21715, 38414, 16693, 12069, 15542, 13649, 26239, 23008, 45326, 1770, 28161, 49994, 8417, 39501, 32811, 23775, 29017, 886, 14484, 6079, 27949, 12075, 17021, 26479, 45227, 31860, 17488, 28974, 48042, 26771, 7232, 14221, 2976, 45484, 13500, 13319, 24013, 11228, 103, 42286, 44601, 20723, 28371, 42986, 23281, 30855, 7174, 10485, 49396, 2197, 46011, 13894, 1761, 13270, 20902, 5786, 8630, 20662, 46225]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22413f1f10>\n",
            "Constructing exemplars of class 69\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [3129, 6521, 11180, 23762, 38511, 36095, 16411, 6461, 554, 4223, 851, 44301, 17909, 22670, 7165, 31481, 20249, 46927, 1420, 48821, 34929, 17783, 10479, 28088, 10653, 3479, 45680, 11567, 759, 19871, 13167, 19731, 41127, 20310, 31168, 25360, 43227, 33841, 11554, 26511, 12976, 33019, 16594, 23902, 9189, 29900, 12421, 28883, 14210, 14100, 7071, 16686, 22241, 29599, 29508, 10772, 45811, 21015, 16760, 9865, 39450, 41127, 28883, 22241, 16876, 23762, 30706, 19250, 4217, 23902, 2248, 32177, 5581, 30173, 20496, 11567, 43120, 39244, 16297, 29041, 39788, 35296, 41963, 15128, 37267, 10614, 27583, 7966, 16760, 10888, 1465, 13946, 25238, 23510, 39388, 4217, 199, 21528, 31437, 14220, 851, 42228, 17311, 4439, 19510, 4223, 34282, 26576, 45113, 24263, 11387, 9148, 25544, 16434, 6840, 32177, 17911, 4889, 38785, 19180, 4217, 19731, 34753, 20055, 32437, 12043, 42617, 33664, 14186, 8821, 22619, 48594, 11143, 3907, 18152, 24187, 7654, 39814, 3479, 10852, 20310, 29041, 831, 15260, 34161, 14654, 37312, 43120, 29866, 16958, 16084, 17447, 43120, 49128, 19871, 5048, 45495, 18238, 1847, 10607, 36095, 3469, 13906, 37267, 44224, 28563, 19180, 38407, 24066, 32108, 34753, 15128, 39120, 26754, 49128, 41606, 22619, 20291, 28088, 19731, 10614, 40924, 11057, 6131, 7966, 31042, 27877, 32731, 42026, 29900, 21181, 13506, 4889, 34907, 7662, 39814, 34282, 4201, 36664, 31733]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2246382c50>\n",
            "Constructing exemplars of class 57\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [40996, 44930, 45530, 48712, 13188, 46114, 5512, 35347, 42029, 34568, 23264, 45158, 29120, 30242, 21487, 43367, 1495, 30481, 41912, 10165, 46691, 22360, 11701, 20086, 13333, 4718, 23915, 23751, 24969, 2112, 35845, 37550, 9651, 2333, 18100, 15583, 29645, 28228, 47604, 49036, 16219, 15541, 46465, 4828, 4852, 36651, 39552, 331, 23082, 31881, 16512, 1593, 21822, 25197, 49750, 24287, 34969, 685, 24254, 22472, 46114, 5753, 8773, 18039, 48777, 16422, 43288, 31623, 16089, 47383, 40068, 17260, 36234, 40848, 4702, 14528, 5457, 1634, 48649, 4200, 43067, 11497, 44930, 14528, 30384, 36415, 15171, 33657, 14008, 4990, 19198, 32046, 11219, 46797, 49911, 8309, 47398, 23751, 34294, 6981, 11111, 47929, 2453, 22510, 26755, 27522, 44632, 34659, 41906, 9651, 36057, 15678, 24636, 24969, 44770, 43367, 27522, 18643, 14491, 4579, 1684, 6496, 32550, 592, 6066, 24962, 4835, 47871, 4828, 21228, 5545, 12757, 40553, 32456, 5418, 48099, 20438, 12361, 37202, 45056, 6723, 19979, 12757, 8401, 30481, 14755, 47871, 11662, 42194, 590, 36771, 45831, 38758, 49883, 26024, 14716, 4830, 48653, 17529, 38372, 46558, 46114, 32201, 17736, 37512, 7094, 43367, 45127, 19198, 26024, 33426, 2453, 16557, 45413, 15678, 30349, 35877, 45158, 26234, 49036, 23240, 48401, 38648, 19134, 28228, 43877, 10165, 41608, 5571, 27491, 40116, 17806, 31291, 5512, 43707, 32046, 3399, 15920, 16597, 21228]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223801ba10>\n",
            "Constructing exemplars of class 25\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [11945, 21894, 10686, 22972, 35549, 4034, 19870, 33908, 3826, 26764, 17683, 37566, 4789, 4482, 36174, 6831, 20179, 19995, 33604, 37501, 6268, 2438, 41399, 6233, 25564, 39568, 44627, 19623, 21192, 11541, 1382, 39267, 2578, 42829, 37494, 16155, 20613, 30221, 17629, 17440, 4674, 49824, 44585, 46442, 30530, 43954, 11805, 22666, 1487, 8468, 10276, 42138, 9695, 23362, 5013, 33804, 18838, 7808, 3826, 42829, 22339, 43145, 27047, 25653, 25037, 17968, 43798, 38919, 48092, 20965, 10044, 13651, 19995, 35242, 8619, 28675, 25538, 5555, 4570, 6655, 27449, 29723, 40247, 3669, 49467, 47163, 26321, 33436, 43585, 31943, 32196, 15226, 2690, 14997, 40540, 1926, 27047, 25653, 16741, 25441, 1257, 27058, 13276, 38706, 18923, 4565, 35188, 7405, 703, 19995, 30687, 40273, 39266, 15340, 43145, 44689, 47262, 31480, 43118, 29159, 3595, 29886, 8468, 450, 22160, 241, 15340, 4482, 49866, 1954, 9733, 42280, 47736, 46990, 42829, 47061, 24309, 22854, 13175, 2578, 39267, 35866, 42138, 5653, 43954, 18838, 30836, 17937, 19860, 15632, 33303, 21405, 12020, 41623, 47197, 13175, 33254, 44757, 25441, 6268, 20170, 46461, 47540, 35964, 703, 13580, 31965, 22666, 35242, 33231, 31271, 40620, 30836, 46673, 22666, 49070, 18992, 24982, 40368, 39684, 11859, 28453, 40417, 1471, 26664, 41623, 28675, 37566, 18655, 16563, 27055, 15190, 19645, 24244, 11541, 22181, 6268, 10276, 22302, 46713]\n",
            "x train:  [-0.27958414 -0.33382413 -0.31899416 -0.3075703  -0.36007908 -0.37246212\n",
            " -0.37052172 -0.2600769   0.25233024 -0.27758867]\n",
            "y_train:  [tensor([57]), tensor([27]), tensor([78]), tensor([27]), tensor([97]), tensor([78]), tensor([69]), tensor([97]), tensor([57]), tensor([97]), tensor([97]), tensor([27]), tensor([30]), tensor([78]), tensor([97]), tensor([82]), tensor([57]), tensor([69]), tensor([25]), tensor([86]), tensor([25]), tensor([25]), tensor([25]), tensor([78]), tensor([97]), tensor([27]), tensor([82]), tensor([25]), tensor([30]), tensor([82]), tensor([69]), tensor([50]), tensor([69]), tensor([78]), tensor([30]), tensor([27]), tensor([27]), tensor([25]), tensor([27]), tensor([57]), tensor([78]), tensor([97]), tensor([69]), tensor([97]), tensor([30]), tensor([86]), tensor([78]), tensor([82]), tensor([86]), tensor([69]), tensor([25]), tensor([57]), tensor([57]), tensor([27]), tensor([97]), tensor([78]), tensor([97]), tensor([27]), tensor([27]), tensor([50]), tensor([86]), tensor([27]), tensor([25]), tensor([27]), tensor([57]), tensor([57]), tensor([57]), tensor([97]), tensor([78]), tensor([69]), tensor([25]), tensor([69]), tensor([27]), tensor([82]), tensor([25]), tensor([82]), tensor([82]), tensor([50]), tensor([30]), tensor([82]), tensor([97]), tensor([78]), tensor([82]), tensor([25]), tensor([50]), tensor([86]), tensor([27]), tensor([97]), tensor([69]), tensor([50]), tensor([50]), tensor([86]), tensor([86]), tensor([69]), tensor([25]), tensor([57]), tensor([50]), tensor([30]), tensor([50]), tensor([97]), tensor([30]), tensor([27]), tensor([78]), tensor([57]), tensor([50]), tensor([69]), tensor([57]), tensor([78]), tensor([82]), tensor([30]), tensor([30]), tensor([25]), tensor([50]), tensor([25]), tensor([86]), tensor([30]), tensor([78]), tensor([57]), tensor([78]), tensor([25]), tensor([78]), tensor([25]), tensor([78]), tensor([69]), tensor([25]), tensor([30]), tensor([50]), tensor([27]), tensor([30]), tensor([50]), tensor([78]), tensor([57]), tensor([50]), tensor([25]), tensor([86]), tensor([57]), tensor([50]), tensor([69]), tensor([25]), tensor([57]), tensor([30]), tensor([30]), tensor([82]), tensor([86]), tensor([25]), tensor([97]), tensor([78]), tensor([57]), tensor([50]), tensor([50]), tensor([82]), tensor([30]), tensor([57]), tensor([50]), tensor([78]), tensor([97]), tensor([50]), tensor([25]), tensor([82]), tensor([82]), tensor([86]), tensor([86]), tensor([69]), tensor([30]), tensor([86]), tensor([27]), tensor([69]), tensor([27]), tensor([78]), tensor([50]), tensor([27]), tensor([25]), tensor([25]), tensor([50]), tensor([82]), tensor([97]), tensor([25]), tensor([78]), tensor([97]), tensor([86]), tensor([69]), tensor([97]), tensor([57]), tensor([97]), tensor([97]), tensor([86]), tensor([30]), tensor([27]), tensor([25]), tensor([86]), tensor([82]), tensor([27]), tensor([50]), tensor([27]), tensor([27]), tensor([69]), tensor([27]), tensor([27]), tensor([57]), tensor([27]), tensor([30]), tensor([57]), tensor([57]), tensor([25]), tensor([30]), tensor([57]), tensor([78]), tensor([25]), tensor([27]), tensor([86]), tensor([78]), tensor([50]), tensor([50]), tensor([82]), tensor([86]), tensor([30]), tensor([30]), tensor([97]), tensor([30]), tensor([78]), tensor([30]), tensor([82]), tensor([69]), tensor([82]), tensor([69]), tensor([50]), tensor([30]), tensor([30]), tensor([57]), tensor([27]), tensor([30]), tensor([50]), tensor([30]), tensor([50]), tensor([25]), tensor([27]), tensor([82]), tensor([50]), tensor([27]), tensor([50]), tensor([27]), tensor([30]), tensor([82]), tensor([30]), tensor([69]), tensor([69]), tensor([97]), tensor([78]), tensor([57]), tensor([27]), tensor([86]), tensor([57]), tensor([27]), tensor([30]), tensor([82]), tensor([27]), tensor([30]), tensor([97]), tensor([30]), tensor([82]), tensor([25]), tensor([57]), tensor([97]), tensor([97]), tensor([50]), tensor([50]), tensor([30]), tensor([82]), tensor([82]), tensor([57]), tensor([27]), tensor([30]), tensor([27]), tensor([82]), tensor([97]), tensor([82]), tensor([27]), tensor([97]), tensor([78]), tensor([82]), tensor([25]), tensor([69]), tensor([57]), tensor([86]), tensor([50]), tensor([69]), tensor([69]), tensor([97]), tensor([57]), tensor([78]), tensor([27]), tensor([25]), tensor([78]), tensor([25]), tensor([50]), tensor([27]), tensor([82]), tensor([78]), tensor([86]), tensor([86]), tensor([78]), tensor([69]), tensor([50]), tensor([50]), tensor([78]), tensor([57]), tensor([50]), tensor([78]), tensor([82]), tensor([82]), tensor([69]), tensor([30]), tensor([57]), tensor([78]), tensor([30]), tensor([30]), tensor([86]), tensor([50]), tensor([50]), tensor([27]), tensor([57]), tensor([25]), tensor([78]), tensor([78]), tensor([57]), tensor([86]), tensor([25]), tensor([27]), tensor([30]), tensor([57]), tensor([69]), tensor([69]), tensor([86]), tensor([97]), tensor([27]), tensor([30]), tensor([30]), tensor([50]), tensor([69]), tensor([69]), tensor([69]), tensor([69]), tensor([27]), tensor([82]), tensor([57]), tensor([78]), tensor([82]), tensor([50]), tensor([82]), tensor([82]), tensor([82]), tensor([57]), tensor([57]), tensor([78]), tensor([69]), tensor([25]), tensor([86]), tensor([69]), tensor([57]), tensor([25]), tensor([30]), tensor([97]), tensor([57]), tensor([27]), tensor([86]), tensor([97]), tensor([25]), tensor([82]), tensor([82]), tensor([50]), tensor([57]), tensor([78]), tensor([97]), tensor([86]), tensor([86]), tensor([25]), tensor([30]), tensor([69]), tensor([50]), tensor([97]), tensor([97]), tensor([27]), tensor([69]), tensor([82]), tensor([86]), tensor([25]), tensor([30]), tensor([69]), tensor([82]), tensor([82]), tensor([78]), tensor([86]), tensor([30]), tensor([30]), tensor([27]), tensor([78]), tensor([30]), tensor([86]), tensor([57]), tensor([86]), tensor([57]), tensor([82]), tensor([57]), tensor([97]), tensor([97]), tensor([82]), tensor([86]), tensor([97]), tensor([57]), tensor([50]), tensor([97]), tensor([78]), tensor([82]), tensor([30]), tensor([78]), tensor([86]), tensor([82]), tensor([57]), tensor([50]), tensor([97]), tensor([97]), tensor([82]), tensor([25]), tensor([30]), tensor([78]), tensor([78]), tensor([30]), tensor([69]), tensor([27]), tensor([86]), tensor([27]), tensor([78]), tensor([27]), tensor([78]), tensor([97]), tensor([50]), tensor([25]), tensor([97]), tensor([27]), tensor([25]), tensor([27]), tensor([78]), tensor([50]), tensor([30]), tensor([86]), tensor([86]), tensor([82]), tensor([50]), tensor([78]), tensor([30]), tensor([69]), tensor([27]), tensor([78]), tensor([30]), tensor([97]), tensor([69]), tensor([57]), tensor([30]), tensor([86]), tensor([25]), tensor([78]), tensor([78]), tensor([27]), tensor([50]), tensor([82]), tensor([78]), tensor([97]), tensor([50]), tensor([82]), tensor([82]), tensor([82]), tensor([57]), tensor([30]), tensor([30]), tensor([97]), tensor([27]), tensor([86]), tensor([27]), tensor([69]), tensor([69]), tensor([97]), tensor([57]), tensor([97]), tensor([57]), tensor([27]), tensor([57]), tensor([50]), tensor([86]), tensor([69]), tensor([86]), tensor([69]), tensor([27]), tensor([57]), tensor([97]), tensor([86]), tensor([30]), tensor([69]), tensor([86]), tensor([57]), tensor([82]), tensor([78]), tensor([69]), tensor([50]), tensor([27]), tensor([30]), tensor([69]), tensor([78]), tensor([27]), tensor([78]), tensor([86]), tensor([25]), tensor([82]), tensor([27]), tensor([30]), tensor([30]), tensor([82]), tensor([69]), tensor([69]), tensor([82]), tensor([78]), tensor([69]), tensor([50]), tensor([27]), tensor([25]), tensor([57]), tensor([50]), tensor([78]), tensor([86]), tensor([86]), tensor([78]), tensor([30]), tensor([25]), tensor([97]), tensor([57]), tensor([30]), tensor([57]), tensor([82]), tensor([50]), tensor([25]), tensor([27]), tensor([97]), tensor([78]), tensor([27]), tensor([30]), tensor([27]), tensor([86]), tensor([50]), tensor([78]), tensor([27]), tensor([57]), tensor([69]), tensor([69]), tensor([97]), tensor([50]), tensor([97]), tensor([86]), tensor([25]), tensor([50]), tensor([50]), tensor([50]), tensor([27]), tensor([97]), tensor([30]), tensor([78]), tensor([25]), tensor([78]), tensor([57]), tensor([30]), tensor([69]), tensor([86]), tensor([78]), tensor([57]), tensor([25]), tensor([69]), tensor([86]), tensor([27]), tensor([97]), tensor([50]), tensor([30]), tensor([78]), tensor([97]), tensor([50]), tensor([69]), tensor([57]), tensor([57]), tensor([30]), tensor([97]), tensor([82]), tensor([30]), tensor([78]), tensor([78]), tensor([78]), tensor([57]), tensor([82]), tensor([82]), tensor([30]), tensor([82]), tensor([86]), tensor([50]), tensor([86]), tensor([50]), tensor([97]), tensor([27]), tensor([25]), tensor([57]), tensor([25]), tensor([69]), tensor([69]), tensor([50]), tensor([25]), tensor([30]), tensor([30]), tensor([27]), tensor([25]), tensor([78]), tensor([69]), tensor([57]), tensor([78]), tensor([27]), tensor([78]), tensor([25]), tensor([25]), tensor([27]), tensor([97]), tensor([25]), tensor([30]), tensor([97]), tensor([82]), tensor([27]), tensor([57]), tensor([25]), tensor([78]), tensor([57]), tensor([78]), tensor([50]), tensor([57]), tensor([69]), tensor([69]), tensor([27]), tensor([97]), tensor([57]), tensor([78]), tensor([82]), tensor([82]), tensor([57]), tensor([82]), tensor([30]), tensor([50]), tensor([57]), tensor([50]), tensor([30]), tensor([78]), tensor([30]), tensor([97]), tensor([25]), tensor([82]), tensor([82]), tensor([97]), tensor([78]), tensor([69]), tensor([27]), tensor([30]), tensor([57]), tensor([97]), tensor([57]), tensor([78]), tensor([78]), tensor([57]), tensor([57]), tensor([57]), tensor([69]), tensor([86]), tensor([69]), tensor([86]), tensor([57]), tensor([86]), tensor([86]), tensor([69]), tensor([82]), tensor([86]), tensor([50]), tensor([50]), tensor([78]), tensor([86]), tensor([57]), tensor([57]), tensor([50]), tensor([86]), tensor([27]), tensor([27]), tensor([69]), tensor([86]), tensor([86]), tensor([78]), tensor([97]), tensor([78]), tensor([82]), tensor([78]), tensor([25]), tensor([82]), tensor([82]), tensor([25]), tensor([97]), tensor([86]), tensor([57]), tensor([86]), tensor([25]), tensor([50]), tensor([82]), tensor([82]), tensor([82]), tensor([25]), tensor([69]), tensor([57]), tensor([78]), tensor([97]), tensor([82]), tensor([82]), tensor([86]), tensor([30]), tensor([27]), tensor([50]), tensor([97]), tensor([69]), tensor([97]), tensor([78]), tensor([25]), tensor([50]), tensor([86]), tensor([27]), tensor([25]), tensor([69]), tensor([86]), tensor([30]), tensor([25]), tensor([86]), tensor([50]), tensor([30]), tensor([86]), tensor([25]), tensor([86]), tensor([27]), tensor([57]), tensor([27]), tensor([30]), tensor([30]), tensor([27]), tensor([27]), tensor([78]), tensor([69]), tensor([69]), tensor([50]), tensor([50]), tensor([30]), tensor([25]), tensor([82]), tensor([50]), tensor([25]), tensor([30]), tensor([69]), tensor([50]), tensor([97]), tensor([78]), tensor([78]), tensor([86]), tensor([50]), tensor([50]), tensor([78]), tensor([25]), tensor([78]), tensor([27]), tensor([27]), tensor([30]), tensor([50]), tensor([97]), tensor([97]), tensor([50]), tensor([25]), tensor([78]), tensor([30]), tensor([82]), tensor([30]), tensor([78]), tensor([78]), tensor([57]), tensor([82]), tensor([69]), tensor([82]), tensor([97]), tensor([78]), tensor([69]), tensor([27]), tensor([82]), tensor([86]), tensor([78]), tensor([25]), tensor([78]), tensor([30]), tensor([27]), tensor([27]), tensor([78]), tensor([25]), tensor([57]), tensor([57]), tensor([57]), tensor([69]), tensor([78]), tensor([78]), tensor([78]), tensor([30]), tensor([78]), tensor([25]), tensor([27]), tensor([50]), tensor([27]), tensor([25]), tensor([78]), tensor([97]), tensor([78]), tensor([27]), tensor([78]), tensor([69]), tensor([57]), tensor([97]), tensor([86]), tensor([27]), tensor([69]), tensor([57]), tensor([25]), tensor([82]), tensor([50]), tensor([97]), tensor([30]), tensor([57]), tensor([86]), tensor([57]), tensor([78]), tensor([86]), tensor([25]), tensor([50]), tensor([97]), tensor([57]), tensor([27]), tensor([82]), tensor([57]), tensor([27]), tensor([69]), tensor([78]), tensor([86]), tensor([69]), tensor([97]), tensor([25]), tensor([57]), tensor([69]), tensor([27]), tensor([57]), tensor([30]), tensor([97]), tensor([86]), tensor([30]), tensor([97]), tensor([69]), tensor([97]), tensor([50]), tensor([57]), tensor([27]), tensor([27]), tensor([69]), tensor([86]), tensor([78]), tensor([86]), tensor([86]), tensor([69]), tensor([27]), tensor([27]), tensor([30]), tensor([25]), tensor([82]), tensor([82]), tensor([25]), tensor([25]), tensor([57]), tensor([69]), tensor([78]), tensor([82]), tensor([69]), tensor([27]), tensor([82]), tensor([25]), tensor([82]), tensor([30]), tensor([30]), tensor([69]), tensor([86]), tensor([78]), tensor([86]), tensor([86]), tensor([50]), tensor([27]), tensor([30]), tensor([25]), tensor([57]), tensor([97]), tensor([50]), tensor([86]), tensor([97]), tensor([25]), tensor([82]), tensor([86]), tensor([30]), tensor([97]), tensor([50]), tensor([78]), tensor([57]), tensor([86]), tensor([50]), tensor([69]), tensor([57]), tensor([50]), tensor([97]), tensor([86]), tensor([50]), tensor([69]), tensor([50]), tensor([69]), tensor([27]), tensor([57]), tensor([97]), tensor([97]), tensor([97]), tensor([78]), tensor([57]), tensor([82]), tensor([86]), tensor([25]), tensor([27]), tensor([78]), tensor([57]), tensor([97]), tensor([25]), tensor([82]), tensor([97]), tensor([86]), tensor([50]), tensor([25]), tensor([25]), tensor([97]), tensor([30]), tensor([78]), tensor([97]), tensor([78]), tensor([30]), tensor([97]), tensor([69]), tensor([30]), tensor([82]), tensor([27]), tensor([57]), tensor([69]), tensor([25]), tensor([82]), tensor([82]), tensor([82]), tensor([27]), tensor([50]), tensor([97]), tensor([69]), tensor([30]), tensor([86]), tensor([86]), tensor([82]), tensor([82]), tensor([86]), tensor([78]), tensor([50]), tensor([57]), tensor([30]), tensor([30]), tensor([25]), tensor([97]), tensor([57]), tensor([27]), tensor([86]), tensor([25]), tensor([82]), tensor([97]), tensor([25]), tensor([78]), tensor([69]), tensor([78]), tensor([27]), tensor([97]), tensor([50]), tensor([97]), tensor([82]), tensor([97]), tensor([27]), tensor([27]), tensor([27]), tensor([27]), tensor([97]), tensor([25]), tensor([97]), tensor([86]), tensor([25]), tensor([82]), tensor([78]), tensor([78]), tensor([69]), tensor([86]), tensor([30]), tensor([30]), tensor([57]), tensor([82]), tensor([69]), tensor([82]), tensor([78]), tensor([25]), tensor([69]), tensor([30]), tensor([78]), tensor([97]), tensor([25]), tensor([78]), tensor([25]), tensor([25]), tensor([86]), tensor([25]), tensor([50]), tensor([78]), tensor([97]), tensor([78]), tensor([27]), tensor([50]), tensor([25]), tensor([50]), tensor([86]), tensor([30]), tensor([27]), tensor([86]), tensor([50]), tensor([30]), tensor([97]), tensor([57]), tensor([57]), tensor([27]), tensor([69]), tensor([82]), tensor([69]), tensor([25]), tensor([69]), tensor([25]), tensor([69]), tensor([27]), tensor([25]), tensor([78]), tensor([30]), tensor([27]), tensor([30]), tensor([27]), tensor([50]), tensor([27]), tensor([69]), tensor([82]), tensor([69]), tensor([86]), tensor([78]), tensor([27]), tensor([86]), tensor([86]), tensor([27]), tensor([30]), tensor([69]), tensor([30]), tensor([27]), tensor([86]), tensor([50]), tensor([30]), tensor([69]), tensor([97]), tensor([27]), tensor([25]), tensor([78]), tensor([86]), tensor([27]), tensor([78]), tensor([30]), tensor([27]), tensor([30]), tensor([50]), tensor([86]), tensor([30]), tensor([78]), tensor([25]), tensor([97]), tensor([86]), tensor([97]), tensor([30]), tensor([86]), tensor([82]), tensor([69]), tensor([86]), tensor([25]), tensor([57]), tensor([30]), tensor([30]), tensor([25]), tensor([78]), tensor([86]), tensor([86]), tensor([86]), tensor([27]), tensor([78]), tensor([50]), tensor([69]), tensor([86]), tensor([57]), tensor([82]), tensor([50]), tensor([57]), tensor([82]), tensor([97]), tensor([69]), tensor([82]), tensor([25]), tensor([30]), tensor([30]), tensor([86]), tensor([82]), tensor([25]), tensor([97]), tensor([78]), tensor([50]), tensor([30]), tensor([86]), tensor([30]), tensor([86]), tensor([86]), tensor([69]), tensor([25]), tensor([57]), tensor([69]), tensor([82]), tensor([27]), tensor([82]), tensor([78]), tensor([50]), tensor([57]), tensor([25]), tensor([57]), tensor([27]), tensor([86]), tensor([50]), tensor([50]), tensor([82]), tensor([30]), tensor([69]), tensor([25]), tensor([86]), tensor([50]), tensor([78]), tensor([57]), tensor([30]), tensor([25]), tensor([97]), tensor([30]), tensor([82]), tensor([57]), tensor([78]), tensor([78]), tensor([25]), tensor([82]), tensor([25]), tensor([78]), tensor([30]), tensor([25]), tensor([50]), tensor([30]), tensor([82]), tensor([27]), tensor([27]), tensor([86]), tensor([30]), tensor([27]), tensor([82]), tensor([82]), tensor([97]), tensor([50]), tensor([69]), tensor([97]), tensor([69]), tensor([82]), tensor([69]), tensor([86]), tensor([97]), tensor([78]), tensor([57]), tensor([97]), tensor([86]), tensor([97]), tensor([97]), tensor([97]), tensor([57]), tensor([78]), tensor([69]), tensor([57]), tensor([78]), tensor([30]), tensor([86]), tensor([78]), tensor([97]), tensor([86]), tensor([50]), tensor([78]), tensor([86]), tensor([30]), tensor([57]), tensor([82]), tensor([57]), tensor([97]), tensor([97]), tensor([97]), tensor([27]), tensor([69]), tensor([27]), tensor([50]), tensor([97]), tensor([50]), tensor([30]), tensor([78]), tensor([30]), tensor([97]), tensor([86]), tensor([82]), tensor([57]), tensor([97]), tensor([97]), tensor([86]), tensor([86]), tensor([97]), tensor([57]), tensor([25]), tensor([97]), tensor([57]), tensor([82]), tensor([30]), tensor([78]), tensor([27]), tensor([86]), tensor([69]), tensor([30]), tensor([69]), tensor([57]), tensor([25]), tensor([50]), tensor([25]), tensor([78]), tensor([69]), tensor([69]), tensor([30]), tensor([27]), tensor([25]), tensor([69]), tensor([69]), tensor([82]), tensor([97]), tensor([69]), tensor([82]), tensor([25]), tensor([50]), tensor([69]), tensor([82]), tensor([82]), tensor([25]), tensor([78]), tensor([97]), tensor([50]), tensor([69]), tensor([30]), tensor([86]), tensor([25]), tensor([82]), tensor([25]), tensor([50]), tensor([30]), tensor([78]), tensor([25]), tensor([86]), tensor([97]), tensor([50]), tensor([78]), tensor([97]), tensor([25]), tensor([82]), tensor([82]), tensor([57]), tensor([78]), tensor([97]), tensor([30]), tensor([57]), tensor([82]), tensor([57]), tensor([27]), tensor([86]), tensor([50]), tensor([57]), tensor([50]), tensor([82]), tensor([86]), tensor([27]), tensor([69]), tensor([69]), tensor([69]), tensor([27]), tensor([50]), tensor([25]), tensor([97]), tensor([86]), tensor([78]), tensor([27]), tensor([78]), tensor([82]), tensor([69]), tensor([57]), tensor([97]), tensor([57]), tensor([57]), tensor([50]), tensor([82]), tensor([57]), tensor([82]), tensor([78]), tensor([82]), tensor([78]), tensor([82]), tensor([86]), tensor([97]), tensor([82]), tensor([27]), tensor([69]), tensor([86]), tensor([82]), tensor([50]), tensor([57]), tensor([78]), tensor([25]), tensor([27]), tensor([25]), tensor([86]), tensor([69]), tensor([86]), tensor([57]), tensor([78]), tensor([25]), tensor([25]), tensor([50]), tensor([97]), tensor([50]), tensor([27]), tensor([27]), tensor([27]), tensor([97]), tensor([27]), tensor([27]), tensor([86]), tensor([97]), tensor([97]), tensor([97]), tensor([86]), tensor([25]), tensor([50]), tensor([25]), tensor([30]), tensor([69]), tensor([25]), tensor([82]), tensor([50]), tensor([50]), tensor([50]), tensor([69]), tensor([82]), tensor([82]), tensor([25]), tensor([50]), tensor([86]), tensor([30]), tensor([30]), tensor([86]), tensor([30]), tensor([30]), tensor([86]), tensor([50]), tensor([57]), tensor([25]), tensor([30]), tensor([57]), tensor([27]), tensor([50]), tensor([27]), tensor([25]), tensor([27]), tensor([78]), tensor([30]), tensor([69]), tensor([27]), tensor([25]), tensor([57]), tensor([97]), tensor([82]), tensor([86]), tensor([78]), tensor([97]), tensor([27]), tensor([30]), tensor([97]), tensor([86]), tensor([25]), tensor([97]), tensor([69]), tensor([27]), tensor([69]), tensor([82]), tensor([27]), tensor([30]), tensor([86]), tensor([25]), tensor([69]), tensor([57]), tensor([78]), tensor([78]), tensor([25]), tensor([57]), tensor([30]), tensor([78]), tensor([30]), tensor([97]), tensor([69]), tensor([50]), tensor([97]), tensor([27]), tensor([27]), tensor([25]), tensor([82]), tensor([86]), tensor([69]), tensor([69]), tensor([97]), tensor([78]), tensor([57]), tensor([57]), tensor([69]), tensor([82]), tensor([50]), tensor([97]), tensor([82]), tensor([50]), tensor([69]), tensor([25]), tensor([97]), tensor([25]), tensor([78]), tensor([50]), tensor([78]), tensor([86]), tensor([50]), tensor([27]), tensor([57]), tensor([50]), tensor([78]), tensor([57]), tensor([57]), tensor([57]), tensor([69]), tensor([97]), tensor([50]), tensor([27]), tensor([57]), tensor([30]), tensor([57]), tensor([78]), tensor([50]), tensor([82]), tensor([69]), tensor([78]), tensor([86]), tensor([30]), tensor([25]), tensor([57]), tensor([27]), tensor([97]), tensor([69]), tensor([78]), tensor([27]), tensor([86]), tensor([82]), tensor([69]), tensor([97]), tensor([30]), tensor([97]), tensor([50]), tensor([86]), tensor([69]), tensor([86]), tensor([82]), tensor([50]), tensor([97]), tensor([82]), tensor([86]), tensor([69]), tensor([82]), tensor([69]), tensor([78]), tensor([86]), tensor([50]), tensor([30]), tensor([50]), tensor([30]), tensor([86]), tensor([82]), tensor([97]), tensor([86]), tensor([97]), tensor([97]), tensor([25]), tensor([69]), tensor([25]), tensor([97]), tensor([78]), tensor([27]), tensor([97]), tensor([50]), tensor([86]), tensor([30]), tensor([86]), tensor([69]), tensor([78]), tensor([86]), tensor([86]), tensor([78]), tensor([82]), tensor([86]), tensor([86]), tensor([50]), tensor([50]), tensor([50]), tensor([30]), tensor([50]), tensor([25]), tensor([78]), tensor([69]), tensor([97]), tensor([57]), tensor([25]), tensor([82]), tensor([27]), tensor([57]), tensor([30]), tensor([50]), tensor([27]), tensor([82]), tensor([27]), tensor([57]), tensor([27]), tensor([50]), tensor([50]), tensor([78]), tensor([50]), tensor([30]), tensor([78]), tensor([30]), tensor([57]), tensor([57]), tensor([86]), tensor([27]), tensor([78]), tensor([30]), tensor([27]), tensor([30]), tensor([25]), tensor([78]), tensor([69]), tensor([50]), tensor([97]), tensor([97]), tensor([30]), tensor([27]), tensor([27]), tensor([86]), tensor([82]), tensor([86]), tensor([78]), tensor([25]), tensor([78]), tensor([97]), tensor([69]), tensor([86]), tensor([69]), tensor([50]), tensor([82]), tensor([69]), tensor([69]), tensor([78]), tensor([50]), tensor([57]), tensor([30]), tensor([27]), tensor([69]), tensor([82]), tensor([57]), tensor([82]), tensor([57]), tensor([69]), tensor([69]), tensor([27]), tensor([97]), tensor([25]), tensor([50]), tensor([27]), tensor([97]), tensor([27]), tensor([25]), tensor([30]), tensor([50]), tensor([25]), tensor([69]), tensor([97]), tensor([25]), tensor([97]), tensor([25]), tensor([30]), tensor([27]), tensor([57]), tensor([30]), tensor([30]), tensor([86]), tensor([97]), tensor([57]), tensor([97]), tensor([50]), tensor([82]), tensor([82]), tensor([86]), tensor([82]), tensor([78]), tensor([86]), tensor([57]), tensor([50]), tensor([69]), tensor([25]), tensor([86]), tensor([50]), tensor([82]), tensor([69]), tensor([86]), tensor([57]), tensor([78]), tensor([30]), tensor([27]), tensor([30]), tensor([57]), tensor([97]), tensor([30]), tensor([27]), tensor([82]), tensor([97]), tensor([86]), tensor([27]), tensor([69]), tensor([27]), tensor([50]), tensor([57]), tensor([25]), tensor([27]), tensor([57]), tensor([25]), tensor([97]), tensor([27]), tensor([57]), tensor([82]), tensor([69]), tensor([25]), tensor([50]), tensor([78]), tensor([97]), tensor([50]), tensor([86]), tensor([82]), tensor([69]), tensor([57]), tensor([78]), tensor([57]), tensor([50]), tensor([25]), tensor([69]), tensor([86]), tensor([27]), tensor([97]), tensor([69]), tensor([25]), tensor([25]), tensor([82]), tensor([50]), tensor([82]), tensor([27]), tensor([86]), tensor([69]), tensor([27]), tensor([69]), tensor([82]), tensor([30]), tensor([57]), tensor([69]), tensor([82]), tensor([25]), tensor([50]), tensor([97]), tensor([82]), tensor([82]), tensor([50]), tensor([30]), tensor([82]), tensor([97]), tensor([25]), tensor([97]), tensor([57]), tensor([57]), tensor([69]), tensor([57]), tensor([97]), tensor([82]), tensor([50]), tensor([86]), tensor([25]), tensor([97]), tensor([78]), tensor([69]), tensor([82]), tensor([30]), tensor([27]), tensor([82]), tensor([27]), tensor([97]), tensor([69]), tensor([82]), tensor([25]), tensor([69]), tensor([69]), tensor([69]), tensor([97]), tensor([69]), tensor([97]), tensor([30]), tensor([78]), tensor([57]), tensor([25]), tensor([69]), tensor([82]), tensor([25]), tensor([27]), tensor([82]), tensor([57]), tensor([50]), tensor([27]), tensor([30]), tensor([82]), tensor([86]), tensor([86]), tensor([25]), tensor([57]), tensor([57]), tensor([50]), tensor([50]), tensor([86]), tensor([27]), tensor([69]), tensor([82]), tensor([30]), tensor([97]), tensor([50]), tensor([50]), tensor([30]), tensor([78]), tensor([78]), tensor([27]), tensor([30]), tensor([50]), tensor([27]), tensor([25]), tensor([69]), tensor([69]), tensor([57]), tensor([78]), tensor([78]), tensor([27]), tensor([86]), tensor([57]), tensor([78]), tensor([27]), tensor([50]), tensor([50]), tensor([57]), tensor([57]), tensor([50]), tensor([97]), tensor([86]), tensor([97]), tensor([30]), tensor([97]), tensor([30]), tensor([50]), tensor([69]), tensor([86]), tensor([97]), tensor([86]), tensor([25]), tensor([25]), tensor([82]), tensor([30]), tensor([25]), tensor([30]), tensor([97]), tensor([69]), tensor([82]), tensor([86]), tensor([82]), tensor([30]), tensor([57]), tensor([50]), tensor([25]), tensor([78]), tensor([82]), tensor([97]), tensor([50]), tensor([27]), tensor([25]), tensor([25]), tensor([30]), tensor([25]), tensor([30]), tensor([30]), tensor([30]), tensor([50]), tensor([69]), tensor([27]), tensor([69]), tensor([25]), tensor([25]), tensor([69]), tensor([25]), tensor([30]), tensor([69]), tensor([78]), tensor([57]), tensor([78]), tensor([50]), tensor([57]), tensor([50]), tensor([30]), tensor([30]), tensor([25]), tensor([30]), tensor([57]), tensor([69]), tensor([57]), tensor([57]), tensor([78]), tensor([86]), tensor([25]), tensor([82]), tensor([50]), tensor([78]), tensor([57]), tensor([50]), tensor([82]), tensor([82]), tensor([69]), tensor([97]), tensor([27]), tensor([86]), tensor([86]), tensor([27]), tensor([97]), tensor([57]), tensor([78]), tensor([50]), tensor([86]), tensor([97]), tensor([82]), tensor([25]), tensor([97]), tensor([82]), tensor([69]), tensor([69]), tensor([50]), tensor([69]), tensor([86]), tensor([27]), tensor([30]), tensor([57]), tensor([78]), tensor([30]), tensor([97]), tensor([25]), tensor([30]), tensor([82]), tensor([50]), tensor([86]), tensor([27]), tensor([57]), tensor([27]), tensor([30]), tensor([82]), tensor([27]), tensor([57]), tensor([25]), tensor([97]), tensor([69]), tensor([27]), tensor([97]), tensor([27]), tensor([25]), tensor([57]), tensor([30]), tensor([57]), tensor([25]), tensor([86]), tensor([97]), tensor([30]), tensor([57]), tensor([78]), tensor([78]), tensor([50]), tensor([30]), tensor([97]), tensor([25]), tensor([50]), tensor([82]), tensor([25]), tensor([82]), tensor([25]), tensor([69]), tensor([82]), tensor([69]), tensor([50]), tensor([82]), tensor([57]), tensor([69]), tensor([86]), tensor([50]), tensor([25]), tensor([57]), tensor([82]), tensor([27]), tensor([25]), tensor([25]), tensor([57]), tensor([57]), tensor([50]), tensor([82]), tensor([25]), tensor([57]), tensor([78]), tensor([25]), tensor([25]), tensor([30]), tensor([82]), tensor([50]), tensor([25]), tensor([25]), tensor([82]), tensor([69]), tensor([86]), tensor([69]), tensor([86]), tensor([57]), tensor([86]), tensor([86]), tensor([57]), tensor([86]), tensor([86]), tensor([97]), tensor([25]), tensor([78]), tensor([50]), tensor([78]), tensor([50]), tensor([57]), tensor([30]), tensor([30]), tensor([86]), tensor([86]), tensor([50]), tensor([82]), tensor([86]), tensor([69]), tensor([82]), tensor([69]), tensor([97]), tensor([86]), tensor([69])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.78 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.808\n",
            "TEST ALL:  0.808\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  2000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 95, 25, 27, 30, 34, 49, 50, 57, 58, 59, 60, 68, 69, 78, 81, 82, 86, 88, 13]\n",
            "TRAIN_SET CLASSES:  [95, 59, 58, 34, 81, 49, 13, 88, 68, 60]\n",
            "VALIDATION CLASSES:  [60, 59, 58, 49, 34, 95, 88, 81, 13, 68]\n",
            "GROUP:  2\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [95, 59, 58, 34, 81, 49, 13, 88, 68, 60]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  20\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.5137197375297546\n",
            "Train step - Step 10, Loss 0.15527212619781494\n",
            "Train step - Step 20, Loss 0.1620972603559494\n",
            "Train step - Step 30, Loss 0.14635789394378662\n",
            "Train step - Step 40, Loss 0.15947948396205902\n",
            "Train step - Step 50, Loss 0.14003886282444\n",
            "Train epoch - Accuracy: 0.3100719424460432 Loss: 0.17604783110481373 Corrects: 2155\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.135798841714859\n",
            "Train step - Step 70, Loss 0.13709400594234467\n",
            "Train step - Step 80, Loss 0.11814415454864502\n",
            "Train step - Step 90, Loss 0.13351868093013763\n",
            "Train step - Step 100, Loss 0.1228102445602417\n",
            "Train epoch - Accuracy: 0.37237410071942445 Loss: 0.13111206749574744 Corrects: 2588\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11647393554449081\n",
            "Train step - Step 120, Loss 0.11900182068347931\n",
            "Train step - Step 130, Loss 0.11732804775238037\n",
            "Train step - Step 140, Loss 0.12590499222278595\n",
            "Train step - Step 150, Loss 0.11622827500104904\n",
            "Train step - Step 160, Loss 0.1222437173128128\n",
            "Train epoch - Accuracy: 0.40302158273381294 Loss: 0.1238487555140214 Corrects: 2801\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.12576769292354584\n",
            "Train step - Step 180, Loss 0.117875836789608\n",
            "Train step - Step 190, Loss 0.10939595848321915\n",
            "Train step - Step 200, Loss 0.11352682113647461\n",
            "Train step - Step 210, Loss 0.1227969154715538\n",
            "Train epoch - Accuracy: 0.4343884892086331 Loss: 0.11971386516265732 Corrects: 3019\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11465467512607574\n",
            "Train step - Step 230, Loss 0.1235346719622612\n",
            "Train step - Step 240, Loss 0.11164980381727219\n",
            "Train step - Step 250, Loss 0.12655311822891235\n",
            "Train step - Step 260, Loss 0.12204866856336594\n",
            "Train step - Step 270, Loss 0.10593034327030182\n",
            "Train epoch - Accuracy: 0.46388489208633094 Loss: 0.11722736817469699 Corrects: 3224\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1274828016757965\n",
            "Train step - Step 290, Loss 0.11482997238636017\n",
            "Train step - Step 300, Loss 0.1154828816652298\n",
            "Train step - Step 310, Loss 0.10806825011968613\n",
            "Train step - Step 320, Loss 0.10392715036869049\n",
            "Train epoch - Accuracy: 0.48676258992805754 Loss: 0.11355792100695397 Corrects: 3383\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11730537563562393\n",
            "Train step - Step 340, Loss 0.12456243485212326\n",
            "Train step - Step 350, Loss 0.12611590325832367\n",
            "Train step - Step 360, Loss 0.10982400178909302\n",
            "Train step - Step 370, Loss 0.11163318157196045\n",
            "Train step - Step 380, Loss 0.11227124184370041\n",
            "Train epoch - Accuracy: 0.5046043165467626 Loss: 0.11165039151478157 Corrects: 3507\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.11253364384174347\n",
            "Train step - Step 400, Loss 0.11447291821241379\n",
            "Train step - Step 410, Loss 0.10710430145263672\n",
            "Train step - Step 420, Loss 0.11158579587936401\n",
            "Train step - Step 430, Loss 0.11459939926862717\n",
            "Train epoch - Accuracy: 0.5207194244604316 Loss: 0.10996882746331126 Corrects: 3619\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10490003973245621\n",
            "Train step - Step 450, Loss 0.10794421285390854\n",
            "Train step - Step 460, Loss 0.1155400201678276\n",
            "Train step - Step 470, Loss 0.10816049575805664\n",
            "Train step - Step 480, Loss 0.11086416244506836\n",
            "Train step - Step 490, Loss 0.11548230797052383\n",
            "Train epoch - Accuracy: 0.5320863309352518 Loss: 0.10759653664535755 Corrects: 3698\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10682572424411774\n",
            "Train step - Step 510, Loss 0.11029054969549179\n",
            "Train step - Step 520, Loss 0.1085776686668396\n",
            "Train step - Step 530, Loss 0.1104159876704216\n",
            "Train step - Step 540, Loss 0.1039591059088707\n",
            "Train epoch - Accuracy: 0.5461870503597123 Loss: 0.10673047166505306 Corrects: 3796\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.10894034057855606\n",
            "Train step - Step 560, Loss 0.10895290225744247\n",
            "Train step - Step 570, Loss 0.1118004098534584\n",
            "Train step - Step 580, Loss 0.11070596426725388\n",
            "Train step - Step 590, Loss 0.10156487673521042\n",
            "Train step - Step 600, Loss 0.09430040419101715\n",
            "Train epoch - Accuracy: 0.5575539568345323 Loss: 0.10503135061306919 Corrects: 3875\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.1005595251917839\n",
            "Train step - Step 620, Loss 0.10481525957584381\n",
            "Train step - Step 630, Loss 0.10602650791406631\n",
            "Train step - Step 640, Loss 0.10052080452442169\n",
            "Train step - Step 650, Loss 0.10943281650543213\n",
            "Train epoch - Accuracy: 0.5730935251798561 Loss: 0.10282688811957408 Corrects: 3983\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.09834938496351242\n",
            "Train step - Step 670, Loss 0.09769809991121292\n",
            "Train step - Step 680, Loss 0.09821218252182007\n",
            "Train step - Step 690, Loss 0.10797180980443954\n",
            "Train step - Step 700, Loss 0.0941043272614479\n",
            "Train step - Step 710, Loss 0.09698782116174698\n",
            "Train epoch - Accuracy: 0.5781294964028777 Loss: 0.10171778334559296 Corrects: 4018\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10962522029876709\n",
            "Train step - Step 730, Loss 0.10139738768339157\n",
            "Train step - Step 740, Loss 0.09661175310611725\n",
            "Train step - Step 750, Loss 0.10276100784540176\n",
            "Train step - Step 760, Loss 0.09553369879722595\n",
            "Train epoch - Accuracy: 0.5922302158273381 Loss: 0.10114398369042993 Corrects: 4116\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10056030750274658\n",
            "Train step - Step 780, Loss 0.09527494758367538\n",
            "Train step - Step 790, Loss 0.09898130595684052\n",
            "Train step - Step 800, Loss 0.09152210503816605\n",
            "Train step - Step 810, Loss 0.09602291882038116\n",
            "Train step - Step 820, Loss 0.09175743907690048\n",
            "Train epoch - Accuracy: 0.6056115107913669 Loss: 0.09850410102082671 Corrects: 4209\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.09368482977151871\n",
            "Train step - Step 840, Loss 0.08973021805286407\n",
            "Train step - Step 850, Loss 0.08805853128433228\n",
            "Train step - Step 860, Loss 0.1074579730629921\n",
            "Train step - Step 870, Loss 0.10363306850194931\n",
            "Train epoch - Accuracy: 0.6187050359712231 Loss: 0.0979529124069557 Corrects: 4300\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.10176260769367218\n",
            "Train step - Step 890, Loss 0.09938796609640121\n",
            "Train step - Step 900, Loss 0.09452502429485321\n",
            "Train step - Step 910, Loss 0.08878082036972046\n",
            "Train step - Step 920, Loss 0.09494564682245255\n",
            "Train step - Step 930, Loss 0.10756053775548935\n",
            "Train epoch - Accuracy: 0.6146762589928058 Loss: 0.09672370919649549 Corrects: 4272\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.09367267787456512\n",
            "Train step - Step 950, Loss 0.09806215018033981\n",
            "Train step - Step 960, Loss 0.09274570643901825\n",
            "Train step - Step 970, Loss 0.07879046350717545\n",
            "Train step - Step 980, Loss 0.08649956434965134\n",
            "Train epoch - Accuracy: 0.6235971223021582 Loss: 0.0971101653854624 Corrects: 4334\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.09523584693670273\n",
            "Train step - Step 1000, Loss 0.08574391901493073\n",
            "Train step - Step 1010, Loss 0.10065052658319473\n",
            "Train step - Step 1020, Loss 0.09243523329496384\n",
            "Train step - Step 1030, Loss 0.10573585331439972\n",
            "Train step - Step 1040, Loss 0.09262379258871078\n",
            "Train epoch - Accuracy: 0.6378417266187051 Loss: 0.09517672079715797 Corrects: 4433\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.10534518957138062\n",
            "Train step - Step 1060, Loss 0.09342121332883835\n",
            "Train step - Step 1070, Loss 0.10277354717254639\n",
            "Train step - Step 1080, Loss 0.09624594449996948\n",
            "Train step - Step 1090, Loss 0.09371928870677948\n",
            "Train epoch - Accuracy: 0.6450359712230216 Loss: 0.09521175100434598 Corrects: 4483\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.09534984081983566\n",
            "Train step - Step 1110, Loss 0.10372570902109146\n",
            "Train step - Step 1120, Loss 0.09274481981992722\n",
            "Train step - Step 1130, Loss 0.0925784781575203\n",
            "Train step - Step 1140, Loss 0.09150991588830948\n",
            "Train step - Step 1150, Loss 0.11130104213953018\n",
            "Train epoch - Accuracy: 0.6461870503597122 Loss: 0.09433283052641711 Corrects: 4491\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.09895253926515579\n",
            "Train step - Step 1170, Loss 0.09334152936935425\n",
            "Train step - Step 1180, Loss 0.0956624373793602\n",
            "Train step - Step 1190, Loss 0.09620903432369232\n",
            "Train step - Step 1200, Loss 0.10206875950098038\n",
            "Train epoch - Accuracy: 0.6607194244604316 Loss: 0.09341554357422341 Corrects: 4592\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.08560919016599655\n",
            "Train step - Step 1220, Loss 0.09864766150712967\n",
            "Train step - Step 1230, Loss 0.09393327683210373\n",
            "Train step - Step 1240, Loss 0.09488432109355927\n",
            "Train step - Step 1250, Loss 0.11246214061975479\n",
            "Train step - Step 1260, Loss 0.09086866676807404\n",
            "Train epoch - Accuracy: 0.662589928057554 Loss: 0.09228127417804526 Corrects: 4605\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.0799490213394165\n",
            "Train step - Step 1280, Loss 0.08642221987247467\n",
            "Train step - Step 1290, Loss 0.09554249048233032\n",
            "Train step - Step 1300, Loss 0.08790846914052963\n",
            "Train step - Step 1310, Loss 0.09130074828863144\n",
            "Train epoch - Accuracy: 0.6735251798561152 Loss: 0.09153262235492253 Corrects: 4681\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.08753395080566406\n",
            "Train step - Step 1330, Loss 0.09305932372808456\n",
            "Train step - Step 1340, Loss 0.0920829102396965\n",
            "Train step - Step 1350, Loss 0.08910984545946121\n",
            "Train step - Step 1360, Loss 0.09821899235248566\n",
            "Train step - Step 1370, Loss 0.09530873596668243\n",
            "Train epoch - Accuracy: 0.681294964028777 Loss: 0.09002043465701796 Corrects: 4735\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.08706448972225189\n",
            "Train step - Step 1390, Loss 0.08805632591247559\n",
            "Train step - Step 1400, Loss 0.09423217922449112\n",
            "Train step - Step 1410, Loss 0.07923392206430435\n",
            "Train step - Step 1420, Loss 0.09010037779808044\n",
            "Train epoch - Accuracy: 0.6853237410071943 Loss: 0.09072114170455246 Corrects: 4763\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.09863287210464478\n",
            "Train step - Step 1440, Loss 0.08719305694103241\n",
            "Train step - Step 1450, Loss 0.08623050898313522\n",
            "Train step - Step 1460, Loss 0.08364180475473404\n",
            "Train step - Step 1470, Loss 0.0849689394235611\n",
            "Train step - Step 1480, Loss 0.09269847720861435\n",
            "Train epoch - Accuracy: 0.6854676258992806 Loss: 0.08931897576121117 Corrects: 4764\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.09634333848953247\n",
            "Train step - Step 1500, Loss 0.08717544376850128\n",
            "Train step - Step 1510, Loss 0.08811482042074203\n",
            "Train step - Step 1520, Loss 0.08923672884702682\n",
            "Train step - Step 1530, Loss 0.08147948235273361\n",
            "Train epoch - Accuracy: 0.7011510791366906 Loss: 0.08768172647884424 Corrects: 4873\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.07794263958930969\n",
            "Train step - Step 1550, Loss 0.09044281393289566\n",
            "Train step - Step 1560, Loss 0.08175473660230637\n",
            "Train step - Step 1570, Loss 0.0930468961596489\n",
            "Train step - Step 1580, Loss 0.08658254146575928\n",
            "Train step - Step 1590, Loss 0.07934921234846115\n",
            "Train epoch - Accuracy: 0.6992805755395683 Loss: 0.08738481337861191 Corrects: 4860\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.09282620996236801\n",
            "Train step - Step 1610, Loss 0.08408072590827942\n",
            "Train step - Step 1620, Loss 0.09264273941516876\n",
            "Train step - Step 1630, Loss 0.08586331456899643\n",
            "Train step - Step 1640, Loss 0.08752615004777908\n",
            "Train epoch - Accuracy: 0.697841726618705 Loss: 0.08725031874805904 Corrects: 4850\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.07930278033018112\n",
            "Train step - Step 1660, Loss 0.08849040418863297\n",
            "Train step - Step 1670, Loss 0.08729168027639389\n",
            "Train step - Step 1680, Loss 0.08867409825325012\n",
            "Train step - Step 1690, Loss 0.07749109715223312\n",
            "Train step - Step 1700, Loss 0.09094701707363129\n",
            "Train epoch - Accuracy: 0.7106474820143885 Loss: 0.08703734857144116 Corrects: 4939\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.09048929065465927\n",
            "Train step - Step 1720, Loss 0.08356504887342453\n",
            "Train step - Step 1730, Loss 0.08424090594053268\n",
            "Train step - Step 1740, Loss 0.08474788814783096\n",
            "Train step - Step 1750, Loss 0.08735012263059616\n",
            "Train epoch - Accuracy: 0.7136690647482015 Loss: 0.08584581870826886 Corrects: 4960\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.07971066236495972\n",
            "Train step - Step 1770, Loss 0.08698461204767227\n",
            "Train step - Step 1780, Loss 0.08507969230413437\n",
            "Train step - Step 1790, Loss 0.08864626288414001\n",
            "Train step - Step 1800, Loss 0.08239445835351944\n",
            "Train step - Step 1810, Loss 0.07583171129226685\n",
            "Train epoch - Accuracy: 0.7253237410071942 Loss: 0.08448212118886357 Corrects: 5041\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.07364539057016373\n",
            "Train step - Step 1830, Loss 0.08390919119119644\n",
            "Train step - Step 1840, Loss 0.07670706510543823\n",
            "Train step - Step 1850, Loss 0.08950991183519363\n",
            "Train step - Step 1860, Loss 0.08510259538888931\n",
            "Train epoch - Accuracy: 0.7266187050359713 Loss: 0.0840875696749996 Corrects: 5050\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.08284159749746323\n",
            "Train step - Step 1880, Loss 0.08583543449640274\n",
            "Train step - Step 1890, Loss 0.09001516550779343\n",
            "Train step - Step 1900, Loss 0.08738614618778229\n",
            "Train step - Step 1910, Loss 0.08585526794195175\n",
            "Train step - Step 1920, Loss 0.08405327051877975\n",
            "Train epoch - Accuracy: 0.7333812949640288 Loss: 0.0834215130587276 Corrects: 5097\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.07540019601583481\n",
            "Train step - Step 1940, Loss 0.07869986444711685\n",
            "Train step - Step 1950, Loss 0.0729476809501648\n",
            "Train step - Step 1960, Loss 0.08516658842563629\n",
            "Train step - Step 1970, Loss 0.08709259331226349\n",
            "Train epoch - Accuracy: 0.7381294964028777 Loss: 0.08377414759328897 Corrects: 5130\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.07943244278430939\n",
            "Train step - Step 1990, Loss 0.07961037009954453\n",
            "Train step - Step 2000, Loss 0.07295819371938705\n",
            "Train step - Step 2010, Loss 0.08755698055028915\n",
            "Train step - Step 2020, Loss 0.09211256355047226\n",
            "Train step - Step 2030, Loss 0.08329223841428757\n",
            "Train epoch - Accuracy: 0.740431654676259 Loss: 0.08308536692488966 Corrects: 5146\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.08170788735151291\n",
            "Train step - Step 2050, Loss 0.08275917172431946\n",
            "Train step - Step 2060, Loss 0.0858304426074028\n",
            "Train step - Step 2070, Loss 0.07858647406101227\n",
            "Train step - Step 2080, Loss 0.08792652934789658\n",
            "Train epoch - Accuracy: 0.7369784172661871 Loss: 0.08184769358351934 Corrects: 5122\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.08494587242603302\n",
            "Train step - Step 2100, Loss 0.07336889207363129\n",
            "Train step - Step 2110, Loss 0.0828673243522644\n",
            "Train step - Step 2120, Loss 0.07628609240055084\n",
            "Train step - Step 2130, Loss 0.08665778487920761\n",
            "Train step - Step 2140, Loss 0.07487447559833527\n",
            "Train epoch - Accuracy: 0.7541007194244604 Loss: 0.08176011660973803 Corrects: 5241\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.0885625034570694\n",
            "Train step - Step 2160, Loss 0.08166667073965073\n",
            "Train step - Step 2170, Loss 0.08018147945404053\n",
            "Train step - Step 2180, Loss 0.08001047372817993\n",
            "Train step - Step 2190, Loss 0.08148421347141266\n",
            "Train epoch - Accuracy: 0.7515107913669065 Loss: 0.08158057080755989 Corrects: 5223\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.07706590741872787\n",
            "Train step - Step 2210, Loss 0.0797080472111702\n",
            "Train step - Step 2220, Loss 0.08511817455291748\n",
            "Train step - Step 2230, Loss 0.087377168238163\n",
            "Train step - Step 2240, Loss 0.08352716267108917\n",
            "Train step - Step 2250, Loss 0.07630030065774918\n",
            "Train epoch - Accuracy: 0.7571223021582734 Loss: 0.0802971484978422 Corrects: 5262\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.08549987524747849\n",
            "Train step - Step 2270, Loss 0.08611471205949783\n",
            "Train step - Step 2280, Loss 0.07016594707965851\n",
            "Train step - Step 2290, Loss 0.07858879119157791\n",
            "Train step - Step 2300, Loss 0.08604902774095535\n",
            "Train epoch - Accuracy: 0.759136690647482 Loss: 0.08114769571547886 Corrects: 5276\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.08030302822589874\n",
            "Train step - Step 2320, Loss 0.07497315108776093\n",
            "Train step - Step 2330, Loss 0.07517413049936295\n",
            "Train step - Step 2340, Loss 0.08480717986822128\n",
            "Train step - Step 2350, Loss 0.07881420105695724\n",
            "Train step - Step 2360, Loss 0.07360482960939407\n",
            "Train epoch - Accuracy: 0.76431654676259 Loss: 0.08045893929201922 Corrects: 5312\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.08918626606464386\n",
            "Train step - Step 2380, Loss 0.07610037922859192\n",
            "Train step - Step 2390, Loss 0.07993558794260025\n",
            "Train step - Step 2400, Loss 0.07343622297048569\n",
            "Train step - Step 2410, Loss 0.07503028959035873\n",
            "Train epoch - Accuracy: 0.7658992805755396 Loss: 0.08003820805884093 Corrects: 5323\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.08134033530950546\n",
            "Train step - Step 2430, Loss 0.08050155639648438\n",
            "Train step - Step 2440, Loss 0.08664305508136749\n",
            "Train step - Step 2450, Loss 0.07922056317329407\n",
            "Train step - Step 2460, Loss 0.09436456859111786\n",
            "Train step - Step 2470, Loss 0.07809104770421982\n",
            "Train epoch - Accuracy: 0.7706474820143885 Loss: 0.07974407131508958 Corrects: 5356\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.08621776103973389\n",
            "Train step - Step 2490, Loss 0.06996627151966095\n",
            "Train step - Step 2500, Loss 0.07676687836647034\n",
            "Train step - Step 2510, Loss 0.07017969340085983\n",
            "Train step - Step 2520, Loss 0.08751892298460007\n",
            "Train epoch - Accuracy: 0.7730935251798561 Loss: 0.07828936831771041 Corrects: 5373\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.08088164776563644\n",
            "Train step - Step 2540, Loss 0.07686486840248108\n",
            "Train step - Step 2550, Loss 0.08857851475477219\n",
            "Train step - Step 2560, Loss 0.07915546745061874\n",
            "Train step - Step 2570, Loss 0.07420473545789719\n",
            "Train step - Step 2580, Loss 0.08481122553348541\n",
            "Train epoch - Accuracy: 0.7755395683453238 Loss: 0.078529702918135 Corrects: 5390\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.08124800771474838\n",
            "Train step - Step 2600, Loss 0.08041898161172867\n",
            "Train step - Step 2610, Loss 0.08211817592382431\n",
            "Train step - Step 2620, Loss 0.07901903241872787\n",
            "Train step - Step 2630, Loss 0.07663803547620773\n",
            "Train epoch - Accuracy: 0.7728057553956834 Loss: 0.07713004355379145 Corrects: 5371\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.07974462956190109\n",
            "Train step - Step 2650, Loss 0.0830555334687233\n",
            "Train step - Step 2660, Loss 0.08138811588287354\n",
            "Train step - Step 2670, Loss 0.08925288170576096\n",
            "Train step - Step 2680, Loss 0.08188947290182114\n",
            "Train step - Step 2690, Loss 0.07767104357481003\n",
            "Train epoch - Accuracy: 0.7994244604316547 Loss: 0.07578067874522518 Corrects: 5556\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.07252953201532364\n",
            "Train step - Step 2710, Loss 0.05989137291908264\n",
            "Train step - Step 2720, Loss 0.06925400346517563\n",
            "Train step - Step 2730, Loss 0.06764595955610275\n",
            "Train step - Step 2740, Loss 0.07116127014160156\n",
            "Train epoch - Accuracy: 0.8048920863309352 Loss: 0.071172800604388 Corrects: 5594\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.07476639002561569\n",
            "Train step - Step 2760, Loss 0.07601749151945114\n",
            "Train step - Step 2770, Loss 0.06429257988929749\n",
            "Train step - Step 2780, Loss 0.0783190131187439\n",
            "Train step - Step 2790, Loss 0.07094205915927887\n",
            "Train step - Step 2800, Loss 0.06262075155973434\n",
            "Train epoch - Accuracy: 0.8105035971223021 Loss: 0.06938832835542212 Corrects: 5633\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.07437986135482788\n",
            "Train step - Step 2820, Loss 0.07001225650310516\n",
            "Train step - Step 2830, Loss 0.06506583839654922\n",
            "Train step - Step 2840, Loss 0.06593982130289078\n",
            "Train step - Step 2850, Loss 0.07529112696647644\n",
            "Train epoch - Accuracy: 0.8125179856115108 Loss: 0.06980720868213572 Corrects: 5647\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.06447569280862808\n",
            "Train step - Step 2870, Loss 0.07000371068716049\n",
            "Train step - Step 2880, Loss 0.06963162124156952\n",
            "Train step - Step 2890, Loss 0.06636413186788559\n",
            "Train step - Step 2900, Loss 0.06860849261283875\n",
            "Train step - Step 2910, Loss 0.07622908800840378\n",
            "Train epoch - Accuracy: 0.816115107913669 Loss: 0.06936502665924511 Corrects: 5672\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.06749660521745682\n",
            "Train step - Step 2930, Loss 0.06826024502515793\n",
            "Train step - Step 2940, Loss 0.061849117279052734\n",
            "Train step - Step 2950, Loss 0.07017137855291367\n",
            "Train step - Step 2960, Loss 0.06735862046480179\n",
            "Train epoch - Accuracy: 0.8162589928057554 Loss: 0.06874214718667723 Corrects: 5673\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.07207958400249481\n",
            "Train step - Step 2980, Loss 0.0719502866268158\n",
            "Train step - Step 2990, Loss 0.06928153336048126\n",
            "Train step - Step 3000, Loss 0.06428641080856323\n",
            "Train step - Step 3010, Loss 0.06831766664981842\n",
            "Train step - Step 3020, Loss 0.07425983250141144\n",
            "Train epoch - Accuracy: 0.8201438848920863 Loss: 0.06837133668952709 Corrects: 5700\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.07646727561950684\n",
            "Train step - Step 3040, Loss 0.0709725171327591\n",
            "Train step - Step 3050, Loss 0.061700787395238876\n",
            "Train step - Step 3060, Loss 0.06753735989332199\n",
            "Train step - Step 3070, Loss 0.07541318982839584\n",
            "Train epoch - Accuracy: 0.818705035971223 Loss: 0.06870097133538706 Corrects: 5690\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.07458435744047165\n",
            "Train step - Step 3090, Loss 0.06673649698495865\n",
            "Train step - Step 3100, Loss 0.065317802131176\n",
            "Train step - Step 3110, Loss 0.07203247398138046\n",
            "Train step - Step 3120, Loss 0.06310570240020752\n",
            "Train step - Step 3130, Loss 0.06544651836156845\n",
            "Train epoch - Accuracy: 0.8191366906474821 Loss: 0.06800679594707146 Corrects: 5693\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.06858061999082565\n",
            "Train step - Step 3150, Loss 0.06900080293416977\n",
            "Train step - Step 3160, Loss 0.06967704743146896\n",
            "Train step - Step 3170, Loss 0.07758313417434692\n",
            "Train step - Step 3180, Loss 0.06984340399503708\n",
            "Train epoch - Accuracy: 0.820863309352518 Loss: 0.06762577895852302 Corrects: 5705\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.06835265457630157\n",
            "Train step - Step 3200, Loss 0.07019121944904327\n",
            "Train step - Step 3210, Loss 0.07439161837100983\n",
            "Train step - Step 3220, Loss 0.0673198327422142\n",
            "Train step - Step 3230, Loss 0.06697966158390045\n",
            "Train step - Step 3240, Loss 0.06831631064414978\n",
            "Train epoch - Accuracy: 0.8237410071942446 Loss: 0.06768142867645771 Corrects: 5725\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.06446557492017746\n",
            "Train step - Step 3260, Loss 0.07153867930173874\n",
            "Train step - Step 3270, Loss 0.06854569166898727\n",
            "Train step - Step 3280, Loss 0.0750897005200386\n",
            "Train step - Step 3290, Loss 0.06718166172504425\n",
            "Train epoch - Accuracy: 0.8270503597122302 Loss: 0.06745510075160925 Corrects: 5748\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.06075084209442139\n",
            "Train step - Step 3310, Loss 0.06793849915266037\n",
            "Train step - Step 3320, Loss 0.06748151034116745\n",
            "Train step - Step 3330, Loss 0.06385751068592072\n",
            "Train step - Step 3340, Loss 0.06405927985906601\n",
            "Train step - Step 3350, Loss 0.06225118786096573\n",
            "Train epoch - Accuracy: 0.8299280575539568 Loss: 0.06744642545422204 Corrects: 5768\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.06337190419435501\n",
            "Train step - Step 3370, Loss 0.06923902034759521\n",
            "Train step - Step 3380, Loss 0.0682426244020462\n",
            "Train step - Step 3390, Loss 0.0659913420677185\n",
            "Train step - Step 3400, Loss 0.0750669464468956\n",
            "Train epoch - Accuracy: 0.8280575539568346 Loss: 0.06662913817295925 Corrects: 5755\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.06196798011660576\n",
            "Train step - Step 3420, Loss 0.06485550850629807\n",
            "Train step - Step 3430, Loss 0.06348340958356857\n",
            "Train step - Step 3440, Loss 0.06361221522092819\n",
            "Train step - Step 3450, Loss 0.06026061251759529\n",
            "Train step - Step 3460, Loss 0.06681904941797256\n",
            "Train epoch - Accuracy: 0.8302158273381295 Loss: 0.06655740247355948 Corrects: 5770\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.072975292801857\n",
            "Train step - Step 3480, Loss 0.05933220311999321\n",
            "Train step - Step 3490, Loss 0.059193920344114304\n",
            "Train step - Step 3500, Loss 0.06796402484178543\n",
            "Train step - Step 3510, Loss 0.062430489808321\n",
            "Train epoch - Accuracy: 0.8309352517985612 Loss: 0.06638119916049696 Corrects: 5775\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.0608590729534626\n",
            "Train step - Step 3530, Loss 0.07800799608230591\n",
            "Train step - Step 3540, Loss 0.05905953049659729\n",
            "Train step - Step 3550, Loss 0.06221465393900871\n",
            "Train step - Step 3560, Loss 0.07207079231739044\n",
            "Train step - Step 3570, Loss 0.06744766235351562\n",
            "Train epoch - Accuracy: 0.837410071942446 Loss: 0.06568886065440212 Corrects: 5820\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.06388998031616211\n",
            "Train step - Step 3590, Loss 0.057940542697906494\n",
            "Train step - Step 3600, Loss 0.0702398270368576\n",
            "Train step - Step 3610, Loss 0.06380406767129898\n",
            "Train step - Step 3620, Loss 0.06191299483180046\n",
            "Train epoch - Accuracy: 0.8329496402877697 Loss: 0.06560417303507277 Corrects: 5789\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.05899148806929588\n",
            "Train step - Step 3640, Loss 0.060915183275938034\n",
            "Train step - Step 3650, Loss 0.06338294595479965\n",
            "Train step - Step 3660, Loss 0.06035848334431648\n",
            "Train step - Step 3670, Loss 0.06344914436340332\n",
            "Train step - Step 3680, Loss 0.0763983279466629\n",
            "Train epoch - Accuracy: 0.837841726618705 Loss: 0.06511312590871783 Corrects: 5823\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.06606154143810272\n",
            "Train step - Step 3700, Loss 0.06260736286640167\n",
            "Train step - Step 3710, Loss 0.0638287290930748\n",
            "Train step - Step 3720, Loss 0.07076075673103333\n",
            "Train step - Step 3730, Loss 0.06363384425640106\n",
            "Train epoch - Accuracy: 0.8356834532374101 Loss: 0.06538867134413273 Corrects: 5808\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.0670810341835022\n",
            "Train step - Step 3750, Loss 0.062466394156217575\n",
            "Train step - Step 3760, Loss 0.06771600246429443\n",
            "Train step - Step 3770, Loss 0.06633460521697998\n",
            "Train step - Step 3780, Loss 0.05785433202981949\n",
            "Train step - Step 3790, Loss 0.05846219137310982\n",
            "Train epoch - Accuracy: 0.8364028776978417 Loss: 0.06521887555396814 Corrects: 5813\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.06871316581964493\n",
            "Train step - Step 3810, Loss 0.06317370384931564\n",
            "Train step - Step 3820, Loss 0.060373373329639435\n",
            "Train step - Step 3830, Loss 0.05292872339487076\n",
            "Train step - Step 3840, Loss 0.06337867677211761\n",
            "Train epoch - Accuracy: 0.8332374100719424 Loss: 0.06565725827388626 Corrects: 5791\n",
            "Training finished in 391.5239861011505 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60]\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238246690>\n",
            "Constructing exemplars of class 95\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [32186, 26713, 38023, 42809, 45990, 42270, 3547, 22104, 34010, 46326, 33476, 44573, 24463, 44400, 16062, 35680, 4692, 33093, 6109, 41274, 34789, 43747, 31116, 36160, 6485, 36315, 1046, 9398, 12520, 151, 8536, 25703, 6860, 18987, 41953, 7551, 22863, 8358, 7046, 46579, 8166, 24552, 12686, 29929, 6505, 3321, 9249, 32678, 20478, 26438, 9249, 19353, 48605, 21403, 36988, 261, 9422, 20410, 38023, 5657, 23873, 2028, 44254, 6501, 15488, 49037, 8148, 1246, 15850, 1304, 44254, 19214, 49109, 42728, 23225, 20104, 38072, 49683, 29397, 4466, 48605, 16946, 24974, 47545, 31610, 30193, 15850, 28993, 37094, 29176, 22863, 4692, 45775, 32001, 4281, 46459, 15696, 49059, 14016, 1568]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238250250>\n",
            "Constructing exemplars of class 59\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [23545, 45838, 44095, 39867, 20216, 19474, 37533, 21624, 49401, 36223, 982, 37719, 29305, 20481, 25142, 46102, 8931, 8676, 5672, 37490, 8113, 22436, 45468, 29352, 5848, 28258, 16001, 17943, 48988, 28526, 16288, 40888, 49789, 42330, 27824, 34455, 25774, 18865, 19474, 24261, 15079, 2834, 45659, 5848, 1927, 42060, 14413, 6571, 6283, 39867, 28724, 31607, 37228, 21624, 14671, 46599, 17943, 43511, 35106, 34163, 47226, 13105, 13249, 32614, 38064, 14358, 20491, 6100, 13873, 849, 14478, 27824, 26113, 27601, 48062, 3488, 16709, 45339, 16295, 37719, 15381, 7118, 14376, 47268, 16295, 41678, 31150, 28208, 34504, 5504, 32778, 31097, 5277, 33936, 38957, 16446, 12254, 38064, 8271, 44581]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224140d290>\n",
            "Constructing exemplars of class 58\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [37824, 12360, 8420, 32617, 37677, 9675, 8635, 35887, 28288, 1868, 33374, 17397, 16728, 21254, 49640, 4060, 15070, 7012, 33641, 40195, 46080, 25374, 17308, 40385, 2937, 38588, 21533, 15471, 23558, 4204, 3247, 21251, 46859, 6213, 17912, 806, 32988, 40195, 1276, 12305, 22875, 15921, 46859, 19550, 38641, 4843, 27657, 38394, 2436, 31306, 14966, 19340, 14563, 26996, 2646, 30320, 1364, 4898, 4470, 46080, 2646, 22402, 30648, 5971, 9540, 48857, 47002, 31122, 42827, 16657, 10885, 38296, 5611, 3916, 37709, 46535, 27465, 43421, 32867, 41401, 22831, 34107, 3450, 27231, 6423, 16795, 36494, 27665, 26743, 37653, 34611, 46406, 15142, 49640, 12219, 38663, 25074, 6171, 48857, 10685]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223821bb10>\n",
            "Constructing exemplars of class 34\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [17608, 23639, 10770, 7312, 9271, 8406, 40004, 17632, 8112, 28868, 24929, 17839, 35219, 43022, 28339, 36114, 20437, 43552, 8406, 18397, 27418, 38997, 5064, 17549, 8949, 4556, 19416, 3773, 42158, 49973, 1938, 41170, 38174, 22132, 8494, 40789, 19437, 21205, 11594, 1394, 31527, 42036, 31794, 49978, 18275, 3283, 8396, 32312, 3181, 3512, 1447, 17581, 16662, 18727, 23992, 12228, 14310, 49973, 45164, 42049, 42189, 36177, 43683, 16299, 21112, 40789, 26595, 8494, 19311, 31783, 22477, 13481, 28868, 20461, 1938, 12267, 35708, 4001, 13758, 3868, 120, 30789, 40779, 26432, 22089, 27238, 33555, 10911, 9271, 25992, 21662, 11804, 13758, 28439, 1938, 45960, 19440, 16944, 14459, 28672]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2231718b10>\n",
            "Constructing exemplars of class 81\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [7373, 33201, 17952, 19383, 26521, 41277, 21025, 29807, 15624, 41112, 37599, 8559, 9254, 33016, 37741, 4954, 32660, 9281, 24275, 12683, 20487, 14338, 41058, 31727, 22344, 12346, 39790, 20328, 24481, 34515, 24965, 6258, 22076, 13354, 29415, 22144, 22512, 10584, 12751, 15149, 1429, 4904, 36593, 36497, 16499, 186, 43371, 14660, 37917, 9275, 25550, 21118, 11575, 24640, 20967, 45181, 7533, 3802, 21829, 39569, 12246, 25283, 29807, 48255, 14621, 47085, 20081, 29471, 30365, 18666, 26745, 17913, 48259, 7208, 41058, 12588, 11885, 19430, 19146, 39819, 41333, 33718, 29751, 33428, 9348, 48259, 35097, 5539, 9696, 20657, 37252, 29408, 21025, 38668, 11675, 675, 15624, 23286, 20470, 12746]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a95a10>\n",
            "Constructing exemplars of class 49\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [40611, 44090, 45236, 26777, 14246, 23516, 45650, 17241, 18112, 45369, 17776, 47850, 33744, 13062, 12860, 27122, 34446, 17524, 36563, 14536, 17737, 29462, 31721, 46595, 31350, 47029, 27276, 38402, 30596, 34982, 34092, 14890, 2265, 32239, 14929, 23516, 2920, 515, 20833, 27289, 34303, 11517, 29567, 2372, 36073, 24605, 7209, 1140, 21923, 41320, 23625, 18996, 36643, 6747, 16363, 23913, 28384, 2219, 46973, 14929, 41836, 33565, 46931, 7785, 42740, 29777, 8498, 18189, 3638, 22247, 27888, 46767, 23625, 18112, 19048, 204, 23342, 6287, 34801, 5529, 25737, 29567, 3750, 12189, 2294, 49966, 27289, 38731, 1542, 35550, 42904, 11517, 5061, 8181, 31578, 22292, 467, 6287, 3638, 43738]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232f37510>\n",
            "Constructing exemplars of class 13\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [20536, 11395, 35183, 12091, 29418, 38523, 33916, 49198, 29916, 10130, 47791, 28335, 39012, 38685, 35365, 49326, 27555, 24097, 35201, 7422, 30495, 40220, 786, 24313, 18481, 12655, 38685, 43496, 47819, 35821, 5411, 37720, 31336, 19445, 40043, 40047, 49326, 16607, 7509, 34252, 32936, 28765, 41936, 28334, 240, 3803, 5108, 29916, 24621, 39918, 24202, 29766, 36769, 46129, 21373, 18285, 7509, 12379, 3798, 41944, 7397, 27002, 5243, 2471, 3300, 32018, 40139, 38685, 240, 39012, 19445, 33026, 18334, 32257, 13536, 10298, 46223, 12900, 39903, 34191, 41936, 33845, 33287, 19537, 15928, 24534, 21193, 10848, 40220, 45728, 10735, 3908, 5175, 16053, 46223, 33048, 41936, 13171, 20524, 40043]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22437a4090>\n",
            "Constructing exemplars of class 88\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [18125, 3372, 25616, 10043, 47259, 410, 39507, 32080, 31666, 19590, 25659, 21681, 1766, 5859, 20173, 37988, 23488, 42247, 5092, 31216, 18631, 34079, 48573, 3992, 42150, 48057, 8729, 30573, 19558, 18424, 10554, 20369, 30652, 10708, 31529, 41775, 10636, 22995, 23703, 49375, 27419, 22568, 36967, 23220, 37571, 48977, 17428, 27437, 48434, 18283, 27437, 42342, 26433, 3494, 25097, 45933, 28487, 34538, 37945, 44312, 19590, 31666, 10708, 39639, 11344, 2680, 49131, 1638, 26724, 31872, 17206, 11935, 8284, 34045, 7691, 14905, 29762, 38485, 45452, 18258, 39639, 49844, 38821, 15437, 39998, 337, 41869, 44936, 25092, 33890, 32562, 32702, 1215, 41108, 33302, 42342, 40569, 5859, 19775, 27300]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a95a10>\n",
            "Constructing exemplars of class 68\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [12280, 15573, 21958, 47878, 11225, 9030, 16427, 29487, 43742, 28450, 7778, 21198, 38968, 2450, 26732, 31044, 39862, 19692, 17868, 16168, 10601, 209, 44458, 17859, 32181, 46100, 45044, 18947, 31919, 31749, 37974, 31765, 7963, 30341, 20256, 7915, 26327, 39615, 28655, 19380, 45993, 33990, 47799, 31765, 32628, 33189, 9347, 24338, 3170, 37930, 12826, 6449, 4361, 36856, 7915, 45020, 20734, 28217, 5934, 21628, 28450, 4361, 22454, 37932, 17950, 42956, 45613, 39173, 47074, 7963, 45020, 7915, 37274, 19149, 8137, 31953, 44939, 25899, 27702, 17950, 46305, 4930, 36010, 24915, 32918, 10351, 28443, 12920, 17614, 13132, 41640, 32693, 15864, 23689, 3845, 22188, 44639, 23724, 932, 37253]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232feeb90>\n",
            "Constructing exemplars of class 60\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [34786, 28818, 15514, 3685, 3872, 38494, 2389, 34008, 39924, 7364, 34100, 41753, 29787, 32347, 49469, 37184, 11340, 42186, 38947, 11013, 13067, 20062, 321, 1335, 48517, 20729, 23440, 4172, 23213, 997, 7814, 6904, 29787, 29843, 29774, 3674, 23484, 27501, 31284, 39410, 35494, 25424, 18485, 25648, 23213, 19902, 44937, 14350, 27836, 46939, 35234, 25766, 27545, 23267, 34587, 39957, 20752, 24959, 12940, 35461, 33985, 39171, 24862, 27635, 36949, 38662, 30095, 18047, 32966, 16305, 23195, 25237, 38947, 9301, 33985, 8694, 34100, 17797, 18379, 30018, 10801, 8295, 17604, 18246, 2354, 16794, 34954, 28793, 4172, 45827, 27836, 20798, 39241, 21800, 4465, 25213, 17543, 5795, 34587, 3685]\n",
            "x train:  [-0.12761475 -0.15750642 -0.23773655 -0.08929659 -0.0321173  -0.2115917\n",
            " -0.08551868 -0.144941   -0.05664165 -0.0589585  -0.16049495 -0.46802476\n",
            " -0.10960289  0.2163591  -0.23706165 -0.3072646  -0.30117854 -0.23612618\n",
            " -0.2998108  -0.3507351 ]\n",
            "y_train:  [tensor([34]), tensor([78]), tensor([50]), tensor([50]), tensor([27]), tensor([13]), tensor([86]), tensor([58]), tensor([69]), tensor([49]), tensor([34]), tensor([27]), tensor([60]), tensor([82]), tensor([82]), tensor([49]), tensor([58]), tensor([97]), tensor([82]), tensor([57]), tensor([86]), tensor([69]), tensor([97]), tensor([95]), tensor([13]), tensor([30]), tensor([97]), tensor([78]), tensor([34]), tensor([25]), tensor([69]), tensor([58]), tensor([69]), tensor([78]), tensor([68]), tensor([86]), tensor([27]), tensor([25]), tensor([69]), tensor([30]), tensor([69]), tensor([59]), tensor([78]), tensor([69]), tensor([78]), tensor([30]), tensor([88]), tensor([95]), tensor([13]), tensor([25]), tensor([68]), tensor([58]), tensor([82]), tensor([57]), tensor([97]), tensor([13]), tensor([13]), tensor([68]), tensor([95]), tensor([27]), tensor([27]), tensor([60]), tensor([97]), tensor([95]), tensor([82]), tensor([69]), tensor([50]), tensor([59]), tensor([13]), tensor([57]), tensor([30]), tensor([97]), tensor([97]), tensor([49]), tensor([60]), tensor([81]), tensor([25]), tensor([97]), tensor([13]), tensor([95]), tensor([50]), tensor([25]), tensor([88]), tensor([69]), tensor([25]), tensor([50]), tensor([95]), tensor([50]), tensor([86]), tensor([25]), tensor([86]), tensor([82]), tensor([50]), tensor([58]), tensor([81]), tensor([97]), tensor([57]), tensor([82]), tensor([69]), tensor([27]), tensor([27]), tensor([69]), tensor([59]), tensor([34]), tensor([25]), tensor([88]), tensor([50]), tensor([49]), tensor([57]), tensor([50]), tensor([58]), tensor([27]), tensor([27]), tensor([58]), tensor([82]), tensor([82]), tensor([68]), tensor([34]), tensor([82]), tensor([59]), tensor([97]), tensor([57]), tensor([78]), tensor([68]), tensor([95]), tensor([25]), tensor([88]), tensor([59]), tensor([49]), tensor([97]), tensor([78]), tensor([27]), tensor([86]), tensor([86]), tensor([88]), tensor([78]), tensor([81]), tensor([34]), tensor([50]), tensor([57]), tensor([25]), tensor([13]), tensor([25]), tensor([78]), tensor([25]), tensor([86]), tensor([68]), tensor([49]), tensor([95]), tensor([27]), tensor([59]), tensor([27]), tensor([59]), tensor([88]), tensor([60]), tensor([30]), tensor([27]), tensor([97]), tensor([60]), tensor([13]), tensor([13]), tensor([78]), tensor([27]), tensor([69]), tensor([13]), tensor([68]), tensor([88]), tensor([50]), tensor([13]), tensor([82]), tensor([58]), tensor([81]), tensor([27]), tensor([60]), tensor([58]), tensor([82]), tensor([86]), tensor([30]), tensor([13]), tensor([69]), tensor([59]), tensor([30]), tensor([49]), tensor([69]), tensor([68]), tensor([78]), tensor([95]), tensor([13]), tensor([58]), tensor([78]), tensor([59]), tensor([49]), tensor([86]), tensor([88]), tensor([88]), tensor([86]), tensor([58]), tensor([49]), tensor([86]), tensor([25]), tensor([49]), tensor([57]), tensor([78]), tensor([25]), tensor([82]), tensor([49]), tensor([82]), tensor([88]), tensor([86]), tensor([50]), tensor([50]), tensor([25]), tensor([25]), tensor([82]), tensor([59]), tensor([97]), tensor([68]), tensor([25]), tensor([30]), tensor([30]), tensor([27]), tensor([30]), tensor([78]), tensor([82]), tensor([68]), tensor([95]), tensor([30]), tensor([50]), tensor([13]), tensor([68]), tensor([86]), tensor([95]), tensor([95]), tensor([49]), tensor([82]), tensor([57]), tensor([49]), tensor([58]), tensor([69]), tensor([95]), tensor([88]), tensor([81]), tensor([60]), tensor([27]), tensor([50]), tensor([86]), tensor([27]), tensor([69]), tensor([49]), tensor([82]), tensor([97]), tensor([69]), tensor([27]), tensor([88]), tensor([95]), tensor([30]), tensor([27]), tensor([88]), tensor([58]), tensor([49]), tensor([60]), tensor([50]), tensor([25]), tensor([86]), tensor([81]), tensor([25]), tensor([49]), tensor([13]), tensor([81]), tensor([78]), tensor([34]), tensor([59]), tensor([82]), tensor([13]), tensor([88]), tensor([50]), tensor([88]), tensor([25]), tensor([59]), tensor([69]), tensor([30]), tensor([68]), tensor([30]), tensor([69]), tensor([13]), tensor([59]), tensor([88]), tensor([60]), tensor([34]), tensor([81]), tensor([95]), tensor([78]), tensor([58]), tensor([60]), tensor([25]), tensor([86]), tensor([30]), tensor([30]), tensor([58]), tensor([81]), tensor([49]), tensor([58]), tensor([60]), tensor([58]), tensor([82]), tensor([68]), tensor([97]), tensor([25]), tensor([59]), tensor([78]), tensor([25]), tensor([58]), tensor([95]), tensor([81]), tensor([13]), tensor([59]), tensor([81]), tensor([69]), tensor([57]), tensor([50]), tensor([78]), tensor([88]), tensor([86]), tensor([60]), tensor([82]), tensor([58]), tensor([25]), tensor([50]), tensor([69]), tensor([95]), tensor([13]), tensor([30]), tensor([68]), tensor([97]), tensor([68]), tensor([50]), tensor([34]), tensor([30]), tensor([57]), tensor([97]), tensor([86]), tensor([86]), tensor([57]), tensor([57]), tensor([81]), tensor([68]), tensor([95]), tensor([68]), tensor([30]), tensor([86]), tensor([88]), tensor([13]), tensor([97]), tensor([50]), tensor([49]), tensor([49]), tensor([60]), tensor([68]), tensor([50]), tensor([25]), tensor([78]), tensor([78]), tensor([60]), tensor([49]), tensor([49]), tensor([50]), tensor([59]), tensor([59]), tensor([58]), tensor([86]), tensor([49]), tensor([88]), tensor([88]), tensor([97]), tensor([57]), tensor([82]), tensor([57]), tensor([13]), tensor([49]), tensor([49]), tensor([95]), tensor([27]), tensor([95]), tensor([97]), tensor([13]), tensor([59]), tensor([82]), tensor([58]), tensor([30]), tensor([30]), tensor([97]), tensor([60]), tensor([57]), tensor([60]), tensor([58]), tensor([13]), tensor([25]), tensor([34]), tensor([59]), tensor([86]), tensor([81]), tensor([69]), tensor([59]), tensor([49]), tensor([13]), tensor([49]), tensor([25]), tensor([34]), tensor([88]), tensor([13]), tensor([13]), tensor([30]), tensor([60]), tensor([50]), tensor([50]), tensor([57]), tensor([69]), tensor([86]), tensor([97]), tensor([58]), tensor([69]), tensor([68]), tensor([82]), tensor([60]), tensor([86]), tensor([60]), tensor([88]), tensor([88]), tensor([50]), tensor([50]), tensor([49]), tensor([49]), tensor([50]), tensor([81]), tensor([49]), tensor([68]), tensor([69]), tensor([50]), tensor([60]), tensor([58]), tensor([34]), tensor([69]), tensor([59]), tensor([88]), tensor([58]), tensor([97]), tensor([30]), tensor([88]), tensor([49]), tensor([27]), tensor([68]), tensor([82]), tensor([60]), tensor([13]), tensor([58]), tensor([57]), tensor([69]), tensor([34]), tensor([95]), tensor([30]), tensor([57]), tensor([57]), tensor([57]), tensor([34]), tensor([81]), tensor([50]), tensor([50]), tensor([60]), tensor([86]), tensor([81]), tensor([69]), tensor([78]), tensor([27]), tensor([58]), tensor([30]), tensor([95]), tensor([68]), tensor([82]), tensor([30]), tensor([58]), tensor([82]), tensor([81]), tensor([27]), tensor([88]), tensor([81]), tensor([50]), tensor([25]), tensor([13]), tensor([78]), tensor([50]), tensor([27]), tensor([81]), tensor([86]), tensor([58]), tensor([25]), tensor([34]), tensor([30]), tensor([34]), tensor([86]), tensor([59]), tensor([82]), tensor([81]), tensor([49]), tensor([58]), tensor([81]), tensor([27]), tensor([25]), tensor([50]), tensor([57]), tensor([57]), tensor([82]), tensor([25]), tensor([68]), tensor([59]), tensor([34]), tensor([78]), tensor([58]), tensor([69]), tensor([81]), tensor([86]), tensor([68]), tensor([88]), tensor([25]), tensor([97]), tensor([86]), tensor([34]), tensor([86]), tensor([34]), tensor([50]), tensor([68]), tensor([69]), tensor([25]), tensor([30]), tensor([58]), tensor([25]), tensor([50]), tensor([25]), tensor([34]), tensor([58]), tensor([50]), tensor([95]), tensor([69]), tensor([34]), tensor([86]), tensor([27]), tensor([86]), tensor([34]), tensor([30]), tensor([68]), tensor([78]), tensor([88]), tensor([25]), tensor([13]), tensor([27]), tensor([68]), tensor([57]), tensor([69]), tensor([57]), tensor([58]), tensor([34]), tensor([97]), tensor([81]), tensor([78]), tensor([58]), tensor([60]), tensor([50]), tensor([69]), tensor([27]), tensor([57]), tensor([57]), tensor([27]), tensor([88]), tensor([30]), tensor([88]), tensor([34]), tensor([34]), tensor([49]), tensor([97]), tensor([86]), tensor([78]), tensor([58]), tensor([59]), tensor([95]), tensor([58]), tensor([49]), tensor([59]), tensor([57]), tensor([49]), tensor([86]), tensor([50]), tensor([82]), tensor([86]), tensor([68]), tensor([78]), tensor([50]), tensor([81]), tensor([30]), tensor([69]), tensor([34]), tensor([82]), tensor([69]), tensor([27]), tensor([95]), tensor([13]), tensor([49]), tensor([78]), tensor([34]), tensor([27]), tensor([60]), tensor([25]), tensor([58]), tensor([30]), tensor([49]), tensor([97]), tensor([34]), tensor([60]), tensor([95]), tensor([27]), tensor([49]), tensor([13]), tensor([78]), tensor([86]), tensor([59]), tensor([82]), tensor([97]), tensor([95]), tensor([78]), tensor([81]), tensor([50]), tensor([69]), tensor([34]), tensor([68]), tensor([58]), tensor([60]), tensor([78]), tensor([68]), tensor([82]), tensor([34]), tensor([30]), tensor([68]), tensor([60]), tensor([34]), tensor([59]), tensor([50]), tensor([97]), tensor([27]), tensor([13]), tensor([25]), tensor([95]), tensor([30]), tensor([78]), tensor([68]), tensor([88]), tensor([82]), tensor([27]), tensor([30]), tensor([58]), tensor([68]), tensor([81]), tensor([50]), tensor([82]), tensor([34]), tensor([81]), tensor([25]), tensor([88]), tensor([68]), tensor([82]), tensor([95]), tensor([60]), tensor([58]), tensor([25]), tensor([69]), tensor([25]), tensor([49]), tensor([50]), tensor([95]), tensor([68]), tensor([82]), tensor([69]), tensor([13]), tensor([60]), tensor([57]), tensor([60]), tensor([30]), tensor([50]), tensor([34]), tensor([78]), tensor([30]), tensor([27]), tensor([59]), tensor([88]), tensor([97]), tensor([86]), tensor([34]), tensor([50]), tensor([13]), tensor([95]), tensor([59]), tensor([59]), tensor([30]), tensor([57]), tensor([69]), tensor([60]), tensor([30]), tensor([95]), tensor([49]), tensor([60]), tensor([13]), tensor([78]), tensor([30]), tensor([50]), tensor([97]), tensor([58]), tensor([13]), tensor([78]), tensor([68]), tensor([69]), tensor([68]), tensor([50]), tensor([13]), tensor([88]), tensor([57]), tensor([13]), tensor([13]), tensor([59]), tensor([68]), tensor([95]), tensor([50]), tensor([97]), tensor([57]), tensor([13]), tensor([82]), tensor([58]), tensor([86]), tensor([82]), tensor([88]), tensor([30]), tensor([60]), tensor([27]), tensor([50]), tensor([13]), tensor([57]), tensor([58]), tensor([78]), tensor([13]), tensor([13]), tensor([88]), tensor([88]), tensor([49]), tensor([78]), tensor([86]), tensor([13]), tensor([25]), tensor([34]), tensor([69]), tensor([57]), tensor([57]), tensor([49]), tensor([59]), tensor([13]), tensor([13]), tensor([13]), tensor([58]), tensor([49]), tensor([25]), tensor([50]), tensor([59]), tensor([57]), tensor([30]), tensor([50]), tensor([78]), tensor([95]), tensor([34]), tensor([27]), tensor([25]), tensor([81]), tensor([34]), tensor([78]), tensor([97]), tensor([81]), tensor([86]), tensor([59]), tensor([34]), tensor([88]), tensor([25]), tensor([34]), tensor([27]), tensor([69]), tensor([81]), tensor([97]), tensor([58]), tensor([82]), tensor([60]), tensor([68]), tensor([68]), tensor([13]), tensor([60]), tensor([30]), tensor([68]), tensor([30]), tensor([88]), tensor([58]), tensor([13]), tensor([68]), tensor([57]), tensor([30]), tensor([88]), tensor([50]), tensor([81]), tensor([78]), tensor([60]), tensor([57]), tensor([97]), tensor([81]), tensor([95]), tensor([25]), tensor([78]), tensor([86]), tensor([60]), tensor([30]), tensor([69]), tensor([82]), tensor([27]), tensor([34]), tensor([86]), tensor([30]), tensor([59]), tensor([25]), tensor([30]), tensor([81]), tensor([81]), tensor([69]), tensor([82]), tensor([34]), tensor([59]), tensor([97]), tensor([30]), tensor([27]), tensor([88]), tensor([69]), tensor([97]), tensor([88]), tensor([25]), tensor([58]), tensor([68]), tensor([30]), tensor([59]), tensor([81]), tensor([60]), tensor([95]), tensor([95]), tensor([25]), tensor([88]), tensor([25]), tensor([68]), tensor([68]), tensor([81]), tensor([50]), tensor([81]), tensor([59]), tensor([57]), tensor([60]), tensor([34]), tensor([95]), tensor([50]), tensor([57]), tensor([27]), tensor([50]), tensor([57]), tensor([68]), tensor([82]), tensor([78]), tensor([88]), tensor([34]), tensor([34]), tensor([27]), tensor([57]), tensor([82]), tensor([57]), tensor([86]), tensor([13]), tensor([25]), tensor([34]), tensor([59]), tensor([86]), tensor([59]), tensor([82]), tensor([49]), tensor([82]), tensor([57]), tensor([13]), tensor([86]), tensor([58]), tensor([78]), tensor([50]), tensor([49]), tensor([34]), tensor([82]), tensor([88]), tensor([78]), tensor([57]), tensor([82]), tensor([34]), tensor([97]), tensor([13]), tensor([68]), tensor([97]), tensor([86]), tensor([78]), tensor([82]), tensor([27]), tensor([69]), tensor([49]), tensor([95]), tensor([57]), tensor([50]), tensor([30]), tensor([81]), tensor([34]), tensor([13]), tensor([34]), tensor([95]), tensor([25]), tensor([78]), tensor([81]), tensor([69]), tensor([25]), tensor([59]), tensor([60]), tensor([50]), tensor([27]), tensor([68]), tensor([57]), tensor([97]), tensor([59]), tensor([95]), tensor([95]), tensor([30]), tensor([57]), tensor([57]), tensor([69]), tensor([97]), tensor([68]), tensor([69]), tensor([78]), tensor([27]), tensor([27]), tensor([97]), tensor([34]), tensor([25]), tensor([30]), tensor([97]), tensor([82]), tensor([57]), tensor([50]), tensor([25]), tensor([58]), tensor([81]), tensor([50]), tensor([50]), tensor([49]), tensor([57]), tensor([68]), tensor([95]), tensor([68]), tensor([82]), tensor([57]), tensor([13]), tensor([81]), tensor([81]), tensor([88]), tensor([69]), tensor([81]), tensor([13]), tensor([81]), tensor([60]), tensor([49]), tensor([81]), tensor([27]), tensor([57]), tensor([59]), tensor([95]), tensor([68]), tensor([57]), tensor([68]), tensor([30]), tensor([95]), tensor([50]), tensor([49]), tensor([34]), tensor([88]), tensor([25]), tensor([30]), tensor([13]), tensor([95]), tensor([86]), tensor([13]), tensor([34]), tensor([59]), tensor([13]), tensor([30]), tensor([13]), tensor([59]), tensor([57]), tensor([60]), tensor([13]), tensor([34]), tensor([95]), tensor([88]), tensor([59]), tensor([59]), tensor([95]), tensor([34]), tensor([81]), tensor([60]), tensor([81]), tensor([58]), tensor([86]), tensor([88]), tensor([27]), tensor([68]), tensor([97]), tensor([57]), tensor([78]), tensor([86]), tensor([50]), tensor([30]), tensor([88]), tensor([60]), tensor([50]), tensor([13]), tensor([13]), tensor([60]), tensor([95]), tensor([59]), tensor([25]), tensor([57]), tensor([88]), tensor([88]), tensor([25]), tensor([49]), tensor([82]), tensor([27]), tensor([58]), tensor([86]), tensor([97]), tensor([68]), tensor([81]), tensor([59]), tensor([27]), tensor([60]), tensor([50]), tensor([58]), tensor([78]), tensor([78]), tensor([68]), tensor([58]), tensor([88]), tensor([34]), tensor([27]), tensor([95]), tensor([34]), tensor([97]), tensor([69]), tensor([78]), tensor([97]), tensor([49]), tensor([57]), tensor([95]), tensor([97]), tensor([57]), tensor([50]), tensor([27]), tensor([30]), tensor([86]), tensor([81]), tensor([60]), tensor([97]), tensor([13]), tensor([81]), tensor([95]), tensor([49]), tensor([50]), tensor([49]), tensor([59]), tensor([68]), tensor([49]), tensor([30]), tensor([82]), tensor([82]), tensor([82]), tensor([50]), tensor([95]), tensor([86]), tensor([81]), tensor([68]), tensor([60]), tensor([58]), tensor([13]), tensor([50]), tensor([59]), tensor([49]), tensor([34]), tensor([25]), tensor([34]), tensor([81]), tensor([13]), tensor([49]), tensor([59]), tensor([60]), tensor([60]), tensor([25]), tensor([49]), tensor([81]), tensor([30]), tensor([59]), tensor([69]), tensor([69]), tensor([86]), tensor([59]), tensor([49]), tensor([95]), tensor([57]), tensor([50]), tensor([68]), tensor([50]), tensor([69]), tensor([60]), tensor([82]), tensor([97]), tensor([30]), tensor([58]), tensor([69]), tensor([88]), tensor([13]), tensor([34]), tensor([27]), tensor([86]), tensor([68]), tensor([95]), tensor([34]), tensor([95]), tensor([30]), tensor([58]), tensor([25]), tensor([95]), tensor([88]), tensor([49]), tensor([58]), tensor([25]), tensor([30]), tensor([88]), tensor([82]), tensor([13]), tensor([86]), tensor([58]), tensor([68]), tensor([57]), tensor([49]), tensor([78]), tensor([34]), tensor([59]), tensor([97]), tensor([86]), tensor([78]), tensor([88]), tensor([82]), tensor([34]), tensor([88]), tensor([81]), tensor([97]), tensor([59]), tensor([27]), tensor([58]), tensor([60]), tensor([68]), tensor([69]), tensor([60]), tensor([34]), tensor([68]), tensor([81]), tensor([27]), tensor([27]), tensor([49]), tensor([13]), tensor([58]), tensor([69]), tensor([57]), tensor([97]), tensor([30]), tensor([60]), tensor([34]), tensor([25]), tensor([60]), tensor([34]), tensor([49]), tensor([78]), tensor([30]), tensor([13]), tensor([34]), tensor([69]), tensor([34]), tensor([78]), tensor([50]), tensor([27]), tensor([50]), tensor([88]), tensor([58]), tensor([57]), tensor([49]), tensor([30]), tensor([27]), tensor([68]), tensor([88]), tensor([60]), tensor([58]), tensor([25]), tensor([97]), tensor([81]), tensor([57]), tensor([34]), tensor([69]), tensor([95]), tensor([30]), tensor([78]), tensor([60]), tensor([25]), tensor([88]), tensor([69]), tensor([82]), tensor([81]), tensor([34]), tensor([88]), tensor([82]), tensor([86]), tensor([25]), tensor([82]), tensor([49]), tensor([68]), tensor([97]), tensor([49]), tensor([81]), tensor([97]), tensor([95]), tensor([88]), tensor([34]), tensor([68]), tensor([13]), tensor([95]), tensor([68]), tensor([88]), tensor([58]), tensor([58]), tensor([58]), tensor([13]), tensor([97]), tensor([95]), tensor([95]), tensor([34]), tensor([27]), tensor([57]), tensor([97]), tensor([25]), tensor([97]), tensor([81]), tensor([57]), tensor([82]), tensor([78]), tensor([97]), tensor([82]), tensor([69]), tensor([81]), tensor([27]), tensor([27]), tensor([60]), tensor([97]), tensor([78]), tensor([68]), tensor([68]), tensor([97]), tensor([97]), tensor([49]), tensor([69]), tensor([95]), tensor([82]), tensor([57]), tensor([57]), tensor([86]), tensor([25]), tensor([34]), tensor([81]), tensor([57]), tensor([58]), tensor([88]), tensor([86]), tensor([60]), tensor([88]), tensor([78]), tensor([68]), tensor([97]), tensor([50]), tensor([57]), tensor([49]), tensor([68]), tensor([50]), tensor([68]), tensor([69]), tensor([60]), tensor([57]), tensor([49]), tensor([78]), tensor([95]), tensor([49]), tensor([59]), tensor([27]), tensor([49]), tensor([25]), tensor([82]), tensor([57]), tensor([88]), tensor([58]), tensor([78]), tensor([30]), tensor([59]), tensor([86]), tensor([60]), tensor([13]), tensor([57]), tensor([57]), tensor([95]), tensor([68]), tensor([25]), tensor([60]), tensor([78]), tensor([25]), tensor([60]), tensor([69]), tensor([49]), tensor([95]), tensor([34]), tensor([59]), tensor([78]), tensor([78]), tensor([34]), tensor([49]), tensor([86]), tensor([27]), tensor([49]), tensor([88]), tensor([86]), tensor([97]), tensor([78]), tensor([82]), tensor([88]), tensor([68]), tensor([95]), tensor([69]), tensor([95]), tensor([13]), tensor([82]), tensor([69]), tensor([97]), tensor([86]), tensor([68]), tensor([27]), tensor([86]), tensor([57]), tensor([68]), tensor([49]), tensor([78]), tensor([57]), tensor([60]), tensor([50]), tensor([82]), tensor([88]), tensor([86]), tensor([82]), tensor([88]), tensor([34]), tensor([59]), tensor([49]), tensor([81]), tensor([78]), tensor([25]), tensor([88]), tensor([34]), tensor([58]), tensor([34]), tensor([58]), tensor([49]), tensor([81]), tensor([68]), tensor([86]), tensor([82]), tensor([97]), tensor([97]), tensor([95]), tensor([69]), tensor([88]), tensor([57]), tensor([78]), tensor([78]), tensor([95]), tensor([27]), tensor([78]), tensor([88]), tensor([27]), tensor([95]), tensor([49]), tensor([81]), tensor([86]), tensor([59]), tensor([27]), tensor([50]), tensor([95]), tensor([34]), tensor([57]), tensor([97]), tensor([50]), tensor([86]), tensor([58]), tensor([30]), tensor([57]), tensor([82]), tensor([78]), tensor([30]), tensor([68]), tensor([81]), tensor([30]), tensor([57]), tensor([86]), tensor([81]), tensor([49]), tensor([88]), tensor([78]), tensor([30]), tensor([81]), tensor([25]), tensor([88]), tensor([13]), tensor([78]), tensor([57]), tensor([27]), tensor([58]), tensor([78]), tensor([27]), tensor([81]), tensor([81]), tensor([78]), tensor([86]), tensor([34]), tensor([50]), tensor([57]), tensor([97]), tensor([49]), tensor([95]), tensor([13]), tensor([88]), tensor([78]), tensor([69]), tensor([68]), tensor([82]), tensor([97]), tensor([81]), tensor([69]), tensor([60]), tensor([25]), tensor([69]), tensor([82]), tensor([13]), tensor([88]), tensor([81]), tensor([57]), tensor([60]), tensor([59]), tensor([82]), tensor([86]), tensor([78]), tensor([68]), tensor([95]), tensor([82]), tensor([97]), tensor([81]), tensor([58]), tensor([68]), tensor([95]), tensor([30]), tensor([60]), tensor([82]), tensor([59]), tensor([58]), tensor([59]), tensor([69]), tensor([30]), tensor([50]), tensor([27]), tensor([58]), tensor([82]), tensor([60]), tensor([30]), tensor([49]), tensor([86]), tensor([82]), tensor([86]), tensor([81]), tensor([13]), tensor([82]), tensor([78]), tensor([50]), tensor([88]), tensor([59]), tensor([58]), tensor([60]), tensor([69]), tensor([27]), tensor([88]), tensor([58]), tensor([34]), tensor([86]), tensor([97]), tensor([68]), tensor([59]), tensor([69]), tensor([95]), tensor([97]), tensor([59]), tensor([86]), tensor([27]), tensor([27]), tensor([69]), tensor([13]), tensor([60]), tensor([97]), tensor([58]), tensor([27]), tensor([81]), tensor([25]), tensor([97]), tensor([58]), tensor([59]), tensor([30]), tensor([50]), tensor([88]), tensor([59]), tensor([82]), tensor([50]), tensor([25]), tensor([49]), tensor([50]), tensor([34]), tensor([78]), tensor([86]), tensor([60]), tensor([25]), tensor([81]), tensor([13]), tensor([49]), tensor([27]), tensor([30]), tensor([60]), tensor([78]), tensor([95]), tensor([13]), tensor([95]), tensor([82]), tensor([88]), tensor([27]), tensor([69]), tensor([78]), tensor([58]), tensor([78]), tensor([30]), tensor([58]), tensor([50]), tensor([27]), tensor([59]), tensor([69]), tensor([95]), tensor([58]), tensor([57]), tensor([27]), tensor([86]), tensor([78]), tensor([82]), tensor([59]), tensor([78]), tensor([95]), tensor([60]), tensor([95]), tensor([86]), tensor([57]), tensor([30]), tensor([59]), tensor([82]), tensor([50]), tensor([27]), tensor([88]), tensor([82]), tensor([81]), tensor([78]), tensor([49]), tensor([27]), tensor([97]), tensor([95]), tensor([59]), tensor([69]), tensor([68]), tensor([13]), tensor([97]), tensor([27]), tensor([86]), tensor([81]), tensor([81]), tensor([34]), tensor([81]), tensor([27]), tensor([50]), tensor([50]), tensor([57]), tensor([68]), tensor([27]), tensor([27]), tensor([69]), tensor([86]), tensor([60]), tensor([25]), tensor([58]), tensor([57]), tensor([58]), tensor([60]), tensor([69]), tensor([81]), tensor([86]), tensor([59]), tensor([69]), tensor([88]), tensor([81]), tensor([68]), tensor([97]), tensor([59]), tensor([30]), tensor([60]), tensor([97]), tensor([34]), tensor([95]), tensor([30]), tensor([88]), tensor([13]), tensor([97]), tensor([95]), tensor([25]), tensor([97]), tensor([86]), tensor([34]), tensor([60]), tensor([30]), tensor([95]), tensor([25]), tensor([81]), tensor([49]), tensor([57]), tensor([59]), tensor([68]), tensor([88]), tensor([95]), tensor([49]), tensor([60]), tensor([69]), tensor([69]), tensor([86]), tensor([50]), tensor([25]), tensor([81]), tensor([95]), tensor([50]), tensor([25]), tensor([27]), tensor([58]), tensor([34]), tensor([68]), tensor([86]), tensor([49]), tensor([30]), tensor([95]), tensor([27]), tensor([81]), tensor([34]), tensor([58]), tensor([60]), tensor([69]), tensor([13]), tensor([82]), tensor([13]), tensor([57]), tensor([68]), tensor([49]), tensor([30]), tensor([25]), tensor([13]), tensor([58]), tensor([59]), tensor([69]), tensor([81]), tensor([82]), tensor([50]), tensor([81]), tensor([88]), tensor([27]), tensor([97]), tensor([13]), tensor([86]), tensor([58]), tensor([49]), tensor([34]), tensor([60]), tensor([68]), tensor([86]), tensor([58]), tensor([50]), tensor([69]), tensor([97]), tensor([97]), tensor([60]), tensor([88]), tensor([25]), tensor([97]), tensor([82]), tensor([25]), tensor([13]), tensor([27]), tensor([49]), tensor([95]), tensor([82]), tensor([59]), tensor([57]), tensor([57]), tensor([68]), tensor([59]), tensor([25]), tensor([58]), tensor([25]), tensor([78]), tensor([30]), tensor([58]), tensor([60]), tensor([95]), tensor([82]), tensor([57]), tensor([59]), tensor([34]), tensor([88]), tensor([59]), tensor([30]), tensor([27]), tensor([60]), tensor([34]), tensor([13]), tensor([25]), tensor([95]), tensor([49]), tensor([88]), tensor([95]), tensor([30]), tensor([49]), tensor([82]), tensor([81]), tensor([59]), tensor([86]), tensor([68]), tensor([60]), tensor([69]), tensor([13]), tensor([88]), tensor([27]), tensor([60]), tensor([30]), tensor([68]), tensor([88]), tensor([25]), tensor([86]), tensor([86]), tensor([95]), tensor([60]), tensor([88]), tensor([60]), tensor([58]), tensor([57]), tensor([30]), tensor([86]), tensor([34]), tensor([69]), tensor([59]), tensor([81]), tensor([69]), tensor([81]), tensor([13]), tensor([82]), tensor([95]), tensor([59]), tensor([34]), tensor([59]), tensor([57]), tensor([34]), tensor([81]), tensor([78]), tensor([97]), tensor([58]), tensor([59]), tensor([30]), tensor([69]), tensor([81]), tensor([60]), tensor([69]), tensor([57]), tensor([69]), tensor([68]), tensor([27]), tensor([27]), tensor([82]), tensor([68]), tensor([50]), tensor([88]), tensor([69]), tensor([49]), tensor([81]), tensor([30]), tensor([30]), tensor([81]), tensor([78]), tensor([59]), tensor([30]), tensor([50]), tensor([25]), tensor([97]), tensor([82]), tensor([68]), tensor([25]), tensor([58]), tensor([69]), tensor([59]), tensor([82]), tensor([69]), tensor([97]), tensor([34]), tensor([97]), tensor([25]), tensor([81]), tensor([88]), tensor([60]), tensor([13]), tensor([50]), tensor([25]), tensor([30]), tensor([78]), tensor([68]), tensor([59]), tensor([57]), tensor([25]), tensor([78]), tensor([34]), tensor([57]), tensor([78]), tensor([68]), tensor([57]), tensor([69]), tensor([25]), tensor([59]), tensor([49]), tensor([60]), tensor([86]), tensor([97]), tensor([60]), tensor([86]), tensor([69]), tensor([59]), tensor([78]), tensor([49]), tensor([60]), tensor([60]), tensor([57]), tensor([49]), tensor([27]), tensor([95]), tensor([49]), tensor([30]), tensor([50]), tensor([78]), tensor([60]), tensor([88]), tensor([13]), tensor([78]), tensor([49]), tensor([88]), tensor([97]), tensor([86]), tensor([58]), tensor([57]), tensor([81]), tensor([69]), tensor([34]), tensor([97]), tensor([27]), tensor([59]), tensor([88]), tensor([58]), tensor([59]), tensor([86]), tensor([60]), tensor([50]), tensor([69]), tensor([30]), tensor([34]), tensor([27]), tensor([25]), tensor([68]), tensor([27]), tensor([59]), tensor([82]), tensor([95]), tensor([81]), tensor([49]), tensor([82]), tensor([25]), tensor([97]), tensor([13]), tensor([49]), tensor([81]), tensor([59]), tensor([34]), tensor([30]), tensor([59]), tensor([78]), tensor([34]), tensor([58]), tensor([78]), tensor([30]), tensor([82]), tensor([58]), tensor([25]), tensor([78]), tensor([13]), tensor([97]), tensor([13]), tensor([95]), tensor([25]), tensor([50]), tensor([59]), tensor([60]), tensor([86]), tensor([81]), tensor([86]), tensor([78]), tensor([86]), tensor([60]), tensor([25]), tensor([82]), tensor([69]), tensor([95]), tensor([81]), tensor([50]), tensor([69]), tensor([13]), tensor([49]), tensor([59]), tensor([97]), tensor([78]), tensor([13]), tensor([97]), tensor([97]), tensor([27]), tensor([88]), tensor([30]), tensor([60]), tensor([58]), tensor([34]), tensor([13]), tensor([82]), tensor([95]), tensor([88]), tensor([86]), tensor([34]), tensor([60]), tensor([95]), tensor([68]), tensor([59]), tensor([27]), tensor([86]), tensor([59]), tensor([81]), tensor([60]), tensor([30]), tensor([68]), tensor([78]), tensor([27]), tensor([13]), tensor([97]), tensor([49]), tensor([58]), tensor([82]), tensor([97])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.78 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.78\n",
            "TEST ALL:  0.7615\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  3000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 95, 24, 30, 34, 36, 50, 58, 60, 68, 72, 78, 80, 82, 86, 88, 94, 3, 13, 25, 27, 35, 37, 45, 49, 57, 59, 69, 81, 10]\n",
            "TRAIN_SET CLASSES:  [35, 3, 94, 10, 45, 37, 80, 72, 36, 24]\n",
            "VALIDATION CLASSES:  [45, 37, 36, 35, 94, 24, 80, 10, 72, 3]\n",
            "GROUP:  3\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [35, 3, 94, 10, 45, 37, 80, 72, 36, 24]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  30\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.3655283749103546\n",
            "Train step - Step 10, Loss 0.1471903920173645\n",
            "Train step - Step 20, Loss 0.13680699467658997\n",
            "Train step - Step 30, Loss 0.13708963990211487\n",
            "Train step - Step 40, Loss 0.1262548714876175\n",
            "Train step - Step 50, Loss 0.13335397839546204\n",
            "Train epoch - Accuracy: 0.27784172661870504 Loss: 0.1551868379716393 Corrects: 1931\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12387491017580032\n",
            "Train step - Step 70, Loss 0.12180088460445404\n",
            "Train step - Step 80, Loss 0.11773530393838882\n",
            "Train step - Step 90, Loss 0.12467190623283386\n",
            "Train step - Step 100, Loss 0.12684659659862518\n",
            "Train epoch - Accuracy: 0.33913669064748203 Loss: 0.12049913643933029 Corrects: 2357\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11842267215251923\n",
            "Train step - Step 120, Loss 0.11390971392393112\n",
            "Train step - Step 130, Loss 0.11250793188810349\n",
            "Train step - Step 140, Loss 0.12058088183403015\n",
            "Train step - Step 150, Loss 0.1177511215209961\n",
            "Train step - Step 160, Loss 0.11764604598283768\n",
            "Train epoch - Accuracy: 0.3543884892086331 Loss: 0.1172574834686389 Corrects: 2463\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.11415193229913712\n",
            "Train step - Step 180, Loss 0.10946138203144073\n",
            "Train step - Step 190, Loss 0.11654669791460037\n",
            "Train step - Step 200, Loss 0.10938170552253723\n",
            "Train step - Step 210, Loss 0.10694452375173569\n",
            "Train epoch - Accuracy: 0.3912230215827338 Loss: 0.11464503289555474 Corrects: 2719\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.12029964476823807\n",
            "Train step - Step 230, Loss 0.10585962235927582\n",
            "Train step - Step 240, Loss 0.10988374054431915\n",
            "Train step - Step 250, Loss 0.1186482384800911\n",
            "Train step - Step 260, Loss 0.12558776140213013\n",
            "Train step - Step 270, Loss 0.10924842953681946\n",
            "Train epoch - Accuracy: 0.4116546762589928 Loss: 0.11270299019787809 Corrects: 2861\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.11761637032032013\n",
            "Train step - Step 290, Loss 0.10602931678295135\n",
            "Train step - Step 300, Loss 0.11248591542243958\n",
            "Train step - Step 310, Loss 0.11223272979259491\n",
            "Train step - Step 320, Loss 0.11122937500476837\n",
            "Train epoch - Accuracy: 0.4251798561151079 Loss: 0.11163229740780892 Corrects: 2955\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.1139887124300003\n",
            "Train step - Step 340, Loss 0.10825194418430328\n",
            "Train step - Step 350, Loss 0.11099588125944138\n",
            "Train step - Step 360, Loss 0.11344196647405624\n",
            "Train step - Step 370, Loss 0.1148267388343811\n",
            "Train step - Step 380, Loss 0.10766288638114929\n",
            "Train epoch - Accuracy: 0.4389928057553957 Loss: 0.11020940079320249 Corrects: 3051\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.1028960794210434\n",
            "Train step - Step 400, Loss 0.11122769117355347\n",
            "Train step - Step 410, Loss 0.11673148721456528\n",
            "Train step - Step 420, Loss 0.11290652304887772\n",
            "Train step - Step 430, Loss 0.10281186550855637\n",
            "Train epoch - Accuracy: 0.45366906474820146 Loss: 0.10975612811476207 Corrects: 3153\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.111140176653862\n",
            "Train step - Step 450, Loss 0.10554233938455582\n",
            "Train step - Step 460, Loss 0.10790922492742538\n",
            "Train step - Step 470, Loss 0.11385266482830048\n",
            "Train step - Step 480, Loss 0.10535689443349838\n",
            "Train step - Step 490, Loss 0.10356055945158005\n",
            "Train epoch - Accuracy: 0.4660431654676259 Loss: 0.1083947665249701 Corrects: 3239\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10899171233177185\n",
            "Train step - Step 510, Loss 0.1006239503622055\n",
            "Train step - Step 520, Loss 0.11163724213838577\n",
            "Train step - Step 530, Loss 0.11440769582986832\n",
            "Train step - Step 540, Loss 0.11593274027109146\n",
            "Train epoch - Accuracy: 0.4753956834532374 Loss: 0.10845811480026452 Corrects: 3304\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.09824761003255844\n",
            "Train step - Step 560, Loss 0.11270561069250107\n",
            "Train step - Step 570, Loss 0.10148702561855316\n",
            "Train step - Step 580, Loss 0.10986015945672989\n",
            "Train step - Step 590, Loss 0.10251349955797195\n",
            "Train step - Step 600, Loss 0.09963274002075195\n",
            "Train epoch - Accuracy: 0.4856115107913669 Loss: 0.10724863022780247 Corrects: 3375\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.11300929635763168\n",
            "Train step - Step 620, Loss 0.10432419180870056\n",
            "Train step - Step 630, Loss 0.10660799592733383\n",
            "Train step - Step 640, Loss 0.10557826608419418\n",
            "Train step - Step 650, Loss 0.10019223392009735\n",
            "Train epoch - Accuracy: 0.49151079136690645 Loss: 0.10647151007283506 Corrects: 3416\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10154450684785843\n",
            "Train step - Step 670, Loss 0.11291321367025375\n",
            "Train step - Step 680, Loss 0.10507655888795853\n",
            "Train step - Step 690, Loss 0.10679423809051514\n",
            "Train step - Step 700, Loss 0.09995181113481522\n",
            "Train step - Step 710, Loss 0.11621768027544022\n",
            "Train epoch - Accuracy: 0.5004316546762589 Loss: 0.10659612375412056 Corrects: 3478\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.1129692941904068\n",
            "Train step - Step 730, Loss 0.10840572416782379\n",
            "Train step - Step 740, Loss 0.10696452856063843\n",
            "Train step - Step 750, Loss 0.1134609580039978\n",
            "Train step - Step 760, Loss 0.09605546295642853\n",
            "Train epoch - Accuracy: 0.5148201438848921 Loss: 0.10538749699755538 Corrects: 3578\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10500804334878922\n",
            "Train step - Step 780, Loss 0.10041309148073196\n",
            "Train step - Step 790, Loss 0.10621672123670578\n",
            "Train step - Step 800, Loss 0.09452123194932938\n",
            "Train step - Step 810, Loss 0.1046031191945076\n",
            "Train step - Step 820, Loss 0.10263539850711823\n",
            "Train epoch - Accuracy: 0.5135251798561151 Loss: 0.10535967927184894 Corrects: 3569\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.10115336626768112\n",
            "Train step - Step 840, Loss 0.10726671665906906\n",
            "Train step - Step 850, Loss 0.10240891575813293\n",
            "Train step - Step 860, Loss 0.10599426180124283\n",
            "Train step - Step 870, Loss 0.10086945444345474\n",
            "Train epoch - Accuracy: 0.5178417266187051 Loss: 0.10457348813470319 Corrects: 3599\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.10069550573825836\n",
            "Train step - Step 890, Loss 0.1000175029039383\n",
            "Train step - Step 900, Loss 0.11421291530132294\n",
            "Train step - Step 910, Loss 0.10426384955644608\n",
            "Train step - Step 920, Loss 0.11191707104444504\n",
            "Train step - Step 930, Loss 0.09919759631156921\n",
            "Train epoch - Accuracy: 0.523453237410072 Loss: 0.10382477703497564 Corrects: 3638\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.09736958891153336\n",
            "Train step - Step 950, Loss 0.10539144277572632\n",
            "Train step - Step 960, Loss 0.11228587478399277\n",
            "Train step - Step 970, Loss 0.10139790177345276\n",
            "Train step - Step 980, Loss 0.09946922957897186\n",
            "Train epoch - Accuracy: 0.5372661870503597 Loss: 0.10308113813614674 Corrects: 3734\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.11096440255641937\n",
            "Train step - Step 1000, Loss 0.10394163429737091\n",
            "Train step - Step 1010, Loss 0.09588644653558731\n",
            "Train step - Step 1020, Loss 0.10948914289474487\n",
            "Train step - Step 1030, Loss 0.09932424128055573\n",
            "Train step - Step 1040, Loss 0.09465715289115906\n",
            "Train epoch - Accuracy: 0.541726618705036 Loss: 0.1027594394070639 Corrects: 3765\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.09919654577970505\n",
            "Train step - Step 1060, Loss 0.10855916887521744\n",
            "Train step - Step 1070, Loss 0.09877190738916397\n",
            "Train step - Step 1080, Loss 0.10317105054855347\n",
            "Train step - Step 1090, Loss 0.10234267264604568\n",
            "Train epoch - Accuracy: 0.5415827338129496 Loss: 0.10174281511804183 Corrects: 3764\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10130622237920761\n",
            "Train step - Step 1110, Loss 0.09804131835699081\n",
            "Train step - Step 1120, Loss 0.09498488157987595\n",
            "Train step - Step 1130, Loss 0.09302910417318344\n",
            "Train step - Step 1140, Loss 0.10222765058279037\n",
            "Train step - Step 1150, Loss 0.10424795001745224\n",
            "Train epoch - Accuracy: 0.5483453237410072 Loss: 0.10219252356093564 Corrects: 3811\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.1056530624628067\n",
            "Train step - Step 1170, Loss 0.11063896119594574\n",
            "Train step - Step 1180, Loss 0.09998775273561478\n",
            "Train step - Step 1190, Loss 0.10549645870923996\n",
            "Train step - Step 1200, Loss 0.09957142919301987\n",
            "Train epoch - Accuracy: 0.5509352517985612 Loss: 0.10219472217474053 Corrects: 3829\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.09297382831573486\n",
            "Train step - Step 1220, Loss 0.10460666567087173\n",
            "Train step - Step 1230, Loss 0.09586283564567566\n",
            "Train step - Step 1240, Loss 0.1017710492014885\n",
            "Train step - Step 1250, Loss 0.10229744017124176\n",
            "Train step - Step 1260, Loss 0.10842575132846832\n",
            "Train epoch - Accuracy: 0.5627338129496403 Loss: 0.10110514364439807 Corrects: 3911\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.09625657647848129\n",
            "Train step - Step 1280, Loss 0.10300273448228836\n",
            "Train step - Step 1290, Loss 0.10354281216859818\n",
            "Train step - Step 1300, Loss 0.096091628074646\n",
            "Train step - Step 1310, Loss 0.10330094397068024\n",
            "Train epoch - Accuracy: 0.5690647482014388 Loss: 0.10057311272235225 Corrects: 3955\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.09420546889305115\n",
            "Train step - Step 1330, Loss 0.09460929781198502\n",
            "Train step - Step 1340, Loss 0.11530350148677826\n",
            "Train step - Step 1350, Loss 0.09786470234394073\n",
            "Train step - Step 1360, Loss 0.09552475810050964\n",
            "Train step - Step 1370, Loss 0.09297427535057068\n",
            "Train epoch - Accuracy: 0.5748201438848921 Loss: 0.10006986922926182 Corrects: 3995\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.09906842559576035\n",
            "Train step - Step 1390, Loss 0.10348088294267654\n",
            "Train step - Step 1400, Loss 0.10386592149734497\n",
            "Train step - Step 1410, Loss 0.1063530445098877\n",
            "Train step - Step 1420, Loss 0.10246821492910385\n",
            "Train epoch - Accuracy: 0.5743884892086331 Loss: 0.10046292179136825 Corrects: 3992\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.09808224439620972\n",
            "Train step - Step 1440, Loss 0.103947214782238\n",
            "Train step - Step 1450, Loss 0.1048632487654686\n",
            "Train step - Step 1460, Loss 0.10153444856405258\n",
            "Train step - Step 1470, Loss 0.09233013540506363\n",
            "Train step - Step 1480, Loss 0.09243472665548325\n",
            "Train epoch - Accuracy: 0.5831654676258993 Loss: 0.09981255408885667 Corrects: 4053\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.0955040380358696\n",
            "Train step - Step 1500, Loss 0.09586256742477417\n",
            "Train step - Step 1510, Loss 0.10250281542539597\n",
            "Train step - Step 1520, Loss 0.10302609205245972\n",
            "Train step - Step 1530, Loss 0.10499188303947449\n",
            "Train epoch - Accuracy: 0.5860431654676259 Loss: 0.09941293252672223 Corrects: 4073\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.094258151948452\n",
            "Train step - Step 1550, Loss 0.09078412503004074\n",
            "Train step - Step 1560, Loss 0.10212531685829163\n",
            "Train step - Step 1570, Loss 0.09887834638357162\n",
            "Train step - Step 1580, Loss 0.10115248709917068\n",
            "Train step - Step 1590, Loss 0.10857584327459335\n",
            "Train epoch - Accuracy: 0.5928057553956835 Loss: 0.0998282300611194 Corrects: 4120\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.09855499863624573\n",
            "Train step - Step 1610, Loss 0.1116127222776413\n",
            "Train step - Step 1620, Loss 0.10459968447685242\n",
            "Train step - Step 1630, Loss 0.09329255670309067\n",
            "Train step - Step 1640, Loss 0.09954743087291718\n",
            "Train epoch - Accuracy: 0.6030215827338129 Loss: 0.09812683506406468 Corrects: 4191\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.09304775297641754\n",
            "Train step - Step 1660, Loss 0.09510088711977005\n",
            "Train step - Step 1670, Loss 0.10076016187667847\n",
            "Train step - Step 1680, Loss 0.09753479063510895\n",
            "Train step - Step 1690, Loss 0.0993725061416626\n",
            "Train step - Step 1700, Loss 0.09557796269655228\n",
            "Train epoch - Accuracy: 0.5992805755395684 Loss: 0.09778589696335278 Corrects: 4165\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.0976453423500061\n",
            "Train step - Step 1720, Loss 0.09921053797006607\n",
            "Train step - Step 1730, Loss 0.08726401627063751\n",
            "Train step - Step 1740, Loss 0.09796535968780518\n",
            "Train step - Step 1750, Loss 0.09303759783506393\n",
            "Train epoch - Accuracy: 0.6050359712230216 Loss: 0.09779588385237206 Corrects: 4205\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.09382766485214233\n",
            "Train step - Step 1770, Loss 0.09058260172605515\n",
            "Train step - Step 1780, Loss 0.10241126269102097\n",
            "Train step - Step 1790, Loss 0.0931248888373375\n",
            "Train step - Step 1800, Loss 0.10469073057174683\n",
            "Train step - Step 1810, Loss 0.09437908232212067\n",
            "Train epoch - Accuracy: 0.6027338129496402 Loss: 0.09687381631178822 Corrects: 4189\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.10298492014408112\n",
            "Train step - Step 1830, Loss 0.09861698746681213\n",
            "Train step - Step 1840, Loss 0.09226816147565842\n",
            "Train step - Step 1850, Loss 0.10008730739355087\n",
            "Train step - Step 1860, Loss 0.09097132831811905\n",
            "Train epoch - Accuracy: 0.6184172661870504 Loss: 0.09586998703454039 Corrects: 4298\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.09973586350679398\n",
            "Train step - Step 1880, Loss 0.09028739482164383\n",
            "Train step - Step 1890, Loss 0.09504267573356628\n",
            "Train step - Step 1900, Loss 0.09402509033679962\n",
            "Train step - Step 1910, Loss 0.10027351975440979\n",
            "Train step - Step 1920, Loss 0.09207746386528015\n",
            "Train epoch - Accuracy: 0.6224460431654676 Loss: 0.09673701241290826 Corrects: 4326\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.09366109222173691\n",
            "Train step - Step 1940, Loss 0.09102505445480347\n",
            "Train step - Step 1950, Loss 0.10456205904483795\n",
            "Train step - Step 1960, Loss 0.0913504809141159\n",
            "Train step - Step 1970, Loss 0.0922856405377388\n",
            "Train epoch - Accuracy: 0.6271942446043165 Loss: 0.0964507012508756 Corrects: 4359\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.09633725881576538\n",
            "Train step - Step 1990, Loss 0.08794169127941132\n",
            "Train step - Step 2000, Loss 0.09108951687812805\n",
            "Train step - Step 2010, Loss 0.09136001020669937\n",
            "Train step - Step 2020, Loss 0.09422671794891357\n",
            "Train step - Step 2030, Loss 0.09768716990947723\n",
            "Train epoch - Accuracy: 0.6264748201438849 Loss: 0.09580322246328532 Corrects: 4354\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.09953667223453522\n",
            "Train step - Step 2050, Loss 0.09318437427282333\n",
            "Train step - Step 2060, Loss 0.09688735008239746\n",
            "Train step - Step 2070, Loss 0.08771035075187683\n",
            "Train step - Step 2080, Loss 0.10136935114860535\n",
            "Train epoch - Accuracy: 0.6333812949640287 Loss: 0.0954260997077544 Corrects: 4402\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.09112586826086044\n",
            "Train step - Step 2100, Loss 0.09487347304821014\n",
            "Train step - Step 2110, Loss 0.09460693597793579\n",
            "Train step - Step 2120, Loss 0.09404806047677994\n",
            "Train step - Step 2130, Loss 0.0954691469669342\n",
            "Train step - Step 2140, Loss 0.09689813107252121\n",
            "Train epoch - Accuracy: 0.6379856115107914 Loss: 0.09515246644937735 Corrects: 4434\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.09152717888355255\n",
            "Train step - Step 2160, Loss 0.09556018561124802\n",
            "Train step - Step 2170, Loss 0.09677540510892868\n",
            "Train step - Step 2180, Loss 0.09830033779144287\n",
            "Train step - Step 2190, Loss 0.0910569578409195\n",
            "Train epoch - Accuracy: 0.638705035971223 Loss: 0.09496494691363341 Corrects: 4439\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.11007659137248993\n",
            "Train step - Step 2210, Loss 0.09412585943937302\n",
            "Train step - Step 2220, Loss 0.10149621218442917\n",
            "Train step - Step 2230, Loss 0.08651544898748398\n",
            "Train step - Step 2240, Loss 0.09518826007843018\n",
            "Train step - Step 2250, Loss 0.0926475003361702\n",
            "Train epoch - Accuracy: 0.6378417266187051 Loss: 0.09488975748955775 Corrects: 4433\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.09957603365182877\n",
            "Train step - Step 2270, Loss 0.09896735101938248\n",
            "Train step - Step 2280, Loss 0.1034431979060173\n",
            "Train step - Step 2290, Loss 0.10102290660142899\n",
            "Train step - Step 2300, Loss 0.09412673860788345\n",
            "Train epoch - Accuracy: 0.6517985611510791 Loss: 0.09480754027263724 Corrects: 4530\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.09089864045381546\n",
            "Train step - Step 2320, Loss 0.0920552909374237\n",
            "Train step - Step 2330, Loss 0.08583377301692963\n",
            "Train step - Step 2340, Loss 0.09616861492395401\n",
            "Train step - Step 2350, Loss 0.0870760977268219\n",
            "Train step - Step 2360, Loss 0.10272204130887985\n",
            "Train epoch - Accuracy: 0.6545323741007194 Loss: 0.09378804924033529 Corrects: 4549\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.10021774470806122\n",
            "Train step - Step 2380, Loss 0.09053858369588852\n",
            "Train step - Step 2390, Loss 0.08985821157693863\n",
            "Train step - Step 2400, Loss 0.09593766182661057\n",
            "Train step - Step 2410, Loss 0.09891527146100998\n",
            "Train epoch - Accuracy: 0.6499280575539569 Loss: 0.09403922844704964 Corrects: 4517\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.0936519056558609\n",
            "Train step - Step 2430, Loss 0.09517037868499756\n",
            "Train step - Step 2440, Loss 0.08937764167785645\n",
            "Train step - Step 2450, Loss 0.09105692058801651\n",
            "Train step - Step 2460, Loss 0.10124481469392776\n",
            "Train step - Step 2470, Loss 0.09525445848703384\n",
            "Train epoch - Accuracy: 0.6641726618705036 Loss: 0.09367797396809077 Corrects: 4616\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.09882916510105133\n",
            "Train step - Step 2490, Loss 0.0838240534067154\n",
            "Train step - Step 2500, Loss 0.09555936604738235\n",
            "Train step - Step 2510, Loss 0.09115926921367645\n",
            "Train step - Step 2520, Loss 0.09516802430152893\n",
            "Train epoch - Accuracy: 0.6635971223021583 Loss: 0.09255266832576381 Corrects: 4612\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.09424183517694473\n",
            "Train step - Step 2540, Loss 0.08983970433473587\n",
            "Train step - Step 2550, Loss 0.10016480088233948\n",
            "Train step - Step 2560, Loss 0.10226918011903763\n",
            "Train step - Step 2570, Loss 0.0934678465127945\n",
            "Train step - Step 2580, Loss 0.09142544120550156\n",
            "Train epoch - Accuracy: 0.6585611510791367 Loss: 0.0925947022095001 Corrects: 4577\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.0950917899608612\n",
            "Train step - Step 2600, Loss 0.09257205575704575\n",
            "Train step - Step 2610, Loss 0.0933300033211708\n",
            "Train step - Step 2620, Loss 0.0928674265742302\n",
            "Train step - Step 2630, Loss 0.09252287447452545\n",
            "Train epoch - Accuracy: 0.663021582733813 Loss: 0.09248385411586693 Corrects: 4608\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.09395809471607208\n",
            "Train step - Step 2650, Loss 0.09360671043395996\n",
            "Train step - Step 2660, Loss 0.09106128662824631\n",
            "Train step - Step 2670, Loss 0.08956111967563629\n",
            "Train step - Step 2680, Loss 0.09626634418964386\n",
            "Train step - Step 2690, Loss 0.09340263158082962\n",
            "Train epoch - Accuracy: 0.6733812949640288 Loss: 0.09249979703546428 Corrects: 4680\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.08575862646102905\n",
            "Train step - Step 2710, Loss 0.08900928497314453\n",
            "Train step - Step 2720, Loss 0.08676702529191971\n",
            "Train step - Step 2730, Loss 0.09200768172740936\n",
            "Train step - Step 2740, Loss 0.09473780542612076\n",
            "Train epoch - Accuracy: 0.6893525179856115 Loss: 0.08918260546468144 Corrects: 4791\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.08488817512989044\n",
            "Train step - Step 2760, Loss 0.08888044208288193\n",
            "Train step - Step 2770, Loss 0.08875874429941177\n",
            "Train step - Step 2780, Loss 0.08743438869714737\n",
            "Train step - Step 2790, Loss 0.08491537719964981\n",
            "Train step - Step 2800, Loss 0.09103624522686005\n",
            "Train epoch - Accuracy: 0.6897841726618705 Loss: 0.08829260634646999 Corrects: 4794\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.09502662718296051\n",
            "Train step - Step 2820, Loss 0.08533711731433868\n",
            "Train step - Step 2830, Loss 0.08565380424261093\n",
            "Train step - Step 2840, Loss 0.08459056913852692\n",
            "Train step - Step 2850, Loss 0.09001649916172028\n",
            "Train epoch - Accuracy: 0.6955395683453237 Loss: 0.08811420190677369 Corrects: 4834\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.08176196366548538\n",
            "Train step - Step 2870, Loss 0.08539918810129166\n",
            "Train step - Step 2880, Loss 0.08468573540449142\n",
            "Train step - Step 2890, Loss 0.08469324558973312\n",
            "Train step - Step 2900, Loss 0.0980789065361023\n",
            "Train step - Step 2910, Loss 0.08363844454288483\n",
            "Train epoch - Accuracy: 0.6959712230215828 Loss: 0.08787912537511304 Corrects: 4837\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.09174506366252899\n",
            "Train step - Step 2930, Loss 0.09530851989984512\n",
            "Train step - Step 2940, Loss 0.0849711075425148\n",
            "Train step - Step 2950, Loss 0.0904371365904808\n",
            "Train step - Step 2960, Loss 0.09332605451345444\n",
            "Train epoch - Accuracy: 0.6968345323741008 Loss: 0.08788327982743009 Corrects: 4843\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.08981061726808548\n",
            "Train step - Step 2980, Loss 0.0884333923459053\n",
            "Train step - Step 2990, Loss 0.09038124233484268\n",
            "Train step - Step 3000, Loss 0.08734169602394104\n",
            "Train step - Step 3010, Loss 0.08466026186943054\n",
            "Train step - Step 3020, Loss 0.08701649308204651\n",
            "Train epoch - Accuracy: 0.698705035971223 Loss: 0.08762596082129924 Corrects: 4856\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.09550351649522781\n",
            "Train step - Step 3040, Loss 0.09735383093357086\n",
            "Train step - Step 3050, Loss 0.08832011371850967\n",
            "Train step - Step 3060, Loss 0.09377045184373856\n",
            "Train step - Step 3070, Loss 0.08814537525177002\n",
            "Train epoch - Accuracy: 0.6956834532374101 Loss: 0.08775687511018712 Corrects: 4835\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.08800650388002396\n",
            "Train step - Step 3090, Loss 0.08652743697166443\n",
            "Train step - Step 3100, Loss 0.08019568771123886\n",
            "Train step - Step 3110, Loss 0.08407812565565109\n",
            "Train step - Step 3120, Loss 0.08106923848390579\n",
            "Train step - Step 3130, Loss 0.08312807232141495\n",
            "Train epoch - Accuracy: 0.6972661870503597 Loss: 0.0870231265258446 Corrects: 4846\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.09392258524894714\n",
            "Train step - Step 3150, Loss 0.0873727947473526\n",
            "Train step - Step 3160, Loss 0.08179916441440582\n",
            "Train step - Step 3170, Loss 0.08278728276491165\n",
            "Train step - Step 3180, Loss 0.08383549004793167\n",
            "Train epoch - Accuracy: 0.6966906474820144 Loss: 0.08745538196760974 Corrects: 4842\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.09326320141553879\n",
            "Train step - Step 3200, Loss 0.0893259271979332\n",
            "Train step - Step 3210, Loss 0.08353132754564285\n",
            "Train step - Step 3220, Loss 0.0878303125500679\n",
            "Train step - Step 3230, Loss 0.08217602968215942\n",
            "Train step - Step 3240, Loss 0.09065430611371994\n",
            "Train epoch - Accuracy: 0.700863309352518 Loss: 0.08699049244038493 Corrects: 4871\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.08708683401346207\n",
            "Train step - Step 3260, Loss 0.08945474773645401\n",
            "Train step - Step 3270, Loss 0.08780074119567871\n",
            "Train step - Step 3280, Loss 0.08549186587333679\n",
            "Train step - Step 3290, Loss 0.09311715513467789\n",
            "Train epoch - Accuracy: 0.6975539568345324 Loss: 0.08664893234805238 Corrects: 4848\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.08992630988359451\n",
            "Train step - Step 3310, Loss 0.08583366870880127\n",
            "Train step - Step 3320, Loss 0.09877697378396988\n",
            "Train step - Step 3330, Loss 0.09101920574903488\n",
            "Train step - Step 3340, Loss 0.07805202901363373\n",
            "Train step - Step 3350, Loss 0.08680865168571472\n",
            "Train epoch - Accuracy: 0.7076258992805755 Loss: 0.0866389168573798 Corrects: 4918\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.07882194966077805\n",
            "Train step - Step 3370, Loss 0.0871996134519577\n",
            "Train step - Step 3380, Loss 0.09110616892576218\n",
            "Train step - Step 3390, Loss 0.08478055149316788\n",
            "Train step - Step 3400, Loss 0.08752996474504471\n",
            "Train epoch - Accuracy: 0.6949640287769784 Loss: 0.08705895822254016 Corrects: 4830\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.09262833744287491\n",
            "Train step - Step 3420, Loss 0.088290274143219\n",
            "Train step - Step 3430, Loss 0.08387775719165802\n",
            "Train step - Step 3440, Loss 0.08439023047685623\n",
            "Train step - Step 3450, Loss 0.09179317951202393\n",
            "Train step - Step 3460, Loss 0.08415535092353821\n",
            "Train epoch - Accuracy: 0.7037410071942446 Loss: 0.08676867697950748 Corrects: 4891\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.08715655654668808\n",
            "Train step - Step 3480, Loss 0.09122674912214279\n",
            "Train step - Step 3490, Loss 0.08417436480522156\n",
            "Train step - Step 3500, Loss 0.08957863599061966\n",
            "Train step - Step 3510, Loss 0.08199720829725266\n",
            "Train epoch - Accuracy: 0.7005755395683453 Loss: 0.08650843660822875 Corrects: 4869\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.08872242271900177\n",
            "Train step - Step 3530, Loss 0.09293386340141296\n",
            "Train step - Step 3540, Loss 0.08072957396507263\n",
            "Train step - Step 3550, Loss 0.08275590091943741\n",
            "Train step - Step 3560, Loss 0.08734768629074097\n",
            "Train step - Step 3570, Loss 0.08845309168100357\n",
            "Train epoch - Accuracy: 0.7058992805755395 Loss: 0.0859767212627603 Corrects: 4906\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.08808489143848419\n",
            "Train step - Step 3590, Loss 0.08836689591407776\n",
            "Train step - Step 3600, Loss 0.08136056363582611\n",
            "Train step - Step 3610, Loss 0.08639241009950638\n",
            "Train step - Step 3620, Loss 0.08243419975042343\n",
            "Train epoch - Accuracy: 0.7050359712230215 Loss: 0.08593380964702839 Corrects: 4900\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.08930964767932892\n",
            "Train step - Step 3640, Loss 0.08721937984228134\n",
            "Train step - Step 3650, Loss 0.09154876321554184\n",
            "Train step - Step 3660, Loss 0.09662441164255142\n",
            "Train step - Step 3670, Loss 0.08625861257314682\n",
            "Train step - Step 3680, Loss 0.08180764317512512\n",
            "Train epoch - Accuracy: 0.7070503597122302 Loss: 0.08617740971150158 Corrects: 4914\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.08417168259620667\n",
            "Train step - Step 3700, Loss 0.09297706931829453\n",
            "Train step - Step 3710, Loss 0.08687571436166763\n",
            "Train step - Step 3720, Loss 0.08168043941259384\n",
            "Train step - Step 3730, Loss 0.08433074504137039\n",
            "Train epoch - Accuracy: 0.7125179856115108 Loss: 0.08591447647955777 Corrects: 4952\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.08867671340703964\n",
            "Train step - Step 3750, Loss 0.08356068283319473\n",
            "Train step - Step 3760, Loss 0.0884842723608017\n",
            "Train step - Step 3770, Loss 0.07670504599809647\n",
            "Train step - Step 3780, Loss 0.0785355344414711\n",
            "Train step - Step 3790, Loss 0.09497208148241043\n",
            "Train epoch - Accuracy: 0.7063309352517986 Loss: 0.08616313581629623 Corrects: 4909\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.08938184380531311\n",
            "Train step - Step 3810, Loss 0.08601180464029312\n",
            "Train step - Step 3820, Loss 0.09116224199533463\n",
            "Train step - Step 3830, Loss 0.08707931637763977\n",
            "Train step - Step 3840, Loss 0.08212748169898987\n",
            "Train epoch - Accuracy: 0.7096402877697842 Loss: 0.08627890648601724 Corrects: 4932\n",
            "Training finished in 391.86886191368103 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24]\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238246450>\n",
            "Constructing exemplars of class 35\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [33483, 29607, 7990, 39415, 3351, 49967, 37302, 10115, 45119, 33176, 31045, 23578, 34763, 49876, 15283, 49687, 17219, 40091, 39716, 20375, 1757, 35224, 35721, 33372, 5196, 40344, 3012, 17870, 3973, 19766, 1109, 28429, 46064, 7388, 3456, 21591, 48836, 26190, 47645, 35495, 25400, 33661, 27490, 3137, 10552, 10445, 40091, 29072, 45720, 23218, 3351, 49876, 1266, 28737, 23218, 47650, 33831, 43572, 11937, 25565, 43095, 36741, 40110, 35495, 16599, 7158]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e45250>\n",
            "Constructing exemplars of class 3\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [1174, 27383, 5360, 37028, 18509, 21545, 27153, 4410, 34113, 30884, 27542, 21545, 12671, 21904, 35835, 40302, 26514, 44290, 10513, 5173, 1637, 48182, 28978, 13146, 16490, 26708, 18216, 612, 2085, 49954, 29385, 35315, 13735, 40708, 35799, 41083, 12671, 46182, 6868, 35435, 16011, 12034, 20805, 2459, 20654, 8365, 16147, 18562, 45058, 46182, 19469, 21545, 21232, 19050, 28981, 23120, 27931, 17546, 23456, 13911, 31510, 47026, 46726, 49376, 47454, 45475]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e45a10>\n",
            "Constructing exemplars of class 94\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [37068, 2309, 18304, 36318, 20021, 5905, 1305, 43459, 37175, 49542, 30002, 2298, 22407, 15797, 44145, 4278, 45593, 11974, 16046, 21717, 41750, 39422, 38694, 7307, 27701, 3559, 4278, 29636, 21693, 11391, 40927, 44310, 9278, 3701, 15538, 25415, 42625, 45955, 9223, 31562, 2608, 17962, 24818, 1466, 13250, 41683, 8487, 46875, 10808, 31910, 3363, 44720, 14488, 43612, 11974, 8164, 9714, 9056, 925, 2246, 18675, 7189, 39857, 44997, 4061, 30951]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224548f250>\n",
            "Constructing exemplars of class 10\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [29329, 18383, 23295, 12442, 23498, 19042, 2600, 36955, 42544, 18346, 47187, 19864, 26126, 19470, 39793, 41961, 33623, 33805, 11218, 21167, 12703, 28393, 39934, 40883, 24971, 43313, 23675, 47995, 22611, 38293, 13554, 28944, 41600, 28098, 14588, 42860, 5193, 17835, 5218, 6203, 42493, 7596, 9250, 12703, 8612, 24971, 12347, 11858, 3682, 23324, 307, 45738, 9250, 45122, 8458, 48217, 2078, 33749, 39956, 18361, 47052, 4797, 22107, 30042, 40997, 36762]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e9b910>\n",
            "Constructing exemplars of class 45\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [34770, 49984, 32346, 26914, 39094, 7197, 4174, 23395, 39566, 47639, 32627, 14226, 42601, 48033, 19487, 46928, 40478, 21605, 28850, 10949, 25282, 6508, 22072, 36852, 33646, 16982, 22680, 41, 17675, 3098, 26244, 39674, 37743, 23244, 49570, 34572, 33006, 31279, 21039, 22291, 23832, 24699, 15485, 24069, 24407, 14112, 28149, 28415, 37039, 41696, 48780, 35740, 26212, 13776, 12914, 21433, 35423, 3073, 48362, 17893, 13041, 31056, 45863, 16152, 28638, 35505]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224548f250>\n",
            "Constructing exemplars of class 37\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [14781, 48555, 25575, 10839, 40493, 42135, 30344, 1461, 42883, 13941, 21386, 1657, 6396, 43721, 9279, 14746, 20162, 34643, 29920, 868, 41533, 40346, 40493, 2948, 12229, 13164, 35125, 23237, 49767, 26645, 42430, 13164, 27854, 27789, 17778, 22155, 38632, 5952, 4050, 39283, 780, 11152, 11145, 28419, 868, 1915, 9279, 12498, 33630, 12524, 6208, 49775, 693, 12157, 17409, 5635, 47950, 20260, 2035, 23012, 1556, 41199, 39760, 39021, 36043, 20176]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223293af10>\n",
            "Constructing exemplars of class 80\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [40393, 31171, 37506, 48667, 16315, 43167, 6819, 40095, 12081, 42652, 27436, 48899, 33998, 21576, 17949, 47797, 46027, 19901, 43609, 30148, 10210, 28719, 1157, 42206, 20818, 46829, 48095, 48667, 30177, 38368, 192, 30524, 41806, 13998, 30933, 36652, 8076, 4360, 648, 6706, 45814, 19341, 38932, 7032, 26730, 46522, 39933, 28539, 20442, 19088, 22449, 34060, 31244, 15339, 28987, 32587, 41469, 8094, 29481, 6819, 44518, 11791, 16315, 35914, 1194, 43204]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224548f250>\n",
            "Constructing exemplars of class 72\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [46472, 24223, 15650, 17951, 12744, 20337, 35222, 39035, 40775, 32996, 15862, 42792, 48680, 175, 27794, 25002, 34443, 25901, 10182, 10763, 26457, 7615, 41207, 35718, 15977, 28531, 366, 2849, 37914, 31270, 33599, 48535, 43569, 22639, 8530, 21925, 31199, 13753, 49399, 26363, 16065, 2629, 38476, 46052, 43200, 6808, 47020, 39047, 3899, 34381, 26457, 9011, 23232, 32816, 48053, 41758, 7850, 25029, 15163, 28194, 15650, 7661, 22880, 23650, 42453, 33412]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223824ad10>\n",
            "Constructing exemplars of class 36\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [49703, 34628, 23384, 40990, 26779, 14586, 16245, 8743, 24229, 9143, 6985, 35484, 43855, 35377, 32061, 74, 14331, 42505, 21836, 34405, 36638, 14988, 44878, 1953, 13336, 46806, 24857, 24152, 29941, 1886, 29101, 18611, 23256, 37555, 48720, 2505, 26003, 26288, 21513, 14818, 45277, 15161, 37870, 17194, 6555, 1878, 43770, 20508, 41854, 15368, 42239, 33871, 12823, 40566, 12969, 4758, 31117, 46692, 49511, 9678, 18648, 12063, 33223, 21085, 9965, 29226]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223190a290>\n",
            "Constructing exemplars of class 24\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [47115, 5501, 35673, 29672, 41770, 37031, 37108, 34527, 21804, 11015, 7175, 40015, 30448, 36752, 21892, 9738, 21978, 31264, 20750, 3715, 42167, 46749, 8516, 28113, 26151, 24610, 2366, 24647, 38058, 1496, 45952, 23737, 44519, 23061, 49481, 23997, 17584, 4279, 42106, 23802, 43138, 44782, 44835, 44700, 39052, 34279, 8083, 11672, 49477, 40015, 3717, 37173, 16008, 26741, 1524, 1863, 44117, 23199, 38564, 1718, 8440, 37040, 3261, 17834, 11511, 9119]\n",
            "x train:  [-0.13962898 -0.15788312  0.10308097 -0.13827321 -0.14207406 -0.18010783\n",
            " -0.19979425 -0.13976938 -0.09247549 -0.09836888 -0.31739867 -0.23496416\n",
            " -0.06538626 -0.183022   -0.12628408 -0.16400474  0.06208349 -0.19973294\n",
            " -0.31905657 -0.21433294 -0.12914461 -0.14619474 -0.22592269 -0.09664242\n",
            " -0.12398364 -0.10073257 -0.1983899  -0.21789515 -0.257017   -0.30104747]\n",
            "y_train:  [tensor([13]), tensor([81]), tensor([80]), tensor([30]), tensor([97]), tensor([94]), tensor([81]), tensor([59]), tensor([49]), tensor([68]), tensor([81]), tensor([81]), tensor([30]), tensor([88]), tensor([35]), tensor([49]), tensor([36]), tensor([88]), tensor([13]), tensor([10]), tensor([24]), tensor([50]), tensor([59]), tensor([95]), tensor([69]), tensor([49]), tensor([57]), tensor([13]), tensor([50]), tensor([72]), tensor([81]), tensor([34]), tensor([95]), tensor([80]), tensor([24]), tensor([3]), tensor([30]), tensor([35]), tensor([13]), tensor([49]), tensor([69]), tensor([25]), tensor([81]), tensor([49]), tensor([36]), tensor([30]), tensor([94]), tensor([81]), tensor([60]), tensor([81]), tensor([88]), tensor([59]), tensor([27]), tensor([94]), tensor([95]), tensor([69]), tensor([82]), tensor([50]), tensor([68]), tensor([72]), tensor([10]), tensor([27]), tensor([81]), tensor([50]), tensor([27]), tensor([45]), tensor([45]), tensor([30]), tensor([50]), tensor([34]), tensor([45]), tensor([50]), tensor([59]), tensor([97]), tensor([35]), tensor([80]), tensor([82]), tensor([36]), tensor([49]), tensor([50]), tensor([24]), tensor([57]), tensor([57]), tensor([34]), tensor([25]), tensor([49]), tensor([34]), tensor([35]), tensor([10]), tensor([72]), tensor([35]), tensor([82]), tensor([45]), tensor([36]), tensor([50]), tensor([59]), tensor([69]), tensor([13]), tensor([72]), tensor([78]), tensor([36]), tensor([59]), tensor([78]), tensor([80]), tensor([86]), tensor([25]), tensor([24]), tensor([59]), tensor([57]), tensor([45]), tensor([78]), tensor([36]), tensor([25]), tensor([49]), tensor([68]), tensor([10]), tensor([72]), tensor([68]), tensor([25]), tensor([88]), tensor([36]), tensor([10]), tensor([97]), tensor([68]), tensor([35]), tensor([13]), tensor([49]), tensor([30]), tensor([81]), tensor([81]), tensor([68]), tensor([45]), tensor([34]), tensor([49]), tensor([35]), tensor([35]), tensor([78]), tensor([35]), tensor([68]), tensor([68]), tensor([82]), tensor([69]), tensor([69]), tensor([68]), tensor([25]), tensor([72]), tensor([58]), tensor([80]), tensor([3]), tensor([13]), tensor([36]), tensor([13]), tensor([86]), tensor([82]), tensor([34]), tensor([69]), tensor([37]), tensor([27]), tensor([30]), tensor([68]), tensor([45]), tensor([45]), tensor([25]), tensor([3]), tensor([10]), tensor([59]), tensor([94]), tensor([80]), tensor([45]), tensor([94]), tensor([86]), tensor([37]), tensor([3]), tensor([97]), tensor([69]), tensor([97]), tensor([81]), tensor([13]), tensor([24]), tensor([94]), tensor([94]), tensor([27]), tensor([60]), tensor([80]), tensor([3]), tensor([69]), tensor([81]), tensor([27]), tensor([97]), tensor([45]), tensor([82]), tensor([50]), tensor([88]), tensor([60]), tensor([78]), tensor([95]), tensor([59]), tensor([45]), tensor([95]), tensor([97]), tensor([10]), tensor([60]), tensor([57]), tensor([82]), tensor([72]), tensor([24]), tensor([27]), tensor([36]), tensor([59]), tensor([59]), tensor([80]), tensor([36]), tensor([94]), tensor([30]), tensor([95]), tensor([57]), tensor([81]), tensor([49]), tensor([49]), tensor([13]), tensor([24]), tensor([3]), tensor([13]), tensor([94]), tensor([13]), tensor([30]), tensor([86]), tensor([68]), tensor([88]), tensor([82]), tensor([57]), tensor([60]), tensor([88]), tensor([57]), tensor([36]), tensor([86]), tensor([95]), tensor([97]), tensor([57]), tensor([36]), tensor([81]), tensor([81]), tensor([72]), tensor([68]), tensor([3]), tensor([36]), tensor([34]), tensor([57]), tensor([81]), tensor([3]), tensor([27]), tensor([95]), tensor([69]), tensor([30]), tensor([95]), tensor([72]), tensor([80]), tensor([25]), tensor([60]), tensor([58]), tensor([58]), tensor([94]), tensor([58]), tensor([57]), tensor([59]), tensor([60]), tensor([97]), tensor([59]), tensor([35]), tensor([59]), tensor([36]), tensor([58]), tensor([45]), tensor([13]), tensor([27]), tensor([81]), tensor([27]), tensor([60]), tensor([69]), tensor([25]), tensor([57]), tensor([97]), tensor([58]), tensor([82]), tensor([82]), tensor([35]), tensor([37]), tensor([82]), tensor([45]), tensor([80]), tensor([36]), tensor([57]), tensor([24]), tensor([24]), tensor([95]), tensor([81]), tensor([25]), tensor([88]), tensor([80]), tensor([57]), tensor([36]), tensor([45]), tensor([34]), tensor([58]), tensor([37]), tensor([94]), tensor([36]), tensor([59]), tensor([82]), tensor([58]), tensor([10]), tensor([35]), tensor([27]), tensor([27]), tensor([94]), tensor([24]), tensor([50]), tensor([78]), tensor([97]), tensor([57]), tensor([49]), tensor([95]), tensor([50]), tensor([88]), tensor([68]), tensor([30]), tensor([94]), tensor([3]), tensor([34]), tensor([95]), tensor([13]), tensor([24]), tensor([3]), tensor([82]), tensor([60]), tensor([10]), tensor([3]), tensor([37]), tensor([10]), tensor([82]), tensor([88]), tensor([88]), tensor([35]), tensor([35]), tensor([72]), tensor([24]), tensor([34]), tensor([81]), tensor([49]), tensor([25]), tensor([25]), tensor([34]), tensor([94]), tensor([30]), tensor([81]), tensor([34]), tensor([34]), tensor([25]), tensor([10]), tensor([49]), tensor([45]), tensor([88]), tensor([94]), tensor([81]), tensor([27]), tensor([88]), tensor([36]), tensor([69]), tensor([36]), tensor([50]), tensor([35]), tensor([49]), tensor([69]), tensor([3]), tensor([81]), tensor([36]), tensor([24]), tensor([82]), tensor([30]), tensor([36]), tensor([82]), tensor([58]), tensor([81]), tensor([24]), tensor([24]), tensor([81]), tensor([59]), tensor([13]), tensor([35]), tensor([3]), tensor([78]), tensor([30]), tensor([36]), tensor([94]), tensor([59]), tensor([94]), tensor([59]), tensor([57]), tensor([3]), tensor([30]), tensor([68]), tensor([13]), tensor([25]), tensor([10]), tensor([95]), tensor([69]), tensor([86]), tensor([50]), tensor([45]), tensor([60]), tensor([72]), tensor([97]), tensor([82]), tensor([72]), tensor([25]), tensor([30]), tensor([34]), tensor([68]), tensor([94]), tensor([94]), tensor([68]), tensor([68]), tensor([69]), tensor([50]), tensor([72]), tensor([45]), tensor([3]), tensor([3]), tensor([34]), tensor([80]), tensor([24]), tensor([59]), tensor([25]), tensor([58]), tensor([3]), tensor([45]), tensor([86]), tensor([10]), tensor([37]), tensor([37]), tensor([88]), tensor([95]), tensor([58]), tensor([57]), tensor([82]), tensor([95]), tensor([94]), tensor([3]), tensor([94]), tensor([80]), tensor([59]), tensor([94]), tensor([80]), tensor([25]), tensor([30]), tensor([25]), tensor([88]), tensor([30]), tensor([50]), tensor([45]), tensor([88]), tensor([34]), tensor([59]), tensor([78]), tensor([30]), tensor([94]), tensor([36]), tensor([80]), tensor([49]), tensor([80]), tensor([58]), tensor([25]), tensor([50]), tensor([86]), tensor([49]), tensor([35]), tensor([13]), tensor([72]), tensor([68]), tensor([82]), tensor([49]), tensor([81]), tensor([69]), tensor([78]), tensor([95]), tensor([81]), tensor([59]), tensor([95]), tensor([69]), tensor([24]), tensor([97]), tensor([36]), tensor([10]), tensor([37]), tensor([94]), tensor([97]), tensor([69]), tensor([57]), tensor([72]), tensor([3]), tensor([59]), tensor([60]), tensor([94]), tensor([97]), tensor([97]), tensor([3]), tensor([3]), tensor([34]), tensor([45]), tensor([94]), tensor([80]), tensor([69]), tensor([94]), tensor([97]), tensor([72]), tensor([57]), tensor([81]), tensor([24]), tensor([78]), tensor([13]), tensor([60]), tensor([50]), tensor([58]), tensor([30]), tensor([81]), tensor([88]), tensor([60]), tensor([10]), tensor([81]), tensor([45]), tensor([34]), tensor([82]), tensor([34]), tensor([24]), tensor([27]), tensor([58]), tensor([25]), tensor([45]), tensor([24]), tensor([88]), tensor([78]), tensor([97]), tensor([25]), tensor([35]), tensor([10]), tensor([35]), tensor([81]), tensor([69]), tensor([36]), tensor([57]), tensor([45]), tensor([95]), tensor([36]), tensor([24]), tensor([86]), tensor([58]), tensor([82]), tensor([35]), tensor([72]), tensor([49]), tensor([81]), tensor([27]), tensor([57]), tensor([34]), tensor([69]), tensor([59]), tensor([49]), tensor([36]), tensor([34]), tensor([72]), tensor([97]), tensor([37]), tensor([59]), tensor([69]), tensor([36]), tensor([35]), tensor([49]), tensor([59]), tensor([24]), tensor([69]), tensor([60]), tensor([27]), tensor([35]), tensor([50]), tensor([50]), tensor([80]), tensor([25]), tensor([94]), tensor([35]), tensor([95]), tensor([49]), tensor([69]), tensor([94]), tensor([59]), tensor([24]), tensor([78]), tensor([10]), tensor([86]), tensor([60]), tensor([69]), tensor([60]), tensor([34]), tensor([97]), tensor([30]), tensor([95]), tensor([35]), tensor([58]), tensor([80]), tensor([72]), tensor([30]), tensor([68]), tensor([35]), tensor([13]), tensor([59]), tensor([45]), tensor([24]), tensor([88]), tensor([24]), tensor([78]), tensor([80]), tensor([59]), tensor([88]), tensor([78]), tensor([68]), tensor([88]), tensor([45]), tensor([78]), tensor([35]), tensor([80]), tensor([34]), tensor([58]), tensor([69]), tensor([69]), tensor([68]), tensor([36]), tensor([81]), tensor([60]), tensor([86]), tensor([36]), tensor([58]), tensor([35]), tensor([35]), tensor([50]), tensor([86]), tensor([50]), tensor([78]), tensor([78]), tensor([72]), tensor([94]), tensor([72]), tensor([13]), tensor([88]), tensor([30]), tensor([36]), tensor([80]), tensor([95]), tensor([37]), tensor([88]), tensor([86]), tensor([86]), tensor([72]), tensor([37]), tensor([72]), tensor([35]), tensor([88]), tensor([27]), tensor([25]), tensor([81]), tensor([50]), tensor([86]), tensor([49]), tensor([81]), tensor([94]), tensor([34]), tensor([88]), tensor([45]), tensor([30]), tensor([97]), tensor([13]), tensor([59]), tensor([49]), tensor([45]), tensor([13]), tensor([72]), tensor([60]), tensor([50]), tensor([86]), tensor([97]), tensor([69]), tensor([36]), tensor([36]), tensor([37]), tensor([49]), tensor([10]), tensor([27]), tensor([3]), tensor([3]), tensor([13]), tensor([97]), tensor([45]), tensor([60]), tensor([13]), tensor([25]), tensor([88]), tensor([50]), tensor([86]), tensor([50]), tensor([27]), tensor([80]), tensor([58]), tensor([88]), tensor([72]), tensor([35]), tensor([68]), tensor([57]), tensor([45]), tensor([72]), tensor([49]), tensor([37]), tensor([37]), tensor([60]), tensor([37]), tensor([86]), tensor([69]), tensor([35]), tensor([57]), tensor([88]), tensor([3]), tensor([69]), tensor([81]), tensor([13]), tensor([58]), tensor([27]), tensor([36]), tensor([72]), tensor([45]), tensor([80]), tensor([50]), tensor([25]), tensor([13]), tensor([97]), tensor([37]), tensor([59]), tensor([10]), tensor([30]), tensor([36]), tensor([58]), tensor([58]), tensor([3]), tensor([37]), tensor([37]), tensor([25]), tensor([50]), tensor([34]), tensor([24]), tensor([88]), tensor([37]), tensor([10]), tensor([10]), tensor([88]), tensor([50]), tensor([27]), tensor([49]), tensor([59]), tensor([30]), tensor([80]), tensor([3]), tensor([30]), tensor([37]), tensor([27]), tensor([82]), tensor([78]), tensor([82]), tensor([30]), tensor([72]), tensor([27]), tensor([45]), tensor([10]), tensor([59]), tensor([10]), tensor([57]), tensor([30]), tensor([35]), tensor([81]), tensor([37]), tensor([88]), tensor([49]), tensor([35]), tensor([10]), tensor([50]), tensor([68]), tensor([88]), tensor([97]), tensor([37]), tensor([3]), tensor([27]), tensor([34]), tensor([37]), tensor([94]), tensor([24]), tensor([37]), tensor([49]), tensor([3]), tensor([59]), tensor([58]), tensor([13]), tensor([59]), tensor([95]), tensor([60]), tensor([57]), tensor([36]), tensor([37]), tensor([69]), tensor([13]), tensor([59]), tensor([95]), tensor([95]), tensor([25]), tensor([68]), tensor([37]), tensor([30]), tensor([59]), tensor([68]), tensor([30]), tensor([3]), tensor([35]), tensor([80]), tensor([49]), tensor([25]), tensor([27]), tensor([88]), tensor([50]), tensor([60]), tensor([94]), tensor([13]), tensor([81]), tensor([37]), tensor([80]), tensor([10]), tensor([24]), tensor([78]), tensor([86]), tensor([30]), tensor([35]), tensor([94]), tensor([13]), tensor([95]), tensor([72]), tensor([24]), tensor([94]), tensor([82]), tensor([30]), tensor([50]), tensor([69]), tensor([81]), tensor([49]), tensor([88]), tensor([80]), tensor([60]), tensor([59]), tensor([88]), tensor([25]), tensor([57]), tensor([25]), tensor([35]), tensor([78]), tensor([88]), tensor([80]), tensor([68]), tensor([60]), tensor([34]), tensor([37]), tensor([57]), tensor([86]), tensor([86]), tensor([50]), tensor([35]), tensor([37]), tensor([35]), tensor([27]), tensor([30]), tensor([78]), tensor([49]), tensor([69]), tensor([59]), tensor([88]), tensor([58]), tensor([3]), tensor([34]), tensor([80]), tensor([95]), tensor([60]), tensor([72]), tensor([68]), tensor([45]), tensor([60]), tensor([80]), tensor([49]), tensor([34]), tensor([27]), tensor([94]), tensor([68]), tensor([78]), tensor([27]), tensor([25]), tensor([86]), tensor([94]), tensor([45]), tensor([37]), tensor([49]), tensor([97]), tensor([24]), tensor([68]), tensor([78]), tensor([60]), tensor([80]), tensor([78]), tensor([35]), tensor([81]), tensor([72]), tensor([58]), tensor([3]), tensor([34]), tensor([3]), tensor([97]), tensor([97]), tensor([95]), tensor([86]), tensor([24]), tensor([80]), tensor([59]), tensor([94]), tensor([30]), tensor([82]), tensor([58]), tensor([80]), tensor([86]), tensor([57]), tensor([78]), tensor([35]), tensor([25]), tensor([10]), tensor([78]), tensor([82]), tensor([94]), tensor([95]), tensor([97]), tensor([86]), tensor([97]), tensor([59]), tensor([50]), tensor([78]), tensor([58]), tensor([57]), tensor([80]), tensor([13]), tensor([24]), tensor([59]), tensor([59]), tensor([72]), tensor([97]), tensor([34]), tensor([95]), tensor([80]), tensor([13]), tensor([78]), tensor([45]), tensor([60]), tensor([60]), tensor([10]), tensor([60]), tensor([78]), tensor([60]), tensor([80]), tensor([80]), tensor([72]), tensor([10]), tensor([25]), tensor([60]), tensor([24]), tensor([82]), tensor([50]), tensor([80]), tensor([86]), tensor([60]), tensor([57]), tensor([49]), tensor([78]), tensor([49]), tensor([69]), tensor([45]), tensor([50]), tensor([69]), tensor([68]), tensor([35]), tensor([10]), tensor([69]), tensor([30]), tensor([72]), tensor([95]), tensor([86]), tensor([69]), tensor([45]), tensor([60]), tensor([86]), tensor([35]), tensor([35]), tensor([86]), tensor([57]), tensor([24]), tensor([34]), tensor([95]), tensor([10]), tensor([88]), tensor([69]), tensor([95]), tensor([49]), tensor([30]), tensor([27]), tensor([25]), tensor([27]), tensor([27]), tensor([82]), tensor([78]), tensor([50]), tensor([95]), tensor([36]), tensor([35]), tensor([58]), tensor([81]), tensor([13]), tensor([45]), tensor([81]), tensor([97]), tensor([94]), tensor([68]), tensor([68]), tensor([81]), tensor([95]), tensor([60]), tensor([37]), tensor([3]), tensor([13]), tensor([50]), tensor([25]), tensor([94]), tensor([80]), tensor([35]), tensor([82]), tensor([30]), tensor([36]), tensor([27]), tensor([37]), tensor([13]), tensor([94]), tensor([59]), tensor([37]), tensor([34]), tensor([13]), tensor([78]), tensor([72]), tensor([57]), tensor([78]), tensor([78]), tensor([36]), tensor([25]), tensor([10]), tensor([34]), tensor([10]), tensor([97]), tensor([95]), tensor([37]), tensor([86]), tensor([10]), tensor([30]), tensor([82]), tensor([95]), tensor([24]), tensor([60]), tensor([24]), tensor([27]), tensor([30]), tensor([24]), tensor([80]), tensor([24]), tensor([27]), tensor([3]), tensor([36]), tensor([88]), tensor([68]), tensor([37]), tensor([60]), tensor([36]), tensor([37]), tensor([68]), tensor([82]), tensor([68]), tensor([82]), tensor([97]), tensor([88]), tensor([69]), tensor([10]), tensor([72]), tensor([45]), tensor([82]), tensor([94]), tensor([45]), tensor([37]), tensor([69]), tensor([86]), tensor([25]), tensor([34]), tensor([88]), tensor([35]), tensor([95]), tensor([10]), tensor([94]), tensor([10]), tensor([25]), tensor([50]), tensor([69]), tensor([78]), tensor([88]), tensor([57]), tensor([80]), tensor([59]), tensor([45]), tensor([45]), tensor([94]), tensor([30]), tensor([68]), tensor([25]), tensor([27]), tensor([30]), tensor([97]), tensor([35]), tensor([82]), tensor([82]), tensor([10]), tensor([10]), tensor([24]), tensor([35]), tensor([88]), tensor([13]), tensor([95]), tensor([95]), tensor([50]), tensor([49]), tensor([37]), tensor([37]), tensor([95]), tensor([86]), tensor([88]), tensor([34]), tensor([78]), tensor([45]), tensor([27]), tensor([36]), tensor([50]), tensor([49]), tensor([97]), tensor([86]), tensor([50]), tensor([37]), tensor([36]), tensor([78]), tensor([10]), tensor([80]), tensor([34]), tensor([37]), tensor([86]), tensor([86]), tensor([82]), tensor([27]), tensor([60]), tensor([30]), tensor([30]), tensor([58]), tensor([49]), tensor([97]), tensor([45]), tensor([49]), tensor([34]), tensor([72]), tensor([3]), tensor([80]), tensor([94]), tensor([24]), tensor([69]), tensor([10]), tensor([24]), tensor([37]), tensor([86]), tensor([59]), tensor([81]), tensor([58]), tensor([35]), tensor([37]), tensor([25]), tensor([57]), tensor([35]), tensor([25]), tensor([50]), tensor([80]), tensor([95]), tensor([81]), tensor([78]), tensor([60]), tensor([95]), tensor([58]), tensor([60]), tensor([81]), tensor([3]), tensor([59]), tensor([58]), tensor([58]), tensor([95]), tensor([24]), tensor([60]), tensor([68]), tensor([27]), tensor([59]), tensor([68]), tensor([58]), tensor([3]), tensor([36]), tensor([88]), tensor([69]), tensor([45]), tensor([45]), tensor([27]), tensor([37]), tensor([34]), tensor([94]), tensor([24]), tensor([49]), tensor([72]), tensor([82]), tensor([60]), tensor([69]), tensor([57]), tensor([49]), tensor([13]), tensor([36]), tensor([45]), tensor([58]), tensor([69]), tensor([57]), tensor([72]), tensor([30]), tensor([49]), tensor([10]), tensor([35]), tensor([13]), tensor([95]), tensor([60]), tensor([13]), tensor([35]), tensor([45]), tensor([50]), tensor([45]), tensor([25]), tensor([86]), tensor([58]), tensor([80]), tensor([50]), tensor([13]), tensor([86]), tensor([69]), tensor([82]), tensor([81]), tensor([57]), tensor([60]), tensor([50]), tensor([45]), tensor([78]), tensor([72]), tensor([3]), tensor([81]), tensor([3]), tensor([78]), tensor([60]), tensor([24]), tensor([57]), tensor([57]), tensor([81]), tensor([3]), tensor([35]), tensor([10]), tensor([81]), tensor([88]), tensor([10]), tensor([68]), tensor([3]), tensor([27]), tensor([58]), tensor([27]), tensor([24]), tensor([50]), tensor([97]), tensor([27]), tensor([69]), tensor([34]), tensor([30]), tensor([58]), tensor([69]), tensor([60]), tensor([80]), tensor([13]), tensor([72]), tensor([60]), tensor([82]), tensor([10]), tensor([27]), tensor([97]), tensor([24]), tensor([36]), tensor([13]), tensor([34]), tensor([13]), tensor([81]), tensor([57]), tensor([60]), tensor([49]), tensor([10]), tensor([78]), tensor([81]), tensor([10]), tensor([60]), tensor([50]), tensor([3]), tensor([45]), tensor([10]), tensor([59]), tensor([81]), tensor([36]), tensor([24]), tensor([95]), tensor([25]), tensor([69]), tensor([97]), tensor([78]), tensor([78]), tensor([3]), tensor([27]), tensor([57]), tensor([24]), tensor([95]), tensor([60]), tensor([57]), tensor([69]), tensor([69]), tensor([57]), tensor([36]), tensor([13]), tensor([81]), tensor([94]), tensor([10]), tensor([57]), tensor([3]), tensor([57]), tensor([49]), tensor([30]), tensor([59]), tensor([69]), tensor([34]), tensor([57]), tensor([34]), tensor([68]), tensor([57]), tensor([81]), tensor([69]), tensor([78]), tensor([82]), tensor([25]), tensor([78]), tensor([97]), tensor([82]), tensor([49]), tensor([81]), tensor([45]), tensor([49]), tensor([37]), tensor([35]), tensor([86]), tensor([88]), tensor([80]), tensor([57]), tensor([82]), tensor([13]), tensor([25]), tensor([34]), tensor([30]), tensor([69]), tensor([97]), tensor([3]), tensor([36]), tensor([86]), tensor([30]), tensor([37]), tensor([94]), tensor([80]), tensor([37]), tensor([57]), tensor([58]), tensor([80]), tensor([3]), tensor([86]), tensor([95]), tensor([35]), tensor([60]), tensor([58]), tensor([3]), tensor([13]), tensor([13]), tensor([57]), tensor([68]), tensor([58]), tensor([34]), tensor([82]), tensor([68]), tensor([57]), tensor([45]), tensor([81]), tensor([86]), tensor([69]), tensor([97]), tensor([58]), tensor([97]), tensor([36]), tensor([27]), tensor([34]), tensor([80]), tensor([24]), tensor([45]), tensor([24]), tensor([68]), tensor([97]), tensor([72]), tensor([69]), tensor([97]), tensor([13]), tensor([82]), tensor([82]), tensor([27]), tensor([50]), tensor([27]), tensor([72]), tensor([68]), tensor([86]), tensor([95]), tensor([94]), tensor([86]), tensor([34]), tensor([78]), tensor([10]), tensor([13]), tensor([58]), tensor([88]), tensor([78]), tensor([97]), tensor([50]), tensor([81]), tensor([58]), tensor([78]), tensor([27]), tensor([30]), tensor([95]), tensor([49]), tensor([37]), tensor([80]), tensor([59]), tensor([24]), tensor([58]), tensor([69]), tensor([36]), tensor([86]), tensor([68]), tensor([88]), tensor([88]), tensor([25]), tensor([37]), tensor([88]), tensor([24]), tensor([60]), tensor([88]), tensor([86]), tensor([35]), tensor([88]), tensor([37]), tensor([97]), tensor([36]), tensor([30]), tensor([10]), tensor([86]), tensor([25]), tensor([37]), tensor([58]), tensor([68]), tensor([60]), tensor([3]), tensor([24]), tensor([82]), tensor([86]), tensor([27]), tensor([58]), tensor([50]), tensor([36]), tensor([3]), tensor([50]), tensor([34]), tensor([59]), tensor([30]), tensor([80]), tensor([58]), tensor([27]), tensor([3]), tensor([95]), tensor([37]), tensor([58]), tensor([37]), tensor([58]), tensor([68]), tensor([95]), tensor([49]), tensor([37]), tensor([25]), tensor([59]), tensor([49]), tensor([27]), tensor([3]), tensor([86]), tensor([58]), tensor([82]), tensor([97]), tensor([30]), tensor([68]), tensor([68]), tensor([27]), tensor([97]), tensor([95]), tensor([82]), tensor([60]), tensor([86]), tensor([36]), tensor([95]), tensor([78]), tensor([97]), tensor([3]), tensor([59]), tensor([27]), tensor([34]), tensor([35]), tensor([80]), tensor([49]), tensor([34]), tensor([72]), tensor([34]), tensor([88]), tensor([78]), tensor([69]), tensor([88]), tensor([24]), tensor([34]), tensor([88]), tensor([81]), tensor([13]), tensor([82]), tensor([58]), tensor([36]), tensor([95]), tensor([50]), tensor([27]), tensor([57]), tensor([86]), tensor([94]), tensor([80]), tensor([86]), tensor([60]), tensor([34]), tensor([13]), tensor([72]), tensor([78]), tensor([57]), tensor([82]), tensor([95]), tensor([57]), tensor([35]), tensor([36]), tensor([69]), tensor([72]), tensor([45]), tensor([25]), tensor([78]), tensor([58]), tensor([30]), tensor([10]), tensor([13]), tensor([57]), tensor([24]), tensor([82]), tensor([60]), tensor([72]), tensor([68]), tensor([58]), tensor([3]), tensor([24]), tensor([25]), tensor([37]), tensor([88]), tensor([24]), tensor([86]), tensor([95]), tensor([60]), tensor([58]), tensor([60]), tensor([30]), tensor([95]), tensor([36]), tensor([86]), tensor([94]), tensor([59]), tensor([27]), tensor([94]), tensor([97]), tensor([69]), tensor([13]), tensor([36]), tensor([82]), tensor([94]), tensor([10]), tensor([82]), tensor([10]), tensor([50]), tensor([50]), tensor([49]), tensor([24]), tensor([94]), tensor([82]), tensor([58]), tensor([25]), tensor([68]), tensor([78]), tensor([3]), tensor([68]), tensor([49]), tensor([78]), tensor([80]), tensor([72]), tensor([24]), tensor([95]), tensor([30]), tensor([60]), tensor([58]), tensor([97]), tensor([34]), tensor([60]), tensor([13]), tensor([72]), tensor([25]), tensor([25]), tensor([88]), tensor([13]), tensor([25]), tensor([24]), tensor([36]), tensor([59]), tensor([13]), tensor([97]), tensor([97]), tensor([50]), tensor([27]), tensor([82]), tensor([25]), tensor([24]), tensor([86]), tensor([57]), tensor([57]), tensor([59]), tensor([45]), tensor([86]), tensor([78]), tensor([57]), tensor([68]), tensor([69]), tensor([97]), tensor([10]), tensor([97]), tensor([78]), tensor([81]), tensor([10]), tensor([34]), tensor([68]), tensor([10]), tensor([10]), tensor([58]), tensor([34]), tensor([58]), tensor([49]), tensor([69]), tensor([60]), tensor([24]), tensor([82]), tensor([50]), tensor([30]), tensor([72]), tensor([78]), tensor([45]), tensor([37]), tensor([81]), tensor([88]), tensor([69]), tensor([86]), tensor([49]), tensor([97]), tensor([45]), tensor([27]), tensor([59]), tensor([10]), tensor([80]), tensor([72]), tensor([80]), tensor([50]), tensor([72]), tensor([97]), tensor([86]), tensor([37]), tensor([68]), tensor([49]), tensor([37]), tensor([86]), tensor([25]), tensor([82]), tensor([81]), tensor([86]), tensor([69]), tensor([24]), tensor([69]), tensor([30]), tensor([49]), tensor([3]), tensor([78]), tensor([68]), tensor([24]), tensor([94]), tensor([59]), tensor([27]), tensor([50]), tensor([27]), tensor([57]), tensor([45]), tensor([10]), tensor([80]), tensor([94]), tensor([88]), tensor([72]), tensor([13]), tensor([88]), tensor([30]), tensor([59]), tensor([45]), tensor([57]), tensor([72]), tensor([36]), tensor([45]), tensor([30]), tensor([30]), tensor([34]), tensor([34]), tensor([82]), tensor([3]), tensor([3]), tensor([27]), tensor([86]), tensor([72]), tensor([3]), tensor([72]), tensor([57]), tensor([80]), tensor([27]), tensor([45]), tensor([88]), tensor([86]), tensor([24]), tensor([95]), tensor([60]), tensor([50]), tensor([72]), tensor([13]), tensor([94]), tensor([94]), tensor([72]), tensor([27]), tensor([68]), tensor([50]), tensor([25]), tensor([34]), tensor([50]), tensor([50]), tensor([27]), tensor([59]), tensor([3]), tensor([10]), tensor([13]), tensor([80]), tensor([49]), tensor([58]), tensor([72]), tensor([81]), tensor([82]), tensor([10]), tensor([88]), tensor([59]), tensor([59]), tensor([50]), tensor([97]), tensor([58]), tensor([78]), tensor([13]), tensor([82]), tensor([10]), tensor([68]), tensor([25]), tensor([35]), tensor([50]), tensor([50]), tensor([97]), tensor([49]), tensor([13]), tensor([37]), tensor([57]), tensor([30]), tensor([35]), tensor([35]), tensor([78]), tensor([25]), tensor([86]), tensor([3]), tensor([30]), tensor([37]), tensor([37]), tensor([94]), tensor([68]), tensor([94]), tensor([50]), tensor([49]), tensor([86]), tensor([88]), tensor([25]), tensor([25]), tensor([57]), tensor([60]), tensor([88]), tensor([97]), tensor([78]), tensor([36]), tensor([80]), tensor([25]), tensor([94]), tensor([58]), tensor([60]), tensor([72]), tensor([35]), tensor([49]), tensor([95]), tensor([10]), tensor([25]), tensor([25]), tensor([97]), tensor([37]), tensor([30]), tensor([81]), tensor([35]), tensor([68]), tensor([95]), tensor([34]), tensor([58]), tensor([13]), tensor([80]), tensor([25]), tensor([49]), tensor([82]), tensor([94]), tensor([34]), tensor([88]), tensor([95]), tensor([80]), tensor([30]), tensor([35]), tensor([68]), tensor([13]), tensor([34]), tensor([59]), tensor([35]), tensor([24]), tensor([30]), tensor([82]), tensor([82]), tensor([35]), tensor([97]), tensor([37]), tensor([78]), tensor([72]), tensor([58]), tensor([36]), tensor([45]), tensor([97]), tensor([10]), tensor([34]), tensor([49]), tensor([27]), tensor([68]), tensor([82]), tensor([3]), tensor([36]), tensor([78]), tensor([72]), tensor([49]), tensor([58]), tensor([72]), tensor([94]), tensor([68]), tensor([13]), tensor([95]), tensor([3]), tensor([95]), tensor([58]), tensor([72]), tensor([72]), tensor([81]), tensor([36]), tensor([59]), tensor([57]), tensor([78]), tensor([81]), tensor([78]), tensor([57]), tensor([60]), tensor([34]), tensor([94]), tensor([37]), tensor([3]), tensor([3]), tensor([82]), tensor([80]), tensor([27]), tensor([68]), tensor([68]), tensor([13]), tensor([57]), tensor([97]), tensor([78]), tensor([45]), tensor([86]), tensor([95]), tensor([34]), tensor([69]), tensor([45]), tensor([10]), tensor([37]), tensor([68]), tensor([45]), tensor([82]), tensor([10]), tensor([34]), tensor([3]), tensor([95]), tensor([27]), tensor([94]), tensor([3]), tensor([25]), tensor([72]), tensor([45]), tensor([80]), tensor([60]), tensor([86]), tensor([36]), tensor([37]), tensor([36]), tensor([59])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.66 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.629\n",
            "TEST ALL:  0.6686666666666666\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  4000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 95, 92, 88, 86, 82, 80, 78, 72, 68, 60, 58, 52, 50, 36, 34, 30, 24, 12, 94, 96, 98, 45, 81, 77, 69, 59, 57, 53, 49, 37, 3, 35, 33, 31, 27, 25, 13, 11, 10]\n",
            "TRAIN_SET CLASSES:  [31, 11, 98, 77, 53, 33, 96, 92, 52, 12]\n",
            "VALIDATION CLASSES:  [53, 52, 98, 33, 96, 31, 92, 77, 12, 11]\n",
            "GROUP:  4\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [31, 11, 98, 77, 53, 33, 96, 92, 52, 12]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "num classes till now:  40\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.3073346018791199\n",
            "Train step - Step 10, Loss 0.1480991691350937\n",
            "Train step - Step 20, Loss 0.13023869693279266\n",
            "Train step - Step 30, Loss 0.12014985084533691\n",
            "Train step - Step 40, Loss 0.11557386070489883\n",
            "Train step - Step 50, Loss 0.11761804670095444\n",
            "Train epoch - Accuracy: 0.2253968253968254 Loss: 0.14208179342454302 Corrects: 1562\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.11956510692834854\n",
            "Train step - Step 70, Loss 0.11104216426610947\n",
            "Train step - Step 80, Loss 0.11280784755945206\n",
            "Train step - Step 90, Loss 0.11579904705286026\n",
            "Train step - Step 100, Loss 0.11548564583063126\n",
            "Train epoch - Accuracy: 0.25266955266955266 Loss: 0.11274286488347927 Corrects: 1751\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.10096504539251328\n",
            "Train step - Step 120, Loss 0.12171955406665802\n",
            "Train step - Step 130, Loss 0.1140647754073143\n",
            "Train step - Step 140, Loss 0.10412990301847458\n",
            "Train step - Step 150, Loss 0.11500344425439835\n",
            "Train step - Step 160, Loss 0.10406651347875595\n",
            "Train epoch - Accuracy: 0.26810966810966813 Loss: 0.11041771090047872 Corrects: 1858\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.10949307680130005\n",
            "Train step - Step 180, Loss 0.11347679048776627\n",
            "Train step - Step 190, Loss 0.10351679474115372\n",
            "Train step - Step 200, Loss 0.10616955906152725\n",
            "Train step - Step 210, Loss 0.11427489668130875\n",
            "Train epoch - Accuracy: 0.2867243867243867 Loss: 0.10821211563173311 Corrects: 1987\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.10581977665424347\n",
            "Train step - Step 230, Loss 0.10697080194950104\n",
            "Train step - Step 240, Loss 0.11016473919153214\n",
            "Train step - Step 250, Loss 0.1080569252371788\n",
            "Train step - Step 260, Loss 0.10822806507349014\n",
            "Train step - Step 270, Loss 0.10467226803302765\n",
            "Train epoch - Accuracy: 0.30447330447330445 Loss: 0.10708875214683718 Corrects: 2110\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.10313472896814346\n",
            "Train step - Step 290, Loss 0.10525494068861008\n",
            "Train step - Step 300, Loss 0.10753007233142853\n",
            "Train step - Step 310, Loss 0.09284785389900208\n",
            "Train step - Step 320, Loss 0.10813412815332413\n",
            "Train epoch - Accuracy: 0.31154401154401157 Loss: 0.10655235765128253 Corrects: 2159\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11063102632761002\n",
            "Train step - Step 340, Loss 0.1032189279794693\n",
            "Train step - Step 350, Loss 0.10238444060087204\n",
            "Train step - Step 360, Loss 0.09718789160251617\n",
            "Train step - Step 370, Loss 0.10745420306921005\n",
            "Train step - Step 380, Loss 0.1108352541923523\n",
            "Train epoch - Accuracy: 0.33015873015873015 Loss: 0.10541329168181054 Corrects: 2288\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.11108193546533585\n",
            "Train step - Step 400, Loss 0.1079028844833374\n",
            "Train step - Step 410, Loss 0.10221685469150543\n",
            "Train step - Step 420, Loss 0.10259101539850235\n",
            "Train step - Step 430, Loss 0.09454160928726196\n",
            "Train epoch - Accuracy: 0.3329004329004329 Loss: 0.10546921327774658 Corrects: 2307\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.1103404089808464\n",
            "Train step - Step 450, Loss 0.10151410102844238\n",
            "Train step - Step 460, Loss 0.10482936352491379\n",
            "Train step - Step 470, Loss 0.10243437439203262\n",
            "Train step - Step 480, Loss 0.1061353012919426\n",
            "Train step - Step 490, Loss 0.09856237471103668\n",
            "Train epoch - Accuracy: 0.33910533910533913 Loss: 0.10420990554242252 Corrects: 2350\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10924811661243439\n",
            "Train step - Step 510, Loss 0.10205985605716705\n",
            "Train step - Step 520, Loss 0.10572909563779831\n",
            "Train step - Step 530, Loss 0.10204048454761505\n",
            "Train step - Step 540, Loss 0.10180198401212692\n",
            "Train epoch - Accuracy: 0.3464646464646465 Loss: 0.1046870245540469 Corrects: 2401\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.10944072157144547\n",
            "Train step - Step 560, Loss 0.11173780262470245\n",
            "Train step - Step 570, Loss 0.09572450071573257\n",
            "Train step - Step 580, Loss 0.10566937178373337\n",
            "Train step - Step 590, Loss 0.11440491676330566\n",
            "Train step - Step 600, Loss 0.10373914241790771\n",
            "Train epoch - Accuracy: 0.3577200577200577 Loss: 0.1042647104328673 Corrects: 2479\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.10635427385568619\n",
            "Train step - Step 620, Loss 0.09808206558227539\n",
            "Train step - Step 630, Loss 0.10784973949193954\n",
            "Train step - Step 640, Loss 0.10120978206396103\n",
            "Train step - Step 650, Loss 0.10698049515485764\n",
            "Train epoch - Accuracy: 0.3660894660894661 Loss: 0.103577994457399 Corrects: 2537\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10872111469507217\n",
            "Train step - Step 670, Loss 0.09544453769922256\n",
            "Train step - Step 680, Loss 0.1096452847123146\n",
            "Train step - Step 690, Loss 0.09726341813802719\n",
            "Train step - Step 700, Loss 0.09812896698713303\n",
            "Train step - Step 710, Loss 0.10519436746835709\n",
            "Train epoch - Accuracy: 0.37676767676767675 Loss: 0.10337531611129835 Corrects: 2611\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10167764872312546\n",
            "Train step - Step 730, Loss 0.1072809249162674\n",
            "Train step - Step 740, Loss 0.11063728481531143\n",
            "Train step - Step 750, Loss 0.09839712828397751\n",
            "Train step - Step 760, Loss 0.09868873655796051\n",
            "Train epoch - Accuracy: 0.38225108225108223 Loss: 0.1026655432164755 Corrects: 2649\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10115231573581696\n",
            "Train step - Step 780, Loss 0.10021913051605225\n",
            "Train step - Step 790, Loss 0.09926768392324448\n",
            "Train step - Step 800, Loss 0.10027740150690079\n",
            "Train step - Step 810, Loss 0.10009435564279556\n",
            "Train step - Step 820, Loss 0.10086234658956528\n",
            "Train epoch - Accuracy: 0.38441558441558443 Loss: 0.10209871838662188 Corrects: 2664\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.09181171655654907\n",
            "Train step - Step 840, Loss 0.10229893028736115\n",
            "Train step - Step 850, Loss 0.10149097442626953\n",
            "Train step - Step 860, Loss 0.0968412309885025\n",
            "Train step - Step 870, Loss 0.10262322425842285\n",
            "Train epoch - Accuracy: 0.3930735930735931 Loss: 0.10216522926806028 Corrects: 2724\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.09923968464136124\n",
            "Train step - Step 890, Loss 0.0982104241847992\n",
            "Train step - Step 900, Loss 0.10101731866598129\n",
            "Train step - Step 910, Loss 0.09948103874921799\n",
            "Train step - Step 920, Loss 0.1038048192858696\n",
            "Train step - Step 930, Loss 0.09447764605283737\n",
            "Train epoch - Accuracy: 0.4002886002886003 Loss: 0.10227622474082793 Corrects: 2774\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.10148904472589493\n",
            "Train step - Step 950, Loss 0.09863685816526413\n",
            "Train step - Step 960, Loss 0.10014193505048752\n",
            "Train step - Step 970, Loss 0.09791777282953262\n",
            "Train step - Step 980, Loss 0.10515346378087997\n",
            "Train epoch - Accuracy: 0.39855699855699855 Loss: 0.10174654174977739 Corrects: 2762\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.0977393090724945\n",
            "Train step - Step 1000, Loss 0.09715157747268677\n",
            "Train step - Step 1010, Loss 0.09563546627759933\n",
            "Train step - Step 1020, Loss 0.10423469543457031\n",
            "Train step - Step 1030, Loss 0.10386121273040771\n",
            "Train step - Step 1040, Loss 0.09685119241476059\n",
            "Train epoch - Accuracy: 0.40894660894660895 Loss: 0.10105522846504723 Corrects: 2834\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.09621112048625946\n",
            "Train step - Step 1060, Loss 0.10888013988733292\n",
            "Train step - Step 1070, Loss 0.099806047976017\n",
            "Train step - Step 1080, Loss 0.1054057851433754\n",
            "Train step - Step 1090, Loss 0.10287012159824371\n",
            "Train epoch - Accuracy: 0.41284271284271284 Loss: 0.10114753580454625 Corrects: 2861\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.09727699309587479\n",
            "Train step - Step 1110, Loss 0.09593977779150009\n",
            "Train step - Step 1120, Loss 0.09867918491363525\n",
            "Train step - Step 1130, Loss 0.09354998171329498\n",
            "Train step - Step 1140, Loss 0.09836740046739578\n",
            "Train step - Step 1150, Loss 0.10430946201086044\n",
            "Train epoch - Accuracy: 0.4288600288600289 Loss: 0.1009411845114324 Corrects: 2972\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10636980831623077\n",
            "Train step - Step 1170, Loss 0.10506885498762131\n",
            "Train step - Step 1180, Loss 0.09726681560277939\n",
            "Train step - Step 1190, Loss 0.10805258899927139\n",
            "Train step - Step 1200, Loss 0.09933515638113022\n",
            "Train epoch - Accuracy: 0.42655122655122657 Loss: 0.10050496612534379 Corrects: 2956\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.09644343703985214\n",
            "Train step - Step 1220, Loss 0.10693464428186417\n",
            "Train step - Step 1230, Loss 0.09511859714984894\n",
            "Train step - Step 1240, Loss 0.10007991641759872\n",
            "Train step - Step 1250, Loss 0.10014473646879196\n",
            "Train step - Step 1260, Loss 0.1005738154053688\n",
            "Train epoch - Accuracy: 0.43102453102453103 Loss: 0.1002027116167597 Corrects: 2987\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.1025499701499939\n",
            "Train step - Step 1280, Loss 0.10265964269638062\n",
            "Train step - Step 1290, Loss 0.09789279848337173\n",
            "Train step - Step 1300, Loss 0.09562303125858307\n",
            "Train step - Step 1310, Loss 0.09099739044904709\n",
            "Train epoch - Accuracy: 0.43232323232323233 Loss: 0.09960526055716849 Corrects: 2996\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.09537853300571442\n",
            "Train step - Step 1330, Loss 0.09826133400201797\n",
            "Train step - Step 1340, Loss 0.10436160862445831\n",
            "Train step - Step 1350, Loss 0.09823613613843918\n",
            "Train step - Step 1360, Loss 0.09901461750268936\n",
            "Train step - Step 1370, Loss 0.10087913274765015\n",
            "Train epoch - Accuracy: 0.4406926406926407 Loss: 0.09989547381153355 Corrects: 3054\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.09808685630559921\n",
            "Train step - Step 1390, Loss 0.09831305593252182\n",
            "Train step - Step 1400, Loss 0.09382259845733643\n",
            "Train step - Step 1410, Loss 0.09444605559110641\n",
            "Train step - Step 1420, Loss 0.10030974447727203\n",
            "Train epoch - Accuracy: 0.44372294372294374 Loss: 0.09953289036282902 Corrects: 3075\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.10691952705383301\n",
            "Train step - Step 1440, Loss 0.10131946951150894\n",
            "Train step - Step 1450, Loss 0.10133900493383408\n",
            "Train step - Step 1460, Loss 0.10075552761554718\n",
            "Train step - Step 1470, Loss 0.09960083663463593\n",
            "Train step - Step 1480, Loss 0.10413713753223419\n",
            "Train epoch - Accuracy: 0.44516594516594515 Loss: 0.10080906451823564 Corrects: 3085\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.09905374050140381\n",
            "Train step - Step 1500, Loss 0.10154397785663605\n",
            "Train step - Step 1510, Loss 0.09310275316238403\n",
            "Train step - Step 1520, Loss 0.09694653004407883\n",
            "Train step - Step 1530, Loss 0.11009764671325684\n",
            "Train epoch - Accuracy: 0.4531024531024531 Loss: 0.09917264888855974 Corrects: 3140\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.10101651400327682\n",
            "Train step - Step 1550, Loss 0.09937717765569687\n",
            "Train step - Step 1560, Loss 0.1013263687491417\n",
            "Train step - Step 1570, Loss 0.095622219145298\n",
            "Train step - Step 1580, Loss 0.09873232245445251\n",
            "Train step - Step 1590, Loss 0.09185225516557693\n",
            "Train epoch - Accuracy: 0.45656565656565656 Loss: 0.09846686403687727 Corrects: 3164\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.11031820625066757\n",
            "Train step - Step 1610, Loss 0.09628962725400925\n",
            "Train step - Step 1620, Loss 0.09936636686325073\n",
            "Train step - Step 1630, Loss 0.09174177050590515\n",
            "Train step - Step 1640, Loss 0.1013450175523758\n",
            "Train epoch - Accuracy: 0.45916305916305916 Loss: 0.09943630806295387 Corrects: 3182\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.09426383674144745\n",
            "Train step - Step 1660, Loss 0.10330326855182648\n",
            "Train step - Step 1670, Loss 0.09845548868179321\n",
            "Train step - Step 1680, Loss 0.09715045988559723\n",
            "Train step - Step 1690, Loss 0.1036992073059082\n",
            "Train step - Step 1700, Loss 0.09432192891836166\n",
            "Train epoch - Accuracy: 0.4665223665223665 Loss: 0.09832121624919071 Corrects: 3233\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.0961555615067482\n",
            "Train step - Step 1720, Loss 0.09692354500293732\n",
            "Train step - Step 1730, Loss 0.09616636484861374\n",
            "Train step - Step 1740, Loss 0.09876399487257004\n",
            "Train step - Step 1750, Loss 0.09453659504652023\n",
            "Train epoch - Accuracy: 0.4668109668109668 Loss: 0.09819515299960477 Corrects: 3235\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.09560181200504303\n",
            "Train step - Step 1770, Loss 0.09465458244085312\n",
            "Train step - Step 1780, Loss 0.09735988080501556\n",
            "Train step - Step 1790, Loss 0.0949755385518074\n",
            "Train step - Step 1800, Loss 0.10063965618610382\n",
            "Train step - Step 1810, Loss 0.09421682357788086\n",
            "Train epoch - Accuracy: 0.4746031746031746 Loss: 0.0977420178627727 Corrects: 3289\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.09949290007352829\n",
            "Train step - Step 1830, Loss 0.09804785251617432\n",
            "Train step - Step 1840, Loss 0.09120408445596695\n",
            "Train step - Step 1850, Loss 0.10210692882537842\n",
            "Train step - Step 1860, Loss 0.10320629924535751\n",
            "Train epoch - Accuracy: 0.4743145743145743 Loss: 0.09823688956738207 Corrects: 3287\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.09535720944404602\n",
            "Train step - Step 1880, Loss 0.09315773099660873\n",
            "Train step - Step 1890, Loss 0.09699176996946335\n",
            "Train step - Step 1900, Loss 0.10058505833148956\n",
            "Train step - Step 1910, Loss 0.09439336508512497\n",
            "Train step - Step 1920, Loss 0.09726909548044205\n",
            "Train epoch - Accuracy: 0.47503607503607503 Loss: 0.09761840320295757 Corrects: 3292\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.09700318425893784\n",
            "Train step - Step 1940, Loss 0.10350699722766876\n",
            "Train step - Step 1950, Loss 0.1042802557349205\n",
            "Train step - Step 1960, Loss 0.1007702499628067\n",
            "Train step - Step 1970, Loss 0.0960497185587883\n",
            "Train epoch - Accuracy: 0.4832611832611833 Loss: 0.0975247573134359 Corrects: 3349\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.0995960459113121\n",
            "Train step - Step 1990, Loss 0.09890419989824295\n",
            "Train step - Step 2000, Loss 0.09924816340208054\n",
            "Train step - Step 2010, Loss 0.10442771762609482\n",
            "Train step - Step 2020, Loss 0.09527073800563812\n",
            "Train step - Step 2030, Loss 0.1119525209069252\n",
            "Train epoch - Accuracy: 0.48787878787878786 Loss: 0.09782196283340454 Corrects: 3381\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.09880878776311874\n",
            "Train step - Step 2050, Loss 0.0917138084769249\n",
            "Train step - Step 2060, Loss 0.0970720574259758\n",
            "Train step - Step 2070, Loss 0.09437098354101181\n",
            "Train step - Step 2080, Loss 0.09208861738443375\n",
            "Train epoch - Accuracy: 0.4871572871572872 Loss: 0.09768979359971841 Corrects: 3376\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.09233410656452179\n",
            "Train step - Step 2100, Loss 0.10121779888868332\n",
            "Train step - Step 2110, Loss 0.10158912092447281\n",
            "Train step - Step 2120, Loss 0.0979093536734581\n",
            "Train step - Step 2130, Loss 0.10225389152765274\n",
            "Train step - Step 2140, Loss 0.09455644339323044\n",
            "Train epoch - Accuracy: 0.49855699855699853 Loss: 0.09748673806931893 Corrects: 3455\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.09596476703882217\n",
            "Train step - Step 2160, Loss 0.09842758625745773\n",
            "Train step - Step 2170, Loss 0.0992061197757721\n",
            "Train step - Step 2180, Loss 0.09637937694787979\n",
            "Train step - Step 2190, Loss 0.09887118637561798\n",
            "Train epoch - Accuracy: 0.5053391053391053 Loss: 0.0971109595585179 Corrects: 3502\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.09126700460910797\n",
            "Train step - Step 2210, Loss 0.09742414951324463\n",
            "Train step - Step 2220, Loss 0.10693866014480591\n",
            "Train step - Step 2230, Loss 0.09405491501092911\n",
            "Train step - Step 2240, Loss 0.0967523455619812\n",
            "Train step - Step 2250, Loss 0.09396063536405563\n",
            "Train epoch - Accuracy: 0.49393939393939396 Loss: 0.09674559909027892 Corrects: 3423\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.09733986109495163\n",
            "Train step - Step 2270, Loss 0.0905914306640625\n",
            "Train step - Step 2280, Loss 0.09753473848104477\n",
            "Train step - Step 2290, Loss 0.09721497446298599\n",
            "Train step - Step 2300, Loss 0.09175533056259155\n",
            "Train epoch - Accuracy: 0.5092352092352093 Loss: 0.09713010327812568 Corrects: 3529\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.09656532853841782\n",
            "Train step - Step 2320, Loss 0.09329584985971451\n",
            "Train step - Step 2330, Loss 0.10366035997867584\n",
            "Train step - Step 2340, Loss 0.0958186611533165\n",
            "Train step - Step 2350, Loss 0.09515539556741714\n",
            "Train step - Step 2360, Loss 0.09603734314441681\n",
            "Train epoch - Accuracy: 0.5050505050505051 Loss: 0.09685444028088541 Corrects: 3500\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.09467434883117676\n",
            "Train step - Step 2380, Loss 0.09631792455911636\n",
            "Train step - Step 2390, Loss 0.09425117075443268\n",
            "Train step - Step 2400, Loss 0.09982263296842575\n",
            "Train step - Step 2410, Loss 0.09597712755203247\n",
            "Train epoch - Accuracy: 0.5124098124098124 Loss: 0.09623493250529315 Corrects: 3551\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.09113726764917374\n",
            "Train step - Step 2430, Loss 0.09242258965969086\n",
            "Train step - Step 2440, Loss 0.09291740506887436\n",
            "Train step - Step 2450, Loss 0.09430890530347824\n",
            "Train step - Step 2460, Loss 0.08821886032819748\n",
            "Train step - Step 2470, Loss 0.0990137830376625\n",
            "Train epoch - Accuracy: 0.5131313131313131 Loss: 0.0961864707746891 Corrects: 3556\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.09657149761915207\n",
            "Train step - Step 2490, Loss 0.09433891624212265\n",
            "Train step - Step 2500, Loss 0.09282097220420837\n",
            "Train step - Step 2510, Loss 0.0940684899687767\n",
            "Train step - Step 2520, Loss 0.09670952707529068\n",
            "Train epoch - Accuracy: 0.515007215007215 Loss: 0.09611561370618416 Corrects: 3569\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.0988488644361496\n",
            "Train step - Step 2540, Loss 0.09696682542562485\n",
            "Train step - Step 2550, Loss 0.09623509645462036\n",
            "Train step - Step 2560, Loss 0.09986110776662827\n",
            "Train step - Step 2570, Loss 0.10041172802448273\n",
            "Train step - Step 2580, Loss 0.102719746530056\n",
            "Train epoch - Accuracy: 0.5253968253968254 Loss: 0.0957125021932273 Corrects: 3641\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.09741155803203583\n",
            "Train step - Step 2600, Loss 0.09216010570526123\n",
            "Train step - Step 2610, Loss 0.09637414664030075\n",
            "Train step - Step 2620, Loss 0.09358341246843338\n",
            "Train step - Step 2630, Loss 0.10094467550516129\n",
            "Train epoch - Accuracy: 0.5321789321789322 Loss: 0.09551383651051171 Corrects: 3688\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.09432730823755264\n",
            "Train step - Step 2650, Loss 0.08999278396368027\n",
            "Train step - Step 2660, Loss 0.0986882895231247\n",
            "Train step - Step 2670, Loss 0.09716159105300903\n",
            "Train step - Step 2680, Loss 0.100896455347538\n",
            "Train step - Step 2690, Loss 0.09381072968244553\n",
            "Train epoch - Accuracy: 0.5357864357864358 Loss: 0.09532553843003502 Corrects: 3713\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.09680300951004028\n",
            "Train step - Step 2710, Loss 0.0990324541926384\n",
            "Train step - Step 2720, Loss 0.09866403043270111\n",
            "Train step - Step 2730, Loss 0.08957703411579132\n",
            "Train step - Step 2740, Loss 0.09261628985404968\n",
            "Train epoch - Accuracy: 0.5341991341991342 Loss: 0.09354934946111099 Corrects: 3702\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.09428530931472778\n",
            "Train step - Step 2760, Loss 0.09089644253253937\n",
            "Train step - Step 2770, Loss 0.08993344753980637\n",
            "Train step - Step 2780, Loss 0.08447644114494324\n",
            "Train step - Step 2790, Loss 0.09393709897994995\n",
            "Train step - Step 2800, Loss 0.0995442271232605\n",
            "Train epoch - Accuracy: 0.5366522366522366 Loss: 0.09331502225061859 Corrects: 3719\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.09498847275972366\n",
            "Train step - Step 2820, Loss 0.08946675062179565\n",
            "Train step - Step 2830, Loss 0.0881929025053978\n",
            "Train step - Step 2840, Loss 0.09293469786643982\n",
            "Train step - Step 2850, Loss 0.09044908732175827\n",
            "Train epoch - Accuracy: 0.5522366522366522 Loss: 0.09286967732689598 Corrects: 3827\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.08868025243282318\n",
            "Train step - Step 2870, Loss 0.09488091617822647\n",
            "Train step - Step 2880, Loss 0.0995616614818573\n",
            "Train step - Step 2890, Loss 0.09401190280914307\n",
            "Train step - Step 2900, Loss 0.09237315505743027\n",
            "Train step - Step 2910, Loss 0.09339506924152374\n",
            "Train epoch - Accuracy: 0.5415584415584416 Loss: 0.09321577552851144 Corrects: 3753\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.08922895044088364\n",
            "Train step - Step 2930, Loss 0.09169792383909225\n",
            "Train step - Step 2940, Loss 0.08825687319040298\n",
            "Train step - Step 2950, Loss 0.09228914231061935\n",
            "Train step - Step 2960, Loss 0.08830014616250992\n",
            "Train epoch - Accuracy: 0.5526695526695526 Loss: 0.09322876889167238 Corrects: 3830\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.09444893896579742\n",
            "Train step - Step 2980, Loss 0.09270517528057098\n",
            "Train step - Step 2990, Loss 0.09125449508428574\n",
            "Train step - Step 3000, Loss 0.09402626752853394\n",
            "Train step - Step 3010, Loss 0.09743861854076385\n",
            "Train step - Step 3020, Loss 0.08824288100004196\n",
            "Train epoch - Accuracy: 0.543001443001443 Loss: 0.09293593450585141 Corrects: 3763\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.08723610639572144\n",
            "Train step - Step 3040, Loss 0.09257420152425766\n",
            "Train step - Step 3050, Loss 0.08960682898759842\n",
            "Train step - Step 3060, Loss 0.09354548901319504\n",
            "Train step - Step 3070, Loss 0.09095194190740585\n",
            "Train epoch - Accuracy: 0.5431457431457432 Loss: 0.09211607575201541 Corrects: 3764\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.08957380056381226\n",
            "Train step - Step 3090, Loss 0.09344011545181274\n",
            "Train step - Step 3100, Loss 0.08739560097455978\n",
            "Train step - Step 3110, Loss 0.10008648782968521\n",
            "Train step - Step 3120, Loss 0.09030144661664963\n",
            "Train step - Step 3130, Loss 0.09074580669403076\n",
            "Train epoch - Accuracy: 0.5556998556998557 Loss: 0.09267354704987951 Corrects: 3851\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.08734912425279617\n",
            "Train step - Step 3150, Loss 0.0885373055934906\n",
            "Train step - Step 3160, Loss 0.09001864492893219\n",
            "Train step - Step 3170, Loss 0.09557676315307617\n",
            "Train step - Step 3180, Loss 0.09071671962738037\n",
            "Train epoch - Accuracy: 0.5418470418470418 Loss: 0.0925060353388346 Corrects: 3755\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.09464231878519058\n",
            "Train step - Step 3200, Loss 0.09572774171829224\n",
            "Train step - Step 3210, Loss 0.0915985032916069\n",
            "Train step - Step 3220, Loss 0.09168683737516403\n",
            "Train step - Step 3230, Loss 0.09162979573011398\n",
            "Train step - Step 3240, Loss 0.09659158438444138\n",
            "Train epoch - Accuracy: 0.5507936507936508 Loss: 0.09281958529099414 Corrects: 3817\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.08960920572280884\n",
            "Train step - Step 3260, Loss 0.08637744188308716\n",
            "Train step - Step 3270, Loss 0.09524865448474884\n",
            "Train step - Step 3280, Loss 0.09869902580976486\n",
            "Train step - Step 3290, Loss 0.09311310946941376\n",
            "Train epoch - Accuracy: 0.5542568542568542 Loss: 0.0922262026784568 Corrects: 3841\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.08904316276311874\n",
            "Train step - Step 3310, Loss 0.09020593017339706\n",
            "Train step - Step 3320, Loss 0.09071778506040573\n",
            "Train step - Step 3330, Loss 0.09596502780914307\n",
            "Train step - Step 3340, Loss 0.08906444162130356\n",
            "Train step - Step 3350, Loss 0.08740934729576111\n",
            "Train epoch - Accuracy: 0.5471861471861472 Loss: 0.0926636230549943 Corrects: 3792\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.09533378481864929\n",
            "Train step - Step 3370, Loss 0.09181796759366989\n",
            "Train step - Step 3380, Loss 0.09714767336845398\n",
            "Train step - Step 3390, Loss 0.08787932991981506\n",
            "Train step - Step 3400, Loss 0.09818834066390991\n",
            "Train epoch - Accuracy: 0.5477633477633478 Loss: 0.09254343082851489 Corrects: 3796\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.08641214668750763\n",
            "Train step - Step 3420, Loss 0.09167183190584183\n",
            "Train step - Step 3430, Loss 0.09157409518957138\n",
            "Train step - Step 3440, Loss 0.08931421488523483\n",
            "Train step - Step 3450, Loss 0.09531664103269577\n",
            "Train step - Step 3460, Loss 0.09456660598516464\n",
            "Train epoch - Accuracy: 0.5595959595959596 Loss: 0.09278860530811987 Corrects: 3878\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.09325321763753891\n",
            "Train step - Step 3480, Loss 0.09583693742752075\n",
            "Train step - Step 3490, Loss 0.09236392378807068\n",
            "Train step - Step 3500, Loss 0.08953793346881866\n",
            "Train step - Step 3510, Loss 0.09123405069112778\n",
            "Train epoch - Accuracy: 0.5525252525252525 Loss: 0.0920817723973489 Corrects: 3829\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.08786027878522873\n",
            "Train step - Step 3530, Loss 0.08874095976352692\n",
            "Train step - Step 3540, Loss 0.10168089717626572\n",
            "Train step - Step 3550, Loss 0.0891566053032875\n",
            "Train step - Step 3560, Loss 0.09224238246679306\n",
            "Train step - Step 3570, Loss 0.09911925345659256\n",
            "Train epoch - Accuracy: 0.5506493506493506 Loss: 0.09190083130398526 Corrects: 3816\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.08482461422681808\n",
            "Train step - Step 3590, Loss 0.09803100675344467\n",
            "Train step - Step 3600, Loss 0.09786628931760788\n",
            "Train step - Step 3610, Loss 0.0825764462351799\n",
            "Train step - Step 3620, Loss 0.08680054545402527\n",
            "Train epoch - Accuracy: 0.554978354978355 Loss: 0.09191817584921959 Corrects: 3846\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.08175988495349884\n",
            "Train step - Step 3640, Loss 0.09481433779001236\n",
            "Train step - Step 3650, Loss 0.09139343351125717\n",
            "Train step - Step 3660, Loss 0.10146579891443253\n",
            "Train step - Step 3670, Loss 0.09209644794464111\n",
            "Train step - Step 3680, Loss 0.09385232627391815\n",
            "Train epoch - Accuracy: 0.551948051948052 Loss: 0.09188630887096234 Corrects: 3825\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.09047018736600876\n",
            "Train step - Step 3700, Loss 0.09408275783061981\n",
            "Train step - Step 3710, Loss 0.08903385698795319\n",
            "Train step - Step 3720, Loss 0.09319835156202316\n",
            "Train step - Step 3730, Loss 0.09326326847076416\n",
            "Train epoch - Accuracy: 0.5581529581529582 Loss: 0.09187070429540128 Corrects: 3868\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.08610884100198746\n",
            "Train step - Step 3750, Loss 0.0929359719157219\n",
            "Train step - Step 3760, Loss 0.08689181506633759\n",
            "Train step - Step 3770, Loss 0.0958409458398819\n",
            "Train step - Step 3780, Loss 0.08985867351293564\n",
            "Train step - Step 3790, Loss 0.08801876753568649\n",
            "Train epoch - Accuracy: 0.5608946608946609 Loss: 0.09176041794803752 Corrects: 3887\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.09162988513708115\n",
            "Train step - Step 3810, Loss 0.09269370138645172\n",
            "Train step - Step 3820, Loss 0.09265007078647614\n",
            "Train step - Step 3830, Loss 0.0964227095246315\n",
            "Train step - Step 3840, Loss 0.0839262381196022\n",
            "Train epoch - Accuracy: 0.5565656565656566 Loss: 0.0917451428214537 Corrects: 3857\n",
            "Training finished in 394.04053378105164 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12]\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a91110>\n",
            "Constructing exemplars of class 31\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [27608, 43659, 45735, 18319, 10264, 39658, 14187, 42240, 20503, 35417, 9751, 39763, 40510, 577, 12848, 43388, 32942, 12992, 44187, 48403, 35975, 20730, 14539, 13151, 9753, 13459, 28480, 42087, 15529, 20777, 40691, 27670, 40252, 48626, 8073, 17216, 4710, 41840, 33089, 23690, 29212, 29539, 41495, 24566, 15771, 21101, 13961, 35620, 7567, 16115]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22442a9510>\n",
            "Constructing exemplars of class 11\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [4897, 49105, 20085, 15865, 7458, 38189, 367, 26462, 16452, 33575, 36056, 20769, 3, 9023, 25957, 3227, 1368, 48891, 7693, 28452, 35464, 30176, 21793, 42551, 16581, 5619, 36180, 2423, 41156, 36488, 12481, 27340, 26880, 10855, 7885, 24853, 15539, 29728, 40971, 33108, 48891, 2419, 39356, 13508, 23647, 11909, 39843, 13999, 44073, 45868]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224468fe90>\n",
            "Constructing exemplars of class 98\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [42690, 5299, 42097, 9619, 49448, 34236, 32663, 36700, 19014, 14675, 657, 45929, 32663, 7033, 39330, 12289, 38887, 31312, 11092, 44549, 40002, 13020, 9881, 7263, 35778, 22129, 30599, 33973, 34740, 47852, 18208, 41402, 48986, 5738, 48221, 11547, 2106, 22507, 3011, 40002, 41829, 26782, 29909, 39449, 18876, 32017, 24587, 32462, 41970, 15118]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22afb73a50>\n",
            "Constructing exemplars of class 77\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [13547, 43871, 37689, 9098, 35045, 5255, 8467, 8840, 41679, 27822, 29258, 37355, 31643, 992, 5699, 40491, 32844, 13791, 32869, 42729, 6332, 27473, 20773, 47167, 21701, 10396, 6656, 46407, 23121, 38440, 22896, 45253, 21941, 17033, 10835, 6500, 22465, 8461, 20893, 13263, 539, 21383, 47203, 18773, 48564, 30310, 46407, 22131, 28847, 36550]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2243f0aa50>\n",
            "Constructing exemplars of class 53\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [44630, 43029, 18438, 48992, 15773, 22135, 3825, 27872, 19337, 24716, 38892, 30518, 2708, 29794, 44055, 4447, 35266, 30053, 9417, 21665, 12652, 32479, 14688, 20633, 49992, 13245, 10821, 41619, 15483, 30519, 44868, 46646, 43519, 5933, 11352, 28831, 8198, 11550, 2379, 18106, 26054, 28185, 40277, 48159, 19243, 31218, 37017, 37739, 31087, 30519]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2243757550>\n",
            "Constructing exemplars of class 33\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [32055, 11467, 154, 37027, 19528, 22526, 47986, 43132, 28066, 38607, 32074, 47490, 49772, 1332, 26253, 15370, 7284, 36341, 18663, 22509, 6182, 5145, 25604, 45691, 7894, 19844, 834, 19528, 27107, 44156, 49835, 42756, 24006, 25604, 20376, 16691, 2284, 29588, 819, 8831, 44220, 21227, 36538, 47897, 11467, 49993, 30831, 4513, 5407, 33149]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a5af50>\n",
            "Constructing exemplars of class 96\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [15588, 16698, 21651, 36334, 30255, 15523, 25669, 45594, 30847, 25675, 21935, 31046, 45779, 13450, 30204, 20076, 3575, 25886, 8274, 47334, 9244, 22752, 47766, 16426, 20531, 35316, 45607, 10026, 10348, 19209, 17725, 41293, 38091, 34022, 5568, 27953, 12665, 14192, 35853, 5097, 26739, 49507, 29970, 39740, 37342, 46882, 999, 20758, 43076, 39951]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238218350>\n",
            "Constructing exemplars of class 92\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [11549, 29991, 42542, 39393, 31054, 30317, 48481, 47900, 48370, 8027, 34778, 17050, 9671, 2243, 41693, 18710, 23096, 8964, 42443, 12128, 4808, 40868, 3404, 36876, 6660, 27112, 3418, 27252, 27136, 8964, 26679, 29877, 16912, 15497, 31696, 8774, 34162, 24796, 25121, 14277, 1056, 38742, 34554, 14714, 25644, 5334, 2678, 24476, 47350, 38651]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232ee5210>\n",
            "Constructing exemplars of class 52\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [48718, 43144, 29331, 36704, 27974, 4661, 34015, 37200, 41662, 9971, 11295, 41163, 14629, 35930, 258, 10450, 33921, 37091, 4103, 41003, 3282, 39369, 45117, 13699, 47204, 6939, 7842, 4077, 20324, 41159, 15762, 20869, 26984, 37179, 36475, 48284, 2308, 34461, 39113, 13662, 41642, 29785, 3770, 25972, 34574, 38464, 9344, 44637, 47087, 36835]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2243757550>\n",
            "Constructing exemplars of class 12\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [4133, 22039, 16898, 1805, 1877, 5796, 30424, 38424, 21150, 46788, 29908, 40992, 10034, 43012, 13408, 13818, 1016, 40877, 42152, 38015, 6835, 11891, 11482, 16677, 34032, 18814, 18568, 12178, 24292, 47504, 12497, 19275, 36386, 49421, 2269, 11590, 1230, 8749, 30222, 37906, 41827, 6003, 49977, 26192, 27596, 4115, 39732, 7825, 24144, 20126]\n",
            "x train:  [-0.02059093 -0.06732447 -0.10034069 -0.09277993 -0.01150913 -0.11669035\n",
            " -0.12046364 -0.05698308 -0.07635212 -0.15259278 -0.19799119  0.04024148\n",
            " -0.239633   -0.21351177 -0.16089296 -0.15952861 -0.16811673 -0.1649122\n",
            " -0.03320993 -0.13229288 -0.16527365 -0.18694079 -0.2587738  -0.18745014\n",
            " -0.10967166 -0.08390739 -0.14917962 -0.07950906 -0.1550511  -0.20178099\n",
            " -0.25627798 -0.2381275  -0.17097144 -0.19202118 -0.19456273 -0.14712217\n",
            " -0.09356982 -0.27231842  0.06785555 -0.18505909]\n",
            "y_train:  [tensor([52]), tensor([35]), tensor([50]), tensor([97]), tensor([59]), tensor([3]), tensor([13]), tensor([53]), tensor([45]), tensor([77]), tensor([82]), tensor([95]), tensor([49]), tensor([10]), tensor([69]), tensor([34]), tensor([36]), tensor([24]), tensor([88]), tensor([10]), tensor([53]), tensor([95]), tensor([78]), tensor([92]), tensor([35]), tensor([94]), tensor([37]), tensor([31]), tensor([52]), tensor([33]), tensor([80]), tensor([33]), tensor([34]), tensor([34]), tensor([80]), tensor([24]), tensor([33]), tensor([58]), tensor([30]), tensor([57]), tensor([25]), tensor([82]), tensor([25]), tensor([37]), tensor([52]), tensor([3]), tensor([97]), tensor([12]), tensor([33]), tensor([13]), tensor([24]), tensor([78]), tensor([30]), tensor([59]), tensor([86]), tensor([52]), tensor([11]), tensor([13]), tensor([95]), tensor([49]), tensor([31]), tensor([80]), tensor([34]), tensor([52]), tensor([37]), tensor([24]), tensor([36]), tensor([25]), tensor([80]), tensor([36]), tensor([24]), tensor([33]), tensor([58]), tensor([81]), tensor([98]), tensor([72]), tensor([25]), tensor([13]), tensor([97]), tensor([94]), tensor([98]), tensor([52]), tensor([92]), tensor([94]), tensor([33]), tensor([60]), tensor([52]), tensor([33]), tensor([36]), tensor([31]), tensor([92]), tensor([72]), tensor([50]), tensor([30]), tensor([37]), tensor([36]), tensor([11]), tensor([95]), tensor([53]), tensor([37]), tensor([35]), tensor([13]), tensor([80]), tensor([72]), tensor([33]), tensor([57]), tensor([98]), tensor([36]), tensor([35]), tensor([97]), tensor([94]), tensor([52]), tensor([25]), tensor([34]), tensor([52]), tensor([24]), tensor([59]), tensor([82]), tensor([27]), tensor([97]), tensor([52]), tensor([30]), tensor([36]), tensor([59]), tensor([35]), tensor([57]), tensor([35]), tensor([58]), tensor([13]), tensor([33]), tensor([24]), tensor([57]), tensor([78]), tensor([92]), tensor([50]), tensor([98]), tensor([35]), tensor([97]), tensor([92]), tensor([33]), tensor([27]), tensor([78]), tensor([78]), tensor([78]), tensor([88]), tensor([49]), tensor([59]), tensor([13]), tensor([59]), tensor([72]), tensor([12]), tensor([82]), tensor([98]), tensor([24]), tensor([35]), tensor([78]), tensor([53]), tensor([88]), tensor([88]), tensor([35]), tensor([34]), tensor([77]), tensor([86]), tensor([3]), tensor([78]), tensor([59]), tensor([37]), tensor([24]), tensor([31]), tensor([86]), tensor([95]), tensor([69]), tensor([69]), tensor([37]), tensor([98]), tensor([45]), tensor([13]), tensor([98]), tensor([81]), tensor([52]), tensor([69]), tensor([24]), tensor([57]), tensor([77]), tensor([45]), tensor([60]), tensor([10]), tensor([52]), tensor([77]), tensor([35]), tensor([94]), tensor([36]), tensor([94]), tensor([53]), tensor([27]), tensor([77]), tensor([50]), tensor([37]), tensor([69]), tensor([31]), tensor([94]), tensor([37]), tensor([88]), tensor([77]), tensor([53]), tensor([50]), tensor([92]), tensor([50]), tensor([34]), tensor([96]), tensor([34]), tensor([52]), tensor([25]), tensor([58]), tensor([12]), tensor([30]), tensor([13]), tensor([81]), tensor([60]), tensor([53]), tensor([58]), tensor([25]), tensor([27]), tensor([59]), tensor([24]), tensor([98]), tensor([50]), tensor([33]), tensor([72]), tensor([35]), tensor([94]), tensor([94]), tensor([86]), tensor([96]), tensor([13]), tensor([36]), tensor([95]), tensor([72]), tensor([57]), tensor([34]), tensor([3]), tensor([36]), tensor([33]), tensor([35]), tensor([88]), tensor([10]), tensor([60]), tensor([97]), tensor([52]), tensor([57]), tensor([69]), tensor([33]), tensor([50]), tensor([37]), tensor([50]), tensor([24]), tensor([12]), tensor([95]), tensor([88]), tensor([31]), tensor([11]), tensor([95]), tensor([45]), tensor([25]), tensor([50]), tensor([80]), tensor([3]), tensor([27]), tensor([72]), tensor([94]), tensor([86]), tensor([50]), tensor([72]), tensor([3]), tensor([96]), tensor([25]), tensor([94]), tensor([27]), tensor([11]), tensor([34]), tensor([57]), tensor([57]), tensor([50]), tensor([45]), tensor([25]), tensor([36]), tensor([34]), tensor([88]), tensor([11]), tensor([3]), tensor([33]), tensor([60]), tensor([78]), tensor([30]), tensor([27]), tensor([10]), tensor([77]), tensor([30]), tensor([30]), tensor([31]), tensor([12]), tensor([11]), tensor([11]), tensor([58]), tensor([92]), tensor([35]), tensor([92]), tensor([49]), tensor([49]), tensor([12]), tensor([95]), tensor([13]), tensor([34]), tensor([60]), tensor([33]), tensor([77]), tensor([88]), tensor([69]), tensor([72]), tensor([34]), tensor([45]), tensor([81]), tensor([58]), tensor([58]), tensor([25]), tensor([92]), tensor([60]), tensor([58]), tensor([82]), tensor([27]), tensor([98]), tensor([37]), tensor([50]), tensor([80]), tensor([82]), tensor([25]), tensor([25]), tensor([68]), tensor([69]), tensor([96]), tensor([57]), tensor([78]), tensor([25]), tensor([33]), tensor([82]), tensor([34]), tensor([3]), tensor([49]), tensor([49]), tensor([78]), tensor([98]), tensor([95]), tensor([86]), tensor([31]), tensor([57]), tensor([45]), tensor([82]), tensor([11]), tensor([97]), tensor([58]), tensor([57]), tensor([50]), tensor([25]), tensor([88]), tensor([31]), tensor([53]), tensor([36]), tensor([94]), tensor([86]), tensor([27]), tensor([69]), tensor([77]), tensor([10]), tensor([31]), tensor([94]), tensor([86]), tensor([68]), tensor([3]), tensor([31]), tensor([12]), tensor([3]), tensor([53]), tensor([24]), tensor([92]), tensor([35]), tensor([95]), tensor([82]), tensor([24]), tensor([27]), tensor([59]), tensor([69]), tensor([35]), tensor([60]), tensor([98]), tensor([77]), tensor([50]), tensor([81]), tensor([98]), tensor([52]), tensor([77]), tensor([58]), tensor([96]), tensor([72]), tensor([11]), tensor([97]), tensor([59]), tensor([50]), tensor([10]), tensor([96]), tensor([25]), tensor([10]), tensor([58]), tensor([80]), tensor([25]), tensor([98]), tensor([58]), tensor([57]), tensor([37]), tensor([12]), tensor([57]), tensor([12]), tensor([77]), tensor([96]), tensor([80]), tensor([81]), tensor([30]), tensor([33]), tensor([96]), tensor([82]), tensor([82]), tensor([35]), tensor([49]), tensor([68]), tensor([78]), tensor([49]), tensor([36]), tensor([59]), tensor([37]), tensor([59]), tensor([49]), tensor([72]), tensor([58]), tensor([27]), tensor([33]), tensor([24]), tensor([88]), tensor([69]), tensor([60]), tensor([45]), tensor([80]), tensor([35]), tensor([72]), tensor([10]), tensor([35]), tensor([95]), tensor([59]), tensor([94]), tensor([96]), tensor([52]), tensor([49]), tensor([31]), tensor([27]), tensor([95]), tensor([97]), tensor([98]), tensor([31]), tensor([68]), tensor([11]), tensor([81]), tensor([30]), tensor([53]), tensor([11]), tensor([96]), tensor([10]), tensor([34]), tensor([35]), tensor([68]), tensor([33]), tensor([49]), tensor([27]), tensor([53]), tensor([27]), tensor([98]), tensor([94]), tensor([98]), tensor([52]), tensor([3]), tensor([24]), tensor([45]), tensor([72]), tensor([31]), tensor([95]), tensor([52]), tensor([69]), tensor([53]), tensor([60]), tensor([30]), tensor([58]), tensor([59]), tensor([59]), tensor([80]), tensor([96]), tensor([34]), tensor([33]), tensor([88]), tensor([78]), tensor([52]), tensor([80]), tensor([13]), tensor([13]), tensor([81]), tensor([36]), tensor([92]), tensor([33]), tensor([34]), tensor([94]), tensor([24]), tensor([88]), tensor([68]), tensor([36]), tensor([80]), tensor([60]), tensor([13]), tensor([52]), tensor([88]), tensor([69]), tensor([94]), tensor([34]), tensor([82]), tensor([69]), tensor([53]), tensor([35]), tensor([37]), tensor([35]), tensor([72]), tensor([95]), tensor([27]), tensor([81]), tensor([30]), tensor([94]), tensor([53]), tensor([94]), tensor([77]), tensor([50]), tensor([25]), tensor([69]), tensor([31]), tensor([95]), tensor([95]), tensor([97]), tensor([53]), tensor([10]), tensor([94]), tensor([49]), tensor([34]), tensor([37]), tensor([60]), tensor([92]), tensor([98]), tensor([86]), tensor([30]), tensor([37]), tensor([12]), tensor([97]), tensor([27]), tensor([78]), tensor([30]), tensor([50]), tensor([13]), tensor([80]), tensor([96]), tensor([27]), tensor([96]), tensor([35]), tensor([52]), tensor([34]), tensor([96]), tensor([97]), tensor([3]), tensor([10]), tensor([58]), tensor([13]), tensor([53]), tensor([78]), tensor([81]), tensor([68]), tensor([12]), tensor([10]), tensor([33]), tensor([98]), tensor([53]), tensor([53]), tensor([10]), tensor([92]), tensor([88]), tensor([11]), tensor([50]), tensor([3]), tensor([33]), tensor([3]), tensor([45]), tensor([10]), tensor([10]), tensor([82]), tensor([37]), tensor([30]), tensor([30]), tensor([68]), tensor([68]), tensor([58]), tensor([78]), tensor([50]), tensor([13]), tensor([53]), tensor([59]), tensor([59]), tensor([98]), tensor([57]), tensor([68]), tensor([10]), tensor([31]), tensor([52]), tensor([10]), tensor([53]), tensor([3]), tensor([34]), tensor([92]), tensor([68]), tensor([68]), tensor([97]), tensor([72]), tensor([3]), tensor([97]), tensor([72]), tensor([59]), tensor([80]), tensor([86]), tensor([31]), tensor([59]), tensor([53]), tensor([78]), tensor([59]), tensor([30]), tensor([3]), tensor([36]), tensor([57]), tensor([77]), tensor([52]), tensor([34]), tensor([57]), tensor([25]), tensor([59]), tensor([36]), tensor([3]), tensor([34]), tensor([86]), tensor([13]), tensor([36]), tensor([57]), tensor([31]), tensor([31]), tensor([30]), tensor([52]), tensor([59]), tensor([98]), tensor([12]), tensor([98]), tensor([3]), tensor([80]), tensor([77]), tensor([37]), tensor([96]), tensor([69]), tensor([27]), tensor([58]), tensor([95]), tensor([3]), tensor([94]), tensor([52]), tensor([25]), tensor([88]), tensor([11]), tensor([11]), tensor([96]), tensor([37]), tensor([12]), tensor([11]), tensor([33]), tensor([30]), tensor([72]), tensor([92]), tensor([92]), tensor([50]), tensor([78]), tensor([13]), tensor([60]), tensor([57]), tensor([13]), tensor([59]), tensor([98]), tensor([80]), tensor([80]), tensor([94]), tensor([78]), tensor([97]), tensor([59]), tensor([34]), tensor([31]), tensor([80]), tensor([35]), tensor([45]), tensor([45]), tensor([57]), tensor([80]), tensor([72]), tensor([96]), tensor([52]), tensor([60]), tensor([92]), tensor([34]), tensor([80]), tensor([58]), tensor([82]), tensor([77]), tensor([68]), tensor([37]), tensor([69]), tensor([12]), tensor([57]), tensor([69]), tensor([81]), tensor([36]), tensor([49]), tensor([68]), tensor([77]), tensor([68]), tensor([10]), tensor([37]), tensor([30]), tensor([77]), tensor([82]), tensor([69]), tensor([12]), tensor([36]), tensor([3]), tensor([88]), tensor([3]), tensor([77]), tensor([86]), tensor([68]), tensor([11]), tensor([88]), tensor([52]), tensor([35]), tensor([96]), tensor([78]), tensor([80]), tensor([98]), tensor([45]), tensor([36]), tensor([58]), tensor([80]), tensor([12]), tensor([33]), tensor([88]), tensor([77]), tensor([81]), tensor([31]), tensor([45]), tensor([59]), tensor([88]), tensor([49]), tensor([96]), tensor([82]), tensor([45]), tensor([30]), tensor([27]), tensor([34]), tensor([88]), tensor([31]), tensor([68]), tensor([88]), tensor([59]), tensor([33]), tensor([58]), tensor([36]), tensor([31]), tensor([31]), tensor([96]), tensor([92]), tensor([58]), tensor([82]), tensor([24]), tensor([13]), tensor([81]), tensor([94]), tensor([10]), tensor([98]), tensor([96]), tensor([59]), tensor([31]), tensor([45]), tensor([11]), tensor([95]), tensor([60]), tensor([81]), tensor([31]), tensor([52]), tensor([33]), tensor([96]), tensor([36]), tensor([95]), tensor([45]), tensor([24]), tensor([88]), tensor([27]), tensor([60]), tensor([24]), tensor([24]), tensor([25]), tensor([36]), tensor([36]), tensor([78]), tensor([92]), tensor([98]), tensor([94]), tensor([80]), tensor([92]), tensor([92]), tensor([59]), tensor([24]), tensor([52]), tensor([49]), tensor([69]), tensor([24]), tensor([10]), tensor([97]), tensor([82]), tensor([97]), tensor([96]), tensor([25]), tensor([34]), tensor([34]), tensor([68]), tensor([80]), tensor([97]), tensor([95]), tensor([82]), tensor([77]), tensor([92]), tensor([45]), tensor([24]), tensor([37]), tensor([50]), tensor([49]), tensor([57]), tensor([3]), tensor([80]), tensor([98]), tensor([77]), tensor([96]), tensor([53]), tensor([45]), tensor([88]), tensor([57]), tensor([60]), tensor([53]), tensor([50]), tensor([35]), tensor([68]), tensor([81]), tensor([92]), tensor([96]), tensor([68]), tensor([35]), tensor([81]), tensor([69]), tensor([45]), tensor([33]), tensor([96]), tensor([96]), tensor([12]), tensor([50]), tensor([10]), tensor([12]), tensor([92]), tensor([57]), tensor([57]), tensor([92]), tensor([78]), tensor([86]), tensor([27]), tensor([45]), tensor([81]), tensor([24]), tensor([94]), tensor([77]), tensor([13]), tensor([94]), tensor([69]), tensor([13]), tensor([80]), tensor([60]), tensor([96]), tensor([50]), tensor([10]), tensor([57]), tensor([50]), tensor([3]), tensor([49]), tensor([82]), tensor([82]), tensor([60]), tensor([60]), tensor([52]), tensor([31]), tensor([57]), tensor([59]), tensor([96]), tensor([68]), tensor([25]), tensor([97]), tensor([58]), tensor([10]), tensor([30]), tensor([69]), tensor([36]), tensor([60]), tensor([97]), tensor([12]), tensor([72]), tensor([34]), tensor([86]), tensor([13]), tensor([81]), tensor([78]), tensor([72]), tensor([30]), tensor([95]), tensor([53]), tensor([25]), tensor([12]), tensor([3]), tensor([49]), tensor([77]), tensor([96]), tensor([31]), tensor([81]), tensor([24]), tensor([78]), tensor([80]), tensor([95]), tensor([97]), tensor([95]), tensor([13]), tensor([94]), tensor([34]), tensor([31]), tensor([69]), tensor([77]), tensor([97]), tensor([53]), tensor([11]), tensor([50]), tensor([11]), tensor([78]), tensor([82]), tensor([92]), tensor([52]), tensor([97]), tensor([25]), tensor([12]), tensor([97]), tensor([98]), tensor([11]), tensor([3]), tensor([97]), tensor([88]), tensor([35]), tensor([37]), tensor([78]), tensor([95]), tensor([92]), tensor([37]), tensor([37]), tensor([31]), tensor([92]), tensor([98]), tensor([77]), tensor([24]), tensor([3]), tensor([12]), tensor([49]), tensor([31]), tensor([68]), tensor([37]), tensor([80]), tensor([33]), tensor([3]), tensor([49]), tensor([50]), tensor([92]), tensor([27]), tensor([69]), tensor([81]), tensor([24]), tensor([88]), tensor([27]), tensor([49]), tensor([45]), tensor([49]), tensor([98]), tensor([81]), tensor([13]), tensor([78]), tensor([25]), tensor([97]), tensor([97]), tensor([12]), tensor([69]), tensor([96]), tensor([3]), tensor([33]), tensor([95]), tensor([57]), tensor([81]), tensor([97]), tensor([86]), tensor([24]), tensor([60]), tensor([13]), tensor([77]), tensor([59]), tensor([59]), tensor([13]), tensor([10]), tensor([25]), tensor([57]), tensor([36]), tensor([82]), tensor([86]), tensor([86]), tensor([53]), tensor([68]), tensor([77]), tensor([58]), tensor([25]), tensor([80]), tensor([95]), tensor([82]), tensor([81]), tensor([96]), tensor([35]), tensor([27]), tensor([52]), tensor([53]), tensor([13]), tensor([53]), tensor([10]), tensor([27]), tensor([77]), tensor([95]), tensor([45]), tensor([58]), tensor([68]), tensor([11]), tensor([3]), tensor([53]), tensor([33]), tensor([72]), tensor([92]), tensor([49]), tensor([35]), tensor([50]), tensor([60]), tensor([95]), tensor([69]), tensor([45]), tensor([35]), tensor([88]), tensor([10]), tensor([82]), tensor([50]), tensor([94]), tensor([59]), tensor([94]), tensor([60]), tensor([81]), tensor([78]), tensor([53]), tensor([31]), tensor([27]), tensor([27]), tensor([69]), tensor([78]), tensor([92]), tensor([60]), tensor([3]), tensor([78]), tensor([81]), tensor([13]), tensor([86]), tensor([34]), tensor([49]), tensor([25]), tensor([12]), tensor([12]), tensor([33]), tensor([81]), tensor([33]), tensor([53]), tensor([57]), tensor([86]), tensor([12]), tensor([88]), tensor([11]), tensor([69]), tensor([86]), tensor([80]), tensor([50]), tensor([81]), tensor([30]), tensor([78]), tensor([82]), tensor([35]), tensor([98]), tensor([11]), tensor([27]), tensor([77]), tensor([53]), tensor([34]), tensor([96]), tensor([92]), tensor([69]), tensor([30]), tensor([60]), tensor([92]), tensor([49]), tensor([88]), tensor([37]), tensor([97]), tensor([27]), tensor([45]), tensor([53]), tensor([88]), tensor([81]), tensor([11]), tensor([27]), tensor([59]), tensor([11]), tensor([98]), tensor([81]), tensor([11]), tensor([45]), tensor([58]), tensor([86]), tensor([82]), tensor([11]), tensor([77]), tensor([30]), tensor([25]), tensor([72]), tensor([36]), tensor([81]), tensor([88]), tensor([68]), tensor([69]), tensor([24]), tensor([11]), tensor([68]), tensor([77]), tensor([45]), tensor([50]), tensor([92]), tensor([35]), tensor([11]), tensor([31]), tensor([36]), tensor([27]), tensor([50]), tensor([13]), tensor([60]), tensor([98]), tensor([37]), tensor([45]), tensor([95]), tensor([72]), tensor([33]), tensor([60]), tensor([49]), tensor([69]), tensor([49]), tensor([31]), tensor([37]), tensor([60]), tensor([78]), tensor([30]), tensor([33]), tensor([96]), tensor([95]), tensor([37]), tensor([34]), tensor([82]), tensor([45]), tensor([95]), tensor([3]), tensor([53]), tensor([68]), tensor([68]), tensor([94]), tensor([3]), tensor([88]), tensor([24]), tensor([24]), tensor([24]), tensor([57]), tensor([92]), tensor([25]), tensor([95]), tensor([72]), tensor([34]), tensor([80]), tensor([82]), tensor([24]), tensor([68]), tensor([45]), tensor([78]), tensor([57]), tensor([81]), tensor([98]), tensor([11]), tensor([81]), tensor([92]), tensor([30]), tensor([10]), tensor([58]), tensor([13]), tensor([52]), tensor([77]), tensor([3]), tensor([72]), tensor([30]), tensor([31]), tensor([35]), tensor([52]), tensor([45]), tensor([97]), tensor([86]), tensor([60]), tensor([31]), tensor([98]), tensor([52]), tensor([77]), tensor([80]), tensor([53]), tensor([3]), tensor([97]), tensor([86]), tensor([97]), tensor([58]), tensor([31]), tensor([68]), tensor([11]), tensor([77]), tensor([13]), tensor([36]), tensor([96]), tensor([80]), tensor([31]), tensor([36]), tensor([96]), tensor([58]), tensor([33]), tensor([58]), tensor([30]), tensor([92]), tensor([53]), tensor([35]), tensor([58]), tensor([12]), tensor([97]), tensor([59]), tensor([36]), tensor([97]), tensor([97]), tensor([96]), tensor([72]), tensor([50]), tensor([82]), tensor([53]), tensor([37]), tensor([13]), tensor([52]), tensor([27]), tensor([95]), tensor([82]), tensor([97]), tensor([69]), tensor([49]), tensor([34]), tensor([30]), tensor([95]), tensor([94]), tensor([24]), tensor([57]), tensor([58]), tensor([68]), tensor([72]), tensor([45]), tensor([25]), tensor([69]), tensor([37]), tensor([77]), tensor([97]), tensor([31]), tensor([37]), tensor([94]), tensor([24]), tensor([36]), tensor([60]), tensor([36]), tensor([82]), tensor([37]), tensor([98]), tensor([49]), tensor([27]), tensor([80]), tensor([86]), tensor([86]), tensor([50]), tensor([81]), tensor([86]), tensor([72]), tensor([60]), tensor([30]), tensor([12]), tensor([77]), tensor([86]), tensor([88]), tensor([59]), tensor([77]), tensor([98]), tensor([82]), tensor([88]), tensor([77]), tensor([37]), tensor([96]), tensor([72]), tensor([82]), tensor([59]), tensor([10]), tensor([37]), tensor([35]), tensor([12]), tensor([59]), tensor([10]), tensor([49]), tensor([92]), tensor([49]), tensor([49]), tensor([35]), tensor([27]), tensor([92]), tensor([80]), tensor([37]), tensor([97]), tensor([94]), tensor([81]), tensor([12]), tensor([58]), tensor([82]), tensor([86]), tensor([33]), tensor([27]), tensor([10]), tensor([81]), tensor([92]), tensor([11]), tensor([77]), tensor([3]), tensor([98]), tensor([53]), tensor([13]), tensor([3]), tensor([60]), tensor([98]), tensor([57]), tensor([80]), tensor([98]), tensor([78]), tensor([88]), tensor([78]), tensor([68]), tensor([3]), tensor([95]), tensor([97]), tensor([82]), tensor([72]), tensor([86]), tensor([88]), tensor([78]), tensor([53]), tensor([24]), tensor([72]), tensor([72]), tensor([35]), tensor([95]), tensor([94]), tensor([81]), tensor([36]), tensor([88]), tensor([60]), tensor([53]), tensor([35]), tensor([80]), tensor([69]), tensor([25]), tensor([34]), tensor([82]), tensor([45]), tensor([78]), tensor([3]), tensor([13]), tensor([12]), tensor([86]), tensor([12]), tensor([50]), tensor([95]), tensor([97]), tensor([45]), tensor([86]), tensor([53]), tensor([81]), tensor([80]), tensor([77]), tensor([10]), tensor([50]), tensor([94]), tensor([34]), tensor([52]), tensor([13]), tensor([94]), tensor([81]), tensor([86]), tensor([49]), tensor([24]), tensor([12]), tensor([10]), tensor([10]), tensor([12]), tensor([33]), tensor([98]), tensor([24]), tensor([36]), tensor([80]), tensor([69]), tensor([34]), tensor([96]), tensor([30]), tensor([36]), tensor([88]), tensor([33]), tensor([34]), tensor([11]), tensor([25]), tensor([35]), tensor([72]), tensor([58]), tensor([82]), tensor([95]), tensor([60]), tensor([80]), tensor([58]), tensor([72]), tensor([52]), tensor([92]), tensor([30]), tensor([30]), tensor([86]), tensor([10]), tensor([82]), tensor([12]), tensor([27]), tensor([49]), tensor([13]), tensor([49]), tensor([10]), tensor([86]), tensor([57]), tensor([95]), tensor([50]), tensor([37]), tensor([31]), tensor([37]), tensor([52]), tensor([34]), tensor([27]), tensor([3]), tensor([96]), tensor([59]), tensor([25]), tensor([58]), tensor([72]), tensor([3]), tensor([45]), tensor([50]), tensor([45]), tensor([11]), tensor([59]), tensor([60]), tensor([50]), tensor([50]), tensor([33]), tensor([27]), tensor([53]), tensor([97]), tensor([88]), tensor([37]), tensor([81]), tensor([53]), tensor([36]), tensor([10]), tensor([57]), tensor([12]), tensor([33]), tensor([52]), tensor([27]), tensor([97]), tensor([81]), tensor([68]), tensor([36]), tensor([95]), tensor([82]), tensor([77]), tensor([82]), tensor([24]), tensor([11]), tensor([24]), tensor([96]), tensor([25]), tensor([60]), tensor([27]), tensor([95]), tensor([69]), tensor([35]), tensor([94]), tensor([77]), tensor([24]), tensor([24]), tensor([10]), tensor([58]), tensor([30]), tensor([88]), tensor([80]), tensor([31]), tensor([94]), tensor([30]), tensor([24]), tensor([60]), tensor([49]), tensor([78]), tensor([72]), tensor([94]), tensor([98]), tensor([13]), tensor([68]), tensor([97]), tensor([13]), tensor([30]), tensor([98]), tensor([82]), tensor([53]), tensor([12]), tensor([27]), tensor([52]), tensor([50]), tensor([3]), tensor([57]), tensor([30]), tensor([31]), tensor([81]), tensor([57]), tensor([94]), tensor([57]), tensor([59]), tensor([88]), tensor([31]), tensor([49]), tensor([27]), tensor([68]), tensor([60]), tensor([50]), tensor([88]), tensor([92]), tensor([24]), tensor([36]), tensor([10]), tensor([57]), tensor([53]), tensor([80]), tensor([68]), tensor([25]), tensor([13]), tensor([11]), tensor([80]), tensor([25]), tensor([96]), tensor([96]), tensor([69]), tensor([69]), tensor([52]), tensor([59]), tensor([13]), tensor([94]), tensor([35]), tensor([82]), tensor([78]), tensor([78]), tensor([12]), tensor([60]), tensor([78]), tensor([49]), tensor([59]), tensor([3]), tensor([88]), tensor([95]), tensor([60]), tensor([49]), tensor([13]), tensor([30]), tensor([60]), tensor([27]), tensor([35]), tensor([31]), tensor([30]), tensor([30]), tensor([88]), tensor([37]), tensor([27]), tensor([58]), tensor([30]), tensor([69]), tensor([10]), tensor([82]), tensor([58]), tensor([34]), tensor([36]), tensor([37]), tensor([49]), tensor([36]), tensor([11]), tensor([11]), tensor([37]), tensor([98]), tensor([49]), tensor([25]), tensor([36]), tensor([57]), tensor([80]), tensor([86]), tensor([45]), tensor([35]), tensor([34]), tensor([37]), tensor([78]), tensor([25]), tensor([31]), tensor([58]), tensor([88]), tensor([12]), tensor([81]), tensor([78]), tensor([53]), tensor([59]), tensor([13]), tensor([10]), tensor([57]), tensor([72]), tensor([31]), tensor([86]), tensor([49]), tensor([60]), tensor([49]), tensor([27]), tensor([45]), tensor([3]), tensor([45]), tensor([69]), tensor([81]), tensor([37]), tensor([69]), tensor([96]), tensor([97]), tensor([49]), tensor([58]), tensor([58]), tensor([25]), tensor([68]), tensor([45]), tensor([72]), tensor([50]), tensor([11]), tensor([35]), tensor([58]), tensor([45]), tensor([49]), tensor([58]), tensor([50]), tensor([86]), tensor([36]), tensor([58]), tensor([94]), tensor([13]), tensor([94]), tensor([24]), tensor([33]), tensor([24]), tensor([36]), tensor([13]), tensor([52]), tensor([72]), tensor([94]), tensor([86]), tensor([59]), tensor([69]), tensor([30]), tensor([13]), tensor([86]), tensor([57]), tensor([25]), tensor([72]), tensor([94]), tensor([98]), tensor([57]), tensor([57]), tensor([78]), tensor([45]), tensor([69]), tensor([31]), tensor([50]), tensor([31]), tensor([78]), tensor([81]), tensor([92]), tensor([57]), tensor([72]), tensor([36]), tensor([24]), tensor([86]), tensor([86]), tensor([82]), tensor([95]), tensor([77]), tensor([92]), tensor([3]), tensor([36]), tensor([45]), tensor([25]), tensor([30]), tensor([45]), tensor([10]), tensor([52]), tensor([10]), tensor([34]), tensor([25]), tensor([88]), tensor([25]), tensor([3]), tensor([78]), tensor([69]), tensor([3]), tensor([59]), tensor([92]), tensor([11]), tensor([77]), tensor([80]), tensor([68]), tensor([86]), tensor([86]), tensor([68]), tensor([30]), tensor([78]), tensor([68]), tensor([12]), tensor([88]), tensor([96]), tensor([11]), tensor([81]), tensor([12]), tensor([78]), tensor([69]), tensor([49]), tensor([58]), tensor([3]), tensor([33]), tensor([68]), tensor([82]), tensor([59]), tensor([86]), tensor([68]), tensor([53]), tensor([94]), tensor([10]), tensor([95]), tensor([82]), tensor([31]), tensor([12]), tensor([25]), tensor([34]), tensor([72]), tensor([33]), tensor([10]), tensor([88]), tensor([95]), tensor([88]), tensor([68]), tensor([57]), tensor([12]), tensor([11]), tensor([34]), tensor([12]), tensor([69]), tensor([50]), tensor([11]), tensor([11]), tensor([72]), tensor([98]), tensor([25]), tensor([37]), tensor([80]), tensor([36]), tensor([69]), tensor([33]), tensor([53]), tensor([45]), tensor([33]), tensor([60]), tensor([72]), tensor([10]), tensor([12]), tensor([24]), tensor([92]), tensor([58]), tensor([60]), tensor([49]), tensor([34]), tensor([35]), tensor([24]), tensor([52]), tensor([3]), tensor([34]), tensor([95]), tensor([95]), tensor([25]), tensor([37]), tensor([52]), tensor([50]), tensor([30]), tensor([11]), tensor([12]), tensor([95]), tensor([94]), tensor([82]), tensor([97]), tensor([97]), tensor([25]), tensor([86]), tensor([10]), tensor([35]), tensor([11]), tensor([60]), tensor([81]), tensor([96]), tensor([45]), tensor([33]), tensor([72]), tensor([92]), tensor([72]), tensor([37]), tensor([11]), tensor([45]), tensor([30]), tensor([86]), tensor([45]), tensor([60]), tensor([59]), tensor([60]), tensor([53]), tensor([11]), tensor([59]), tensor([77]), tensor([13]), tensor([11]), tensor([86]), tensor([27]), tensor([81]), tensor([97]), tensor([36]), tensor([68]), tensor([49]), tensor([3]), tensor([30]), tensor([52]), tensor([27]), tensor([68]), tensor([95]), tensor([72]), tensor([92]), tensor([49]), tensor([35]), tensor([31]), tensor([97]), tensor([12]), tensor([68]), tensor([25]), tensor([98]), tensor([96]), tensor([10]), tensor([37]), tensor([60]), tensor([58]), tensor([77]), tensor([33]), tensor([77]), tensor([33]), tensor([50]), tensor([11]), tensor([77]), tensor([69]), tensor([34]), tensor([57]), tensor([94]), tensor([68]), tensor([80]), tensor([30]), tensor([60]), tensor([78]), tensor([97]), tensor([72]), tensor([81]), tensor([96]), tensor([82]), tensor([12]), tensor([58]), tensor([94]), tensor([96]), tensor([13]), tensor([37]), tensor([52]), tensor([86]), tensor([35]), tensor([27]), tensor([92]), tensor([81]), tensor([88]), tensor([98]), tensor([27]), tensor([45]), tensor([94]), tensor([12]), tensor([78]), tensor([68]), tensor([96]), tensor([80]), tensor([52]), tensor([80]), tensor([53]), tensor([10]), tensor([68]), tensor([57]), tensor([10]), tensor([34]), tensor([57]), tensor([35]), tensor([36]), tensor([58]), tensor([82]), tensor([77]), tensor([72]), tensor([50]), tensor([98]), tensor([86]), tensor([72]), tensor([59]), tensor([59]), tensor([68]), tensor([33]), tensor([45]), tensor([35]), tensor([34]), tensor([98]), tensor([13]), tensor([98]), tensor([52]), tensor([92]), tensor([86]), tensor([81]), tensor([12]), tensor([69]), tensor([27])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.62 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.621\n",
            "TEST ALL:  0.60725\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  5000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 92, 57, 53, 49, 45, 37, 33, 25, 21, 13, 96, 88, 91, 80, 72, 68, 60, 52, 36, 32, 24, 16, 12, 65, 69, 77, 81, 67, 59, 35, 31, 27, 11, 7, 3, 98, 94, 86, 82, 78, 62, 58, 50, 46, 34, 30, 10, 97, 0]\n",
            "TRAIN_SET CLASSES:  [91, 67, 7, 62, 46, 65, 21, 32, 16, 0]\n",
            "VALIDATION CLASSES:  [62, 46, 32, 91, 21, 16, 7, 67, 65, 0]\n",
            "GROUP:  5\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [91, 67, 7, 62, 46, 65, 21, 32, 16, 0]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  50\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.2546118497848511\n",
            "Train step - Step 10, Loss 0.13615931570529938\n",
            "Train step - Step 20, Loss 0.13159416615962982\n",
            "Train step - Step 30, Loss 0.13171856105327606\n",
            "Train step - Step 40, Loss 0.12001270055770874\n",
            "Train step - Step 50, Loss 0.11502552032470703\n",
            "Train epoch - Accuracy: 0.19870503597122302 Loss: 0.1392400737632093 Corrects: 1381\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12060771882534027\n",
            "Train step - Step 70, Loss 0.11152762919664383\n",
            "Train step - Step 80, Loss 0.115134596824646\n",
            "Train step - Step 90, Loss 0.12204354256391525\n",
            "Train step - Step 100, Loss 0.11738988012075424\n",
            "Train epoch - Accuracy: 0.22172661870503596 Loss: 0.11667206875283083 Corrects: 1541\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11725146323442459\n",
            "Train step - Step 120, Loss 0.11701810359954834\n",
            "Train step - Step 130, Loss 0.10807836055755615\n",
            "Train step - Step 140, Loss 0.11482053250074387\n",
            "Train step - Step 150, Loss 0.10766826570034027\n",
            "Train step - Step 160, Loss 0.11434187740087509\n",
            "Train epoch - Accuracy: 0.24388489208633093 Loss: 0.11396793855608796 Corrects: 1695\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.1137232780456543\n",
            "Train step - Step 180, Loss 0.1141018271446228\n",
            "Train step - Step 190, Loss 0.11184515058994293\n",
            "Train step - Step 200, Loss 0.11051492393016815\n",
            "Train step - Step 210, Loss 0.11439350992441177\n",
            "Train epoch - Accuracy: 0.2589928057553957 Loss: 0.11304383812619628 Corrects: 1800\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11776764690876007\n",
            "Train step - Step 230, Loss 0.1168118268251419\n",
            "Train step - Step 240, Loss 0.11128567159175873\n",
            "Train step - Step 250, Loss 0.10771572589874268\n",
            "Train step - Step 260, Loss 0.11209642142057419\n",
            "Train step - Step 270, Loss 0.1140456572175026\n",
            "Train epoch - Accuracy: 0.2745323741007194 Loss: 0.11177246631478234 Corrects: 1908\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.11270158737897873\n",
            "Train step - Step 290, Loss 0.10956428200006485\n",
            "Train step - Step 300, Loss 0.10885967314243317\n",
            "Train step - Step 310, Loss 0.11358761787414551\n",
            "Train step - Step 320, Loss 0.11304683238267899\n",
            "Train epoch - Accuracy: 0.2887769784172662 Loss: 0.11135379072144735 Corrects: 2007\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11051391065120697\n",
            "Train step - Step 340, Loss 0.11133944243192673\n",
            "Train step - Step 350, Loss 0.11296160519123077\n",
            "Train step - Step 360, Loss 0.10772325098514557\n",
            "Train step - Step 370, Loss 0.10942546278238297\n",
            "Train step - Step 380, Loss 0.10419360548257828\n",
            "Train epoch - Accuracy: 0.2961151079136691 Loss: 0.11075633823013992 Corrects: 2058\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.1064787432551384\n",
            "Train step - Step 400, Loss 0.11323632299900055\n",
            "Train step - Step 410, Loss 0.11644540727138519\n",
            "Train step - Step 420, Loss 0.11006111651659012\n",
            "Train step - Step 430, Loss 0.10916438698768616\n",
            "Train epoch - Accuracy: 0.3097841726618705 Loss: 0.11011229398653662 Corrects: 2153\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.11058427393436432\n",
            "Train step - Step 450, Loss 0.11143773794174194\n",
            "Train step - Step 460, Loss 0.11020062118768692\n",
            "Train step - Step 470, Loss 0.10667828470468521\n",
            "Train step - Step 480, Loss 0.10353843122720718\n",
            "Train step - Step 490, Loss 0.1102447658777237\n",
            "Train epoch - Accuracy: 0.3130935251798561 Loss: 0.10971390797508707 Corrects: 2176\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10714899003505707\n",
            "Train step - Step 510, Loss 0.11575951427221298\n",
            "Train step - Step 520, Loss 0.10613996535539627\n",
            "Train step - Step 530, Loss 0.11239919066429138\n",
            "Train step - Step 540, Loss 0.11286856234073639\n",
            "Train epoch - Accuracy: 0.3194244604316547 Loss: 0.10918653756165676 Corrects: 2220\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.11135975271463394\n",
            "Train step - Step 560, Loss 0.10916551202535629\n",
            "Train step - Step 570, Loss 0.11007167398929596\n",
            "Train step - Step 580, Loss 0.11253722757101059\n",
            "Train step - Step 590, Loss 0.12087110430002213\n",
            "Train step - Step 600, Loss 0.10854323953390121\n",
            "Train epoch - Accuracy: 0.3368345323741007 Loss: 0.10904771880065794 Corrects: 2341\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.1055738627910614\n",
            "Train step - Step 620, Loss 0.10858270525932312\n",
            "Train step - Step 630, Loss 0.1036655530333519\n",
            "Train step - Step 640, Loss 0.10593398660421371\n",
            "Train step - Step 650, Loss 0.10501333326101303\n",
            "Train epoch - Accuracy: 0.3358273381294964 Loss: 0.10850969069938865 Corrects: 2334\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10211625695228577\n",
            "Train step - Step 670, Loss 0.10245218127965927\n",
            "Train step - Step 680, Loss 0.11370555311441422\n",
            "Train step - Step 690, Loss 0.10928510129451752\n",
            "Train step - Step 700, Loss 0.10459409654140472\n",
            "Train step - Step 710, Loss 0.10210712254047394\n",
            "Train epoch - Accuracy: 0.34043165467625897 Loss: 0.10825281084012642 Corrects: 2366\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10500278323888779\n",
            "Train step - Step 730, Loss 0.10729211568832397\n",
            "Train step - Step 740, Loss 0.10107851028442383\n",
            "Train step - Step 750, Loss 0.10491810739040375\n",
            "Train step - Step 760, Loss 0.10246700793504715\n",
            "Train epoch - Accuracy: 0.35827338129496406 Loss: 0.10836393890192182 Corrects: 2490\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10118608176708221\n",
            "Train step - Step 780, Loss 0.10424835234880447\n",
            "Train step - Step 790, Loss 0.10775035619735718\n",
            "Train step - Step 800, Loss 0.10482561588287354\n",
            "Train step - Step 810, Loss 0.1059117317199707\n",
            "Train step - Step 820, Loss 0.10804218053817749\n",
            "Train epoch - Accuracy: 0.358705035971223 Loss: 0.10800690013513291 Corrects: 2493\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.10545768588781357\n",
            "Train step - Step 840, Loss 0.10348966717720032\n",
            "Train step - Step 850, Loss 0.11306391656398773\n",
            "Train step - Step 860, Loss 0.10653258860111237\n",
            "Train step - Step 870, Loss 0.1034652590751648\n",
            "Train epoch - Accuracy: 0.36935251798561153 Loss: 0.10779346287250519 Corrects: 2567\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.1126464307308197\n",
            "Train step - Step 890, Loss 0.10932713001966476\n",
            "Train step - Step 900, Loss 0.09923704713582993\n",
            "Train step - Step 910, Loss 0.11369603872299194\n",
            "Train step - Step 920, Loss 0.10522542893886566\n",
            "Train step - Step 930, Loss 0.10675191879272461\n",
            "Train epoch - Accuracy: 0.377410071942446 Loss: 0.10717171892630968 Corrects: 2623\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.10794875770807266\n",
            "Train step - Step 950, Loss 0.11031055450439453\n",
            "Train step - Step 960, Loss 0.11418807506561279\n",
            "Train step - Step 970, Loss 0.10578931868076324\n",
            "Train step - Step 980, Loss 0.10592444241046906\n",
            "Train epoch - Accuracy: 0.38316546762589926 Loss: 0.10713583235260395 Corrects: 2663\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.10865499079227448\n",
            "Train step - Step 1000, Loss 0.11040443927049637\n",
            "Train step - Step 1010, Loss 0.109550341963768\n",
            "Train step - Step 1020, Loss 0.10857883095741272\n",
            "Train step - Step 1030, Loss 0.10475587844848633\n",
            "Train step - Step 1040, Loss 0.1125776618719101\n",
            "Train epoch - Accuracy: 0.3915107913669065 Loss: 0.10689446020040581 Corrects: 2721\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.10833892226219177\n",
            "Train step - Step 1060, Loss 0.09979503601789474\n",
            "Train step - Step 1070, Loss 0.10877429693937302\n",
            "Train step - Step 1080, Loss 0.10998522490262985\n",
            "Train step - Step 1090, Loss 0.1000124141573906\n",
            "Train epoch - Accuracy: 0.38633093525179857 Loss: 0.10715703597600512 Corrects: 2685\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10655400156974792\n",
            "Train step - Step 1110, Loss 0.10764048993587494\n",
            "Train step - Step 1120, Loss 0.10941404104232788\n",
            "Train step - Step 1130, Loss 0.10695327818393707\n",
            "Train step - Step 1140, Loss 0.10476546734571457\n",
            "Train step - Step 1150, Loss 0.10755869001150131\n",
            "Train epoch - Accuracy: 0.39467625899280573 Loss: 0.10641350402248849 Corrects: 2743\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10640475153923035\n",
            "Train step - Step 1170, Loss 0.101453997194767\n",
            "Train step - Step 1180, Loss 0.10257808119058609\n",
            "Train step - Step 1190, Loss 0.11068938672542572\n",
            "Train step - Step 1200, Loss 0.10248542577028275\n",
            "Train epoch - Accuracy: 0.40431654676258993 Loss: 0.10661084259371106 Corrects: 2810\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.10491970181465149\n",
            "Train step - Step 1220, Loss 0.10281399637460709\n",
            "Train step - Step 1230, Loss 0.10225324332714081\n",
            "Train step - Step 1240, Loss 0.10905403643846512\n",
            "Train step - Step 1250, Loss 0.11167445778846741\n",
            "Train step - Step 1260, Loss 0.11058954894542694\n",
            "Train epoch - Accuracy: 0.4063309352517986 Loss: 0.10641458415084605 Corrects: 2824\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.103298120200634\n",
            "Train step - Step 1280, Loss 0.10936851054430008\n",
            "Train step - Step 1290, Loss 0.10650234669446945\n",
            "Train step - Step 1300, Loss 0.10302133113145828\n",
            "Train step - Step 1310, Loss 0.10471926629543304\n",
            "Train epoch - Accuracy: 0.4172661870503597 Loss: 0.10611397308196953 Corrects: 2900\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.10818652808666229\n",
            "Train step - Step 1330, Loss 0.10485441237688065\n",
            "Train step - Step 1340, Loss 0.1093021109700203\n",
            "Train step - Step 1350, Loss 0.09931037575006485\n",
            "Train step - Step 1360, Loss 0.10640624165534973\n",
            "Train step - Step 1370, Loss 0.10166700184345245\n",
            "Train epoch - Accuracy: 0.418705035971223 Loss: 0.1058514670438046 Corrects: 2910\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.10649902373552322\n",
            "Train step - Step 1390, Loss 0.10157186537981033\n",
            "Train step - Step 1400, Loss 0.11196543276309967\n",
            "Train step - Step 1410, Loss 0.1108059287071228\n",
            "Train step - Step 1420, Loss 0.10084458440542221\n",
            "Train epoch - Accuracy: 0.4244604316546763 Loss: 0.10580738421609934 Corrects: 2950\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.10161390155553818\n",
            "Train step - Step 1440, Loss 0.10321464389562607\n",
            "Train step - Step 1450, Loss 0.11089616268873215\n",
            "Train step - Step 1460, Loss 0.11461817473173141\n",
            "Train step - Step 1470, Loss 0.10184233635663986\n",
            "Train step - Step 1480, Loss 0.10194941610097885\n",
            "Train epoch - Accuracy: 0.42848920863309353 Loss: 0.10606413916074972 Corrects: 2978\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.10566145926713943\n",
            "Train step - Step 1500, Loss 0.10832327604293823\n",
            "Train step - Step 1510, Loss 0.10777545720338821\n",
            "Train step - Step 1520, Loss 0.10839366912841797\n",
            "Train step - Step 1530, Loss 0.1034741923213005\n",
            "Train epoch - Accuracy: 0.43381294964028777 Loss: 0.10552062838412017 Corrects: 3015\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.101997509598732\n",
            "Train step - Step 1550, Loss 0.1044112965464592\n",
            "Train step - Step 1560, Loss 0.10410559177398682\n",
            "Train step - Step 1570, Loss 0.10611503571271896\n",
            "Train step - Step 1580, Loss 0.10514593869447708\n",
            "Train step - Step 1590, Loss 0.10667693614959717\n",
            "Train epoch - Accuracy: 0.43755395683453235 Loss: 0.1048650146366881 Corrects: 3041\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.1002109944820404\n",
            "Train step - Step 1610, Loss 0.10246577113866806\n",
            "Train step - Step 1620, Loss 0.10254170000553131\n",
            "Train step - Step 1630, Loss 0.09840015321969986\n",
            "Train step - Step 1640, Loss 0.10047109425067902\n",
            "Train epoch - Accuracy: 0.44244604316546765 Loss: 0.10556999857691553 Corrects: 3075\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.10515611618757248\n",
            "Train step - Step 1660, Loss 0.10142489522695541\n",
            "Train step - Step 1670, Loss 0.1102081686258316\n",
            "Train step - Step 1680, Loss 0.10670974850654602\n",
            "Train step - Step 1690, Loss 0.10395299643278122\n",
            "Train step - Step 1700, Loss 0.10129691660404205\n",
            "Train epoch - Accuracy: 0.4507913669064748 Loss: 0.10553267603512291 Corrects: 3133\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.10562264919281006\n",
            "Train step - Step 1720, Loss 0.10295797139406204\n",
            "Train step - Step 1730, Loss 0.10865586996078491\n",
            "Train step - Step 1740, Loss 0.1075352281332016\n",
            "Train step - Step 1750, Loss 0.10591544955968857\n",
            "Train epoch - Accuracy: 0.45568345323741005 Loss: 0.10503640666711245 Corrects: 3167\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.10224958509206772\n",
            "Train step - Step 1770, Loss 0.09611335396766663\n",
            "Train step - Step 1780, Loss 0.10559318959712982\n",
            "Train step - Step 1790, Loss 0.10965368151664734\n",
            "Train step - Step 1800, Loss 0.10362490266561508\n",
            "Train step - Step 1810, Loss 0.10322888940572739\n",
            "Train epoch - Accuracy: 0.4494964028776978 Loss: 0.10477736646322895 Corrects: 3124\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.10302804410457611\n",
            "Train step - Step 1830, Loss 0.10912961512804031\n",
            "Train step - Step 1840, Loss 0.10308963805437088\n",
            "Train step - Step 1850, Loss 0.10445097833871841\n",
            "Train step - Step 1860, Loss 0.11082127690315247\n",
            "Train epoch - Accuracy: 0.45784172661870504 Loss: 0.10477797544474224 Corrects: 3182\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.09918319433927536\n",
            "Train step - Step 1880, Loss 0.11390265822410583\n",
            "Train step - Step 1890, Loss 0.09980857372283936\n",
            "Train step - Step 1900, Loss 0.1085871085524559\n",
            "Train step - Step 1910, Loss 0.10162214189767838\n",
            "Train step - Step 1920, Loss 0.10534927248954773\n",
            "Train epoch - Accuracy: 0.4631654676258993 Loss: 0.10483105499967396 Corrects: 3219\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.10277356207370758\n",
            "Train step - Step 1940, Loss 0.10395488142967224\n",
            "Train step - Step 1950, Loss 0.1040952280163765\n",
            "Train step - Step 1960, Loss 0.1050909087061882\n",
            "Train step - Step 1970, Loss 0.10423200577497482\n",
            "Train epoch - Accuracy: 0.4651798561151079 Loss: 0.10418781641790335 Corrects: 3233\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.10378845036029816\n",
            "Train step - Step 1990, Loss 0.10618409514427185\n",
            "Train step - Step 2000, Loss 0.10576808452606201\n",
            "Train step - Step 2010, Loss 0.10929874330759048\n",
            "Train step - Step 2020, Loss 0.1087755635380745\n",
            "Train step - Step 2030, Loss 0.10377475619316101\n",
            "Train epoch - Accuracy: 0.4646043165467626 Loss: 0.1045873856973305 Corrects: 3229\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.10768083482980728\n",
            "Train step - Step 2050, Loss 0.10192542523145676\n",
            "Train step - Step 2060, Loss 0.09550834447145462\n",
            "Train step - Step 2070, Loss 0.10426560044288635\n",
            "Train step - Step 2080, Loss 0.10458014905452728\n",
            "Train epoch - Accuracy: 0.4660431654676259 Loss: 0.10414780566375033 Corrects: 3239\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.10719403624534607\n",
            "Train step - Step 2100, Loss 0.10480926930904388\n",
            "Train step - Step 2110, Loss 0.10266458243131638\n",
            "Train step - Step 2120, Loss 0.10639040917158127\n",
            "Train step - Step 2130, Loss 0.10165707021951675\n",
            "Train step - Step 2140, Loss 0.10727342963218689\n",
            "Train epoch - Accuracy: 0.47467625899280574 Loss: 0.10396805868731986 Corrects: 3299\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.10078908503055573\n",
            "Train step - Step 2160, Loss 0.09855945408344269\n",
            "Train step - Step 2170, Loss 0.1015959307551384\n",
            "Train step - Step 2180, Loss 0.10977998375892639\n",
            "Train step - Step 2190, Loss 0.10901035368442535\n",
            "Train epoch - Accuracy: 0.47654676258992806 Loss: 0.10382922932827215 Corrects: 3312\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.1024831160902977\n",
            "Train step - Step 2210, Loss 0.1036418154835701\n",
            "Train step - Step 2220, Loss 0.10587634891271591\n",
            "Train step - Step 2230, Loss 0.10515251010656357\n",
            "Train step - Step 2240, Loss 0.1084832176566124\n",
            "Train step - Step 2250, Loss 0.11545612663030624\n",
            "Train epoch - Accuracy: 0.47597122302158273 Loss: 0.1037931456775974 Corrects: 3308\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.1032240092754364\n",
            "Train step - Step 2270, Loss 0.10755809396505356\n",
            "Train step - Step 2280, Loss 0.10702770203351974\n",
            "Train step - Step 2290, Loss 0.10188472270965576\n",
            "Train step - Step 2300, Loss 0.1075555682182312\n",
            "Train epoch - Accuracy: 0.481294964028777 Loss: 0.10403227676590569 Corrects: 3345\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.10388948023319244\n",
            "Train step - Step 2320, Loss 0.10300634801387787\n",
            "Train step - Step 2330, Loss 0.10251758247613907\n",
            "Train step - Step 2340, Loss 0.10904312133789062\n",
            "Train step - Step 2350, Loss 0.10441549867391586\n",
            "Train step - Step 2360, Loss 0.10689807683229446\n",
            "Train epoch - Accuracy: 0.4962589928057554 Loss: 0.1035076899386996 Corrects: 3449\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.1020892858505249\n",
            "Train step - Step 2380, Loss 0.10298406332731247\n",
            "Train step - Step 2390, Loss 0.10735685378313065\n",
            "Train step - Step 2400, Loss 0.10416682809591293\n",
            "Train step - Step 2410, Loss 0.10944070667028427\n",
            "Train epoch - Accuracy: 0.49884892086330934 Loss: 0.10351226571866934 Corrects: 3467\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.09904327988624573\n",
            "Train step - Step 2430, Loss 0.09960681945085526\n",
            "Train step - Step 2440, Loss 0.1028023287653923\n",
            "Train step - Step 2450, Loss 0.10326962918043137\n",
            "Train step - Step 2460, Loss 0.10091658681631088\n",
            "Train step - Step 2470, Loss 0.10841823369264603\n",
            "Train epoch - Accuracy: 0.501726618705036 Loss: 0.10299293209537327 Corrects: 3487\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.10230182111263275\n",
            "Train step - Step 2490, Loss 0.10544493794441223\n",
            "Train step - Step 2500, Loss 0.10156445950269699\n",
            "Train step - Step 2510, Loss 0.10586908459663391\n",
            "Train step - Step 2520, Loss 0.10547954589128494\n",
            "Train epoch - Accuracy: 0.5004316546762589 Loss: 0.10382327295250172 Corrects: 3478\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.09789558500051498\n",
            "Train step - Step 2540, Loss 0.10398639738559723\n",
            "Train step - Step 2550, Loss 0.10854138433933258\n",
            "Train step - Step 2560, Loss 0.10236348956823349\n",
            "Train step - Step 2570, Loss 0.10322756320238113\n",
            "Train step - Step 2580, Loss 0.10409682989120483\n",
            "Train epoch - Accuracy: 0.5069064748201438 Loss: 0.10326409822102073 Corrects: 3523\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.10640695691108704\n",
            "Train step - Step 2600, Loss 0.10679042339324951\n",
            "Train step - Step 2610, Loss 0.10472314804792404\n",
            "Train step - Step 2620, Loss 0.10155800729990005\n",
            "Train step - Step 2630, Loss 0.10125239193439484\n",
            "Train epoch - Accuracy: 0.5021582733812949 Loss: 0.10303364585415065 Corrects: 3490\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.09982692450284958\n",
            "Train step - Step 2650, Loss 0.09921858459711075\n",
            "Train step - Step 2660, Loss 0.10201072692871094\n",
            "Train step - Step 2670, Loss 0.09983152151107788\n",
            "Train step - Step 2680, Loss 0.10175212472677231\n",
            "Train step - Step 2690, Loss 0.0987972691655159\n",
            "Train epoch - Accuracy: 0.5077697841726618 Loss: 0.10269035564266521 Corrects: 3529\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.10062777996063232\n",
            "Train step - Step 2710, Loss 0.10232770442962646\n",
            "Train step - Step 2720, Loss 0.10614404082298279\n",
            "Train step - Step 2730, Loss 0.09616024047136307\n",
            "Train step - Step 2740, Loss 0.09963319450616837\n",
            "Train epoch - Accuracy: 0.523453237410072 Loss: 0.10179586612492157 Corrects: 3638\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.0997505635023117\n",
            "Train step - Step 2760, Loss 0.09913536161184311\n",
            "Train step - Step 2770, Loss 0.10301759839057922\n",
            "Train step - Step 2780, Loss 0.10804865509271622\n",
            "Train step - Step 2790, Loss 0.10104254633188248\n",
            "Train step - Step 2800, Loss 0.10103104263544083\n",
            "Train epoch - Accuracy: 0.5176978417266187 Loss: 0.10134497406028158 Corrects: 3598\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.0993650034070015\n",
            "Train step - Step 2820, Loss 0.09902278333902359\n",
            "Train step - Step 2830, Loss 0.10210242867469788\n",
            "Train step - Step 2840, Loss 0.10025409609079361\n",
            "Train step - Step 2850, Loss 0.10328355431556702\n",
            "Train epoch - Accuracy: 0.522589928057554 Loss: 0.10143507482336579 Corrects: 3632\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.10110879689455032\n",
            "Train step - Step 2870, Loss 0.09742680937051773\n",
            "Train step - Step 2880, Loss 0.09819327294826508\n",
            "Train step - Step 2890, Loss 0.09990246593952179\n",
            "Train step - Step 2900, Loss 0.10404502600431442\n",
            "Train step - Step 2910, Loss 0.09730266034603119\n",
            "Train epoch - Accuracy: 0.5250359712230216 Loss: 0.10116979160325991 Corrects: 3649\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.10198220610618591\n",
            "Train step - Step 2930, Loss 0.10607339441776276\n",
            "Train step - Step 2940, Loss 0.10222390294075012\n",
            "Train step - Step 2950, Loss 0.09296309947967529\n",
            "Train step - Step 2960, Loss 0.10416406393051147\n",
            "Train epoch - Accuracy: 0.5244604316546763 Loss: 0.10133230351715637 Corrects: 3645\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.10411947220563889\n",
            "Train step - Step 2980, Loss 0.10043708235025406\n",
            "Train step - Step 2990, Loss 0.10648401826620102\n",
            "Train step - Step 3000, Loss 0.10726609826087952\n",
            "Train step - Step 3010, Loss 0.10724038630723953\n",
            "Train step - Step 3020, Loss 0.10338069498538971\n",
            "Train epoch - Accuracy: 0.516546762589928 Loss: 0.10112978101205483 Corrects: 3590\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.10373078286647797\n",
            "Train step - Step 3040, Loss 0.10121960192918777\n",
            "Train step - Step 3050, Loss 0.09790748357772827\n",
            "Train step - Step 3060, Loss 0.10050838440656662\n",
            "Train step - Step 3070, Loss 0.0955556258559227\n",
            "Train epoch - Accuracy: 0.520863309352518 Loss: 0.10106191885128296 Corrects: 3620\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.10107851773500443\n",
            "Train step - Step 3090, Loss 0.10109523683786392\n",
            "Train step - Step 3100, Loss 0.09777210652828217\n",
            "Train step - Step 3110, Loss 0.10382112115621567\n",
            "Train step - Step 3120, Loss 0.10031460225582123\n",
            "Train step - Step 3130, Loss 0.09954152256250381\n",
            "Train epoch - Accuracy: 0.52 Loss: 0.1010393875344194 Corrects: 3614\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.10669122636318207\n",
            "Train step - Step 3150, Loss 0.10326750576496124\n",
            "Train step - Step 3160, Loss 0.09739354997873306\n",
            "Train step - Step 3170, Loss 0.10371484607458115\n",
            "Train step - Step 3180, Loss 0.10201778262853622\n",
            "Train epoch - Accuracy: 0.5284892086330936 Loss: 0.10078935200147492 Corrects: 3673\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.09711984544992447\n",
            "Train step - Step 3200, Loss 0.09898725152015686\n",
            "Train step - Step 3210, Loss 0.09897750616073608\n",
            "Train step - Step 3220, Loss 0.09991554915904999\n",
            "Train step - Step 3230, Loss 0.0995786264538765\n",
            "Train step - Step 3240, Loss 0.11074994504451752\n",
            "Train epoch - Accuracy: 0.521294964028777 Loss: 0.10125011191093665 Corrects: 3623\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.09829214215278625\n",
            "Train step - Step 3260, Loss 0.10142118483781815\n",
            "Train step - Step 3270, Loss 0.10470382124185562\n",
            "Train step - Step 3280, Loss 0.09964612126350403\n",
            "Train step - Step 3290, Loss 0.09896053373813629\n",
            "Train epoch - Accuracy: 0.5264748201438849 Loss: 0.10100822784489007 Corrects: 3659\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.10095050930976868\n",
            "Train step - Step 3310, Loss 0.10121738165616989\n",
            "Train step - Step 3320, Loss 0.09830178320407867\n",
            "Train step - Step 3330, Loss 0.10109609365463257\n",
            "Train step - Step 3340, Loss 0.10447227209806442\n",
            "Train step - Step 3350, Loss 0.09692078083753586\n",
            "Train epoch - Accuracy: 0.5290647482014389 Loss: 0.10052659414869418 Corrects: 3677\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.10227256268262863\n",
            "Train step - Step 3370, Loss 0.09711068868637085\n",
            "Train step - Step 3380, Loss 0.10573772341012955\n",
            "Train step - Step 3390, Loss 0.10487230122089386\n",
            "Train step - Step 3400, Loss 0.10265979170799255\n",
            "Train epoch - Accuracy: 0.5302158273381294 Loss: 0.10100606354020482 Corrects: 3685\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.09685061872005463\n",
            "Train step - Step 3420, Loss 0.09906322509050369\n",
            "Train step - Step 3430, Loss 0.09275573492050171\n",
            "Train step - Step 3440, Loss 0.1042419970035553\n",
            "Train step - Step 3450, Loss 0.09637989103794098\n",
            "Train step - Step 3460, Loss 0.1005079597234726\n",
            "Train epoch - Accuracy: 0.5296402877697842 Loss: 0.1010613944075948 Corrects: 3681\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.10029992461204529\n",
            "Train step - Step 3480, Loss 0.10308559983968735\n",
            "Train step - Step 3490, Loss 0.0998394787311554\n",
            "Train step - Step 3500, Loss 0.10230888426303864\n",
            "Train step - Step 3510, Loss 0.10098915547132492\n",
            "Train epoch - Accuracy: 0.5292086330935252 Loss: 0.10081120709721134 Corrects: 3678\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.09623835980892181\n",
            "Train step - Step 3530, Loss 0.11062892526388168\n",
            "Train step - Step 3540, Loss 0.10612928122282028\n",
            "Train step - Step 3550, Loss 0.09784313291311264\n",
            "Train step - Step 3560, Loss 0.0991465151309967\n",
            "Train step - Step 3570, Loss 0.09905070811510086\n",
            "Train epoch - Accuracy: 0.5228776978417267 Loss: 0.10078423799370691 Corrects: 3634\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.10124944150447845\n",
            "Train step - Step 3590, Loss 0.10045324265956879\n",
            "Train step - Step 3600, Loss 0.09300102293491364\n",
            "Train step - Step 3610, Loss 0.09924951195716858\n",
            "Train step - Step 3620, Loss 0.1065649762749672\n",
            "Train epoch - Accuracy: 0.5253237410071943 Loss: 0.10077958438036252 Corrects: 3651\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.10180073231458664\n",
            "Train step - Step 3640, Loss 0.10470789670944214\n",
            "Train step - Step 3650, Loss 0.10322132706642151\n",
            "Train step - Step 3660, Loss 0.09671569615602493\n",
            "Train step - Step 3670, Loss 0.09546029567718506\n",
            "Train step - Step 3680, Loss 0.10450244694948196\n",
            "Train epoch - Accuracy: 0.5336690647482014 Loss: 0.10058183018252147 Corrects: 3709\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.103182353079319\n",
            "Train step - Step 3700, Loss 0.10103137791156769\n",
            "Train step - Step 3710, Loss 0.09099584072828293\n",
            "Train step - Step 3720, Loss 0.10314563661813736\n",
            "Train step - Step 3730, Loss 0.09421086311340332\n",
            "Train epoch - Accuracy: 0.5329496402877698 Loss: 0.1002563196978123 Corrects: 3704\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.09868410229682922\n",
            "Train step - Step 3750, Loss 0.10192018002271652\n",
            "Train step - Step 3760, Loss 0.10092095285654068\n",
            "Train step - Step 3770, Loss 0.09711553156375885\n",
            "Train step - Step 3780, Loss 0.09578556567430496\n",
            "Train step - Step 3790, Loss 0.09848064184188843\n",
            "Train epoch - Accuracy: 0.5276258992805756 Loss: 0.10027281107876798 Corrects: 3667\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.09868211299180984\n",
            "Train step - Step 3810, Loss 0.10438191890716553\n",
            "Train step - Step 3820, Loss 0.09703671932220459\n",
            "Train step - Step 3830, Loss 0.10381843149662018\n",
            "Train step - Step 3840, Loss 0.09997216612100601\n",
            "Train epoch - Accuracy: 0.5289208633093525 Loss: 0.10069348608203929 Corrects: 3676\n",
            "Training finished in 394.65611386299133 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0]\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232ee8c50>\n",
            "Constructing exemplars of class 91\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [34077, 28521, 34389, 3413, 11375, 4065, 36364, 41314, 41298, 35722, 17138, 28172, 30632, 28690, 25757, 35303, 21625, 13439, 47896, 43430, 5265, 17063, 32154, 21340, 3454, 16314, 20049, 44419, 15376, 31831, 16415, 21511, 43460, 32007, 40124, 39392, 34389, 7827, 313, 42716]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238ac7a10>\n",
            "Constructing exemplars of class 67\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [9791, 26411, 5543, 45668, 1247, 46644, 23572, 20614, 30991, 29453, 15324, 32687, 33761, 13575, 10630, 21582, 31732, 21143, 9174, 1101, 26910, 27157, 10946, 34006, 2742, 28209, 27360, 2785, 38326, 2860, 11131, 14249, 29453, 1627, 25966, 1973, 8692, 3818, 28403, 45668]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232eeff50>\n",
            "Constructing exemplars of class 7\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [38690, 2149, 49996, 4884, 606, 44890, 31359, 21660, 38438, 37815, 17493, 13162, 37623, 31764, 40343, 36420, 49939, 22695, 19233, 5819, 42560, 42172, 2149, 31840, 2356, 28351, 40131, 24631, 8306, 37815, 41605, 26064, 45534, 28106, 17791, 37071, 34209, 47101, 35159, 16035]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223822c110>\n",
            "Constructing exemplars of class 62\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [34356, 35609, 4784, 43418, 18170, 32291, 40692, 17468, 21547, 26092, 10731, 6830, 9341, 24885, 42933, 10007, 14909, 41433, 6339, 33591, 29105, 19422, 41603, 18391, 46545, 40891, 21068, 34449, 7935, 47702, 19674, 30713, 3787, 31405, 18696, 20625, 47953, 897, 25417, 719]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22414294d0>\n",
            "Constructing exemplars of class 46\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [41147, 16261, 9803, 12436, 10021, 16394, 48211, 18113, 885, 22659, 44318, 4198, 17212, 19846, 44738, 30397, 18770, 46919, 12019, 27714, 14132, 27411, 11627, 18191, 45758, 28231, 22668, 45493, 28028, 41311, 22439, 27366, 26040, 25003, 21666, 5784, 22305, 44533, 14095, 11990]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238035550>\n",
            "Constructing exemplars of class 65\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [28261, 3807, 11632, 3834, 48464, 14214, 19026, 47299, 23766, 38799, 21975, 22118, 48476, 6411, 44002, 38115, 31038, 12479, 43785, 28543, 17603, 10704, 42332, 44363, 38308, 16505, 17978, 30989, 6255, 48692, 23565, 41727, 28544, 33043, 21059, 11441, 48174, 41321, 34105, 17480]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238048710>\n",
            "Constructing exemplars of class 21\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [32570, 34798, 27534, 20006, 35040, 45390, 25561, 12784, 15874, 215, 14614, 6650, 1333, 13623, 14101, 7092, 48260, 46014, 44316, 12998, 15874, 28967, 5849, 40466, 7358, 33008, 23144, 44201, 11172, 1794, 14404, 7893, 21431, 8572, 17038, 14451, 46251, 15320, 25670, 11031]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2244662210>\n",
            "Constructing exemplars of class 32\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [1732, 33467, 32146, 6698, 47635, 26772, 27951, 21365, 28352, 28321, 36853, 29001, 21249, 40113, 22600, 33857, 7444, 11831, 9834, 8787, 11332, 33705, 17352, 44672, 30068, 48866, 6698, 16745, 20744, 33040, 35436, 38820, 33281, 25811, 33140, 18676, 30294, 15504, 21788, 20356]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232963310>\n",
            "Constructing exemplars of class 16\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [1792, 40901, 34886, 34812, 21303, 39592, 4620, 6972, 25959, 23067, 45357, 44376, 9075, 18377, 30494, 22380, 25541, 14019, 1300, 38638, 20553, 35320, 22464, 23899, 1743, 12643, 9261, 10611, 4687, 15221, 21588, 26400, 3552, 21274, 38681, 20555, 38191, 14425, 12643, 18225]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232edc350>\n",
            "Constructing exemplars of class 0\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [44150, 15990, 30172, 6438, 22165, 47445, 4055, 44972, 24717, 48469, 2126, 28361, 43179, 17882, 38908, 45466, 37056, 31600, 22324, 34042, 27759, 3594, 17727, 35665, 45717, 21725, 41774, 4398, 18064, 42847, 915, 20673, 16540, 42285, 44327, 32714, 47979, 43179, 17390, 27576]\n",
            "x train:  [-0.11418867 -0.06397434 -0.09129415 -0.11471892 -0.1541793   0.0197516\n",
            " -0.02684177 -0.07511663 -0.13134332 -0.03152029 -0.03477676 -0.12528445\n",
            " -0.11254081 -0.09080856 -0.12249456 -0.14362288 -0.00942367 -0.14356144\n",
            " -0.15762042 -0.26915073 -0.05780768 -0.09320907 -0.12336543 -0.10031076\n",
            " -0.15795988 -0.11999255 -0.11741342 -0.06357858 -0.17300823 -0.2699407\n",
            " -0.1191648  -0.04370972 -0.05420838 -0.20760371 -0.27911434 -0.15708354\n",
            " -0.21568969 -0.13978493 -0.21691616 -0.04688708 -0.10786799 -0.1834206\n",
            " -0.28298703 -0.16760553 -0.0432343  -0.09225056 -0.11114978 -0.04877943\n",
            " -0.04576128 -0.25135705]\n",
            "y_train:  [tensor([30]), tensor([37]), tensor([36]), tensor([37]), tensor([69]), tensor([59]), tensor([34]), tensor([35]), tensor([91]), tensor([96]), tensor([34]), tensor([80]), tensor([94]), tensor([53]), tensor([36]), tensor([35]), tensor([81]), tensor([72]), tensor([59]), tensor([11]), tensor([37]), tensor([0]), tensor([57]), tensor([45]), tensor([86]), tensor([12]), tensor([52]), tensor([82]), tensor([33]), tensor([60]), tensor([77]), tensor([24]), tensor([24]), tensor([58]), tensor([12]), tensor([3]), tensor([16]), tensor([81]), tensor([33]), tensor([36]), tensor([88]), tensor([12]), tensor([35]), tensor([31]), tensor([35]), tensor([33]), tensor([45]), tensor([31]), tensor([16]), tensor([12]), tensor([52]), tensor([50]), tensor([24]), tensor([13]), tensor([65]), tensor([0]), tensor([27]), tensor([46]), tensor([67]), tensor([97]), tensor([57]), tensor([68]), tensor([21]), tensor([37]), tensor([97]), tensor([82]), tensor([57]), tensor([11]), tensor([12]), tensor([21]), tensor([57]), tensor([65]), tensor([81]), tensor([86]), tensor([82]), tensor([62]), tensor([62]), tensor([77]), tensor([37]), tensor([98]), tensor([72]), tensor([72]), tensor([60]), tensor([7]), tensor([30]), tensor([95]), tensor([32]), tensor([13]), tensor([78]), tensor([33]), tensor([13]), tensor([57]), tensor([72]), tensor([25]), tensor([60]), tensor([25]), tensor([97]), tensor([82]), tensor([58]), tensor([16]), tensor([60]), tensor([88]), tensor([92]), tensor([88]), tensor([80]), tensor([60]), tensor([88]), tensor([21]), tensor([46]), tensor([0]), tensor([49]), tensor([12]), tensor([30]), tensor([13]), tensor([7]), tensor([49]), tensor([34]), tensor([0]), tensor([45]), tensor([34]), tensor([11]), tensor([97]), tensor([13]), tensor([25]), tensor([7]), tensor([27]), tensor([11]), tensor([62]), tensor([67]), tensor([52]), tensor([96]), tensor([96]), tensor([31]), tensor([36]), tensor([60]), tensor([91]), tensor([27]), tensor([59]), tensor([35]), tensor([80]), tensor([59]), tensor([58]), tensor([67]), tensor([16]), tensor([27]), tensor([24]), tensor([58]), tensor([32]), tensor([50]), tensor([24]), tensor([32]), tensor([24]), tensor([91]), tensor([11]), tensor([45]), tensor([11]), tensor([69]), tensor([96]), tensor([97]), tensor([65]), tensor([96]), tensor([30]), tensor([88]), tensor([72]), tensor([45]), tensor([25]), tensor([72]), tensor([49]), tensor([49]), tensor([97]), tensor([32]), tensor([30]), tensor([95]), tensor([3]), tensor([59]), tensor([31]), tensor([45]), tensor([3]), tensor([94]), tensor([97]), tensor([59]), tensor([78]), tensor([16]), tensor([24]), tensor([97]), tensor([30]), tensor([98]), tensor([81]), tensor([58]), tensor([68]), tensor([52]), tensor([24]), tensor([16]), tensor([58]), tensor([78]), tensor([36]), tensor([58]), tensor([21]), tensor([33]), tensor([92]), tensor([36]), tensor([46]), tensor([77]), tensor([31]), tensor([45]), tensor([49]), tensor([7]), tensor([80]), tensor([67]), tensor([27]), tensor([95]), tensor([92]), tensor([86]), tensor([52]), tensor([92]), tensor([7]), tensor([60]), tensor([98]), tensor([95]), tensor([59]), tensor([34]), tensor([36]), tensor([10]), tensor([82]), tensor([13]), tensor([25]), tensor([65]), tensor([37]), tensor([21]), tensor([91]), tensor([98]), tensor([80]), tensor([36]), tensor([32]), tensor([98]), tensor([69]), tensor([46]), tensor([13]), tensor([98]), tensor([67]), tensor([52]), tensor([27]), tensor([58]), tensor([52]), tensor([32]), tensor([96]), tensor([91]), tensor([65]), tensor([10]), tensor([77]), tensor([53]), tensor([91]), tensor([27]), tensor([30]), tensor([24]), tensor([16]), tensor([96]), tensor([31]), tensor([7]), tensor([95]), tensor([88]), tensor([95]), tensor([96]), tensor([86]), tensor([72]), tensor([68]), tensor([52]), tensor([68]), tensor([52]), tensor([72]), tensor([0]), tensor([69]), tensor([25]), tensor([16]), tensor([31]), tensor([32]), tensor([60]), tensor([7]), tensor([59]), tensor([33]), tensor([94]), tensor([32]), tensor([12]), tensor([96]), tensor([60]), tensor([62]), tensor([10]), tensor([94]), tensor([36]), tensor([34]), tensor([72]), tensor([65]), tensor([13]), tensor([86]), tensor([81]), tensor([35]), tensor([82]), tensor([25]), tensor([60]), tensor([67]), tensor([82]), tensor([53]), tensor([0]), tensor([37]), tensor([58]), tensor([30]), tensor([36]), tensor([91]), tensor([31]), tensor([77]), tensor([0]), tensor([12]), tensor([72]), tensor([35]), tensor([34]), tensor([32]), tensor([96]), tensor([37]), tensor([13]), tensor([46]), tensor([34]), tensor([78]), tensor([27]), tensor([36]), tensor([11]), tensor([25]), tensor([68]), tensor([80]), tensor([80]), tensor([65]), tensor([11]), tensor([57]), tensor([53]), tensor([94]), tensor([58]), tensor([82]), tensor([68]), tensor([7]), tensor([24]), tensor([46]), tensor([27]), tensor([10]), tensor([92]), tensor([31]), tensor([95]), tensor([95]), tensor([36]), tensor([53]), tensor([25]), tensor([68]), tensor([24]), tensor([59]), tensor([62]), tensor([12]), tensor([34]), tensor([52]), tensor([12]), tensor([34]), tensor([67]), tensor([49]), tensor([3]), tensor([98]), tensor([12]), tensor([59]), tensor([24]), tensor([12]), tensor([57]), tensor([35]), tensor([30]), tensor([24]), tensor([68]), tensor([10]), tensor([12]), tensor([7]), tensor([50]), tensor([69]), tensor([59]), tensor([94]), tensor([30]), tensor([92]), tensor([33]), tensor([36]), tensor([21]), tensor([46]), tensor([58]), tensor([72]), tensor([95]), tensor([53]), tensor([62]), tensor([77]), tensor([31]), tensor([86]), tensor([81]), tensor([72]), tensor([95]), tensor([88]), tensor([21]), tensor([13]), tensor([80]), tensor([45]), tensor([68]), tensor([59]), tensor([31]), tensor([32]), tensor([52]), tensor([68]), tensor([31]), tensor([62]), tensor([10]), tensor([94]), tensor([7]), tensor([91]), tensor([52]), tensor([77]), tensor([35]), tensor([50]), tensor([11]), tensor([95]), tensor([78]), tensor([45]), tensor([25]), tensor([57]), tensor([68]), tensor([72]), tensor([45]), tensor([72]), tensor([0]), tensor([13]), tensor([3]), tensor([27]), tensor([52]), tensor([27]), tensor([58]), tensor([0]), tensor([25]), tensor([60]), tensor([94]), tensor([3]), tensor([37]), tensor([65]), tensor([53]), tensor([32]), tensor([81]), tensor([59]), tensor([72]), tensor([69]), tensor([16]), tensor([34]), tensor([3]), tensor([46]), tensor([67]), tensor([77]), tensor([27]), tensor([32]), tensor([49]), tensor([98]), tensor([86]), tensor([53]), tensor([24]), tensor([98]), tensor([33]), tensor([98]), tensor([60]), tensor([30]), tensor([96]), tensor([86]), tensor([60]), tensor([3]), tensor([58]), tensor([69]), tensor([95]), tensor([30]), tensor([31]), tensor([88]), tensor([53]), tensor([24]), tensor([91]), tensor([81]), tensor([53]), tensor([82]), tensor([69]), tensor([35]), tensor([82]), tensor([82]), tensor([68]), tensor([31]), tensor([45]), tensor([91]), tensor([13]), tensor([10]), tensor([59]), tensor([81]), tensor([80]), tensor([31]), tensor([21]), tensor([67]), tensor([34]), tensor([82]), tensor([50]), tensor([49]), tensor([31]), tensor([86]), tensor([37]), tensor([57]), tensor([33]), tensor([68]), tensor([86]), tensor([24]), tensor([77]), tensor([65]), tensor([36]), tensor([37]), tensor([58]), tensor([37]), tensor([92]), tensor([96]), tensor([25]), tensor([96]), tensor([77]), tensor([13]), tensor([11]), tensor([30]), tensor([88]), tensor([46]), tensor([98]), tensor([92]), tensor([10]), tensor([77]), tensor([36]), tensor([46]), tensor([33]), tensor([88]), tensor([52]), tensor([34]), tensor([27]), tensor([80]), tensor([27]), tensor([91]), tensor([98]), tensor([95]), tensor([67]), tensor([45]), tensor([80]), tensor([13]), tensor([57]), tensor([33]), tensor([97]), tensor([94]), tensor([77]), tensor([94]), tensor([31]), tensor([25]), tensor([13]), tensor([65]), tensor([33]), tensor([50]), tensor([97]), tensor([33]), tensor([81]), tensor([77]), tensor([81]), tensor([35]), tensor([92]), tensor([80]), tensor([32]), tensor([30]), tensor([80]), tensor([32]), tensor([13]), tensor([59]), tensor([57]), tensor([31]), tensor([21]), tensor([24]), tensor([80]), tensor([32]), tensor([97]), tensor([65]), tensor([33]), tensor([25]), tensor([12]), tensor([77]), tensor([92]), tensor([30]), tensor([78]), tensor([57]), tensor([11]), tensor([46]), tensor([49]), tensor([52]), tensor([46]), tensor([11]), tensor([58]), tensor([92]), tensor([35]), tensor([49]), tensor([27]), tensor([37]), tensor([68]), tensor([33]), tensor([16]), tensor([46]), tensor([57]), tensor([81]), tensor([30]), tensor([10]), tensor([81]), tensor([24]), tensor([3]), tensor([30]), tensor([12]), tensor([50]), tensor([65]), tensor([98]), tensor([3]), tensor([98]), tensor([58]), tensor([96]), tensor([10]), tensor([92]), tensor([35]), tensor([57]), tensor([65]), tensor([50]), tensor([86]), tensor([92]), tensor([13]), tensor([27]), tensor([34]), tensor([67]), tensor([98]), tensor([10]), tensor([25]), tensor([77]), tensor([30]), tensor([88]), tensor([35]), tensor([72]), tensor([72]), tensor([12]), tensor([97]), tensor([65]), tensor([65]), tensor([92]), tensor([12]), tensor([62]), tensor([58]), tensor([31]), tensor([95]), tensor([94]), tensor([36]), tensor([11]), tensor([65]), tensor([37]), tensor([94]), tensor([65]), tensor([59]), tensor([16]), tensor([7]), tensor([88]), tensor([96]), tensor([86]), tensor([31]), tensor([0]), tensor([35]), tensor([24]), tensor([82]), tensor([81]), tensor([46]), tensor([53]), tensor([94]), tensor([34]), tensor([12]), tensor([3]), tensor([32]), tensor([72]), tensor([0]), tensor([12]), tensor([86]), tensor([46]), tensor([10]), tensor([67]), tensor([10]), tensor([88]), tensor([13]), tensor([78]), tensor([27]), tensor([62]), tensor([62]), tensor([12]), tensor([24]), tensor([68]), tensor([60]), tensor([65]), tensor([60]), tensor([33]), tensor([95]), tensor([72]), tensor([16]), tensor([53]), tensor([46]), tensor([69]), tensor([77]), tensor([27]), tensor([52]), tensor([91]), tensor([25]), tensor([67]), tensor([45]), tensor([53]), tensor([33]), tensor([95]), tensor([34]), tensor([21]), tensor([33]), tensor([10]), tensor([91]), tensor([11]), tensor([81]), tensor([57]), tensor([91]), tensor([46]), tensor([46]), tensor([12]), tensor([49]), tensor([0]), tensor([69]), tensor([86]), tensor([78]), tensor([11]), tensor([92]), tensor([0]), tensor([53]), tensor([45]), tensor([49]), tensor([62]), tensor([16]), tensor([60]), tensor([80]), tensor([11]), tensor([11]), tensor([62]), tensor([91]), tensor([67]), tensor([65]), tensor([10]), tensor([94]), tensor([69]), tensor([36]), tensor([3]), tensor([33]), tensor([94]), tensor([86]), tensor([37]), tensor([86]), tensor([57]), tensor([69]), tensor([77]), tensor([11]), tensor([46]), tensor([53]), tensor([77]), tensor([31]), tensor([45]), tensor([36]), tensor([80]), tensor([37]), tensor([7]), tensor([12]), tensor([65]), tensor([96]), tensor([32]), tensor([50]), tensor([50]), tensor([81]), tensor([59]), tensor([27]), tensor([45]), tensor([82]), tensor([24]), tensor([27]), tensor([57]), tensor([60]), tensor([58]), tensor([10]), tensor([92]), tensor([25]), tensor([86]), tensor([60]), tensor([67]), tensor([24]), tensor([35]), tensor([21]), tensor([24]), tensor([67]), tensor([68]), tensor([62]), tensor([30]), tensor([86]), tensor([3]), tensor([65]), tensor([60]), tensor([78]), tensor([94]), tensor([32]), tensor([59]), tensor([78]), tensor([69]), tensor([67]), tensor([25]), tensor([94]), tensor([62]), tensor([37]), tensor([32]), tensor([60]), tensor([92]), tensor([13]), tensor([57]), tensor([16]), tensor([31]), tensor([37]), tensor([50]), tensor([24]), tensor([97]), tensor([0]), tensor([58]), tensor([36]), tensor([86]), tensor([30]), tensor([67]), tensor([45]), tensor([49]), tensor([21]), tensor([59]), tensor([68]), tensor([25]), tensor([3]), tensor([65]), tensor([78]), tensor([80]), tensor([37]), tensor([65]), tensor([67]), tensor([91]), tensor([21]), tensor([25]), tensor([36]), tensor([97]), tensor([46]), tensor([0]), tensor([57]), tensor([3]), tensor([37]), tensor([58]), tensor([21]), tensor([0]), tensor([21]), tensor([13]), tensor([24]), tensor([27]), tensor([92]), tensor([86]), tensor([67]), tensor([0]), tensor([0]), tensor([62]), tensor([77]), tensor([34]), tensor([3]), tensor([95]), tensor([24]), tensor([33]), tensor([52]), tensor([58]), tensor([59]), tensor([34]), tensor([80]), tensor([68]), tensor([81]), tensor([68]), tensor([62]), tensor([11]), tensor([46]), tensor([37]), tensor([65]), tensor([7]), tensor([98]), tensor([94]), tensor([35]), tensor([88]), tensor([96]), tensor([16]), tensor([13]), tensor([80]), tensor([16]), tensor([21]), tensor([96]), tensor([37]), tensor([69]), tensor([67]), tensor([16]), tensor([25]), tensor([36]), tensor([30]), tensor([49]), tensor([12]), tensor([0]), tensor([32]), tensor([34]), tensor([86]), tensor([59]), tensor([21]), tensor([94]), tensor([94]), tensor([92]), tensor([21]), tensor([50]), tensor([31]), tensor([52]), tensor([62]), tensor([58]), tensor([34]), tensor([68]), tensor([78]), tensor([69]), tensor([78]), tensor([34]), tensor([98]), tensor([72]), tensor([16]), tensor([80]), tensor([37]), tensor([24]), tensor([52]), tensor([35]), tensor([10]), tensor([96]), tensor([16]), tensor([24]), tensor([35]), tensor([7]), tensor([0]), tensor([45]), tensor([97]), tensor([65]), tensor([77]), tensor([53]), tensor([11]), tensor([34]), tensor([3]), tensor([78]), tensor([60]), tensor([91]), tensor([95]), tensor([45]), tensor([78]), tensor([95]), tensor([78]), tensor([31]), tensor([11]), tensor([57]), tensor([13]), tensor([59]), tensor([16]), tensor([37]), tensor([86]), tensor([31]), tensor([58]), tensor([59]), tensor([31]), tensor([10]), tensor([45]), tensor([46]), tensor([67]), tensor([72]), tensor([32]), tensor([21]), tensor([91]), tensor([50]), tensor([57]), tensor([80]), tensor([32]), tensor([45]), tensor([36]), tensor([21]), tensor([49]), tensor([91]), tensor([86]), tensor([49]), tensor([88]), tensor([69]), tensor([65]), tensor([92]), tensor([97]), tensor([72]), tensor([88]), tensor([35]), tensor([81]), tensor([27]), tensor([3]), tensor([46]), tensor([10]), tensor([68]), tensor([88]), tensor([27]), tensor([49]), tensor([86]), tensor([24]), tensor([78]), tensor([86]), tensor([58]), tensor([69]), tensor([27]), tensor([96]), tensor([77]), tensor([50]), tensor([80]), tensor([80]), tensor([46]), tensor([81]), tensor([25]), tensor([52]), tensor([88]), tensor([37]), tensor([30]), tensor([80]), tensor([49]), tensor([46]), tensor([95]), tensor([7]), tensor([92]), tensor([50]), tensor([16]), tensor([88]), tensor([68]), tensor([60]), tensor([49]), tensor([24]), tensor([67]), tensor([68]), tensor([78]), tensor([82]), tensor([82]), tensor([32]), tensor([72]), tensor([97]), tensor([78]), tensor([72]), tensor([53]), tensor([60]), tensor([13]), tensor([60]), tensor([30]), tensor([31]), tensor([78]), tensor([52]), tensor([31]), tensor([92]), tensor([25]), tensor([27]), tensor([32]), tensor([88]), tensor([82]), tensor([98]), tensor([65]), tensor([49]), tensor([60]), tensor([34]), tensor([94]), tensor([50]), tensor([98]), tensor([30]), tensor([50]), tensor([81]), tensor([45]), tensor([3]), tensor([31]), tensor([34]), tensor([98]), tensor([88]), tensor([36]), tensor([94]), tensor([32]), tensor([91]), tensor([91]), tensor([3]), tensor([95]), tensor([31]), tensor([82]), tensor([32]), tensor([7]), tensor([78]), tensor([7]), tensor([97]), tensor([65]), tensor([62]), tensor([69]), tensor([3]), tensor([77]), tensor([7]), tensor([21]), tensor([65]), tensor([7]), tensor([91]), tensor([7]), tensor([50]), tensor([77]), tensor([77]), tensor([25]), tensor([60]), tensor([25]), tensor([12]), tensor([81]), tensor([72]), tensor([68]), tensor([35]), tensor([98]), tensor([91]), tensor([60]), tensor([7]), tensor([78]), tensor([98]), tensor([3]), tensor([65]), tensor([95]), tensor([35]), tensor([12]), tensor([86]), tensor([24]), tensor([88]), tensor([7]), tensor([67]), tensor([34]), tensor([27]), tensor([67]), tensor([62]), tensor([98]), tensor([11]), tensor([46]), tensor([12]), tensor([53]), tensor([92]), tensor([81]), tensor([95]), tensor([12]), tensor([12]), tensor([27]), tensor([33]), tensor([49]), tensor([30]), tensor([3]), tensor([72]), tensor([50]), tensor([35]), tensor([27]), tensor([97]), tensor([52]), tensor([96]), tensor([92]), tensor([59]), tensor([12]), tensor([80]), tensor([67]), tensor([78]), tensor([77]), tensor([32]), tensor([65]), tensor([68]), tensor([53]), tensor([92]), tensor([0]), tensor([58]), tensor([68]), tensor([50]), tensor([91]), tensor([62]), tensor([96]), tensor([3]), tensor([12]), tensor([86]), tensor([13]), tensor([52]), tensor([60]), tensor([35]), tensor([3]), tensor([95]), tensor([82]), tensor([10]), tensor([49]), tensor([58]), tensor([91]), tensor([62]), tensor([82]), tensor([13]), tensor([36]), tensor([94]), tensor([7]), tensor([3]), tensor([72]), tensor([36]), tensor([25]), tensor([97]), tensor([25]), tensor([53]), tensor([91]), tensor([35]), tensor([95]), tensor([52]), tensor([27]), tensor([7]), tensor([98]), tensor([96]), tensor([11]), tensor([34]), tensor([46]), tensor([72]), tensor([59]), tensor([49]), tensor([98]), tensor([35]), tensor([91]), tensor([21]), tensor([80]), tensor([86]), tensor([97]), tensor([13]), tensor([13]), tensor([49]), tensor([88]), tensor([91]), tensor([3]), tensor([35]), tensor([50]), tensor([16]), tensor([81]), tensor([45]), tensor([86]), tensor([30]), tensor([52]), tensor([10]), tensor([72]), tensor([59]), tensor([96]), tensor([50]), tensor([7]), tensor([21]), tensor([97]), tensor([80]), tensor([52]), tensor([91]), tensor([46]), tensor([30]), tensor([82]), tensor([7]), tensor([34]), tensor([91]), tensor([35]), tensor([46]), tensor([34]), tensor([97]), tensor([12]), tensor([21]), tensor([10]), tensor([12]), tensor([60]), tensor([13]), tensor([31]), tensor([53]), tensor([98]), tensor([62]), tensor([96]), tensor([97]), tensor([21]), tensor([36]), tensor([95]), tensor([32]), tensor([27]), tensor([37]), tensor([10]), tensor([65]), tensor([81]), tensor([94]), tensor([45]), tensor([69]), tensor([69]), tensor([59]), tensor([82]), tensor([7]), tensor([80]), tensor([72]), tensor([80]), tensor([24]), tensor([97]), tensor([50]), tensor([12]), tensor([30]), tensor([68]), tensor([94]), tensor([80]), tensor([97]), tensor([31]), tensor([11]), tensor([69]), tensor([25]), tensor([10]), tensor([92]), tensor([69]), tensor([78]), tensor([82]), tensor([57]), tensor([32]), tensor([32]), tensor([3]), tensor([58]), tensor([72]), tensor([96]), tensor([60]), tensor([96]), tensor([33]), tensor([27]), tensor([58]), tensor([97]), tensor([46]), tensor([21]), tensor([46]), tensor([96]), tensor([58]), tensor([60]), tensor([13]), tensor([81]), tensor([58]), tensor([86]), tensor([67]), tensor([82]), tensor([50]), tensor([0]), tensor([21]), tensor([33]), tensor([67]), tensor([0]), tensor([16]), tensor([62]), tensor([96]), tensor([32]), tensor([97]), tensor([72]), tensor([46]), tensor([50]), tensor([27]), tensor([78]), tensor([36]), tensor([52]), tensor([86]), tensor([52]), tensor([37]), tensor([57]), tensor([49]), tensor([45]), tensor([69]), tensor([45]), tensor([88]), tensor([13]), tensor([11]), tensor([81]), tensor([37]), tensor([31]), tensor([60]), tensor([80]), tensor([57]), tensor([98]), tensor([88]), tensor([98]), tensor([92]), tensor([62]), tensor([81]), tensor([94]), tensor([16]), tensor([45]), tensor([45]), tensor([65]), tensor([50]), tensor([52]), tensor([78]), tensor([31]), tensor([69]), tensor([49]), tensor([58]), tensor([57]), tensor([13]), tensor([37]), tensor([68]), tensor([7]), tensor([80]), tensor([52]), tensor([58]), tensor([97]), tensor([46]), tensor([53]), tensor([77]), tensor([46]), tensor([11]), tensor([57]), tensor([57]), tensor([80]), tensor([81]), tensor([50]), tensor([77]), tensor([49]), tensor([37]), tensor([60]), tensor([57]), tensor([10]), tensor([95]), tensor([62]), tensor([52]), tensor([24]), tensor([72]), tensor([88]), tensor([50]), tensor([35]), tensor([69]), tensor([7]), tensor([37]), tensor([95]), tensor([53]), tensor([25]), tensor([35]), tensor([25]), tensor([52]), tensor([34]), tensor([57]), tensor([86]), tensor([13]), tensor([33]), tensor([81]), tensor([82]), tensor([10]), tensor([33]), tensor([60]), tensor([53]), tensor([81]), tensor([91]), tensor([16]), tensor([95]), tensor([60]), tensor([10]), tensor([11]), tensor([25]), tensor([58]), tensor([95]), tensor([7]), tensor([53]), tensor([80]), tensor([3]), tensor([11]), tensor([10]), tensor([11]), tensor([53]), tensor([58]), tensor([21]), tensor([0]), tensor([98]), tensor([92]), tensor([16]), tensor([69]), tensor([94]), tensor([3]), tensor([67]), tensor([92]), tensor([77]), tensor([94]), tensor([59]), tensor([11]), tensor([37]), tensor([33]), tensor([95]), tensor([21]), tensor([57]), tensor([32]), tensor([21]), tensor([80]), tensor([0]), tensor([25]), tensor([96]), tensor([11]), tensor([82]), tensor([68]), tensor([62]), tensor([7]), tensor([77]), tensor([33]), tensor([86]), tensor([36]), tensor([49]), tensor([45]), tensor([27]), tensor([68]), tensor([92]), tensor([0]), tensor([21]), tensor([77]), tensor([0]), tensor([36]), tensor([33]), tensor([3]), tensor([53]), tensor([13]), tensor([96]), tensor([3]), tensor([16]), tensor([80]), tensor([67]), tensor([49]), tensor([88]), tensor([12]), tensor([92]), tensor([27]), tensor([50]), tensor([59]), tensor([53]), tensor([95]), tensor([30]), tensor([35]), tensor([96]), tensor([78]), tensor([97]), tensor([81]), tensor([24]), tensor([37]), tensor([94]), tensor([45]), tensor([62]), tensor([88]), tensor([46]), tensor([0]), tensor([11]), tensor([91]), tensor([11]), tensor([94]), tensor([62]), tensor([10]), tensor([94]), tensor([10]), tensor([97]), tensor([36]), tensor([81]), tensor([80]), tensor([72]), tensor([7]), tensor([45]), tensor([46]), tensor([82]), tensor([69]), tensor([21]), tensor([57]), tensor([31]), tensor([96]), tensor([36]), tensor([11]), tensor([33]), tensor([27]), tensor([88]), tensor([13]), tensor([37]), tensor([69]), tensor([35]), tensor([25]), tensor([16]), tensor([21]), tensor([69]), tensor([78]), tensor([78]), tensor([88]), tensor([94]), tensor([13]), tensor([82]), tensor([86]), tensor([68]), tensor([80]), tensor([78]), tensor([49]), tensor([10]), tensor([77]), tensor([24]), tensor([34]), tensor([16]), tensor([68]), tensor([57]), tensor([7]), tensor([7]), tensor([88]), tensor([98]), tensor([34]), tensor([91]), tensor([91]), tensor([31]), tensor([58]), tensor([67]), tensor([59]), tensor([59]), tensor([95]), tensor([31]), tensor([82]), tensor([58]), tensor([62]), tensor([81]), tensor([27]), tensor([69]), tensor([77]), tensor([25]), tensor([65]), tensor([53]), tensor([77]), tensor([37]), tensor([77]), tensor([34]), tensor([50]), tensor([0]), tensor([97]), tensor([81]), tensor([98]), tensor([21]), tensor([10]), tensor([92]), tensor([69]), tensor([69]), tensor([82]), tensor([97]), tensor([16]), tensor([81]), tensor([62]), tensor([7]), tensor([65]), tensor([82]), tensor([52]), tensor([30]), tensor([52]), tensor([32]), tensor([32]), tensor([53]), tensor([50]), tensor([78]), tensor([12]), tensor([45]), tensor([92]), tensor([0]), tensor([52]), tensor([86]), tensor([21]), tensor([7]), tensor([62]), tensor([69]), tensor([94]), tensor([10]), tensor([88]), tensor([10]), tensor([58]), tensor([97]), tensor([78]), tensor([35]), tensor([67]), tensor([25]), tensor([3]), tensor([59]), tensor([67]), tensor([16]), tensor([50]), tensor([98]), tensor([0]), tensor([92]), tensor([0]), tensor([78]), tensor([32]), tensor([16]), tensor([45]), tensor([33]), tensor([49]), tensor([50]), tensor([34]), tensor([35]), tensor([72]), tensor([50]), tensor([0]), tensor([21]), tensor([59]), tensor([98]), tensor([33]), tensor([69]), tensor([46]), tensor([3]), tensor([62]), tensor([91]), tensor([98]), tensor([72]), tensor([81]), tensor([97]), tensor([24]), tensor([82]), tensor([10]), tensor([72]), tensor([10]), tensor([3]), tensor([30]), tensor([88]), tensor([82]), tensor([62]), tensor([49]), tensor([10]), tensor([30]), tensor([57]), tensor([94]), tensor([95]), tensor([68]), tensor([94]), tensor([32]), tensor([0]), tensor([36]), tensor([53]), tensor([53]), tensor([82]), tensor([59]), tensor([27]), tensor([69]), tensor([78]), tensor([98]), tensor([98]), tensor([49]), tensor([97]), tensor([30]), tensor([11]), tensor([62]), tensor([88]), tensor([32]), tensor([7]), tensor([60]), tensor([50]), tensor([82]), tensor([65]), tensor([13]), tensor([33]), tensor([53]), tensor([94]), tensor([78]), tensor([60]), tensor([68]), tensor([45]), tensor([30]), tensor([3]), tensor([11]), tensor([30]), tensor([92]), tensor([92]), tensor([12]), tensor([21]), tensor([37]), tensor([82]), tensor([81]), tensor([33]), tensor([46]), tensor([11]), tensor([97]), tensor([91]), tensor([11]), tensor([86]), tensor([82]), tensor([52]), tensor([96]), tensor([37]), tensor([53]), tensor([59]), tensor([16]), tensor([77]), tensor([50]), tensor([7]), tensor([31]), tensor([91]), tensor([31]), tensor([59]), tensor([97]), tensor([34]), tensor([77]), tensor([24]), tensor([96]), tensor([95]), tensor([68]), tensor([80]), tensor([82]), tensor([72]), tensor([65]), tensor([62]), tensor([67]), tensor([97]), tensor([57]), tensor([3]), tensor([88]), tensor([88]), tensor([36]), tensor([50]), tensor([53]), tensor([96]), tensor([65]), tensor([58]), tensor([30]), tensor([37]), tensor([92]), tensor([3]), tensor([86]), tensor([30]), tensor([32]), tensor([94]), tensor([21]), tensor([67]), tensor([30]), tensor([45]), tensor([95]), tensor([16]), tensor([68]), tensor([0]), tensor([33]), tensor([91]), tensor([30]), tensor([65]), tensor([78]), tensor([11]), tensor([78]), tensor([77]), tensor([59]), tensor([72]), tensor([50]), tensor([3]), tensor([45]), tensor([67]), tensor([49]), tensor([77]), tensor([13]), tensor([53]), tensor([16]), tensor([25]), tensor([92]), tensor([98]), tensor([95]), tensor([33]), tensor([16]), tensor([33]), tensor([49]), tensor([13]), tensor([88]), tensor([34]), tensor([24]), tensor([36]), tensor([25]), tensor([96]), tensor([32]), tensor([46]), tensor([72]), tensor([7]), tensor([0]), tensor([59]), tensor([52]), tensor([62]), tensor([97]), tensor([36]), tensor([78]), tensor([57]), tensor([52]), tensor([35]), tensor([96]), tensor([10]), tensor([12]), tensor([68]), tensor([46]), tensor([52]), tensor([82]), tensor([0]), tensor([69]), tensor([82]), tensor([92]), tensor([10]), tensor([45]), tensor([69]), tensor([67]), tensor([12]), tensor([37]), tensor([34]), tensor([95]), tensor([31]), tensor([88]), tensor([68]), tensor([95]), tensor([68]), tensor([16]), tensor([88]), tensor([35]), tensor([98]), tensor([25]), tensor([12]), tensor([32]), tensor([53]), tensor([58]), tensor([45]), tensor([52]), tensor([69]), tensor([60]), tensor([34]), tensor([25]), tensor([67]), tensor([86]), tensor([96]), tensor([57]), tensor([24]), tensor([35]), tensor([50]), tensor([57]), tensor([25]), tensor([0]), tensor([57]), tensor([67]), tensor([94]), tensor([81]), tensor([34]), tensor([53]), tensor([13]), tensor([45]), tensor([88]), tensor([21]), tensor([65]), tensor([77]), tensor([80]), tensor([27]), tensor([0]), tensor([10]), tensor([53]), tensor([36]), tensor([13]), tensor([11]), tensor([92]), tensor([57]), tensor([49]), tensor([94]), tensor([21]), tensor([45]), tensor([27]), tensor([81]), tensor([50]), tensor([91]), tensor([57]), tensor([62]), tensor([94]), tensor([16]), tensor([49]), tensor([24]), tensor([86]), tensor([49]), tensor([35]), tensor([98]), tensor([98]), tensor([86]), tensor([62]), tensor([77]), tensor([53]), tensor([58]), tensor([33]), tensor([34]), tensor([35]), tensor([33]), tensor([68]), tensor([60]), tensor([95]), tensor([36]), tensor([33]), tensor([69]), tensor([49]), tensor([78]), tensor([49]), tensor([37]), tensor([36]), tensor([27]), tensor([12]), tensor([16]), tensor([81]), tensor([96]), tensor([91]), tensor([35]), tensor([34]), tensor([33]), tensor([0]), tensor([36]), tensor([7]), tensor([30]), tensor([36]), tensor([78]), tensor([62]), tensor([67]), tensor([69]), tensor([62]), tensor([59]), tensor([59]), tensor([3]), tensor([60]), tensor([16])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.54 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.539\n",
            "TEST ALL:  0.5356\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  6000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 91, 69, 65, 57, 53, 49, 45, 41, 37, 33, 25, 21, 13, 96, 92, 88, 80, 72, 68, 60, 56, 52, 48, 36, 32, 24, 16, 12, 73, 77, 81, 94, 79, 71, 67, 63, 59, 35, 31, 27, 11, 7, 3, 98, 86, 97, 82, 78, 62, 58, 50, 46, 34, 30, 26, 14, 10, 2, 0]\n",
            "TRAIN_SET CLASSES:  [79, 71, 63, 26, 14, 2, 73, 41, 56, 48]\n",
            "VALIDATION CLASSES:  [63, 56, 48, 41, 26, 79, 14, 73, 71, 2]\n",
            "GROUP:  6\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [79, 71, 63, 26, 14, 2, 73, 41, 56, 48]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  60\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.2442663460969925\n",
            "Train step - Step 10, Loss 0.14584596455097198\n",
            "Train step - Step 20, Loss 0.14033186435699463\n",
            "Train step - Step 30, Loss 0.13283905386924744\n",
            "Train step - Step 40, Loss 0.1239665150642395\n",
            "Train step - Step 50, Loss 0.1202162504196167\n",
            "Train epoch - Accuracy: 0.17496402877697842 Loss: 0.14057387321758613 Corrects: 1216\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12514041364192963\n",
            "Train step - Step 70, Loss 0.12050531059503555\n",
            "Train step - Step 80, Loss 0.11861281096935272\n",
            "Train step - Step 90, Loss 0.11431744694709778\n",
            "Train step - Step 100, Loss 0.11040681600570679\n",
            "Train epoch - Accuracy: 0.2018705035971223 Loss: 0.11873220914344994 Corrects: 1403\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11832695454359055\n",
            "Train step - Step 120, Loss 0.11562176793813705\n",
            "Train step - Step 130, Loss 0.11715500056743622\n",
            "Train step - Step 140, Loss 0.11941944807767868\n",
            "Train step - Step 150, Loss 0.11917033791542053\n",
            "Train step - Step 160, Loss 0.1093059778213501\n",
            "Train epoch - Accuracy: 0.22848920863309352 Loss: 0.1161804647497136 Corrects: 1588\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.10862230509519577\n",
            "Train step - Step 180, Loss 0.11505290120840073\n",
            "Train step - Step 190, Loss 0.11433843523263931\n",
            "Train step - Step 200, Loss 0.11374836415052414\n",
            "Train step - Step 210, Loss 0.11280535906553268\n",
            "Train epoch - Accuracy: 0.24532374100719426 Loss: 0.11486285984944954 Corrects: 1705\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.1120816320180893\n",
            "Train step - Step 230, Loss 0.11645577102899551\n",
            "Train step - Step 240, Loss 0.11717190593481064\n",
            "Train step - Step 250, Loss 0.10720402747392654\n",
            "Train step - Step 260, Loss 0.10903088003396988\n",
            "Train step - Step 270, Loss 0.11226630955934525\n",
            "Train epoch - Accuracy: 0.2715107913669065 Loss: 0.11386414785393709 Corrects: 1887\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.11289524286985397\n",
            "Train step - Step 290, Loss 0.11178285628557205\n",
            "Train step - Step 300, Loss 0.12390243262052536\n",
            "Train step - Step 310, Loss 0.11132893711328506\n",
            "Train step - Step 320, Loss 0.10947875678539276\n",
            "Train epoch - Accuracy: 0.2840287769784173 Loss: 0.11294395544760519 Corrects: 1974\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.10851984471082687\n",
            "Train step - Step 340, Loss 0.11461995542049408\n",
            "Train step - Step 350, Loss 0.11471790075302124\n",
            "Train step - Step 360, Loss 0.11315198242664337\n",
            "Train step - Step 370, Loss 0.11139710992574692\n",
            "Train step - Step 380, Loss 0.11600710451602936\n",
            "Train epoch - Accuracy: 0.2997122302158273 Loss: 0.11242575993855222 Corrects: 2083\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.11447665840387344\n",
            "Train step - Step 400, Loss 0.10988873243331909\n",
            "Train step - Step 410, Loss 0.11441420763731003\n",
            "Train step - Step 420, Loss 0.108719602227211\n",
            "Train step - Step 430, Loss 0.1079876497387886\n",
            "Train epoch - Accuracy: 0.3185611510791367 Loss: 0.11175928070819635 Corrects: 2214\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10862608999013901\n",
            "Train step - Step 450, Loss 0.10980255156755447\n",
            "Train step - Step 460, Loss 0.10728000104427338\n",
            "Train step - Step 470, Loss 0.11303501576185226\n",
            "Train step - Step 480, Loss 0.11312711238861084\n",
            "Train step - Step 490, Loss 0.11707466095685959\n",
            "Train epoch - Accuracy: 0.33050359712230215 Loss: 0.11170547083127413 Corrects: 2297\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.11319267004728317\n",
            "Train step - Step 510, Loss 0.11730580776929855\n",
            "Train step - Step 520, Loss 0.11476722359657288\n",
            "Train step - Step 530, Loss 0.11145532876253128\n",
            "Train step - Step 540, Loss 0.11096905916929245\n",
            "Train epoch - Accuracy: 0.34014388489208636 Loss: 0.11099676878117828 Corrects: 2364\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.11501114070415497\n",
            "Train step - Step 560, Loss 0.11128076165914536\n",
            "Train step - Step 570, Loss 0.10295035690069199\n",
            "Train step - Step 580, Loss 0.11827278882265091\n",
            "Train step - Step 590, Loss 0.10658615827560425\n",
            "Train step - Step 600, Loss 0.11134421825408936\n",
            "Train epoch - Accuracy: 0.3493525179856115 Loss: 0.11101375310112246 Corrects: 2428\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.11258593946695328\n",
            "Train step - Step 620, Loss 0.11102600395679474\n",
            "Train step - Step 630, Loss 0.1056322455406189\n",
            "Train step - Step 640, Loss 0.10792068392038345\n",
            "Train step - Step 650, Loss 0.10730436444282532\n",
            "Train epoch - Accuracy: 0.36071942446043165 Loss: 0.11020796057774866 Corrects: 2507\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.11066558212041855\n",
            "Train step - Step 670, Loss 0.11288904398679733\n",
            "Train step - Step 680, Loss 0.10426728427410126\n",
            "Train step - Step 690, Loss 0.10744098573923111\n",
            "Train step - Step 700, Loss 0.10602428019046783\n",
            "Train step - Step 710, Loss 0.10712360590696335\n",
            "Train epoch - Accuracy: 0.3702158273381295 Loss: 0.110066921421521 Corrects: 2573\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10937181115150452\n",
            "Train step - Step 730, Loss 0.10786282271146774\n",
            "Train step - Step 740, Loss 0.10914939641952515\n",
            "Train step - Step 750, Loss 0.10800999402999878\n",
            "Train step - Step 760, Loss 0.11427739262580872\n",
            "Train epoch - Accuracy: 0.38 Loss: 0.10964772231930452 Corrects: 2641\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.11638180166482925\n",
            "Train step - Step 780, Loss 0.10777569562196732\n",
            "Train step - Step 790, Loss 0.11205922067165375\n",
            "Train step - Step 800, Loss 0.10944947600364685\n",
            "Train step - Step 810, Loss 0.10887950658798218\n",
            "Train step - Step 820, Loss 0.11191801726818085\n",
            "Train epoch - Accuracy: 0.381726618705036 Loss: 0.10984239157369668 Corrects: 2653\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.10552877932786942\n",
            "Train step - Step 840, Loss 0.11035443097352982\n",
            "Train step - Step 850, Loss 0.11135648936033249\n",
            "Train step - Step 860, Loss 0.11085517704486847\n",
            "Train step - Step 870, Loss 0.106376051902771\n",
            "Train epoch - Accuracy: 0.39611510791366905 Loss: 0.10980070103415482 Corrects: 2753\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.11063484102487564\n",
            "Train step - Step 890, Loss 0.10849412530660629\n",
            "Train step - Step 900, Loss 0.10343017429113388\n",
            "Train step - Step 910, Loss 0.11363963037729263\n",
            "Train step - Step 920, Loss 0.10777392983436584\n",
            "Train step - Step 930, Loss 0.10521790385246277\n",
            "Train epoch - Accuracy: 0.4015827338129496 Loss: 0.10909019065417831 Corrects: 2791\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.10410507768392563\n",
            "Train step - Step 950, Loss 0.1135193258523941\n",
            "Train step - Step 960, Loss 0.11086706072092056\n",
            "Train step - Step 970, Loss 0.1022353246808052\n",
            "Train step - Step 980, Loss 0.11069587618112564\n",
            "Train epoch - Accuracy: 0.4084892086330935 Loss: 0.109110687636643 Corrects: 2839\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.10590897500514984\n",
            "Train step - Step 1000, Loss 0.10960717499256134\n",
            "Train step - Step 1010, Loss 0.11054231971502304\n",
            "Train step - Step 1020, Loss 0.10458037257194519\n",
            "Train step - Step 1030, Loss 0.1078043207526207\n",
            "Train step - Step 1040, Loss 0.1055213063955307\n",
            "Train epoch - Accuracy: 0.4148201438848921 Loss: 0.10899393879681182 Corrects: 2883\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.10847651958465576\n",
            "Train step - Step 1060, Loss 0.10324785113334656\n",
            "Train step - Step 1070, Loss 0.11076927930116653\n",
            "Train step - Step 1080, Loss 0.1110614761710167\n",
            "Train step - Step 1090, Loss 0.10932018607854843\n",
            "Train epoch - Accuracy: 0.41640287769784173 Loss: 0.10916411236893359 Corrects: 2894\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10988724231719971\n",
            "Train step - Step 1110, Loss 0.10882183909416199\n",
            "Train step - Step 1120, Loss 0.10858450829982758\n",
            "Train step - Step 1130, Loss 0.10944074392318726\n",
            "Train step - Step 1140, Loss 0.10224276036024094\n",
            "Train step - Step 1150, Loss 0.11240540444850922\n",
            "Train epoch - Accuracy: 0.42805755395683454 Loss: 0.10879161368171088 Corrects: 2975\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10751443356275558\n",
            "Train step - Step 1170, Loss 0.10611221194267273\n",
            "Train step - Step 1180, Loss 0.107391357421875\n",
            "Train step - Step 1190, Loss 0.1083715483546257\n",
            "Train step - Step 1200, Loss 0.1022985652089119\n",
            "Train epoch - Accuracy: 0.42762589928057554 Loss: 0.10849828185366212 Corrects: 2972\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.10936697572469711\n",
            "Train step - Step 1220, Loss 0.10384456068277359\n",
            "Train step - Step 1230, Loss 0.1111157238483429\n",
            "Train step - Step 1240, Loss 0.11170855909585953\n",
            "Train step - Step 1250, Loss 0.10750347375869751\n",
            "Train step - Step 1260, Loss 0.10777806490659714\n",
            "Train epoch - Accuracy: 0.43841726618705035 Loss: 0.10846541840824292 Corrects: 3047\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.10915259271860123\n",
            "Train step - Step 1280, Loss 0.10558263212442398\n",
            "Train step - Step 1290, Loss 0.1049615889787674\n",
            "Train step - Step 1300, Loss 0.10334455221891403\n",
            "Train step - Step 1310, Loss 0.11393915116786957\n",
            "Train epoch - Accuracy: 0.44071942446043166 Loss: 0.10813819582299362 Corrects: 3063\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.10948723554611206\n",
            "Train step - Step 1330, Loss 0.10940002650022507\n",
            "Train step - Step 1340, Loss 0.10240117460489273\n",
            "Train step - Step 1350, Loss 0.10407703369855881\n",
            "Train step - Step 1360, Loss 0.10451992601156235\n",
            "Train step - Step 1370, Loss 0.10926217585802078\n",
            "Train epoch - Accuracy: 0.4474820143884892 Loss: 0.10784845602598121 Corrects: 3110\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.10692956298589706\n",
            "Train step - Step 1390, Loss 0.10991809517145157\n",
            "Train step - Step 1400, Loss 0.1044841855764389\n",
            "Train step - Step 1410, Loss 0.10726053267717361\n",
            "Train step - Step 1420, Loss 0.1067446917295456\n",
            "Train epoch - Accuracy: 0.4469064748201439 Loss: 0.10786004447465321 Corrects: 3106\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.10831305384635925\n",
            "Train step - Step 1440, Loss 0.10950174182653427\n",
            "Train step - Step 1450, Loss 0.1047799289226532\n",
            "Train step - Step 1460, Loss 0.10619644075632095\n",
            "Train step - Step 1470, Loss 0.10457178205251694\n",
            "Train step - Step 1480, Loss 0.10805624723434448\n",
            "Train epoch - Accuracy: 0.4486330935251799 Loss: 0.10789981774288973 Corrects: 3118\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.10641077905893326\n",
            "Train step - Step 1500, Loss 0.1042727679014206\n",
            "Train step - Step 1510, Loss 0.10809000581502914\n",
            "Train step - Step 1520, Loss 0.10471265017986298\n",
            "Train step - Step 1530, Loss 0.11293952167034149\n",
            "Train epoch - Accuracy: 0.4520863309352518 Loss: 0.10827980230394885 Corrects: 3142\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.10468224436044693\n",
            "Train step - Step 1550, Loss 0.10379858314990997\n",
            "Train step - Step 1560, Loss 0.10291475802659988\n",
            "Train step - Step 1570, Loss 0.10899843275547028\n",
            "Train step - Step 1580, Loss 0.10869476199150085\n",
            "Train step - Step 1590, Loss 0.11127480119466782\n",
            "Train epoch - Accuracy: 0.46345323741007194 Loss: 0.1076992332635166 Corrects: 3221\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.10699468106031418\n",
            "Train step - Step 1610, Loss 0.11144952476024628\n",
            "Train step - Step 1620, Loss 0.10988409072160721\n",
            "Train step - Step 1630, Loss 0.10252410918474197\n",
            "Train step - Step 1640, Loss 0.11075979471206665\n",
            "Train epoch - Accuracy: 0.4680575539568345 Loss: 0.10719969454000322 Corrects: 3253\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.10623514652252197\n",
            "Train step - Step 1660, Loss 0.10833185166120529\n",
            "Train step - Step 1670, Loss 0.10042989253997803\n",
            "Train step - Step 1680, Loss 0.10430184751749039\n",
            "Train step - Step 1690, Loss 0.10711782425642014\n",
            "Train step - Step 1700, Loss 0.10884369164705276\n",
            "Train epoch - Accuracy: 0.46489208633093526 Loss: 0.10747646863511998 Corrects: 3231\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.10769577324390411\n",
            "Train step - Step 1720, Loss 0.10585575550794601\n",
            "Train step - Step 1730, Loss 0.10965635627508163\n",
            "Train step - Step 1740, Loss 0.10399632900953293\n",
            "Train step - Step 1750, Loss 0.11032389849424362\n",
            "Train epoch - Accuracy: 0.46489208633093526 Loss: 0.107204982016584 Corrects: 3231\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.10365395992994308\n",
            "Train step - Step 1770, Loss 0.10599410533905029\n",
            "Train step - Step 1780, Loss 0.10530189424753189\n",
            "Train step - Step 1790, Loss 0.1059897318482399\n",
            "Train step - Step 1800, Loss 0.11096874624490738\n",
            "Train step - Step 1810, Loss 0.10916835069656372\n",
            "Train epoch - Accuracy: 0.4785611510791367 Loss: 0.107425279593725 Corrects: 3326\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.10638442635536194\n",
            "Train step - Step 1830, Loss 0.1139182448387146\n",
            "Train step - Step 1840, Loss 0.10573162138462067\n",
            "Train step - Step 1850, Loss 0.10492657870054245\n",
            "Train step - Step 1860, Loss 0.10962823778390884\n",
            "Train epoch - Accuracy: 0.48115107913669064 Loss: 0.10707856850872795 Corrects: 3344\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.10702270269393921\n",
            "Train step - Step 1880, Loss 0.10332884639501572\n",
            "Train step - Step 1890, Loss 0.10882677882909775\n",
            "Train step - Step 1900, Loss 0.10891064256429672\n",
            "Train step - Step 1910, Loss 0.10617412626743317\n",
            "Train step - Step 1920, Loss 0.10459358990192413\n",
            "Train epoch - Accuracy: 0.4823021582733813 Loss: 0.10680996696726024 Corrects: 3352\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.09944601356983185\n",
            "Train step - Step 1940, Loss 0.11007899045944214\n",
            "Train step - Step 1950, Loss 0.10211805254220963\n",
            "Train step - Step 1960, Loss 0.10990987718105316\n",
            "Train step - Step 1970, Loss 0.10205283015966415\n",
            "Train epoch - Accuracy: 0.48848920863309353 Loss: 0.10632445169224156 Corrects: 3395\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.11198601871728897\n",
            "Train step - Step 1990, Loss 0.10509084910154343\n",
            "Train step - Step 2000, Loss 0.1079331636428833\n",
            "Train step - Step 2010, Loss 0.1025017648935318\n",
            "Train step - Step 2020, Loss 0.10418720543384552\n",
            "Train step - Step 2030, Loss 0.10260757803916931\n",
            "Train epoch - Accuracy: 0.49237410071942445 Loss: 0.1065158180624461 Corrects: 3422\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.10869959741830826\n",
            "Train step - Step 2050, Loss 0.10322649031877518\n",
            "Train step - Step 2060, Loss 0.10668233036994934\n",
            "Train step - Step 2070, Loss 0.10223214328289032\n",
            "Train step - Step 2080, Loss 0.10680133104324341\n",
            "Train epoch - Accuracy: 0.4956834532374101 Loss: 0.1065323098572038 Corrects: 3445\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.10482995212078094\n",
            "Train step - Step 2100, Loss 0.09853992611169815\n",
            "Train step - Step 2110, Loss 0.10484384745359421\n",
            "Train step - Step 2120, Loss 0.10426056385040283\n",
            "Train step - Step 2130, Loss 0.1032155230641365\n",
            "Train step - Step 2140, Loss 0.11053260415792465\n",
            "Train epoch - Accuracy: 0.501726618705036 Loss: 0.1065673346635249 Corrects: 3487\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.10909273475408554\n",
            "Train step - Step 2160, Loss 0.10038762539625168\n",
            "Train step - Step 2170, Loss 0.10319071263074875\n",
            "Train step - Step 2180, Loss 0.10952822864055634\n",
            "Train step - Step 2190, Loss 0.10421600937843323\n",
            "Train epoch - Accuracy: 0.49712230215827335 Loss: 0.1060817199580961 Corrects: 3455\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.10251683741807938\n",
            "Train step - Step 2210, Loss 0.10549671202898026\n",
            "Train step - Step 2220, Loss 0.10306166857481003\n",
            "Train step - Step 2230, Loss 0.10512357205152512\n",
            "Train step - Step 2240, Loss 0.10781046003103256\n",
            "Train step - Step 2250, Loss 0.10522529482841492\n",
            "Train epoch - Accuracy: 0.5074820143884892 Loss: 0.10642759599059606 Corrects: 3527\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.10458233207464218\n",
            "Train step - Step 2270, Loss 0.10588622093200684\n",
            "Train step - Step 2280, Loss 0.10347797721624374\n",
            "Train step - Step 2290, Loss 0.10429985076189041\n",
            "Train step - Step 2300, Loss 0.1059543788433075\n",
            "Train epoch - Accuracy: 0.5044604316546762 Loss: 0.10611717989976457 Corrects: 3506\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.1110401526093483\n",
            "Train step - Step 2320, Loss 0.1175244078040123\n",
            "Train step - Step 2330, Loss 0.10994697362184525\n",
            "Train step - Step 2340, Loss 0.10375949740409851\n",
            "Train step - Step 2350, Loss 0.11188794672489166\n",
            "Train step - Step 2360, Loss 0.1090795248746872\n",
            "Train epoch - Accuracy: 0.5100719424460431 Loss: 0.10610116195978878 Corrects: 3545\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.1100773960351944\n",
            "Train step - Step 2380, Loss 0.10836124420166016\n",
            "Train step - Step 2390, Loss 0.10175775736570358\n",
            "Train step - Step 2400, Loss 0.10418502986431122\n",
            "Train step - Step 2410, Loss 0.0995899960398674\n",
            "Train epoch - Accuracy: 0.5109352517985611 Loss: 0.10594084232402362 Corrects: 3551\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.1026524007320404\n",
            "Train step - Step 2430, Loss 0.10780471563339233\n",
            "Train step - Step 2440, Loss 0.10661326348781586\n",
            "Train step - Step 2450, Loss 0.10612910985946655\n",
            "Train step - Step 2460, Loss 0.10469069331884384\n",
            "Train step - Step 2470, Loss 0.1019456759095192\n",
            "Train epoch - Accuracy: 0.518273381294964 Loss: 0.10593980796688753 Corrects: 3602\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.10253409296274185\n",
            "Train step - Step 2490, Loss 0.10301919281482697\n",
            "Train step - Step 2500, Loss 0.10767341405153275\n",
            "Train step - Step 2510, Loss 0.10273522138595581\n",
            "Train step - Step 2520, Loss 0.10482942312955856\n",
            "Train epoch - Accuracy: 0.5179856115107914 Loss: 0.105993977105446 Corrects: 3600\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.10983860492706299\n",
            "Train step - Step 2540, Loss 0.10833217948675156\n",
            "Train step - Step 2550, Loss 0.10909033566713333\n",
            "Train step - Step 2560, Loss 0.11145397275686264\n",
            "Train step - Step 2570, Loss 0.10487928986549377\n",
            "Train step - Step 2580, Loss 0.10566049069166183\n",
            "Train epoch - Accuracy: 0.5202877697841727 Loss: 0.10564585519565953 Corrects: 3616\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.10907354950904846\n",
            "Train step - Step 2600, Loss 0.1056540235877037\n",
            "Train step - Step 2610, Loss 0.1073165163397789\n",
            "Train step - Step 2620, Loss 0.11185795068740845\n",
            "Train step - Step 2630, Loss 0.10634885728359222\n",
            "Train epoch - Accuracy: 0.5237410071942447 Loss: 0.10567927594021928 Corrects: 3640\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.11010042577981949\n",
            "Train step - Step 2650, Loss 0.10321551561355591\n",
            "Train step - Step 2660, Loss 0.10750718414783478\n",
            "Train step - Step 2670, Loss 0.10645417124032974\n",
            "Train step - Step 2680, Loss 0.10634351521730423\n",
            "Train step - Step 2690, Loss 0.10164505988359451\n",
            "Train epoch - Accuracy: 0.5224460431654676 Loss: 0.10577943728553306 Corrects: 3631\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.10289950668811798\n",
            "Train step - Step 2710, Loss 0.1066782996058464\n",
            "Train step - Step 2720, Loss 0.1032189279794693\n",
            "Train step - Step 2730, Loss 0.102934829890728\n",
            "Train step - Step 2740, Loss 0.10535632818937302\n",
            "Train epoch - Accuracy: 0.5384172661870503 Loss: 0.10453306268445021 Corrects: 3742\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.09703701734542847\n",
            "Train step - Step 2760, Loss 0.10150238871574402\n",
            "Train step - Step 2770, Loss 0.09918868541717529\n",
            "Train step - Step 2780, Loss 0.10595586895942688\n",
            "Train step - Step 2790, Loss 0.1032341718673706\n",
            "Train step - Step 2800, Loss 0.10154412686824799\n",
            "Train epoch - Accuracy: 0.5424460431654676 Loss: 0.10425035757555379 Corrects: 3770\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.10567530244588852\n",
            "Train step - Step 2820, Loss 0.10129272937774658\n",
            "Train step - Step 2830, Loss 0.10302989184856415\n",
            "Train step - Step 2840, Loss 0.10230647772550583\n",
            "Train step - Step 2850, Loss 0.10765980929136276\n",
            "Train epoch - Accuracy: 0.5394244604316547 Loss: 0.10445779107671847 Corrects: 3749\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.09994404762983322\n",
            "Train step - Step 2870, Loss 0.10503305494785309\n",
            "Train step - Step 2880, Loss 0.10360874235630035\n",
            "Train step - Step 2890, Loss 0.1114845871925354\n",
            "Train step - Step 2900, Loss 0.10729218274354935\n",
            "Train step - Step 2910, Loss 0.10716747492551804\n",
            "Train epoch - Accuracy: 0.5361151079136691 Loss: 0.10437026945163878 Corrects: 3726\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.10507721453905106\n",
            "Train step - Step 2930, Loss 0.100873202085495\n",
            "Train step - Step 2940, Loss 0.10304510593414307\n",
            "Train step - Step 2950, Loss 0.10270886868238449\n",
            "Train step - Step 2960, Loss 0.1004813015460968\n",
            "Train epoch - Accuracy: 0.5351079136690647 Loss: 0.10403672170081585 Corrects: 3719\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.10419385880231857\n",
            "Train step - Step 2980, Loss 0.10447675734758377\n",
            "Train step - Step 2990, Loss 0.10842657834291458\n",
            "Train step - Step 3000, Loss 0.1082792654633522\n",
            "Train step - Step 3010, Loss 0.09951188415288925\n",
            "Train step - Step 3020, Loss 0.10397721081972122\n",
            "Train epoch - Accuracy: 0.5428776978417266 Loss: 0.10415329255860487 Corrects: 3773\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.09942469745874405\n",
            "Train step - Step 3040, Loss 0.10457368195056915\n",
            "Train step - Step 3050, Loss 0.10910530388355255\n",
            "Train step - Step 3060, Loss 0.10535210371017456\n",
            "Train step - Step 3070, Loss 0.10681068897247314\n",
            "Train epoch - Accuracy: 0.5361151079136691 Loss: 0.10449453427422818 Corrects: 3726\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.10506831854581833\n",
            "Train step - Step 3090, Loss 0.10500160604715347\n",
            "Train step - Step 3100, Loss 0.10736943036317825\n",
            "Train step - Step 3110, Loss 0.10472376644611359\n",
            "Train step - Step 3120, Loss 0.10191050916910172\n",
            "Train step - Step 3130, Loss 0.09908831119537354\n",
            "Train epoch - Accuracy: 0.5443165467625899 Loss: 0.10439054412378682 Corrects: 3783\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.10016041249036789\n",
            "Train step - Step 3150, Loss 0.10834866017103195\n",
            "Train step - Step 3160, Loss 0.101277656853199\n",
            "Train step - Step 3170, Loss 0.10087389498949051\n",
            "Train step - Step 3180, Loss 0.10065266489982605\n",
            "Train epoch - Accuracy: 0.5365467625899281 Loss: 0.10424083875023203 Corrects: 3729\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.10461262613534927\n",
            "Train step - Step 3200, Loss 0.1012769564986229\n",
            "Train step - Step 3210, Loss 0.10552255809307098\n",
            "Train step - Step 3220, Loss 0.10599397867918015\n",
            "Train step - Step 3230, Loss 0.1055767610669136\n",
            "Train step - Step 3240, Loss 0.10167320817708969\n",
            "Train epoch - Accuracy: 0.539568345323741 Loss: 0.10405543103492518 Corrects: 3750\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.10442639142274857\n",
            "Train step - Step 3260, Loss 0.1076677069067955\n",
            "Train step - Step 3270, Loss 0.1032116487622261\n",
            "Train step - Step 3280, Loss 0.1076711043715477\n",
            "Train step - Step 3290, Loss 0.10469046235084534\n",
            "Train epoch - Accuracy: 0.537410071942446 Loss: 0.10442194054667041 Corrects: 3735\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.10465382784605026\n",
            "Train step - Step 3310, Loss 0.10854324698448181\n",
            "Train step - Step 3320, Loss 0.10439377278089523\n",
            "Train step - Step 3330, Loss 0.11102534830570221\n",
            "Train step - Step 3340, Loss 0.10497969388961792\n",
            "Train step - Step 3350, Loss 0.10459490120410919\n",
            "Train epoch - Accuracy: 0.5362589928057554 Loss: 0.10454979485101837 Corrects: 3727\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.09972986578941345\n",
            "Train step - Step 3370, Loss 0.106480173766613\n",
            "Train step - Step 3380, Loss 0.10329771041870117\n",
            "Train step - Step 3390, Loss 0.10327710956335068\n",
            "Train step - Step 3400, Loss 0.10726167261600494\n",
            "Train epoch - Accuracy: 0.5407194244604316 Loss: 0.10429192688182104 Corrects: 3758\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.10106861591339111\n",
            "Train step - Step 3420, Loss 0.10791169106960297\n",
            "Train step - Step 3430, Loss 0.1025119498372078\n",
            "Train step - Step 3440, Loss 0.1017877385020256\n",
            "Train step - Step 3450, Loss 0.1052020937204361\n",
            "Train step - Step 3460, Loss 0.10427732020616531\n",
            "Train epoch - Accuracy: 0.5410071942446043 Loss: 0.10426930253239844 Corrects: 3760\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.1023046150803566\n",
            "Train step - Step 3480, Loss 0.09996762126684189\n",
            "Train step - Step 3490, Loss 0.10374172776937485\n",
            "Train step - Step 3500, Loss 0.10735546052455902\n",
            "Train step - Step 3510, Loss 0.10694684088230133\n",
            "Train epoch - Accuracy: 0.5458992805755396 Loss: 0.10404540001059608 Corrects: 3794\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.10264456272125244\n",
            "Train step - Step 3530, Loss 0.10061927139759064\n",
            "Train step - Step 3540, Loss 0.10020523518323898\n",
            "Train step - Step 3550, Loss 0.10114647448062897\n",
            "Train step - Step 3560, Loss 0.10111887007951736\n",
            "Train step - Step 3570, Loss 0.10169950127601624\n",
            "Train epoch - Accuracy: 0.5420143884892087 Loss: 0.10415723025155582 Corrects: 3767\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.10668715089559555\n",
            "Train step - Step 3590, Loss 0.10134349763393402\n",
            "Train step - Step 3600, Loss 0.10726932436227798\n",
            "Train step - Step 3610, Loss 0.1044187843799591\n",
            "Train step - Step 3620, Loss 0.09563486278057098\n",
            "Train epoch - Accuracy: 0.537841726618705 Loss: 0.10382699017902072 Corrects: 3738\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.09867283701896667\n",
            "Train step - Step 3640, Loss 0.10767408460378647\n",
            "Train step - Step 3650, Loss 0.10714616626501083\n",
            "Train step - Step 3660, Loss 0.09764159470796585\n",
            "Train step - Step 3670, Loss 0.10620222985744476\n",
            "Train step - Step 3680, Loss 0.10028119385242462\n",
            "Train epoch - Accuracy: 0.5389928057553957 Loss: 0.10393921988259117 Corrects: 3746\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.1080852746963501\n",
            "Train step - Step 3700, Loss 0.0963902398943901\n",
            "Train step - Step 3710, Loss 0.10441242158412933\n",
            "Train step - Step 3720, Loss 0.10664631426334381\n",
            "Train step - Step 3730, Loss 0.10232873260974884\n",
            "Train epoch - Accuracy: 0.5418705035971223 Loss: 0.103852283073415 Corrects: 3766\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.10500041395425797\n",
            "Train step - Step 3750, Loss 0.10481728613376617\n",
            "Train step - Step 3760, Loss 0.09624437987804413\n",
            "Train step - Step 3770, Loss 0.10794465243816376\n",
            "Train step - Step 3780, Loss 0.10786116868257523\n",
            "Train step - Step 3790, Loss 0.09883418679237366\n",
            "Train epoch - Accuracy: 0.5548201438848921 Loss: 0.10389299639266172 Corrects: 3856\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.10708127915859222\n",
            "Train step - Step 3810, Loss 0.10218454152345657\n",
            "Train step - Step 3820, Loss 0.09390666335821152\n",
            "Train step - Step 3830, Loss 0.1038324385881424\n",
            "Train step - Step 3840, Loss 0.10126802325248718\n",
            "Train epoch - Accuracy: 0.543884892086331 Loss: 0.10390674102006199 Corrects: 3780\n",
            "Training finished in 396.42685079574585 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0, 79, 71, 63, 26, 14, 2, 73, 41, 56, 48]\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238abb890>\n",
            "Constructing exemplars of class 79\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [23707, 12596, 44563, 32829, 4576, 10213, 4225, 19875, 47658, 21411, 14467, 44576, 42255, 14468, 2957, 34523, 6720, 43134, 4341, 640, 4827, 3813, 23184, 34274, 37348, 3269, 24547, 29058, 15651, 22421, 23272, 46969, 29585]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22318d2690>\n",
            "Constructing exemplars of class 71\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [22815, 30157, 1490, 28537, 18474, 11552, 47861, 20117, 3266, 44903, 8651, 20100, 27048, 16023, 42745, 20102, 16822, 49339, 23126, 18420, 47656, 28664, 1069, 19500, 32559, 49339, 45036, 5858, 23016, 37450, 16822, 19406, 9672]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2231568210>\n",
            "Constructing exemplars of class 63\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [41334, 13280, 8717, 4373, 23379, 44665, 10339, 40687, 44657, 45083, 15217, 2089, 46909, 24316, 19815, 40112, 45813, 35155, 9626, 5685, 3027, 9276, 43265, 26448, 21204, 21787, 29802, 24112, 13430, 24060, 39337, 32414, 31700]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224468b510>\n",
            "Constructing exemplars of class 26\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [46538, 29921, 39061, 11743, 31990, 14243, 34956, 47993, 38835, 9998, 7716, 8820, 31550, 42987, 842, 31508, 14547, 33088, 8917, 44882, 29572, 3328, 33567, 5200, 26159, 26285, 36635, 9616, 40415, 10702, 13348, 1530, 5769]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e8f490>\n",
            "Constructing exemplars of class 14\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [621, 17296, 7478, 22266, 48283, 37639, 46164, 2183, 3395, 13340, 12672, 44069, 1935, 33449, 7357, 28863, 16853, 14120, 45748, 22593, 44242, 36486, 48774, 42961, 16988, 27878, 41080, 14920, 30588, 7674, 19122, 37220, 22273]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e9b490>\n",
            "Constructing exemplars of class 2\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [1106, 15946, 31638, 34814, 3878, 18029, 11054, 19819, 37214, 30134, 37960, 2925, 23185, 1511, 25496, 33194, 1895, 46991, 382, 22417, 11903, 13956, 23678, 2352, 37758, 42302, 44202, 31151, 13017, 16964, 41550, 7066, 2051]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f224468b510>\n",
            "Constructing exemplars of class 73\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [22908, 1063, 11323, 17274, 20665, 6651, 47075, 31895, 1423, 7541, 21222, 48102, 2117, 25797, 11532, 28792, 18133, 24665, 45890, 31433, 48102, 18418, 45541, 44211, 8563, 4571, 20054, 7036, 17803, 23605, 15253, 14219, 12658]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238219550>\n",
            "Constructing exemplars of class 41\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [14791, 22570, 41474, 37036, 45574, 36671, 43099, 24615, 17358, 26081, 48727, 45619, 34706, 19253, 26367, 2050, 40716, 42178, 41641, 26608, 6846, 1307, 21843, 33958, 27020, 7713, 998, 40619, 11246, 42081, 1307, 13530, 15782]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223824dc50>\n",
            "Constructing exemplars of class 56\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [12244, 38779, 12972, 26410, 4314, 38889, 15000, 24599, 41268, 29639, 31995, 30262, 42345, 13025, 16420, 21570, 14423, 29524, 4493, 39599, 5433, 27676, 37910, 47360, 9176, 41657, 23447, 39721, 28791, 20038, 15456, 15636, 17822]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2244394210>\n",
            "Constructing exemplars of class 48\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [4654, 45191, 25645, 19743, 20177, 7719, 46288, 41294, 36150, 49492, 195, 11625, 23587, 36957, 9328, 46996, 24182, 2525, 16574, 22581, 6687, 47302, 41800, 33870, 23744, 14306, 48760, 13009, 251, 195, 21067, 152, 4654]\n",
            "x train:  [ 0.03226107 -0.14881769 -0.12394965 -0.08490223 -0.04953309 -0.13368255\n",
            " -0.05037773 -0.12670901 -0.15285337 -0.04527627 -0.19681294  0.03760378\n",
            " -0.08676144 -0.16244866 -0.07754248 -0.14214084 -0.15155344 -0.0814168\n",
            " -0.11174899 -0.09487329 -0.12625481 -0.08111097 -0.16640784 -0.1475813\n",
            " -0.08610184 -0.07939965 -0.11572324 -0.07093013 -0.11739716 -0.17098913\n",
            " -0.11272877 -0.10268337 -0.1794504  -0.13986552 -0.2316634  -0.02480628\n",
            " -0.00627354 -0.17499778 -0.03010319 -0.12563132 -0.161612   -0.09257635\n",
            " -0.12037809 -0.15672836 -0.14632572 -0.10671978 -0.13955654 -0.09691554\n",
            " -0.18812704 -0.16803642 -0.12820397 -0.10843623 -0.03351827 -0.14154449\n",
            " -0.1833487  -0.18828349 -0.14845298 -0.15053593 -0.08673934 -0.21225375]\n",
            "y_train:  [tensor([33]), tensor([92]), tensor([16]), tensor([88]), tensor([88]), tensor([57]), tensor([27]), tensor([78]), tensor([62]), tensor([58]), tensor([91]), tensor([56]), tensor([46]), tensor([95]), tensor([86]), tensor([63]), tensor([10]), tensor([60]), tensor([98]), tensor([37]), tensor([30]), tensor([88]), tensor([3]), tensor([24]), tensor([60]), tensor([81]), tensor([80]), tensor([67]), tensor([73]), tensor([53]), tensor([57]), tensor([58]), tensor([52]), tensor([60]), tensor([91]), tensor([7]), tensor([67]), tensor([12]), tensor([88]), tensor([65]), tensor([7]), tensor([92]), tensor([63]), tensor([58]), tensor([65]), tensor([31]), tensor([57]), tensor([48]), tensor([98]), tensor([33]), tensor([96]), tensor([31]), tensor([14]), tensor([48]), tensor([98]), tensor([62]), tensor([50]), tensor([59]), tensor([36]), tensor([41]), tensor([50]), tensor([53]), tensor([92]), tensor([96]), tensor([21]), tensor([35]), tensor([10]), tensor([49]), tensor([77]), tensor([79]), tensor([16]), tensor([49]), tensor([68]), tensor([94]), tensor([30]), tensor([67]), tensor([26]), tensor([67]), tensor([33]), tensor([34]), tensor([62]), tensor([21]), tensor([35]), tensor([33]), tensor([31]), tensor([81]), tensor([97]), tensor([73]), tensor([36]), tensor([71]), tensor([80]), tensor([46]), tensor([56]), tensor([26]), tensor([26]), tensor([27]), tensor([68]), tensor([68]), tensor([46]), tensor([48]), tensor([62]), tensor([41]), tensor([21]), tensor([71]), tensor([10]), tensor([71]), tensor([34]), tensor([81]), tensor([11]), tensor([32]), tensor([46]), tensor([60]), tensor([11]), tensor([94]), tensor([11]), tensor([72]), tensor([82]), tensor([88]), tensor([48]), tensor([65]), tensor([68]), tensor([34]), tensor([58]), tensor([58]), tensor([24]), tensor([50]), tensor([97]), tensor([41]), tensor([31]), tensor([60]), tensor([78]), tensor([36]), tensor([26]), tensor([14]), tensor([60]), tensor([14]), tensor([11]), tensor([86]), tensor([80]), tensor([16]), tensor([30]), tensor([86]), tensor([67]), tensor([14]), tensor([16]), tensor([25]), tensor([77]), tensor([56]), tensor([80]), tensor([72]), tensor([16]), tensor([71]), tensor([2]), tensor([73]), tensor([37]), tensor([46]), tensor([73]), tensor([58]), tensor([79]), tensor([37]), tensor([82]), tensor([77]), tensor([73]), tensor([34]), tensor([69]), tensor([77]), tensor([36]), tensor([91]), tensor([16]), tensor([48]), tensor([88]), tensor([98]), tensor([78]), tensor([80]), tensor([33]), tensor([98]), tensor([65]), tensor([80]), tensor([31]), tensor([37]), tensor([49]), tensor([97]), tensor([59]), tensor([80]), tensor([50]), tensor([14]), tensor([41]), tensor([50]), tensor([60]), tensor([45]), tensor([36]), tensor([7]), tensor([11]), tensor([45]), tensor([53]), tensor([58]), tensor([91]), tensor([95]), tensor([36]), tensor([24]), tensor([50]), tensor([88]), tensor([60]), tensor([12]), tensor([73]), tensor([81]), tensor([58]), tensor([67]), tensor([63]), tensor([91]), tensor([81]), tensor([96]), tensor([73]), tensor([3]), tensor([33]), tensor([95]), tensor([96]), tensor([36]), tensor([13]), tensor([57]), tensor([36]), tensor([68]), tensor([72]), tensor([53]), tensor([35]), tensor([60]), tensor([26]), tensor([92]), tensor([16]), tensor([96]), tensor([82]), tensor([36]), tensor([11]), tensor([57]), tensor([26]), tensor([73]), tensor([92]), tensor([56]), tensor([45]), tensor([56]), tensor([41]), tensor([65]), tensor([41]), tensor([96]), tensor([26]), tensor([35]), tensor([41]), tensor([60]), tensor([80]), tensor([80]), tensor([0]), tensor([32]), tensor([41]), tensor([32]), tensor([77]), tensor([86]), tensor([62]), tensor([63]), tensor([41]), tensor([35]), tensor([88]), tensor([69]), tensor([94]), tensor([86]), tensor([34]), tensor([62]), tensor([14]), tensor([7]), tensor([96]), tensor([96]), tensor([96]), tensor([27]), tensor([53]), tensor([2]), tensor([2]), tensor([65]), tensor([91]), tensor([14]), tensor([32]), tensor([34]), tensor([31]), tensor([7]), tensor([10]), tensor([71]), tensor([58]), tensor([52]), tensor([12]), tensor([80]), tensor([14]), tensor([56]), tensor([86]), tensor([62]), tensor([16]), tensor([81]), tensor([33]), tensor([33]), tensor([11]), tensor([50]), tensor([73]), tensor([2]), tensor([25]), tensor([14]), tensor([0]), tensor([59]), tensor([79]), tensor([33]), tensor([37]), tensor([48]), tensor([81]), tensor([13]), tensor([14]), tensor([12]), tensor([79]), tensor([53]), tensor([88]), tensor([36]), tensor([71]), tensor([82]), tensor([92]), tensor([0]), tensor([57]), tensor([67]), tensor([53]), tensor([35]), tensor([52]), tensor([63]), tensor([60]), tensor([13]), tensor([98]), tensor([30]), tensor([97]), tensor([41]), tensor([31]), tensor([52]), tensor([91]), tensor([63]), tensor([62]), tensor([49]), tensor([62]), tensor([73]), tensor([53]), tensor([27]), tensor([77]), tensor([12]), tensor([27]), tensor([41]), tensor([7]), tensor([36]), tensor([45]), tensor([36]), tensor([45]), tensor([97]), tensor([49]), tensor([36]), tensor([78]), tensor([35]), tensor([49]), tensor([77]), tensor([68]), tensor([14]), tensor([78]), tensor([30]), tensor([80]), tensor([69]), tensor([82]), tensor([50]), tensor([97]), tensor([30]), tensor([95]), tensor([86]), tensor([69]), tensor([36]), tensor([21]), tensor([13]), tensor([53]), tensor([13]), tensor([2]), tensor([16]), tensor([25]), tensor([48]), tensor([78]), tensor([65]), tensor([92]), tensor([97]), tensor([86]), tensor([65]), tensor([21]), tensor([72]), tensor([88]), tensor([94]), tensor([94]), tensor([25]), tensor([91]), tensor([96]), tensor([82]), tensor([73]), tensor([12]), tensor([95]), tensor([58]), tensor([94]), tensor([50]), tensor([50]), tensor([16]), tensor([63]), tensor([96]), tensor([78]), tensor([69]), tensor([26]), tensor([3]), tensor([11]), tensor([7]), tensor([0]), tensor([71]), tensor([78]), tensor([56]), tensor([34]), tensor([59]), tensor([48]), tensor([45]), tensor([30]), tensor([24]), tensor([79]), tensor([86]), tensor([21]), tensor([72]), tensor([30]), tensor([26]), tensor([52]), tensor([7]), tensor([26]), tensor([72]), tensor([62]), tensor([59]), tensor([62]), tensor([45]), tensor([72]), tensor([2]), tensor([81]), tensor([13]), tensor([94]), tensor([3]), tensor([73]), tensor([86]), tensor([37]), tensor([95]), tensor([25]), tensor([45]), tensor([14]), tensor([41]), tensor([80]), tensor([56]), tensor([16]), tensor([2]), tensor([46]), tensor([77]), tensor([35]), tensor([13]), tensor([58]), tensor([69]), tensor([80]), tensor([52]), tensor([7]), tensor([58]), tensor([32]), tensor([77]), tensor([49]), tensor([48]), tensor([34]), tensor([59]), tensor([21]), tensor([41]), tensor([37]), tensor([68]), tensor([50]), tensor([81]), tensor([48]), tensor([82]), tensor([86]), tensor([68]), tensor([24]), tensor([77]), tensor([63]), tensor([78]), tensor([30]), tensor([25]), tensor([7]), tensor([88]), tensor([91]), tensor([24]), tensor([10]), tensor([48]), tensor([33]), tensor([21]), tensor([27]), tensor([10]), tensor([50]), tensor([78]), tensor([96]), tensor([56]), tensor([3]), tensor([3]), tensor([91]), tensor([97]), tensor([96]), tensor([67]), tensor([79]), tensor([27]), tensor([73]), tensor([72]), tensor([80]), tensor([3]), tensor([91]), tensor([72]), tensor([96]), tensor([95]), tensor([27]), tensor([94]), tensor([10]), tensor([72]), tensor([53]), tensor([34]), tensor([98]), tensor([65]), tensor([34]), tensor([57]), tensor([65]), tensor([7]), tensor([32]), tensor([98]), tensor([12]), tensor([7]), tensor([59]), tensor([27]), tensor([45]), tensor([35]), tensor([11]), tensor([92]), tensor([86]), tensor([25]), tensor([63]), tensor([30]), tensor([73]), tensor([48]), tensor([32]), tensor([46]), tensor([56]), tensor([81]), tensor([79]), tensor([13]), tensor([95]), tensor([68]), tensor([49]), tensor([94]), tensor([65]), tensor([68]), tensor([95]), tensor([3]), tensor([45]), tensor([16]), tensor([50]), tensor([80]), tensor([60]), tensor([94]), tensor([58]), tensor([67]), tensor([77]), tensor([35]), tensor([79]), tensor([25]), tensor([12]), tensor([49]), tensor([16]), tensor([71]), tensor([45]), tensor([30]), tensor([60]), tensor([34]), tensor([62]), tensor([45]), tensor([45]), tensor([80]), tensor([35]), tensor([94]), tensor([27]), tensor([27]), tensor([81]), tensor([79]), tensor([77]), tensor([45]), tensor([21]), tensor([41]), tensor([73]), tensor([33]), tensor([63]), tensor([71]), tensor([41]), tensor([25]), tensor([3]), tensor([77]), tensor([59]), tensor([98]), tensor([14]), tensor([96]), tensor([71]), tensor([71]), tensor([80]), tensor([7]), tensor([35]), tensor([25]), tensor([58]), tensor([67]), tensor([78]), tensor([31]), tensor([7]), tensor([97]), tensor([24]), tensor([86]), tensor([12]), tensor([57]), tensor([96]), tensor([37]), tensor([73]), tensor([2]), tensor([95]), tensor([3]), tensor([24]), tensor([77]), tensor([24]), tensor([46]), tensor([21]), tensor([16]), tensor([11]), tensor([59]), tensor([46]), tensor([73]), tensor([67]), tensor([82]), tensor([11]), tensor([41]), tensor([86]), tensor([35]), tensor([3]), tensor([95]), tensor([52]), tensor([63]), tensor([77]), tensor([10]), tensor([48]), tensor([31]), tensor([72]), tensor([13]), tensor([2]), tensor([91]), tensor([71]), tensor([77]), tensor([78]), tensor([52]), tensor([11]), tensor([73]), tensor([30]), tensor([62]), tensor([34]), tensor([13]), tensor([0]), tensor([35]), tensor([26]), tensor([71]), tensor([82]), tensor([95]), tensor([37]), tensor([79]), tensor([11]), tensor([41]), tensor([31]), tensor([3]), tensor([48]), tensor([59]), tensor([98]), tensor([2]), tensor([13]), tensor([69]), tensor([16]), tensor([65]), tensor([97]), tensor([58]), tensor([32]), tensor([71]), tensor([92]), tensor([10]), tensor([10]), tensor([21]), tensor([53]), tensor([59]), tensor([58]), tensor([81]), tensor([78]), tensor([7]), tensor([98]), tensor([69]), tensor([32]), tensor([27]), tensor([30]), tensor([73]), tensor([69]), tensor([30]), tensor([96]), tensor([92]), tensor([21]), tensor([35]), tensor([13]), tensor([91]), tensor([73]), tensor([80]), tensor([50]), tensor([25]), tensor([49]), tensor([7]), tensor([12]), tensor([50]), tensor([37]), tensor([2]), tensor([53]), tensor([96]), tensor([37]), tensor([52]), tensor([21]), tensor([11]), tensor([49]), tensor([37]), tensor([11]), tensor([98]), tensor([77]), tensor([32]), tensor([33]), tensor([72]), tensor([62]), tensor([96]), tensor([2]), tensor([46]), tensor([68]), tensor([63]), tensor([59]), tensor([21]), tensor([10]), tensor([12]), tensor([46]), tensor([12]), tensor([97]), tensor([34]), tensor([14]), tensor([81]), tensor([82]), tensor([73]), tensor([68]), tensor([53]), tensor([92]), tensor([79]), tensor([72]), tensor([50]), tensor([67]), tensor([11]), tensor([0]), tensor([7]), tensor([26]), tensor([45]), tensor([82]), tensor([25]), tensor([48]), tensor([10]), tensor([81]), tensor([59]), tensor([46]), tensor([13]), tensor([63]), tensor([33]), tensor([13]), tensor([31]), tensor([24]), tensor([56]), tensor([86]), tensor([97]), tensor([81]), tensor([24]), tensor([50]), tensor([63]), tensor([49]), tensor([91]), tensor([46]), tensor([46]), tensor([79]), tensor([36]), tensor([62]), tensor([80]), tensor([94]), tensor([65]), tensor([67]), tensor([26]), tensor([86]), tensor([79]), tensor([25]), tensor([56]), tensor([37]), tensor([52]), tensor([10]), tensor([82]), tensor([80]), tensor([33]), tensor([14]), tensor([96]), tensor([72]), tensor([79]), tensor([21]), tensor([0]), tensor([72]), tensor([36]), tensor([24]), tensor([79]), tensor([2]), tensor([48]), tensor([14]), tensor([81]), tensor([21]), tensor([35]), tensor([92]), tensor([3]), tensor([96]), tensor([58]), tensor([57]), tensor([72]), tensor([67]), tensor([53]), tensor([26]), tensor([52]), tensor([79]), tensor([27]), tensor([30]), tensor([32]), tensor([16]), tensor([98]), tensor([52]), tensor([94]), tensor([52]), tensor([60]), tensor([2]), tensor([45]), tensor([81]), tensor([3]), tensor([3]), tensor([60]), tensor([57]), tensor([95]), tensor([16]), tensor([32]), tensor([31]), tensor([82]), tensor([59]), tensor([80]), tensor([25]), tensor([81]), tensor([59]), tensor([94]), tensor([88]), tensor([31]), tensor([73]), tensor([25]), tensor([26]), tensor([37]), tensor([35]), tensor([95]), tensor([53]), tensor([58]), tensor([11]), tensor([3]), tensor([69]), tensor([67]), tensor([57]), tensor([95]), tensor([10]), tensor([24]), tensor([49]), tensor([71]), tensor([63]), tensor([86]), tensor([33]), tensor([25]), tensor([71]), tensor([49]), tensor([57]), tensor([7]), tensor([82]), tensor([97]), tensor([67]), tensor([72]), tensor([50]), tensor([95]), tensor([31]), tensor([27]), tensor([81]), tensor([71]), tensor([30]), tensor([46]), tensor([97]), tensor([31]), tensor([68]), tensor([24]), tensor([49]), tensor([63]), tensor([92]), tensor([69]), tensor([81]), tensor([33]), tensor([24]), tensor([25]), tensor([53]), tensor([35]), tensor([10]), tensor([79]), tensor([82]), tensor([7]), tensor([2]), tensor([50]), tensor([11]), tensor([65]), tensor([69]), tensor([53]), tensor([86]), tensor([52]), tensor([0]), tensor([98]), tensor([78]), tensor([14]), tensor([11]), tensor([95]), tensor([96]), tensor([92]), tensor([27]), tensor([12]), tensor([88]), tensor([82]), tensor([35]), tensor([37]), tensor([48]), tensor([26]), tensor([36]), tensor([12]), tensor([78]), tensor([57]), tensor([57]), tensor([25]), tensor([62]), tensor([24]), tensor([62]), tensor([92]), tensor([71]), tensor([49]), tensor([32]), tensor([58]), tensor([67]), tensor([49]), tensor([67]), tensor([86]), tensor([24]), tensor([78]), tensor([88]), tensor([27]), tensor([26]), tensor([32]), tensor([95]), tensor([48]), tensor([69]), tensor([24]), tensor([33]), tensor([68]), tensor([68]), tensor([63]), tensor([0]), tensor([37]), tensor([81]), tensor([26]), tensor([13]), tensor([78]), tensor([98]), tensor([69]), tensor([79]), tensor([27]), tensor([36]), tensor([37]), tensor([57]), tensor([30]), tensor([95]), tensor([45]), tensor([77]), tensor([71]), tensor([13]), tensor([46]), tensor([27]), tensor([91]), tensor([67]), tensor([31]), tensor([37]), tensor([10]), tensor([46]), tensor([7]), tensor([56]), tensor([21]), tensor([14]), tensor([37]), tensor([35]), tensor([50]), tensor([48]), tensor([91]), tensor([94]), tensor([94]), tensor([3]), tensor([16]), tensor([37]), tensor([94]), tensor([45]), tensor([57]), tensor([68]), tensor([81]), tensor([2]), tensor([33]), tensor([57]), tensor([63]), tensor([52]), tensor([14]), tensor([26]), tensor([92]), tensor([78]), tensor([41]), tensor([53]), tensor([10]), tensor([58]), tensor([78]), tensor([96]), tensor([79]), tensor([0]), tensor([92]), tensor([69]), tensor([2]), tensor([34]), tensor([34]), tensor([2]), tensor([65]), tensor([86]), tensor([30]), tensor([7]), tensor([62]), tensor([10]), tensor([91]), tensor([31]), tensor([68]), tensor([14]), tensor([94]), tensor([0]), tensor([72]), tensor([27]), tensor([71]), tensor([95]), tensor([62]), tensor([71]), tensor([12]), tensor([32]), tensor([68]), tensor([56]), tensor([30]), tensor([34]), tensor([57]), tensor([3]), tensor([53]), tensor([31]), tensor([25]), tensor([27]), tensor([26]), tensor([3]), tensor([77]), tensor([86]), tensor([41]), tensor([59]), tensor([56]), tensor([56]), tensor([14]), tensor([52]), tensor([71]), tensor([24]), tensor([95]), tensor([11]), tensor([97]), tensor([12]), tensor([45]), tensor([45]), tensor([34]), tensor([88]), tensor([34]), tensor([41]), tensor([33]), tensor([78]), tensor([0]), tensor([96]), tensor([63]), tensor([12]), tensor([95]), tensor([48]), tensor([88]), tensor([57]), tensor([59]), tensor([32]), tensor([56]), tensor([24]), tensor([88]), tensor([12]), tensor([49]), tensor([58]), tensor([37]), tensor([32]), tensor([98]), tensor([81]), tensor([53]), tensor([16]), tensor([68]), tensor([45]), tensor([25]), tensor([56]), tensor([13]), tensor([91]), tensor([35]), tensor([32]), tensor([69]), tensor([48]), tensor([95]), tensor([12]), tensor([95]), tensor([94]), tensor([56]), tensor([11]), tensor([2]), tensor([0]), tensor([65]), tensor([77]), tensor([0]), tensor([65]), tensor([13]), tensor([97]), tensor([7]), tensor([59]), tensor([7]), tensor([31]), tensor([7]), tensor([56]), tensor([14]), tensor([69]), tensor([0]), tensor([79]), tensor([36]), tensor([65]), tensor([21]), tensor([97]), tensor([56]), tensor([21]), tensor([35]), tensor([65]), tensor([53]), tensor([7]), tensor([57]), tensor([2]), tensor([41]), tensor([60]), tensor([45]), tensor([37]), tensor([25]), tensor([91]), tensor([53]), tensor([65]), tensor([37]), tensor([73]), tensor([14]), tensor([53]), tensor([91]), tensor([21]), tensor([36]), tensor([62]), tensor([60]), tensor([35]), tensor([34]), tensor([46]), tensor([67]), tensor([58]), tensor([13]), tensor([21]), tensor([71]), tensor([24]), tensor([46]), tensor([92]), tensor([98]), tensor([12]), tensor([0]), tensor([16]), tensor([97]), tensor([32]), tensor([68]), tensor([31]), tensor([3]), tensor([71]), tensor([36]), tensor([57]), tensor([62]), tensor([81]), tensor([21]), tensor([59]), tensor([95]), tensor([95]), tensor([27]), tensor([77]), tensor([33]), tensor([27]), tensor([30]), tensor([62]), tensor([31]), tensor([3]), tensor([52]), tensor([16]), tensor([96]), tensor([62]), tensor([80]), tensor([88]), tensor([72]), tensor([94]), tensor([52]), tensor([35]), tensor([67]), tensor([21]), tensor([32]), tensor([56]), tensor([98]), tensor([26]), tensor([11]), tensor([14]), tensor([34]), tensor([53]), tensor([45]), tensor([86]), tensor([88]), tensor([31]), tensor([72]), tensor([11]), tensor([60]), tensor([71]), tensor([2]), tensor([57]), tensor([21]), tensor([16]), tensor([34]), tensor([71]), tensor([49]), tensor([96]), tensor([16]), tensor([58]), tensor([81]), tensor([13]), tensor([32]), tensor([82]), tensor([24]), tensor([34]), tensor([2]), tensor([37]), tensor([36]), tensor([25]), tensor([94]), tensor([88]), tensor([25]), tensor([41]), tensor([25]), tensor([86]), tensor([59]), tensor([24]), tensor([97]), tensor([12]), tensor([56]), tensor([78]), tensor([56]), tensor([88]), tensor([98]), tensor([59]), tensor([12]), tensor([0]), tensor([60]), tensor([62]), tensor([67]), tensor([77]), tensor([98]), tensor([52]), tensor([82]), tensor([94]), tensor([80]), tensor([91]), tensor([41]), tensor([30]), tensor([48]), tensor([92]), tensor([96]), tensor([35]), tensor([92]), tensor([26]), tensor([33]), tensor([60]), tensor([62]), tensor([14]), tensor([13]), tensor([36]), tensor([80]), tensor([52]), tensor([59]), tensor([32]), tensor([12]), tensor([0]), tensor([73]), tensor([77]), tensor([56]), tensor([96]), tensor([59]), tensor([48]), tensor([49]), tensor([82]), tensor([11]), tensor([21]), tensor([68]), tensor([31]), tensor([32]), tensor([7]), tensor([78]), tensor([10]), tensor([94]), tensor([48]), tensor([80]), tensor([88]), tensor([69]), tensor([80]), tensor([32]), tensor([68]), tensor([50]), tensor([49]), tensor([52]), tensor([96]), tensor([46]), tensor([65]), tensor([37]), tensor([45]), tensor([72]), tensor([95]), tensor([0]), tensor([31]), tensor([65]), tensor([92]), tensor([98]), tensor([48]), tensor([62]), tensor([50]), tensor([26]), tensor([59]), tensor([33]), tensor([27]), tensor([82]), tensor([79]), tensor([12]), tensor([26]), tensor([98]), tensor([69]), tensor([72]), tensor([12]), tensor([62]), tensor([36]), tensor([48]), tensor([32]), tensor([82]), tensor([2]), tensor([49]), tensor([63]), tensor([0]), tensor([71]), tensor([41]), tensor([32]), tensor([78]), tensor([12]), tensor([91]), tensor([95]), tensor([78]), tensor([13]), tensor([63]), tensor([34]), tensor([30]), tensor([81]), tensor([0]), tensor([24]), tensor([97]), tensor([52]), tensor([12]), tensor([63]), tensor([98]), tensor([59]), tensor([50]), tensor([30]), tensor([77]), tensor([11]), tensor([77]), tensor([49]), tensor([71]), tensor([79]), tensor([73]), tensor([65]), tensor([16]), tensor([45]), tensor([88]), tensor([58]), tensor([68]), tensor([25]), tensor([25]), tensor([82]), tensor([65]), tensor([78]), tensor([67]), tensor([82]), tensor([36]), tensor([34]), tensor([82]), tensor([67]), tensor([30]), tensor([49]), tensor([30]), tensor([80]), tensor([10]), tensor([11]), tensor([2]), tensor([53]), tensor([36]), tensor([91]), tensor([10]), tensor([50]), tensor([46]), tensor([50]), tensor([45]), tensor([63]), tensor([27]), tensor([88]), tensor([81]), tensor([24]), tensor([97]), tensor([48]), tensor([11]), tensor([81]), tensor([12]), tensor([65]), tensor([68]), tensor([24]), tensor([46]), tensor([13]), tensor([30]), tensor([65]), tensor([92]), tensor([30]), tensor([41]), tensor([73]), tensor([26]), tensor([86]), tensor([46]), tensor([48]), tensor([10]), tensor([56]), tensor([35]), tensor([0]), tensor([59]), tensor([31]), tensor([46]), tensor([65]), tensor([52]), tensor([62]), tensor([27]), tensor([72]), tensor([10]), tensor([34]), tensor([7]), tensor([52]), tensor([81]), tensor([33]), tensor([36]), tensor([16]), tensor([34]), tensor([77]), tensor([16]), tensor([7]), tensor([79]), tensor([50]), tensor([67]), tensor([98]), tensor([77]), tensor([48]), tensor([11]), tensor([16]), tensor([60]), tensor([16]), tensor([31]), tensor([35]), tensor([78]), tensor([63]), tensor([68]), tensor([36]), tensor([37]), tensor([69]), tensor([60]), tensor([58]), tensor([45]), tensor([78]), tensor([77]), tensor([41]), tensor([11]), tensor([95]), tensor([88]), tensor([68]), tensor([58]), tensor([80]), tensor([3]), tensor([52]), tensor([67]), tensor([79]), tensor([98]), tensor([95]), tensor([56]), tensor([57]), tensor([50]), tensor([56]), tensor([3]), tensor([24]), tensor([79]), tensor([59]), tensor([68]), tensor([25]), tensor([41]), tensor([3]), tensor([27]), tensor([46]), tensor([57]), tensor([98]), tensor([0]), tensor([60]), tensor([81]), tensor([46]), tensor([98]), tensor([92]), tensor([92]), tensor([92]), tensor([60]), tensor([73]), tensor([78]), tensor([24]), tensor([62]), tensor([86]), tensor([24]), tensor([79]), tensor([3]), tensor([63]), tensor([33]), tensor([95]), tensor([10]), tensor([2]), tensor([69]), tensor([69]), tensor([92]), tensor([50]), tensor([33]), tensor([30]), tensor([65]), tensor([50]), tensor([68]), tensor([79]), tensor([49]), tensor([31]), tensor([98]), tensor([36]), tensor([98]), tensor([33]), tensor([10]), tensor([46]), tensor([77]), tensor([16]), tensor([30]), tensor([94]), tensor([31]), tensor([58]), tensor([60]), tensor([88]), tensor([35]), tensor([3]), tensor([57]), tensor([31]), tensor([14]), tensor([21]), tensor([48]), tensor([53]), tensor([2]), tensor([41]), tensor([13]), tensor([79]), tensor([91]), tensor([2]), tensor([26]), tensor([16]), tensor([33]), tensor([0]), tensor([16]), tensor([45]), tensor([92]), tensor([58]), tensor([52]), tensor([36]), tensor([80]), tensor([33]), tensor([11]), tensor([2]), tensor([88]), tensor([10]), tensor([73]), tensor([91]), tensor([72]), tensor([97]), tensor([0]), tensor([58]), tensor([16]), tensor([7]), tensor([35]), tensor([30]), tensor([52]), tensor([10]), tensor([0]), tensor([56]), tensor([69]), tensor([35]), tensor([59]), tensor([78]), tensor([30]), tensor([86]), tensor([94]), tensor([71]), tensor([86]), tensor([88]), tensor([3]), tensor([91]), tensor([13]), tensor([24]), tensor([31]), tensor([63]), tensor([27]), tensor([81]), tensor([46]), tensor([69]), tensor([53]), tensor([25]), tensor([97]), tensor([97]), tensor([10]), tensor([88]), tensor([79]), tensor([63]), tensor([63]), tensor([49]), tensor([50]), tensor([92]), tensor([0]), tensor([24]), tensor([49]), tensor([77]), tensor([2]), tensor([56]), tensor([49]), tensor([2]), tensor([62]), tensor([97]), tensor([25]), tensor([31]), tensor([14]), tensor([63]), tensor([73]), tensor([46]), tensor([58]), tensor([69]), tensor([91]), tensor([21]), tensor([41]), tensor([34]), tensor([30]), tensor([72]), tensor([25]), tensor([60]), tensor([67]), tensor([48]), tensor([33]), tensor([37]), tensor([25]), tensor([63]), tensor([36]), tensor([79]), tensor([68]), tensor([67]), tensor([41]), tensor([57]), tensor([41]), tensor([65]), tensor([45]), tensor([86]), tensor([14]), tensor([94]), tensor([78]), tensor([86]), tensor([10]), tensor([7]), tensor([59]), tensor([91]), tensor([71]), tensor([24]), tensor([45]), tensor([45]), tensor([52]), tensor([46]), tensor([21]), tensor([82]), tensor([82]), tensor([26]), tensor([49]), tensor([78]), tensor([92]), tensor([13]), tensor([21]), tensor([3]), tensor([97]), tensor([7]), tensor([80]), tensor([65]), tensor([60]), tensor([34]), tensor([26]), tensor([94]), tensor([3]), tensor([32]), tensor([37]), tensor([92]), tensor([11]), tensor([67]), tensor([49]), tensor([68]), tensor([80]), tensor([30]), tensor([72]), tensor([32]), tensor([13]), tensor([78]), tensor([34]), tensor([0]), tensor([60]), tensor([2]), tensor([79]), tensor([79]), tensor([65]), tensor([32]), tensor([72]), tensor([98]), tensor([62]), tensor([26]), tensor([97]), tensor([58]), tensor([27]), tensor([65]), tensor([0]), tensor([94]), tensor([52]), tensor([57]), tensor([34]), tensor([92]), tensor([60]), tensor([2]), tensor([46]), tensor([86]), tensor([33]), tensor([80]), tensor([67]), tensor([57]), tensor([10]), tensor([97]), tensor([7]), tensor([0]), tensor([52]), tensor([97]), tensor([37]), tensor([34]), tensor([35]), tensor([48]), tensor([86]), tensor([60]), tensor([94]), tensor([88]), tensor([37]), tensor([98]), tensor([33]), tensor([58]), tensor([45]), tensor([63]), tensor([68]), tensor([2]), tensor([82]), tensor([14]), tensor([53]), tensor([73]), tensor([16]), tensor([77]), tensor([91]), tensor([53]), tensor([96]), tensor([53]), tensor([13]), tensor([69]), tensor([82]), tensor([88]), tensor([73]), tensor([27]), tensor([71]), tensor([41]), tensor([13]), tensor([91]), tensor([46]), tensor([60]), tensor([21]), tensor([82]), tensor([12]), tensor([65]), tensor([88]), tensor([79]), tensor([72]), tensor([88]), tensor([57]), tensor([59]), tensor([69]), tensor([37]), tensor([7]), tensor([97]), tensor([95]), tensor([10]), tensor([57]), tensor([3]), tensor([60]), tensor([10]), tensor([82]), tensor([53]), tensor([13]), tensor([49]), tensor([56]), tensor([72]), tensor([13]), tensor([49]), tensor([35]), tensor([50]), tensor([27]), tensor([10]), tensor([35]), tensor([72]), tensor([96]), tensor([26]), tensor([59]), tensor([14]), tensor([50]), tensor([77]), tensor([59]), tensor([32]), tensor([91]), tensor([60]), tensor([50]), tensor([12]), tensor([57]), tensor([14]), tensor([3]), tensor([27]), tensor([37]), tensor([26]), tensor([97]), tensor([21]), tensor([94]), tensor([69]), tensor([33]), tensor([11]), tensor([26]), tensor([67]), tensor([72]), tensor([3]), tensor([69]), tensor([11]), tensor([25]), tensor([98]), tensor([62]), tensor([31]), tensor([63]), tensor([63]), tensor([33]), tensor([97]), tensor([80]), tensor([53]), tensor([46]), tensor([41]), tensor([95]), tensor([86]), tensor([71]), tensor([45]), tensor([52]), tensor([67]), tensor([68]), tensor([69]), tensor([26]), tensor([96]), tensor([13]), tensor([69]), tensor([21]), tensor([57]), tensor([81]), tensor([36]), tensor([24]), tensor([12]), tensor([82]), tensor([0]), tensor([14]), tensor([94]), tensor([86]), tensor([32]), tensor([32]), tensor([37]), tensor([77]), tensor([71]), tensor([68]), tensor([94]), tensor([36]), tensor([25]), tensor([12]), tensor([53]), tensor([98]), tensor([32]), tensor([52]), tensor([81]), tensor([69]), tensor([27]), tensor([62]), tensor([82]), tensor([13]), tensor([67]), tensor([3]), tensor([27]), tensor([96]), tensor([12]), tensor([52]), tensor([0]), tensor([94]), tensor([92]), tensor([31]), tensor([82]), tensor([58]), tensor([91]), tensor([97]), tensor([14]), tensor([91]), tensor([56]), tensor([73]), tensor([78]), tensor([72]), tensor([34]), tensor([59]), tensor([57]), tensor([69]), tensor([49]), tensor([48]), tensor([92]), tensor([0]), tensor([72]), tensor([73]), tensor([56]), tensor([21]), tensor([34])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.56 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.518\n",
            "TEST ALL:  0.499\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  7000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [99, 41, 13, 17, 21, 25, 33, 37, 45, 1, 49, 53, 57, 65, 69, 73, 5, 96, 95, 48, 12, 16, 24, 28, 32, 36, 52, 92, 56, 60, 68, 72, 80, 88, 77, 81, 93, 31, 98, 3, 7, 11, 23, 27, 35, 97, 59, 63, 67, 71, 79, 91, 94, 86, 82, 78, 66, 62, 58, 50, 46, 34, 30, 26, 22, 14, 10, 6, 2, 0]\n",
            "TRAIN_SET CLASSES:  [99, 23, 66, 22, 6, 93, 17, 5, 1, 28]\n",
            "VALIDATION CLASSES:  [99, 93, 28, 23, 22, 17, 6, 5, 66, 1]\n",
            "GROUP:  7\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [99, 23, 66, 22, 6, 93, 17, 5, 1, 28]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "num classes till now:  70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.23905381560325623\n",
            "Train step - Step 10, Loss 0.13669203221797943\n",
            "Train step - Step 20, Loss 0.12566988170146942\n",
            "Train step - Step 30, Loss 0.13623523712158203\n",
            "Train step - Step 40, Loss 0.12064658850431442\n",
            "Train step - Step 50, Loss 0.11870060116052628\n",
            "Train epoch - Accuracy: 0.15858585858585858 Loss: 0.13675020844131322 Corrects: 1099\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.11816417425870895\n",
            "Train step - Step 70, Loss 0.11707297712564468\n",
            "Train step - Step 80, Loss 0.12234858423471451\n",
            "Train step - Step 90, Loss 0.11572576314210892\n",
            "Train step - Step 100, Loss 0.11402927339076996\n",
            "Train epoch - Accuracy: 0.18441558441558442 Loss: 0.11625656518886272 Corrects: 1278\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11672504991292953\n",
            "Train step - Step 120, Loss 0.11395057290792465\n",
            "Train step - Step 130, Loss 0.11555652320384979\n",
            "Train step - Step 140, Loss 0.114253930747509\n",
            "Train step - Step 150, Loss 0.12108352035284042\n",
            "Train step - Step 160, Loss 0.11574304103851318\n",
            "Train epoch - Accuracy: 0.2049062049062049 Loss: 0.11414019317079932 Corrects: 1420\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.11349592357873917\n",
            "Train step - Step 180, Loss 0.11320056766271591\n",
            "Train step - Step 190, Loss 0.11165288835763931\n",
            "Train step - Step 200, Loss 0.11014541983604431\n",
            "Train step - Step 210, Loss 0.11781302839517593\n",
            "Train epoch - Accuracy: 0.2274170274170274 Loss: 0.11310364037623137 Corrects: 1576\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.10927753150463104\n",
            "Train step - Step 230, Loss 0.11749820411205292\n",
            "Train step - Step 240, Loss 0.11148890107870102\n",
            "Train step - Step 250, Loss 0.11362306773662567\n",
            "Train step - Step 260, Loss 0.1094914972782135\n",
            "Train step - Step 270, Loss 0.11733068525791168\n",
            "Train epoch - Accuracy: 0.24718614718614718 Loss: 0.11228026891339565 Corrects: 1713\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.10722170025110245\n",
            "Train step - Step 290, Loss 0.10917782038450241\n",
            "Train step - Step 300, Loss 0.1116352528333664\n",
            "Train step - Step 310, Loss 0.10738524794578552\n",
            "Train step - Step 320, Loss 0.10868899524211884\n",
            "Train epoch - Accuracy: 0.26392496392496395 Loss: 0.11129388471186419 Corrects: 1829\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11180974543094635\n",
            "Train step - Step 340, Loss 0.11537542194128036\n",
            "Train step - Step 350, Loss 0.10705467313528061\n",
            "Train step - Step 360, Loss 0.11228669434785843\n",
            "Train step - Step 370, Loss 0.1055174171924591\n",
            "Train step - Step 380, Loss 0.10036098212003708\n",
            "Train epoch - Accuracy: 0.2842712842712843 Loss: 0.11102423524056679 Corrects: 1970\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.11440838128328323\n",
            "Train step - Step 400, Loss 0.11289878934621811\n",
            "Train step - Step 410, Loss 0.10916239768266678\n",
            "Train step - Step 420, Loss 0.1053123027086258\n",
            "Train step - Step 430, Loss 0.10351001471281052\n",
            "Train epoch - Accuracy: 0.29668109668109666 Loss: 0.11054615692099795 Corrects: 2056\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.11375734955072403\n",
            "Train step - Step 450, Loss 0.11584082990884781\n",
            "Train step - Step 460, Loss 0.11376668512821198\n",
            "Train step - Step 470, Loss 0.1157798245549202\n",
            "Train step - Step 480, Loss 0.11458247154951096\n",
            "Train step - Step 490, Loss 0.11065834015607834\n",
            "Train epoch - Accuracy: 0.3088023088023088 Loss: 0.11040312588687927 Corrects: 2140\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10528449714183807\n",
            "Train step - Step 510, Loss 0.10925297439098358\n",
            "Train step - Step 520, Loss 0.11175741255283356\n",
            "Train step - Step 530, Loss 0.11301793903112411\n",
            "Train step - Step 540, Loss 0.10940908640623093\n",
            "Train epoch - Accuracy: 0.31933621933621936 Loss: 0.11004795192124008 Corrects: 2213\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.10854210704565048\n",
            "Train step - Step 560, Loss 0.1072874367237091\n",
            "Train step - Step 570, Loss 0.11487957835197449\n",
            "Train step - Step 580, Loss 0.10507407784461975\n",
            "Train step - Step 590, Loss 0.1107751652598381\n",
            "Train step - Step 600, Loss 0.1129264310002327\n",
            "Train epoch - Accuracy: 0.33246753246753247 Loss: 0.1101049629382757 Corrects: 2304\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.10854898393154144\n",
            "Train step - Step 620, Loss 0.11167825758457184\n",
            "Train step - Step 630, Loss 0.11356958001852036\n",
            "Train step - Step 640, Loss 0.11010760068893433\n",
            "Train step - Step 650, Loss 0.1095995232462883\n",
            "Train epoch - Accuracy: 0.3466089466089466 Loss: 0.10959221203347821 Corrects: 2402\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10523968189954758\n",
            "Train step - Step 670, Loss 0.11429677903652191\n",
            "Train step - Step 680, Loss 0.11170544475317001\n",
            "Train step - Step 690, Loss 0.10945519059896469\n",
            "Train step - Step 700, Loss 0.10543422400951385\n",
            "Train step - Step 710, Loss 0.11072827130556107\n",
            "Train epoch - Accuracy: 0.3588744588744589 Loss: 0.10941567022305031 Corrects: 2487\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10631461441516876\n",
            "Train step - Step 730, Loss 0.10935797542333603\n",
            "Train step - Step 740, Loss 0.10734395682811737\n",
            "Train step - Step 750, Loss 0.11270212382078171\n",
            "Train step - Step 760, Loss 0.10707554221153259\n",
            "Train epoch - Accuracy: 0.3660894660894661 Loss: 0.10952497205520949 Corrects: 2537\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10748269408941269\n",
            "Train step - Step 780, Loss 0.11105639487504959\n",
            "Train step - Step 790, Loss 0.11082019656896591\n",
            "Train step - Step 800, Loss 0.11208800971508026\n",
            "Train step - Step 810, Loss 0.11183154582977295\n",
            "Train step - Step 820, Loss 0.10423801094293594\n",
            "Train epoch - Accuracy: 0.3746031746031746 Loss: 0.10929501628204857 Corrects: 2596\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.11642370373010635\n",
            "Train step - Step 840, Loss 0.11202012002468109\n",
            "Train step - Step 850, Loss 0.1043146550655365\n",
            "Train step - Step 860, Loss 0.10331854224205017\n",
            "Train step - Step 870, Loss 0.11638268083333969\n",
            "Train epoch - Accuracy: 0.3810966810966811 Loss: 0.10888306968354904 Corrects: 2641\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.10221360623836517\n",
            "Train step - Step 890, Loss 0.10938599705696106\n",
            "Train step - Step 900, Loss 0.10376663506031036\n",
            "Train step - Step 910, Loss 0.10772034525871277\n",
            "Train step - Step 920, Loss 0.10598361492156982\n",
            "Train step - Step 930, Loss 0.10724897682666779\n",
            "Train epoch - Accuracy: 0.38152958152958155 Loss: 0.10893794480082276 Corrects: 2644\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.11437917500734329\n",
            "Train step - Step 950, Loss 0.11307124048471451\n",
            "Train step - Step 960, Loss 0.10632944107055664\n",
            "Train step - Step 970, Loss 0.10836666822433472\n",
            "Train step - Step 980, Loss 0.10526572912931442\n",
            "Train epoch - Accuracy: 0.3878787878787879 Loss: 0.10838951432558709 Corrects: 2688\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.10790877044200897\n",
            "Train step - Step 1000, Loss 0.10719441622495651\n",
            "Train step - Step 1010, Loss 0.1111677959561348\n",
            "Train step - Step 1020, Loss 0.11243896931409836\n",
            "Train step - Step 1030, Loss 0.10539543628692627\n",
            "Train step - Step 1040, Loss 0.10814011842012405\n",
            "Train epoch - Accuracy: 0.3966810966810967 Loss: 0.10872463640712557 Corrects: 2749\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.1144513413310051\n",
            "Train step - Step 1060, Loss 0.11085452884435654\n",
            "Train step - Step 1070, Loss 0.10810069739818573\n",
            "Train step - Step 1080, Loss 0.10778366029262543\n",
            "Train step - Step 1090, Loss 0.10574106127023697\n",
            "Train epoch - Accuracy: 0.4049062049062049 Loss: 0.10839163386417502 Corrects: 2806\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.11304909735918045\n",
            "Train step - Step 1110, Loss 0.10901368409395218\n",
            "Train step - Step 1120, Loss 0.10875110328197479\n",
            "Train step - Step 1130, Loss 0.10696336627006531\n",
            "Train step - Step 1140, Loss 0.10775462538003922\n",
            "Train step - Step 1150, Loss 0.1091332733631134\n",
            "Train epoch - Accuracy: 0.41789321789321787 Loss: 0.10849652401726655 Corrects: 2896\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10995805263519287\n",
            "Train step - Step 1170, Loss 0.10626213997602463\n",
            "Train step - Step 1180, Loss 0.10906684398651123\n",
            "Train step - Step 1190, Loss 0.10679514706134796\n",
            "Train step - Step 1200, Loss 0.10738798975944519\n",
            "Train epoch - Accuracy: 0.41515151515151516 Loss: 0.10800335240183455 Corrects: 2877\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.11241528391838074\n",
            "Train step - Step 1220, Loss 0.11339908093214035\n",
            "Train step - Step 1230, Loss 0.10563801229000092\n",
            "Train step - Step 1240, Loss 0.10321281850337982\n",
            "Train step - Step 1250, Loss 0.10639693588018417\n",
            "Train step - Step 1260, Loss 0.10667712241411209\n",
            "Train epoch - Accuracy: 0.4253968253968254 Loss: 0.10777314760726252 Corrects: 2948\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.10701082646846771\n",
            "Train step - Step 1280, Loss 0.10981550067663193\n",
            "Train step - Step 1290, Loss 0.10789947211742401\n",
            "Train step - Step 1300, Loss 0.10550685971975327\n",
            "Train step - Step 1310, Loss 0.11063897609710693\n",
            "Train epoch - Accuracy: 0.42943722943722945 Loss: 0.10805161799187268 Corrects: 2976\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.10839337855577469\n",
            "Train step - Step 1330, Loss 0.10778020322322845\n",
            "Train step - Step 1340, Loss 0.11013632267713547\n",
            "Train step - Step 1350, Loss 0.116443932056427\n",
            "Train step - Step 1360, Loss 0.11258546262979507\n",
            "Train step - Step 1370, Loss 0.10519254207611084\n",
            "Train epoch - Accuracy: 0.43232323232323233 Loss: 0.10796967132008953 Corrects: 2996\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.10821355879306793\n",
            "Train step - Step 1390, Loss 0.10565046966075897\n",
            "Train step - Step 1400, Loss 0.10578867793083191\n",
            "Train step - Step 1410, Loss 0.10440030694007874\n",
            "Train step - Step 1420, Loss 0.10802800208330154\n",
            "Train epoch - Accuracy: 0.43766233766233764 Loss: 0.10787330259277363 Corrects: 3033\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.10289034992456436\n",
            "Train step - Step 1440, Loss 0.10576336830854416\n",
            "Train step - Step 1450, Loss 0.10997714102268219\n",
            "Train step - Step 1460, Loss 0.1127198114991188\n",
            "Train step - Step 1470, Loss 0.1128322184085846\n",
            "Train step - Step 1480, Loss 0.11070875823497772\n",
            "Train epoch - Accuracy: 0.44126984126984126 Loss: 0.1077462325514997 Corrects: 3058\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.10998949408531189\n",
            "Train step - Step 1500, Loss 0.1076158881187439\n",
            "Train step - Step 1510, Loss 0.1073748767375946\n",
            "Train step - Step 1520, Loss 0.10838884860277176\n",
            "Train step - Step 1530, Loss 0.10815633088350296\n",
            "Train epoch - Accuracy: 0.4393939393939394 Loss: 0.10708453486234079 Corrects: 3045\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.10431989282369614\n",
            "Train step - Step 1550, Loss 0.11196158826351166\n",
            "Train step - Step 1560, Loss 0.10388132929801941\n",
            "Train step - Step 1570, Loss 0.1096690222620964\n",
            "Train step - Step 1580, Loss 0.11067643761634827\n",
            "Train step - Step 1590, Loss 0.10823731124401093\n",
            "Train epoch - Accuracy: 0.45411255411255413 Loss: 0.10736609005824828 Corrects: 3147\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.10425244271755219\n",
            "Train step - Step 1610, Loss 0.10105762630701065\n",
            "Train step - Step 1620, Loss 0.11152853071689606\n",
            "Train step - Step 1630, Loss 0.10519065707921982\n",
            "Train step - Step 1640, Loss 0.11110170930624008\n",
            "Train epoch - Accuracy: 0.4595959595959596 Loss: 0.10702860284849335 Corrects: 3185\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.10256393253803253\n",
            "Train step - Step 1660, Loss 0.10618829727172852\n",
            "Train step - Step 1670, Loss 0.10897865146398544\n",
            "Train step - Step 1680, Loss 0.10508022457361221\n",
            "Train step - Step 1690, Loss 0.10394091159105301\n",
            "Train step - Step 1700, Loss 0.11025106906890869\n",
            "Train epoch - Accuracy: 0.46075036075036074 Loss: 0.10704873146389814 Corrects: 3193\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.09884168952703476\n",
            "Train step - Step 1720, Loss 0.10587407648563385\n",
            "Train step - Step 1730, Loss 0.10317546874284744\n",
            "Train step - Step 1740, Loss 0.10856867581605911\n",
            "Train step - Step 1750, Loss 0.10710632055997849\n",
            "Train epoch - Accuracy: 0.46305916305916306 Loss: 0.10694219245539083 Corrects: 3209\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.10161668062210083\n",
            "Train step - Step 1770, Loss 0.11103038489818573\n",
            "Train step - Step 1780, Loss 0.10585368424654007\n",
            "Train step - Step 1790, Loss 0.10127800703048706\n",
            "Train step - Step 1800, Loss 0.10415468364953995\n",
            "Train step - Step 1810, Loss 0.11205240339040756\n",
            "Train epoch - Accuracy: 0.47243867243867244 Loss: 0.10686724371079243 Corrects: 3274\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.11081277579069138\n",
            "Train step - Step 1830, Loss 0.10269741714000702\n",
            "Train step - Step 1840, Loss 0.10338123887777328\n",
            "Train step - Step 1850, Loss 0.10689769685268402\n",
            "Train step - Step 1860, Loss 0.10357385128736496\n",
            "Train epoch - Accuracy: 0.4683982683982684 Loss: 0.10683873234502164 Corrects: 3246\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.10198719799518585\n",
            "Train step - Step 1880, Loss 0.10935787856578827\n",
            "Train step - Step 1890, Loss 0.10575399547815323\n",
            "Train step - Step 1900, Loss 0.10324915498495102\n",
            "Train step - Step 1910, Loss 0.10089002549648285\n",
            "Train step - Step 1920, Loss 0.10693200677633286\n",
            "Train epoch - Accuracy: 0.4746031746031746 Loss: 0.1066380439877166 Corrects: 3289\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.10639332234859467\n",
            "Train step - Step 1940, Loss 0.10353408008813858\n",
            "Train step - Step 1950, Loss 0.10655278712511063\n",
            "Train step - Step 1960, Loss 0.10740801692008972\n",
            "Train step - Step 1970, Loss 0.10025153309106827\n",
            "Train epoch - Accuracy: 0.4797979797979798 Loss: 0.10652030332693978 Corrects: 3325\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.10611645877361298\n",
            "Train step - Step 1990, Loss 0.10515622794628143\n",
            "Train step - Step 2000, Loss 0.10242252051830292\n",
            "Train step - Step 2010, Loss 0.10736774653196335\n",
            "Train step - Step 2020, Loss 0.11215643584728241\n",
            "Train step - Step 2030, Loss 0.10772458463907242\n",
            "Train epoch - Accuracy: 0.47748917748917746 Loss: 0.10678250009535367 Corrects: 3309\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.10718108713626862\n",
            "Train step - Step 2050, Loss 0.1057184487581253\n",
            "Train step - Step 2060, Loss 0.11550445854663849\n",
            "Train step - Step 2070, Loss 0.1058071106672287\n",
            "Train step - Step 2080, Loss 0.10773719847202301\n",
            "Train epoch - Accuracy: 0.48614718614718616 Loss: 0.1064291080828181 Corrects: 3369\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.10447964072227478\n",
            "Train step - Step 2100, Loss 0.11008112877607346\n",
            "Train step - Step 2110, Loss 0.1018894761800766\n",
            "Train step - Step 2120, Loss 0.10331763327121735\n",
            "Train step - Step 2130, Loss 0.1089651882648468\n",
            "Train step - Step 2140, Loss 0.10584106296300888\n",
            "Train epoch - Accuracy: 0.49393939393939396 Loss: 0.10630071486143494 Corrects: 3423\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.09896495193243027\n",
            "Train step - Step 2160, Loss 0.10604452341794968\n",
            "Train step - Step 2170, Loss 0.10163701325654984\n",
            "Train step - Step 2180, Loss 0.11562404781579971\n",
            "Train step - Step 2190, Loss 0.10878325998783112\n",
            "Train epoch - Accuracy: 0.5004329004329005 Loss: 0.10612257264049194 Corrects: 3468\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.10988491773605347\n",
            "Train step - Step 2210, Loss 0.10649748146533966\n",
            "Train step - Step 2220, Loss 0.10439891368150711\n",
            "Train step - Step 2230, Loss 0.10680896043777466\n",
            "Train step - Step 2240, Loss 0.1106579452753067\n",
            "Train step - Step 2250, Loss 0.11473838239908218\n",
            "Train epoch - Accuracy: 0.49783549783549785 Loss: 0.10617659925597399 Corrects: 3450\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.10635926574468613\n",
            "Train step - Step 2270, Loss 0.10813280940055847\n",
            "Train step - Step 2280, Loss 0.10370296984910965\n",
            "Train step - Step 2290, Loss 0.10786183923482895\n",
            "Train step - Step 2300, Loss 0.09901989251375198\n",
            "Train epoch - Accuracy: 0.5001443001443001 Loss: 0.10597402616625741 Corrects: 3466\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.11125518381595612\n",
            "Train step - Step 2320, Loss 0.11249256879091263\n",
            "Train step - Step 2330, Loss 0.10346037149429321\n",
            "Train step - Step 2340, Loss 0.10717558860778809\n",
            "Train step - Step 2350, Loss 0.10855519771575928\n",
            "Train step - Step 2360, Loss 0.10409640520811081\n",
            "Train epoch - Accuracy: 0.49927849927849927 Loss: 0.10631056823814758 Corrects: 3460\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.10899213701486588\n",
            "Train step - Step 2380, Loss 0.10731590539216995\n",
            "Train step - Step 2390, Loss 0.10260216891765594\n",
            "Train step - Step 2400, Loss 0.10718115419149399\n",
            "Train step - Step 2410, Loss 0.11027161777019501\n",
            "Train epoch - Accuracy: 0.5057720057720058 Loss: 0.10575102491984292 Corrects: 3505\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.10854968428611755\n",
            "Train step - Step 2430, Loss 0.10278372466564178\n",
            "Train step - Step 2440, Loss 0.10564633458852768\n",
            "Train step - Step 2450, Loss 0.1069747805595398\n",
            "Train step - Step 2460, Loss 0.10168442875146866\n",
            "Train step - Step 2470, Loss 0.1055731475353241\n",
            "Train epoch - Accuracy: 0.5086580086580087 Loss: 0.10600061210848036 Corrects: 3525\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.11042329668998718\n",
            "Train step - Step 2490, Loss 0.11596393585205078\n",
            "Train step - Step 2500, Loss 0.11067581921815872\n",
            "Train step - Step 2510, Loss 0.10628949850797653\n",
            "Train step - Step 2520, Loss 0.10779070109128952\n",
            "Train epoch - Accuracy: 0.5128427128427129 Loss: 0.10595361838093052 Corrects: 3554\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.10264168679714203\n",
            "Train step - Step 2540, Loss 0.11068255454301834\n",
            "Train step - Step 2550, Loss 0.10390178859233856\n",
            "Train step - Step 2560, Loss 0.10603687167167664\n",
            "Train step - Step 2570, Loss 0.10893668234348297\n",
            "Train step - Step 2580, Loss 0.10955298691987991\n",
            "Train epoch - Accuracy: 0.5112554112554113 Loss: 0.10563884916759672 Corrects: 3543\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.10258053988218307\n",
            "Train step - Step 2600, Loss 0.11176891624927521\n",
            "Train step - Step 2610, Loss 0.10395380854606628\n",
            "Train step - Step 2620, Loss 0.10672685503959656\n",
            "Train step - Step 2630, Loss 0.1096639484167099\n",
            "Train epoch - Accuracy: 0.5202020202020202 Loss: 0.10603604104267743 Corrects: 3605\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.11125434190034866\n",
            "Train step - Step 2650, Loss 0.10737638175487518\n",
            "Train step - Step 2660, Loss 0.10580949485301971\n",
            "Train step - Step 2670, Loss 0.10017507523298264\n",
            "Train step - Step 2680, Loss 0.10049451142549515\n",
            "Train step - Step 2690, Loss 0.11181621253490448\n",
            "Train epoch - Accuracy: 0.5215007215007215 Loss: 0.10551904204904947 Corrects: 3614\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.1046944186091423\n",
            "Train step - Step 2710, Loss 0.10553071647882462\n",
            "Train step - Step 2720, Loss 0.0966661348938942\n",
            "Train step - Step 2730, Loss 0.10687565058469772\n",
            "Train step - Step 2740, Loss 0.09585339576005936\n",
            "Train epoch - Accuracy: 0.5236652236652236 Loss: 0.1051586053717188 Corrects: 3629\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.10482180863618851\n",
            "Train step - Step 2760, Loss 0.11281059682369232\n",
            "Train step - Step 2770, Loss 0.10245628654956818\n",
            "Train step - Step 2780, Loss 0.10945157706737518\n",
            "Train step - Step 2790, Loss 0.11003544926643372\n",
            "Train step - Step 2800, Loss 0.1110195443034172\n",
            "Train epoch - Accuracy: 0.5275613275613276 Loss: 0.10490371373953758 Corrects: 3656\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.10154745727777481\n",
            "Train step - Step 2820, Loss 0.10026615113019943\n",
            "Train step - Step 2830, Loss 0.10709238052368164\n",
            "Train step - Step 2840, Loss 0.10711055248975754\n",
            "Train step - Step 2850, Loss 0.10659123957157135\n",
            "Train epoch - Accuracy: 0.5301587301587302 Loss: 0.1043826165416884 Corrects: 3674\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.10712731629610062\n",
            "Train step - Step 2870, Loss 0.10479109734296799\n",
            "Train step - Step 2880, Loss 0.1030910536646843\n",
            "Train step - Step 2890, Loss 0.1048911064863205\n",
            "Train step - Step 2900, Loss 0.10438178479671478\n",
            "Train step - Step 2910, Loss 0.10160492360591888\n",
            "Train epoch - Accuracy: 0.5311688311688312 Loss: 0.10494440690564559 Corrects: 3681\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.10392527282238007\n",
            "Train step - Step 2930, Loss 0.11251617223024368\n",
            "Train step - Step 2940, Loss 0.1021588072180748\n",
            "Train step - Step 2950, Loss 0.11659739166498184\n",
            "Train step - Step 2960, Loss 0.10634540766477585\n",
            "Train epoch - Accuracy: 0.529004329004329 Loss: 0.10457623665251946 Corrects: 3666\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.10470376908779144\n",
            "Train step - Step 2980, Loss 0.1091940626502037\n",
            "Train step - Step 2990, Loss 0.10007716715335846\n",
            "Train step - Step 3000, Loss 0.1044813022017479\n",
            "Train step - Step 3010, Loss 0.10308678448200226\n",
            "Train step - Step 3020, Loss 0.10858317464590073\n",
            "Train epoch - Accuracy: 0.5412698412698412 Loss: 0.10480361206061913 Corrects: 3751\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.10084272921085358\n",
            "Train step - Step 3040, Loss 0.10405027121305466\n",
            "Train step - Step 3050, Loss 0.10826906561851501\n",
            "Train step - Step 3060, Loss 0.10797958076000214\n",
            "Train step - Step 3070, Loss 0.10353735834360123\n",
            "Train epoch - Accuracy: 0.53997113997114 Loss: 0.10472353563938307 Corrects: 3742\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.10816606879234314\n",
            "Train step - Step 3090, Loss 0.10134518891572952\n",
            "Train step - Step 3100, Loss 0.10429619252681732\n",
            "Train step - Step 3110, Loss 0.10445444285869598\n",
            "Train step - Step 3120, Loss 0.10108164697885513\n",
            "Train step - Step 3130, Loss 0.10371673852205276\n",
            "Train epoch - Accuracy: 0.5298701298701298 Loss: 0.1046231310733985 Corrects: 3672\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.10796496272087097\n",
            "Train step - Step 3150, Loss 0.11430291086435318\n",
            "Train step - Step 3160, Loss 0.10232485830783844\n",
            "Train step - Step 3170, Loss 0.10538000613451004\n",
            "Train step - Step 3180, Loss 0.10346361994743347\n",
            "Train epoch - Accuracy: 0.5326118326118326 Loss: 0.10483229202023488 Corrects: 3691\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.1090182438492775\n",
            "Train step - Step 3200, Loss 0.10338924080133438\n",
            "Train step - Step 3210, Loss 0.10786817967891693\n",
            "Train step - Step 3220, Loss 0.10188180953264236\n",
            "Train step - Step 3230, Loss 0.11008356511592865\n",
            "Train step - Step 3240, Loss 0.10436062514781952\n",
            "Train epoch - Accuracy: 0.5321789321789322 Loss: 0.10433732117667342 Corrects: 3688\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.10669965296983719\n",
            "Train step - Step 3260, Loss 0.10147160291671753\n",
            "Train step - Step 3270, Loss 0.1087813675403595\n",
            "Train step - Step 3280, Loss 0.1048331931233406\n",
            "Train step - Step 3290, Loss 0.10381671786308289\n",
            "Train epoch - Accuracy: 0.5356421356421357 Loss: 0.10451547643667003 Corrects: 3712\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.11279385536909103\n",
            "Train step - Step 3310, Loss 0.10632986575365067\n",
            "Train step - Step 3320, Loss 0.10983812063932419\n",
            "Train step - Step 3330, Loss 0.10311813652515411\n",
            "Train step - Step 3340, Loss 0.10282918810844421\n",
            "Train step - Step 3350, Loss 0.1050148457288742\n",
            "Train epoch - Accuracy: 0.5424242424242425 Loss: 0.10465279817151128 Corrects: 3759\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.10383272916078568\n",
            "Train step - Step 3370, Loss 0.10365050286054611\n",
            "Train step - Step 3380, Loss 0.1047142967581749\n",
            "Train step - Step 3390, Loss 0.10084125399589539\n",
            "Train step - Step 3400, Loss 0.1033913791179657\n",
            "Train epoch - Accuracy: 0.536075036075036 Loss: 0.10451003381442198 Corrects: 3715\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.10040982067584991\n",
            "Train step - Step 3420, Loss 0.10430309176445007\n",
            "Train step - Step 3430, Loss 0.10234085470438004\n",
            "Train step - Step 3440, Loss 0.10904815047979355\n",
            "Train step - Step 3450, Loss 0.10271835327148438\n",
            "Train step - Step 3460, Loss 0.1046258956193924\n",
            "Train epoch - Accuracy: 0.5311688311688312 Loss: 0.10449123301289298 Corrects: 3681\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.10730429738759995\n",
            "Train step - Step 3480, Loss 0.10719473659992218\n",
            "Train step - Step 3490, Loss 0.101272352039814\n",
            "Train step - Step 3500, Loss 0.09932433813810349\n",
            "Train step - Step 3510, Loss 0.10069054365158081\n",
            "Train epoch - Accuracy: 0.5323232323232323 Loss: 0.1040618646648023 Corrects: 3689\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.10769639909267426\n",
            "Train step - Step 3530, Loss 0.10920839756727219\n",
            "Train step - Step 3540, Loss 0.10240297019481659\n",
            "Train step - Step 3550, Loss 0.1027071624994278\n",
            "Train step - Step 3560, Loss 0.10178165137767792\n",
            "Train step - Step 3570, Loss 0.1098657175898552\n",
            "Train epoch - Accuracy: 0.53997113997114 Loss: 0.10418473498043732 Corrects: 3742\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.1006518229842186\n",
            "Train step - Step 3590, Loss 0.10373644530773163\n",
            "Train step - Step 3600, Loss 0.1029662936925888\n",
            "Train step - Step 3610, Loss 0.10270141810178757\n",
            "Train step - Step 3620, Loss 0.1070220023393631\n",
            "Train epoch - Accuracy: 0.537950937950938 Loss: 0.10437883705459804 Corrects: 3728\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.10502548515796661\n",
            "Train step - Step 3640, Loss 0.09845352172851562\n",
            "Train step - Step 3650, Loss 0.10320541262626648\n",
            "Train step - Step 3660, Loss 0.10346918553113937\n",
            "Train step - Step 3670, Loss 0.10055244714021683\n",
            "Train step - Step 3680, Loss 0.10746888816356659\n",
            "Train epoch - Accuracy: 0.5378066378066378 Loss: 0.10427338610393833 Corrects: 3727\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.09659495204687119\n",
            "Train step - Step 3700, Loss 0.09950689971446991\n",
            "Train step - Step 3710, Loss 0.10046959668397903\n",
            "Train step - Step 3720, Loss 0.10236267745494843\n",
            "Train step - Step 3730, Loss 0.10506940633058548\n",
            "Train epoch - Accuracy: 0.5437229437229437 Loss: 0.10419789149733677 Corrects: 3768\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.10439011454582214\n",
            "Train step - Step 3750, Loss 0.104548878967762\n",
            "Train step - Step 3760, Loss 0.10422754287719727\n",
            "Train step - Step 3770, Loss 0.0955454483628273\n",
            "Train step - Step 3780, Loss 0.10125228762626648\n",
            "Train step - Step 3790, Loss 0.1028129830956459\n",
            "Train epoch - Accuracy: 0.5375180375180375 Loss: 0.10422714937257904 Corrects: 3725\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.10145926475524902\n",
            "Train step - Step 3810, Loss 0.10413898527622223\n",
            "Train step - Step 3820, Loss 0.1026320829987526\n",
            "Train step - Step 3830, Loss 0.10568582266569138\n",
            "Train step - Step 3840, Loss 0.10493743419647217\n",
            "Train epoch - Accuracy: 0.5363636363636364 Loss: 0.1043229717454869 Corrects: 3717\n",
            "Training finished in 397.1611394882202 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0, 79, 71, 63, 26, 14, 2, 73, 41, 56, 48, 99, 23, 66, 22, 6, 93, 17, 5, 1, 28]\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2241406a10>\n",
            "Constructing exemplars of class 99\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [41278, 27753, 49261, 32723, 41751, 27067, 5931, 33087, 44587, 38559, 28680, 15258, 22598, 43583, 9401, 3081, 39683, 42350, 10306, 45821, 40593, 21881, 11202, 13864, 44968, 42070, 27288, 25410]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a87710>\n",
            "Constructing exemplars of class 23\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [41454, 32156, 28839, 27202, 18055, 256, 24932, 2606, 12315, 42121, 4426, 10143, 14649, 36279, 31095, 24696, 14782, 7436, 27017, 49695, 14756, 39458, 5561, 17618, 48551, 16805, 6146, 27041]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22443a58d0>\n",
            "Constructing exemplars of class 66\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [41607, 31927, 45544, 38589, 18810, 22020, 10408, 43654, 28150, 3860, 17786, 6454, 13652, 36732, 16672, 49442, 40583, 8948, 19376, 18442, 29485, 5027, 37684, 48867, 47055, 11759, 14357, 24865]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a7e090>\n",
            "Constructing exemplars of class 22\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [32505, 36684, 46997, 6771, 24204, 41791, 632, 8603, 6019, 48778, 35337, 5250, 36675, 33877, 39093, 6300, 18902, 49084, 32505, 11429, 30505, 30411, 22566, 25300, 8638, 16135, 26474, 17646]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223295da50>\n",
            "Constructing exemplars of class 6\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [27003, 49107, 18887, 43542, 5877, 19935, 40740, 29444, 44515, 15088, 24833, 42459, 31184, 714, 21428, 36992, 11211, 21330, 17836, 17637, 38520, 12070, 41015, 25473, 36354, 17226, 27003, 23658]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e9eb10>\n",
            "Constructing exemplars of class 93\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [12110, 1224, 20401, 37430, 44161, 27722, 37996, 22741, 13042, 19033, 16419, 42140, 29068, 19116, 42068, 20940, 37796, 37405, 37996, 10409, 2142, 29069, 38898, 23982, 34734, 23224, 11099, 35203]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a7e090>\n",
            "Constructing exemplars of class 17\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [11578, 1960, 47695, 10730, 42609, 35889, 12582, 29296, 16365, 39926, 23599, 39997, 29864, 2784, 7213, 27474, 47497, 4224, 34726, 38005, 16128, 7457, 20513, 32188, 15967, 25170, 14769, 20283]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223295da50>\n",
            "Constructing exemplars of class 5\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [43966, 33951, 42520, 49813, 7054, 41562, 18916, 35545, 29948, 16480, 1092, 1479, 14402, 5302, 10319, 41324, 721, 26316, 16708, 31979, 1352, 9597, 7203, 44905, 5575, 23315, 9413, 26187]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232eedf10>\n",
            "Constructing exemplars of class 1\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [42934, 29098, 43670, 32884, 35107, 45011, 15859, 15940, 28112, 44775, 41114, 9164, 28293, 41141, 17084, 35826, 39673, 49682, 27655, 12796, 18023, 20090, 13221, 8713, 23518, 5356, 31177, 4348]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22414062d0>\n",
            "Constructing exemplars of class 28\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [36349, 1460, 10363, 15853, 38756, 30041, 16589, 5425, 26455, 36193, 44769, 26121, 40812, 3324, 8144, 32408, 3756, 29347, 35895, 14546, 21493, 35647, 18027, 42992, 33693, 227, 11356, 35169]\n",
            "x train:  [ 0.02063525 -0.07719998 -0.09806208 -0.04323838 -0.08704853 -0.09753458\n",
            " -0.10799194 -0.08815765 -0.12436156 -0.05206656 -0.07654451 -0.06554949\n",
            " -0.04939351 -0.11713692 -0.07628976 -0.02910401 -0.0928578  -0.07540951\n",
            " -0.13081011 -0.16031636 -0.14090507 -0.09848408 -0.2610185  -0.14227481\n",
            " -0.01296751 -0.08655286 -0.12768988 -0.06214459 -0.18437424 -0.03962271\n",
            " -0.08045632 -0.09128373 -0.14082584 -0.09455372 -0.23422652 -0.0765347\n",
            " -0.10741762 -0.14461088 -0.05755614 -0.07072841 -0.03873661 -0.06289662\n",
            " -0.03610712 -0.17738171 -0.14502038 -0.15018372 -0.10695073 -0.07248995\n",
            " -0.16612123 -0.2012673  -0.10491044 -0.12183402 -0.13097905 -0.06426029\n",
            " -0.08714031 -0.18953446 -0.09918445 -0.06584251 -0.09759878 -0.11384255\n",
            " -0.16286543 -0.16630115 -0.16555203 -0.14795946 -0.07279668 -0.01462855\n",
            " -0.13117328 -0.1380324  -0.11128438 -0.24369204]\n",
            "y_train:  [tensor([7]), tensor([33]), tensor([23]), tensor([93]), tensor([92]), tensor([49]), tensor([92]), tensor([98]), tensor([62]), tensor([57]), tensor([5]), tensor([77]), tensor([72]), tensor([26]), tensor([49]), tensor([56]), tensor([56]), tensor([93]), tensor([91]), tensor([72]), tensor([46]), tensor([16]), tensor([7]), tensor([98]), tensor([33]), tensor([91]), tensor([11]), tensor([67]), tensor([80]), tensor([91]), tensor([71]), tensor([66]), tensor([56]), tensor([26]), tensor([48]), tensor([24]), tensor([58]), tensor([79]), tensor([53]), tensor([14]), tensor([30]), tensor([94]), tensor([52]), tensor([58]), tensor([62]), tensor([45]), tensor([67]), tensor([49]), tensor([10]), tensor([7]), tensor([73]), tensor([98]), tensor([41]), tensor([28]), tensor([67]), tensor([22]), tensor([12]), tensor([35]), tensor([48]), tensor([16]), tensor([1]), tensor([53]), tensor([57]), tensor([14]), tensor([88]), tensor([50]), tensor([37]), tensor([45]), tensor([3]), tensor([23]), tensor([66]), tensor([63]), tensor([65]), tensor([2]), tensor([12]), tensor([24]), tensor([66]), tensor([32]), tensor([58]), tensor([80]), tensor([34]), tensor([11]), tensor([16]), tensor([95]), tensor([95]), tensor([12]), tensor([48]), tensor([49]), tensor([17]), tensor([7]), tensor([5]), tensor([17]), tensor([94]), tensor([31]), tensor([52]), tensor([24]), tensor([11]), tensor([25]), tensor([93]), tensor([31]), tensor([45]), tensor([16]), tensor([66]), tensor([63]), tensor([46]), tensor([77]), tensor([96]), tensor([53]), tensor([1]), tensor([53]), tensor([2]), tensor([79]), tensor([0]), tensor([17]), tensor([57]), tensor([41]), tensor([34]), tensor([23]), tensor([41]), tensor([53]), tensor([65]), tensor([26]), tensor([71]), tensor([27]), tensor([67]), tensor([63]), tensor([5]), tensor([96]), tensor([56]), tensor([98]), tensor([97]), tensor([23]), tensor([59]), tensor([82]), tensor([34]), tensor([88]), tensor([71]), tensor([33]), tensor([33]), tensor([13]), tensor([66]), tensor([98]), tensor([14]), tensor([96]), tensor([96]), tensor([62]), tensor([73]), tensor([69]), tensor([17]), tensor([31]), tensor([21]), tensor([46]), tensor([26]), tensor([59]), tensor([21]), tensor([14]), tensor([52]), tensor([45]), tensor([1]), tensor([60]), tensor([6]), tensor([34]), tensor([77]), tensor([48]), tensor([11]), tensor([30]), tensor([53]), tensor([56]), tensor([52]), tensor([94]), tensor([56]), tensor([67]), tensor([68]), tensor([48]), tensor([36]), tensor([63]), tensor([36]), tensor([81]), tensor([77]), tensor([80]), tensor([35]), tensor([98]), tensor([41]), tensor([62]), tensor([82]), tensor([35]), tensor([59]), tensor([12]), tensor([50]), tensor([86]), tensor([62]), tensor([41]), tensor([36]), tensor([59]), tensor([98]), tensor([79]), tensor([16]), tensor([1]), tensor([62]), tensor([77]), tensor([0]), tensor([94]), tensor([58]), tensor([37]), tensor([24]), tensor([12]), tensor([2]), tensor([37]), tensor([95]), tensor([56]), tensor([57]), tensor([97]), tensor([34]), tensor([78]), tensor([92]), tensor([26]), tensor([0]), tensor([22]), tensor([10]), tensor([79]), tensor([3]), tensor([6]), tensor([73]), tensor([57]), tensor([46]), tensor([97]), tensor([58]), tensor([0]), tensor([16]), tensor([67]), tensor([37]), tensor([97]), tensor([46]), tensor([97]), tensor([53]), tensor([7]), tensor([77]), tensor([25]), tensor([50]), tensor([66]), tensor([31]), tensor([67]), tensor([65]), tensor([66]), tensor([33]), tensor([99]), tensor([86]), tensor([50]), tensor([98]), tensor([97]), tensor([71]), tensor([53]), tensor([71]), tensor([22]), tensor([81]), tensor([27]), tensor([33]), tensor([28]), tensor([80]), tensor([88]), tensor([27]), tensor([0]), tensor([93]), tensor([30]), tensor([60]), tensor([35]), tensor([88]), tensor([48]), tensor([63]), tensor([68]), tensor([59]), tensor([7]), tensor([95]), tensor([69]), tensor([93]), tensor([92]), tensor([36]), tensor([14]), tensor([86]), tensor([77]), tensor([45]), tensor([26]), tensor([28]), tensor([5]), tensor([78]), tensor([36]), tensor([53]), tensor([69]), tensor([66]), tensor([65]), tensor([13]), tensor([98]), tensor([71]), tensor([92]), tensor([58]), tensor([82]), tensor([32]), tensor([25]), tensor([82]), tensor([99]), tensor([34]), tensor([57]), tensor([93]), tensor([97]), tensor([68]), tensor([86]), tensor([68]), tensor([92]), tensor([34]), tensor([34]), tensor([80]), tensor([79]), tensor([58]), tensor([91]), tensor([37]), tensor([7]), tensor([78]), tensor([79]), tensor([11]), tensor([80]), tensor([56]), tensor([59]), tensor([35]), tensor([56]), tensor([88]), tensor([73]), tensor([0]), tensor([34]), tensor([17]), tensor([48]), tensor([50]), tensor([78]), tensor([94]), tensor([2]), tensor([12]), tensor([91]), tensor([79]), tensor([0]), tensor([95]), tensor([65]), tensor([2]), tensor([91]), tensor([71]), tensor([57]), tensor([11]), tensor([14]), tensor([99]), tensor([45]), tensor([81]), tensor([81]), tensor([46]), tensor([7]), tensor([79]), tensor([16]), tensor([27]), tensor([56]), tensor([26]), tensor([5]), tensor([73]), tensor([94]), tensor([72]), tensor([10]), tensor([62]), tensor([98]), tensor([36]), tensor([46]), tensor([49]), tensor([10]), tensor([57]), tensor([99]), tensor([69]), tensor([69]), tensor([59]), tensor([91]), tensor([77]), tensor([36]), tensor([60]), tensor([46]), tensor([81]), tensor([52]), tensor([92]), tensor([88]), tensor([32]), tensor([0]), tensor([49]), tensor([88]), tensor([32]), tensor([50]), tensor([5]), tensor([73]), tensor([95]), tensor([73]), tensor([57]), tensor([36]), tensor([48]), tensor([53]), tensor([30]), tensor([36]), tensor([30]), tensor([77]), tensor([49]), tensor([16]), tensor([22]), tensor([37]), tensor([81]), tensor([77]), tensor([12]), tensor([86]), tensor([23]), tensor([68]), tensor([25]), tensor([57]), tensor([16]), tensor([12]), tensor([93]), tensor([69]), tensor([50]), tensor([94]), tensor([63]), tensor([94]), tensor([97]), tensor([59]), tensor([63]), tensor([23]), tensor([32]), tensor([25]), tensor([13]), tensor([97]), tensor([79]), tensor([88]), tensor([35]), tensor([6]), tensor([6]), tensor([60]), tensor([73]), tensor([3]), tensor([79]), tensor([33]), tensor([92]), tensor([14]), tensor([67]), tensor([68]), tensor([69]), tensor([36]), tensor([60]), tensor([3]), tensor([23]), tensor([31]), tensor([21]), tensor([95]), tensor([80]), tensor([58]), tensor([41]), tensor([97]), tensor([36]), tensor([88]), tensor([3]), tensor([13]), tensor([33]), tensor([13]), tensor([14]), tensor([31]), tensor([23]), tensor([41]), tensor([12]), tensor([99]), tensor([37]), tensor([10]), tensor([57]), tensor([62]), tensor([11]), tensor([6]), tensor([2]), tensor([35]), tensor([52]), tensor([91]), tensor([77]), tensor([48]), tensor([41]), tensor([5]), tensor([23]), tensor([22]), tensor([14]), tensor([71]), tensor([59]), tensor([73]), tensor([25]), tensor([66]), tensor([56]), tensor([68]), tensor([45]), tensor([53]), tensor([33]), tensor([27]), tensor([71]), tensor([60]), tensor([36]), tensor([67]), tensor([53]), tensor([41]), tensor([60]), tensor([62]), tensor([2]), tensor([7]), tensor([24]), tensor([35]), tensor([77]), tensor([71]), tensor([27]), tensor([27]), tensor([65]), tensor([49]), tensor([12]), tensor([33]), tensor([62]), tensor([68]), tensor([48]), tensor([88]), tensor([78]), tensor([88]), tensor([58]), tensor([81]), tensor([92]), tensor([56]), tensor([79]), tensor([50]), tensor([37]), tensor([45]), tensor([56]), tensor([26]), tensor([10]), tensor([86]), tensor([80]), tensor([62]), tensor([63]), tensor([2]), tensor([13]), tensor([82]), tensor([5]), tensor([53]), tensor([45]), tensor([16]), tensor([10]), tensor([68]), tensor([46]), tensor([2]), tensor([30]), tensor([49]), tensor([79]), tensor([33]), tensor([10]), tensor([21]), tensor([0]), tensor([56]), tensor([69]), tensor([14]), tensor([37]), tensor([3]), tensor([46]), tensor([98]), tensor([13]), tensor([62]), tensor([28]), tensor([22]), tensor([1]), tensor([92]), tensor([35]), tensor([91]), tensor([27]), tensor([78]), tensor([82]), tensor([80]), tensor([27]), tensor([91]), tensor([67]), tensor([3]), tensor([45]), tensor([50]), tensor([0]), tensor([24]), tensor([81]), tensor([59]), tensor([72]), tensor([88]), tensor([92]), tensor([67]), tensor([69]), tensor([17]), tensor([92]), tensor([41]), tensor([1]), tensor([5]), tensor([78]), tensor([13]), tensor([25]), tensor([71]), tensor([7]), tensor([80]), tensor([86]), tensor([52]), tensor([26]), tensor([66]), tensor([6]), tensor([31]), tensor([6]), tensor([72]), tensor([99]), tensor([94]), tensor([60]), tensor([65]), tensor([25]), tensor([59]), tensor([73]), tensor([7]), tensor([26]), tensor([12]), tensor([62]), tensor([37]), tensor([50]), tensor([82]), tensor([33]), tensor([49]), tensor([62]), tensor([7]), tensor([31]), tensor([50]), tensor([2]), tensor([91]), tensor([78]), tensor([91]), tensor([63]), tensor([22]), tensor([58]), tensor([32]), tensor([49]), tensor([60]), tensor([37]), tensor([37]), tensor([10]), tensor([36]), tensor([59]), tensor([96]), tensor([31]), tensor([93]), tensor([82]), tensor([86]), tensor([80]), tensor([71]), tensor([69]), tensor([88]), tensor([24]), tensor([95]), tensor([73]), tensor([52]), tensor([28]), tensor([7]), tensor([94]), tensor([14]), tensor([48]), tensor([13]), tensor([52]), tensor([77]), tensor([26]), tensor([10]), tensor([32]), tensor([3]), tensor([2]), tensor([95]), tensor([52]), tensor([48]), tensor([79]), tensor([99]), tensor([66]), tensor([41]), tensor([5]), tensor([11]), tensor([56]), tensor([24]), tensor([80]), tensor([99]), tensor([72]), tensor([31]), tensor([6]), tensor([95]), tensor([37]), tensor([6]), tensor([59]), tensor([30]), tensor([53]), tensor([98]), tensor([86]), tensor([88]), tensor([58]), tensor([41]), tensor([98]), tensor([35]), tensor([91]), tensor([60]), tensor([80]), tensor([28]), tensor([60]), tensor([14]), tensor([82]), tensor([16]), tensor([41]), tensor([68]), tensor([97]), tensor([65]), tensor([22]), tensor([95]), tensor([34]), tensor([96]), tensor([98]), tensor([80]), tensor([3]), tensor([2]), tensor([11]), tensor([91]), tensor([34]), tensor([30]), tensor([60]), tensor([82]), tensor([81]), tensor([59]), tensor([78]), tensor([52]), tensor([2]), tensor([45]), tensor([1]), tensor([5]), tensor([24]), tensor([48]), tensor([48]), tensor([11]), tensor([21]), tensor([65]), tensor([94]), tensor([93]), tensor([21]), tensor([33]), tensor([78]), tensor([21]), tensor([69]), tensor([93]), tensor([2]), tensor([37]), tensor([93]), tensor([6]), tensor([24]), tensor([71]), tensor([46]), tensor([53]), tensor([46]), tensor([65]), tensor([23]), tensor([53]), tensor([69]), tensor([59]), tensor([77]), tensor([10]), tensor([65]), tensor([5]), tensor([23]), tensor([46]), tensor([6]), tensor([34]), tensor([41]), tensor([79]), tensor([48]), tensor([95]), tensor([82]), tensor([31]), tensor([0]), tensor([49]), tensor([28]), tensor([99]), tensor([1]), tensor([7]), tensor([72]), tensor([92]), tensor([13]), tensor([17]), tensor([46]), tensor([91]), tensor([63]), tensor([99]), tensor([30]), tensor([11]), tensor([68]), tensor([26]), tensor([21]), tensor([17]), tensor([72]), tensor([67]), tensor([30]), tensor([52]), tensor([13]), tensor([81]), tensor([25]), tensor([81]), tensor([53]), tensor([7]), tensor([21]), tensor([34]), tensor([46]), tensor([92]), tensor([32]), tensor([50]), tensor([62]), tensor([97]), tensor([63]), tensor([92]), tensor([97]), tensor([60]), tensor([0]), tensor([32]), tensor([12]), tensor([7]), tensor([97]), tensor([48]), tensor([25]), tensor([88]), tensor([22]), tensor([56]), tensor([30]), tensor([96]), tensor([94]), tensor([88]), tensor([95]), tensor([0]), tensor([3]), tensor([13]), tensor([27]), tensor([79]), tensor([22]), tensor([30]), tensor([63]), tensor([14]), tensor([53]), tensor([97]), tensor([37]), tensor([27]), tensor([36]), tensor([41]), tensor([31]), tensor([5]), tensor([11]), tensor([25]), tensor([95]), tensor([41]), tensor([52]), tensor([25]), tensor([81]), tensor([25]), tensor([7]), tensor([69]), tensor([80]), tensor([73]), tensor([36]), tensor([5]), tensor([97]), tensor([16]), tensor([41]), tensor([23]), tensor([57]), tensor([37]), tensor([28]), tensor([30]), tensor([69]), tensor([28]), tensor([0]), tensor([11]), tensor([22]), tensor([46]), tensor([69]), tensor([17]), tensor([6]), tensor([50]), tensor([97]), tensor([24]), tensor([81]), tensor([66]), tensor([14]), tensor([1]), tensor([25]), tensor([21]), tensor([30]), tensor([36]), tensor([80]), tensor([71]), tensor([65]), tensor([30]), tensor([67]), tensor([48]), tensor([91]), tensor([25]), tensor([81]), tensor([94]), tensor([66]), tensor([33]), tensor([46]), tensor([92]), tensor([28]), tensor([72]), tensor([14]), tensor([23]), tensor([97]), tensor([17]), tensor([16]), tensor([68]), tensor([88]), tensor([1]), tensor([88]), tensor([86]), tensor([77]), tensor([88]), tensor([46]), tensor([22]), tensor([13]), tensor([63]), tensor([30]), tensor([41]), tensor([58]), tensor([86]), tensor([63]), tensor([72]), tensor([14]), tensor([1]), tensor([82]), tensor([1]), tensor([94]), tensor([50]), tensor([7]), tensor([73]), tensor([58]), tensor([48]), tensor([27]), tensor([37]), tensor([79]), tensor([22]), tensor([66]), tensor([80]), tensor([92]), tensor([24]), tensor([72]), tensor([49]), tensor([98]), tensor([92]), tensor([49]), tensor([98]), tensor([2]), tensor([16]), tensor([60]), tensor([78]), tensor([7]), tensor([69]), tensor([71]), tensor([82]), tensor([31]), tensor([96]), tensor([97]), tensor([57]), tensor([63]), tensor([63]), tensor([82]), tensor([31]), tensor([35]), tensor([63]), tensor([81]), tensor([93]), tensor([27]), tensor([23]), tensor([98]), tensor([69]), tensor([5]), tensor([57]), tensor([72]), tensor([86]), tensor([5]), tensor([59]), tensor([23]), tensor([10]), tensor([31]), tensor([12]), tensor([45]), tensor([21]), tensor([28]), tensor([32]), tensor([35]), tensor([99]), tensor([36]), tensor([66]), tensor([49]), tensor([45]), tensor([17]), tensor([13]), tensor([33]), tensor([12]), tensor([32]), tensor([63]), tensor([22]), tensor([92]), tensor([60]), tensor([53]), tensor([45]), tensor([65]), tensor([17]), tensor([72]), tensor([52]), tensor([52]), tensor([79]), tensor([12]), tensor([13]), tensor([34]), tensor([11]), tensor([45]), tensor([82]), tensor([22]), tensor([6]), tensor([46]), tensor([13]), tensor([99]), tensor([99]), tensor([82]), tensor([62]), tensor([27]), tensor([24]), tensor([63]), tensor([96]), tensor([28]), tensor([33]), tensor([80]), tensor([99]), tensor([82]), tensor([3]), tensor([99]), tensor([17]), tensor([58]), tensor([79]), tensor([57]), tensor([30]), tensor([99]), tensor([78]), tensor([65]), tensor([93]), tensor([95]), tensor([66]), tensor([53]), tensor([25]), tensor([13]), tensor([22]), tensor([5]), tensor([59]), tensor([37]), tensor([95]), tensor([66]), tensor([60]), tensor([3]), tensor([48]), tensor([6]), tensor([96]), tensor([50]), tensor([48]), tensor([91]), tensor([98]), tensor([28]), tensor([81]), tensor([56]), tensor([30]), tensor([3]), tensor([68]), tensor([22]), tensor([23]), tensor([63]), tensor([72]), tensor([58]), tensor([65]), tensor([67]), tensor([10]), tensor([91]), tensor([36]), tensor([24]), tensor([68]), tensor([78]), tensor([5]), tensor([69]), tensor([34]), tensor([94]), tensor([96]), tensor([24]), tensor([98]), tensor([34]), tensor([2]), tensor([33]), tensor([52]), tensor([69]), tensor([81]), tensor([28]), tensor([14]), tensor([96]), tensor([21]), tensor([33]), tensor([21]), tensor([24]), tensor([2]), tensor([72]), tensor([32]), tensor([16]), tensor([27]), tensor([31]), tensor([25]), tensor([66]), tensor([49]), tensor([41]), tensor([17]), tensor([1]), tensor([63]), tensor([62]), tensor([1]), tensor([28]), tensor([10]), tensor([65]), tensor([73]), tensor([11]), tensor([1]), tensor([81]), tensor([65]), tensor([28]), tensor([81]), tensor([45]), tensor([57]), tensor([30]), tensor([49]), tensor([35]), tensor([26]), tensor([59]), tensor([10]), tensor([57]), tensor([77]), tensor([7]), tensor([96]), tensor([36]), tensor([77]), tensor([31]), tensor([73]), tensor([49]), tensor([45]), tensor([35]), tensor([56]), tensor([6]), tensor([6]), tensor([21]), tensor([92]), tensor([86]), tensor([53]), tensor([50]), tensor([92]), tensor([33]), tensor([13]), tensor([34]), tensor([69]), tensor([25]), tensor([27]), tensor([71]), tensor([66]), tensor([35]), tensor([7]), tensor([80]), tensor([45]), tensor([94]), tensor([49]), tensor([96]), tensor([73]), tensor([72]), tensor([53]), tensor([73]), tensor([17]), tensor([48]), tensor([45]), tensor([0]), tensor([63]), tensor([1]), tensor([97]), tensor([32]), tensor([81]), tensor([58]), tensor([78]), tensor([49]), tensor([45]), tensor([77]), tensor([17]), tensor([24]), tensor([50]), tensor([25]), tensor([16]), tensor([26]), tensor([6]), tensor([77]), tensor([67]), tensor([73]), tensor([32]), tensor([69]), tensor([17]), tensor([3]), tensor([91]), tensor([22]), tensor([86]), tensor([88]), tensor([78]), tensor([96]), tensor([21]), tensor([50]), tensor([2]), tensor([94]), tensor([28]), tensor([59]), tensor([60]), tensor([86]), tensor([30]), tensor([46]), tensor([95]), tensor([23]), tensor([26]), tensor([81]), tensor([79]), tensor([7]), tensor([73]), tensor([33]), tensor([25]), tensor([27]), tensor([59]), tensor([10]), tensor([22]), tensor([52]), tensor([26]), tensor([72]), tensor([49]), tensor([12]), tensor([56]), tensor([21]), tensor([80]), tensor([86]), tensor([34]), tensor([94]), tensor([34]), tensor([49]), tensor([35]), tensor([56]), tensor([50]), tensor([58]), tensor([82]), tensor([45]), tensor([48]), tensor([36]), tensor([93]), tensor([52]), tensor([67]), tensor([3]), tensor([62]), tensor([60]), tensor([2]), tensor([88]), tensor([49]), tensor([79]), tensor([86]), tensor([45]), tensor([35]), tensor([65]), tensor([22]), tensor([93]), tensor([30]), tensor([49]), tensor([91]), tensor([0]), tensor([34]), tensor([2]), tensor([16]), tensor([21]), tensor([36]), tensor([45]), tensor([5]), tensor([99]), tensor([97]), tensor([53]), tensor([78]), tensor([25]), tensor([30]), tensor([32]), tensor([33]), tensor([77]), tensor([27]), tensor([73]), tensor([67]), tensor([68]), tensor([17]), tensor([60]), tensor([52]), tensor([6]), tensor([27]), tensor([94]), tensor([93]), tensor([23]), tensor([10]), tensor([32]), tensor([32]), tensor([41]), tensor([7]), tensor([21]), tensor([22]), tensor([7]), tensor([60]), tensor([80]), tensor([28]), tensor([2]), tensor([17]), tensor([96]), tensor([86]), tensor([17]), tensor([32]), tensor([35]), tensor([36]), tensor([23]), tensor([12]), tensor([3]), tensor([88]), tensor([17]), tensor([71]), tensor([57]), tensor([52]), tensor([6]), tensor([95]), tensor([60]), tensor([56]), tensor([14]), tensor([3]), tensor([79]), tensor([60]), tensor([88]), tensor([86]), tensor([0]), tensor([45]), tensor([93]), tensor([94]), tensor([26]), tensor([91]), tensor([59]), tensor([57]), tensor([28]), tensor([62]), tensor([65]), tensor([95]), tensor([11]), tensor([82]), tensor([91]), tensor([3]), tensor([93]), tensor([16]), tensor([30]), tensor([14]), tensor([98]), tensor([91]), tensor([91]), tensor([69]), tensor([26]), tensor([24]), tensor([24]), tensor([98]), tensor([25]), tensor([81]), tensor([60]), tensor([68]), tensor([86]), tensor([63]), tensor([16]), tensor([73]), tensor([32]), tensor([52]), tensor([92]), tensor([13]), tensor([10]), tensor([99]), tensor([56]), tensor([37]), tensor([52]), tensor([58]), tensor([6]), tensor([52]), tensor([34]), tensor([71]), tensor([65]), tensor([98]), tensor([14]), tensor([65]), tensor([52]), tensor([35]), tensor([65]), tensor([93]), tensor([72]), tensor([66]), tensor([12]), tensor([35]), tensor([56]), tensor([99]), tensor([48]), tensor([68]), tensor([21]), tensor([28]), tensor([58]), tensor([82]), tensor([60]), tensor([0]), tensor([52]), tensor([5]), tensor([99]), tensor([63]), tensor([41]), tensor([1]), tensor([37]), tensor([33]), tensor([82]), tensor([11]), tensor([67]), tensor([91]), tensor([71]), tensor([62]), tensor([23]), tensor([22]), tensor([66]), tensor([93]), tensor([3]), tensor([93]), tensor([2]), tensor([0]), tensor([95]), tensor([52]), tensor([65]), tensor([80]), tensor([12]), tensor([73]), tensor([66]), tensor([24]), tensor([14]), tensor([66]), tensor([21]), tensor([12]), tensor([23]), tensor([32]), tensor([67]), tensor([78]), tensor([72]), tensor([14]), tensor([34]), tensor([49]), tensor([58]), tensor([10]), tensor([0]), tensor([1]), tensor([23]), tensor([86]), tensor([94]), tensor([21]), tensor([26]), tensor([10]), tensor([30]), tensor([97]), tensor([48]), tensor([59]), tensor([16]), tensor([24]), tensor([96]), tensor([22]), tensor([53]), tensor([59]), tensor([17]), tensor([41]), tensor([79]), tensor([99]), tensor([1]), tensor([35]), tensor([50]), tensor([80]), tensor([93]), tensor([57]), tensor([60]), tensor([48]), tensor([24]), tensor([31]), tensor([31]), tensor([50]), tensor([17]), tensor([52]), tensor([26]), tensor([67]), tensor([25]), tensor([14]), tensor([46]), tensor([66]), tensor([22]), tensor([79]), tensor([49]), tensor([24]), tensor([52]), tensor([26]), tensor([71]), tensor([24]), tensor([33]), tensor([33]), tensor([31]), tensor([12]), tensor([35]), tensor([41]), tensor([24]), tensor([11]), tensor([68]), tensor([62]), tensor([95]), tensor([1]), tensor([32]), tensor([5]), tensor([98]), tensor([68]), tensor([67]), tensor([99]), tensor([41]), tensor([82]), tensor([63]), tensor([62]), tensor([14]), tensor([98]), tensor([96]), tensor([3]), tensor([69]), tensor([97]), tensor([32]), tensor([17]), tensor([2]), tensor([36]), tensor([16]), tensor([33]), tensor([1]), tensor([69]), tensor([62]), tensor([94]), tensor([57]), tensor([34]), tensor([37]), tensor([77]), tensor([3]), tensor([86]), tensor([16]), tensor([57]), tensor([36]), tensor([22]), tensor([95]), tensor([68]), tensor([78]), tensor([58]), tensor([22]), tensor([22]), tensor([58]), tensor([7]), tensor([21]), tensor([63]), tensor([60]), tensor([69]), tensor([96]), tensor([92]), tensor([16]), tensor([92]), tensor([88]), tensor([72]), tensor([59]), tensor([34]), tensor([34]), tensor([27]), tensor([45]), tensor([6]), tensor([96]), tensor([37]), tensor([10]), tensor([1]), tensor([82]), tensor([34]), tensor([25]), tensor([35]), tensor([27]), tensor([86]), tensor([24]), tensor([27]), tensor([57]), tensor([6]), tensor([12]), tensor([13]), tensor([5]), tensor([11]), tensor([95]), tensor([97]), tensor([88]), tensor([6]), tensor([23]), tensor([72]), tensor([96]), tensor([0]), tensor([21]), tensor([78]), tensor([71]), tensor([65]), tensor([57]), tensor([96]), tensor([80]), tensor([5]), tensor([32]), tensor([13]), tensor([95]), tensor([71]), tensor([79]), tensor([28]), tensor([82]), tensor([13]), tensor([96]), tensor([78]), tensor([13]), tensor([81]), tensor([98]), tensor([0]), tensor([94]), tensor([21]), tensor([92]), tensor([73]), tensor([25]), tensor([82]), tensor([72]), tensor([11]), tensor([86]), tensor([26]), tensor([88]), tensor([71]), tensor([1]), tensor([93]), tensor([27]), tensor([98]), tensor([30]), tensor([99]), tensor([80]), tensor([58]), tensor([2]), tensor([77]), tensor([69]), tensor([3]), tensor([31]), tensor([72]), tensor([71]), tensor([1]), tensor([7]), tensor([10]), tensor([66]), tensor([96]), tensor([92]), tensor([67]), tensor([45]), tensor([25]), tensor([65]), tensor([50]), tensor([14]), tensor([23]), tensor([57]), tensor([68]), tensor([77]), tensor([26]), tensor([11]), tensor([73]), tensor([57]), tensor([98]), tensor([78]), tensor([14]), tensor([96]), tensor([68]), tensor([78]), tensor([72]), tensor([26]), tensor([92]), tensor([86]), tensor([49]), tensor([5]), tensor([13]), tensor([6]), tensor([23]), tensor([79]), tensor([35]), tensor([62]), tensor([67]), tensor([11]), tensor([46]), tensor([69]), tensor([96]), tensor([34]), tensor([1]), tensor([93]), tensor([28]), tensor([16]), tensor([27]), tensor([99]), tensor([10]), tensor([86]), tensor([26]), tensor([68]), tensor([3]), tensor([23]), tensor([78]), tensor([53]), tensor([93]), tensor([0]), tensor([32]), tensor([0]), tensor([5]), tensor([3]), tensor([80]), tensor([81]), tensor([28]), tensor([35]), tensor([2]), tensor([91]), tensor([62]), tensor([71]), tensor([45]), tensor([59]), tensor([96]), tensor([58]), tensor([97]), tensor([0]), tensor([63]), tensor([56]), tensor([46]), tensor([67]), tensor([37]), tensor([71]), tensor([48]), tensor([81]), tensor([3]), tensor([35]), tensor([31]), tensor([2]), tensor([37]), tensor([65]), tensor([16]), tensor([94]), tensor([67]), tensor([65]), tensor([10]), tensor([80]), tensor([13]), tensor([95]), tensor([53]), tensor([28]), tensor([28]), tensor([32]), tensor([11]), tensor([73]), tensor([32]), tensor([46]), tensor([86]), tensor([60]), tensor([6]), tensor([12]), tensor([82]), tensor([10]), tensor([94]), tensor([68]), tensor([86]), tensor([14]), tensor([27]), tensor([67]), tensor([16]), tensor([77]), tensor([21]), tensor([56]), tensor([67]), tensor([0]), tensor([93]), tensor([71]), tensor([60]), tensor([31]), tensor([17]), tensor([37]), tensor([78]), tensor([53]), tensor([12]), tensor([73]), tensor([65]), tensor([79]), tensor([91]), tensor([73]), tensor([1]), tensor([30]), tensor([41]), tensor([96]), tensor([50]), tensor([31]), tensor([3]), tensor([48]), tensor([31]), tensor([0]), tensor([68]), tensor([53]), tensor([95]), tensor([17]), tensor([67]), tensor([12]), tensor([77]), tensor([36]), tensor([46]), tensor([33]), tensor([93]), tensor([7]), tensor([28]), tensor([32]), tensor([11]), tensor([58]), tensor([31]), tensor([37]), tensor([25]), tensor([12]), tensor([24]), tensor([3]), tensor([59]), tensor([34]), tensor([71]), tensor([97]), tensor([66]), tensor([1]), tensor([73]), tensor([80]), tensor([45]), tensor([81]), tensor([41]), tensor([36]), tensor([62]), tensor([17]), tensor([46]), tensor([24]), tensor([13]), tensor([28]), tensor([97]), tensor([68]), tensor([32]), tensor([97]), tensor([35]), tensor([28]), tensor([49]), tensor([99]), tensor([66]), tensor([62]), tensor([99]), tensor([3]), tensor([50]), tensor([94]), tensor([56]), tensor([56]), tensor([23]), tensor([25]), tensor([99]), tensor([82]), tensor([11]), tensor([68]), tensor([77]), tensor([37]), tensor([2]), tensor([50]), tensor([6]), tensor([50]), tensor([21]), tensor([27]), tensor([58]), tensor([57]), tensor([78]), tensor([68]), tensor([26]), tensor([78]), tensor([59]), tensor([23]), tensor([11]), tensor([56]), tensor([66]), tensor([98]), tensor([13]), tensor([88]), tensor([50]), tensor([33]), tensor([5]), tensor([77]), tensor([36]), tensor([78]), tensor([10]), tensor([36]), tensor([30]), tensor([21]), tensor([63]), tensor([22]), tensor([77]), tensor([58]), tensor([14]), tensor([82]), tensor([57]), tensor([27]), tensor([30]), tensor([41]), tensor([2]), tensor([50]), tensor([35]), tensor([59]), tensor([10]), tensor([78]), tensor([6]), tensor([6]), tensor([91]), tensor([95]), tensor([5]), tensor([79]), tensor([10]), tensor([12]), tensor([58]), tensor([21]), tensor([69]), tensor([16]), tensor([27]), tensor([72]), tensor([46]), tensor([72]), tensor([81]), tensor([26]), tensor([46]), tensor([92]), tensor([95]), tensor([7]), tensor([12]), tensor([79]), tensor([62]), tensor([81]), tensor([31]), tensor([48]), tensor([26]), tensor([93]), tensor([17]), tensor([72]), tensor([31]), tensor([0]), tensor([88]), tensor([41]), tensor([94]), tensor([11]), tensor([16]), tensor([99]), tensor([5]), tensor([13]), tensor([94]), tensor([33]), tensor([1]), tensor([11]), tensor([72]), tensor([37])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.54 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.53\n",
            "TEST ALL:  0.47\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  8000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [99, 95, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 65, 69, 73, 77, 81, 93, 13, 5, 1, 48, 4, 12, 16, 24, 28, 32, 36, 52, 96, 56, 60, 68, 72, 80, 88, 92, 97, 2, 6, 55, 11, 23, 27, 31, 35, 39, 47, 59, 3, 63, 67, 71, 75, 79, 87, 91, 7, 98, 10, 46, 14, 22, 26, 30, 34, 38, 42, 50, 94, 58, 62, 66, 70, 78, 82, 86, 0]\n",
            "TRAIN_SET CLASSES:  [87, 75, 55, 47, 39, 70, 42, 38, 29, 4]\n",
            "VALIDATION CLASSES:  [55, 47, 42, 39, 38, 29, 87, 75, 70, 4]\n",
            "GROUP:  8\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [87, 75, 55, 47, 39, 70, 42, 38, 29, 4]\n",
            "Len TOTAL train susbset:  6910\n",
            "training\n",
            "num classes till now:  80\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.18877437710762024\n",
            "Train step - Step 10, Loss 0.13897837698459625\n",
            "Train step - Step 20, Loss 0.13506563007831573\n",
            "Train step - Step 30, Loss 0.12165265530347824\n",
            "Train step - Step 40, Loss 0.11963336914777756\n",
            "Train step - Step 50, Loss 0.11664227396249771\n",
            "Train epoch - Accuracy: 0.15224312590448624 Loss: 0.13181550753789426 Corrects: 1052\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12257011979818344\n",
            "Train step - Step 70, Loss 0.11895585060119629\n",
            "Train step - Step 80, Loss 0.11448218673467636\n",
            "Train step - Step 90, Loss 0.11810316145420074\n",
            "Train step - Step 100, Loss 0.12154944986104965\n",
            "Train epoch - Accuracy: 0.17206946454413893 Loss: 0.11720298824969318 Corrects: 1189\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11729583889245987\n",
            "Train step - Step 120, Loss 0.1137746199965477\n",
            "Train step - Step 130, Loss 0.11708809435367584\n",
            "Train step - Step 140, Loss 0.10993082821369171\n",
            "Train step - Step 150, Loss 0.11672241985797882\n",
            "Train step - Step 160, Loss 0.11470808833837509\n",
            "Train epoch - Accuracy: 0.19942112879884225 Loss: 0.11555831765731751 Corrects: 1378\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.11224160343408585\n",
            "Train step - Step 180, Loss 0.11342840641736984\n",
            "Train step - Step 190, Loss 0.11865609139204025\n",
            "Train step - Step 200, Loss 0.11169041693210602\n",
            "Train step - Step 210, Loss 0.11365269869565964\n",
            "Train epoch - Accuracy: 0.22416787264833574 Loss: 0.11384046411333139 Corrects: 1549\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11927373707294464\n",
            "Train step - Step 230, Loss 0.11188855022192001\n",
            "Train step - Step 240, Loss 0.11578192561864853\n",
            "Train step - Step 250, Loss 0.11404675245285034\n",
            "Train step - Step 260, Loss 0.1136101707816124\n",
            "Train epoch - Accuracy: 0.23950795947901593 Loss: 0.11325907235380192 Corrects: 1655\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 270, Loss 0.114049032330513\n",
            "Train step - Step 280, Loss 0.11596884578466415\n",
            "Train step - Step 290, Loss 0.11020974069833755\n",
            "Train step - Step 300, Loss 0.1126345619559288\n",
            "Train step - Step 310, Loss 0.10639949887990952\n",
            "Train step - Step 320, Loss 0.11154326051473618\n",
            "Train epoch - Accuracy: 0.25007235890014473 Loss: 0.11291875900692602 Corrects: 1728\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11057982593774796\n",
            "Train step - Step 340, Loss 0.11529035866260529\n",
            "Train step - Step 350, Loss 0.11318513005971909\n",
            "Train step - Step 360, Loss 0.10862749069929123\n",
            "Train step - Step 370, Loss 0.10545235127210617\n",
            "Train epoch - Accuracy: 0.2635311143270622 Loss: 0.11264116633602576 Corrects: 1821\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 380, Loss 0.1068003922700882\n",
            "Train step - Step 390, Loss 0.11561935395002365\n",
            "Train step - Step 400, Loss 0.11791229248046875\n",
            "Train step - Step 410, Loss 0.1124526783823967\n",
            "Train step - Step 420, Loss 0.11663001030683517\n",
            "Train step - Step 430, Loss 0.11091581732034683\n",
            "Train epoch - Accuracy: 0.27554269175108537 Loss: 0.11200389404459041 Corrects: 1904\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10814151912927628\n",
            "Train step - Step 450, Loss 0.10851352661848068\n",
            "Train step - Step 460, Loss 0.10829134285449982\n",
            "Train step - Step 470, Loss 0.1099938303232193\n",
            "Train step - Step 480, Loss 0.10776803642511368\n",
            "Train epoch - Accuracy: 0.287698986975398 Loss: 0.11196243667223002 Corrects: 1988\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 490, Loss 0.11525364220142365\n",
            "Train step - Step 500, Loss 0.11156954616308212\n",
            "Train step - Step 510, Loss 0.1119389757514\n",
            "Train step - Step 520, Loss 0.10570404678583145\n",
            "Train step - Step 530, Loss 0.10835955291986465\n",
            "Train epoch - Accuracy: 0.29088277858176553 Loss: 0.11163510792550406 Corrects: 2010\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 540, Loss 0.11030705273151398\n",
            "Train step - Step 550, Loss 0.11722107976675034\n",
            "Train step - Step 560, Loss 0.10809045284986496\n",
            "Train step - Step 570, Loss 0.11295860260725021\n",
            "Train step - Step 580, Loss 0.10874038189649582\n",
            "Train step - Step 590, Loss 0.11010672152042389\n",
            "Train epoch - Accuracy: 0.30274963820549927 Loss: 0.11145392227837042 Corrects: 2092\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 600, Loss 0.11958513408899307\n",
            "Train step - Step 610, Loss 0.10859810560941696\n",
            "Train step - Step 620, Loss 0.11197829246520996\n",
            "Train step - Step 630, Loss 0.10683133453130722\n",
            "Train step - Step 640, Loss 0.11118190735578537\n",
            "Train epoch - Accuracy: 0.3114327062228654 Loss: 0.1112434090671767 Corrects: 2152\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 650, Loss 0.10845135897397995\n",
            "Train step - Step 660, Loss 0.1129557266831398\n",
            "Train step - Step 670, Loss 0.1106153056025505\n",
            "Train step - Step 680, Loss 0.10845481604337692\n",
            "Train step - Step 690, Loss 0.1044943556189537\n",
            "Train step - Step 700, Loss 0.11082782596349716\n",
            "Train epoch - Accuracy: 0.3118668596237337 Loss: 0.11107524241793863 Corrects: 2155\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.11582052707672119\n",
            "Train step - Step 720, Loss 0.113172248005867\n",
            "Train step - Step 730, Loss 0.10862045735120773\n",
            "Train step - Step 740, Loss 0.109480120241642\n",
            "Train step - Step 750, Loss 0.10628721863031387\n",
            "Train epoch - Accuracy: 0.3251808972503618 Loss: 0.11104260347082714 Corrects: 2247\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 760, Loss 0.1101221814751625\n",
            "Train step - Step 770, Loss 0.11296570301055908\n",
            "Train step - Step 780, Loss 0.11297651380300522\n",
            "Train step - Step 790, Loss 0.10916514694690704\n",
            "Train step - Step 800, Loss 0.10859381407499313\n",
            "Train epoch - Accuracy: 0.32141823444283646 Loss: 0.11082374317256138 Corrects: 2221\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 810, Loss 0.11223359405994415\n",
            "Train step - Step 820, Loss 0.10595257580280304\n",
            "Train step - Step 830, Loss 0.10799863189458847\n",
            "Train step - Step 840, Loss 0.10465947538614273\n",
            "Train step - Step 850, Loss 0.11538256704807281\n",
            "Train step - Step 860, Loss 0.11219199001789093\n",
            "Train epoch - Accuracy: 0.331548480463097 Loss: 0.1104595955606825 Corrects: 2291\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 870, Loss 0.10834157466888428\n",
            "Train step - Step 880, Loss 0.10818839073181152\n",
            "Train step - Step 890, Loss 0.10934609174728394\n",
            "Train step - Step 900, Loss 0.10556007921695709\n",
            "Train step - Step 910, Loss 0.10915684700012207\n",
            "Train epoch - Accuracy: 0.3344428364688857 Loss: 0.11048388579043569 Corrects: 2311\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 920, Loss 0.10807575285434723\n",
            "Train step - Step 930, Loss 0.11338410526514053\n",
            "Train step - Step 940, Loss 0.10647459328174591\n",
            "Train step - Step 950, Loss 0.10687962919473648\n",
            "Train step - Step 960, Loss 0.11056814342737198\n",
            "Train step - Step 970, Loss 0.10420294106006622\n",
            "Train epoch - Accuracy: 0.34327062228654126 Loss: 0.11043839996785745 Corrects: 2372\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.11108999699354172\n",
            "Train step - Step 990, Loss 0.10737981647253036\n",
            "Train step - Step 1000, Loss 0.11469914764165878\n",
            "Train step - Step 1010, Loss 0.1078031063079834\n",
            "Train step - Step 1020, Loss 0.10543491691350937\n",
            "Train epoch - Accuracy: 0.3512301013024602 Loss: 0.11007602762339257 Corrects: 2427\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1030, Loss 0.10897510498762131\n",
            "Train step - Step 1040, Loss 0.10563447326421738\n",
            "Train step - Step 1050, Loss 0.10892342776060104\n",
            "Train step - Step 1060, Loss 0.10885541886091232\n",
            "Train step - Step 1070, Loss 0.11035964637994766\n",
            "Train epoch - Accuracy: 0.3577424023154848 Loss: 0.10994110212657282 Corrects: 2472\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1080, Loss 0.11072786897420883\n",
            "Train step - Step 1090, Loss 0.10603529214859009\n",
            "Train step - Step 1100, Loss 0.1095859557390213\n",
            "Train step - Step 1110, Loss 0.11078665405511856\n",
            "Train step - Step 1120, Loss 0.11021792143583298\n",
            "Train step - Step 1130, Loss 0.1121165081858635\n",
            "Train epoch - Accuracy: 0.356150506512301 Loss: 0.10994941983614576 Corrects: 2461\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.10715055465698242\n",
            "Train step - Step 1150, Loss 0.11241938173770905\n",
            "Train step - Step 1160, Loss 0.10953662544488907\n",
            "Train step - Step 1170, Loss 0.1085326224565506\n",
            "Train step - Step 1180, Loss 0.10765276104211807\n",
            "Train epoch - Accuracy: 0.36005788712011577 Loss: 0.1098044949873484 Corrects: 2488\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1190, Loss 0.10631263256072998\n",
            "Train step - Step 1200, Loss 0.11345267295837402\n",
            "Train step - Step 1210, Loss 0.11322277784347534\n",
            "Train step - Step 1220, Loss 0.11319706588983536\n",
            "Train step - Step 1230, Loss 0.1112300381064415\n",
            "Train step - Step 1240, Loss 0.11705620586872101\n",
            "Train epoch - Accuracy: 0.36642547033285094 Loss: 0.10962316451214157 Corrects: 2532\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.11533010005950928\n",
            "Train step - Step 1260, Loss 0.10831663757562637\n",
            "Train step - Step 1270, Loss 0.1098090410232544\n",
            "Train step - Step 1280, Loss 0.10402288287878036\n",
            "Train step - Step 1290, Loss 0.10968201607465744\n",
            "Train epoch - Accuracy: 0.37062228654124457 Loss: 0.10971309324206216 Corrects: 2561\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1300, Loss 0.11268782615661621\n",
            "Train step - Step 1310, Loss 0.10507091134786606\n",
            "Train step - Step 1320, Loss 0.11588820070028305\n",
            "Train step - Step 1330, Loss 0.10493246465921402\n",
            "Train step - Step 1340, Loss 0.10745327919721603\n",
            "Train epoch - Accuracy: 0.373082489146165 Loss: 0.10967385459437902 Corrects: 2578\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1350, Loss 0.10829194635152817\n",
            "Train step - Step 1360, Loss 0.11170685291290283\n",
            "Train step - Step 1370, Loss 0.1088087186217308\n",
            "Train step - Step 1380, Loss 0.10379566252231598\n",
            "Train step - Step 1390, Loss 0.10730981081724167\n",
            "Train step - Step 1400, Loss 0.1109725832939148\n",
            "Train epoch - Accuracy: 0.37858176555716355 Loss: 0.1095731199564016 Corrects: 2616\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.10803400725126266\n",
            "Train step - Step 1420, Loss 0.10524880886077881\n",
            "Train step - Step 1430, Loss 0.10485787689685822\n",
            "Train step - Step 1440, Loss 0.11164146661758423\n",
            "Train step - Step 1450, Loss 0.11047635227441788\n",
            "Train epoch - Accuracy: 0.3782923299565847 Loss: 0.10922025880963689 Corrects: 2614\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1460, Loss 0.11089690774679184\n",
            "Train step - Step 1470, Loss 0.10622145980596542\n",
            "Train step - Step 1480, Loss 0.1070624515414238\n",
            "Train step - Step 1490, Loss 0.11028041690587997\n",
            "Train step - Step 1500, Loss 0.10512547940015793\n",
            "Train step - Step 1510, Loss 0.10645315796136856\n",
            "Train epoch - Accuracy: 0.38219971056439944 Loss: 0.10957249499609432 Corrects: 2641\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1520, Loss 0.10542085021734238\n",
            "Train step - Step 1530, Loss 0.11223254352807999\n",
            "Train step - Step 1540, Loss 0.11242518573999405\n",
            "Train step - Step 1550, Loss 0.11040160804986954\n",
            "Train step - Step 1560, Loss 0.10928987711668015\n",
            "Train epoch - Accuracy: 0.38683068017366135 Loss: 0.10936445188850122 Corrects: 2673\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1570, Loss 0.11210619658231735\n",
            "Train step - Step 1580, Loss 0.1090092808008194\n",
            "Train step - Step 1590, Loss 0.10928191244602203\n",
            "Train step - Step 1600, Loss 0.10851879417896271\n",
            "Train step - Step 1610, Loss 0.11433210223913193\n",
            "Train epoch - Accuracy: 0.39088277858176557 Loss: 0.10947046637017543 Corrects: 2701\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1620, Loss 0.10509796440601349\n",
            "Train step - Step 1630, Loss 0.1100102886557579\n",
            "Train step - Step 1640, Loss 0.10840769857168198\n",
            "Train step - Step 1650, Loss 0.10919354110956192\n",
            "Train step - Step 1660, Loss 0.10850479453802109\n",
            "Train step - Step 1670, Loss 0.10972002893686295\n",
            "Train epoch - Accuracy: 0.40144717800289437 Loss: 0.1089628111587247 Corrects: 2774\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.11038612574338913\n",
            "Train step - Step 1690, Loss 0.11051007360219955\n",
            "Train step - Step 1700, Loss 0.11793017387390137\n",
            "Train step - Step 1710, Loss 0.10449446737766266\n",
            "Train step - Step 1720, Loss 0.11086455732584\n",
            "Train epoch - Accuracy: 0.39652677279305354 Loss: 0.1090566926275942 Corrects: 2740\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1730, Loss 0.10255267471075058\n",
            "Train step - Step 1740, Loss 0.11300136893987656\n",
            "Train step - Step 1750, Loss 0.11322528123855591\n",
            "Train step - Step 1760, Loss 0.10796016454696655\n",
            "Train step - Step 1770, Loss 0.1062893271446228\n",
            "Train step - Step 1780, Loss 0.1146787628531456\n",
            "Train epoch - Accuracy: 0.40434153400868306 Loss: 0.10914624687688568 Corrects: 2794\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1790, Loss 0.10983055084943771\n",
            "Train step - Step 1800, Loss 0.11186392605304718\n",
            "Train step - Step 1810, Loss 0.11308268457651138\n",
            "Train step - Step 1820, Loss 0.10518407821655273\n",
            "Train step - Step 1830, Loss 0.11259114742279053\n",
            "Train epoch - Accuracy: 0.40144717800289437 Loss: 0.10882311843324846 Corrects: 2774\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.10614154487848282\n",
            "Train step - Step 1850, Loss 0.10855139791965485\n",
            "Train step - Step 1860, Loss 0.10690879821777344\n",
            "Train step - Step 1870, Loss 0.1038115993142128\n",
            "Train step - Step 1880, Loss 0.10353495925664902\n",
            "Train epoch - Accuracy: 0.4015918958031838 Loss: 0.1088831196839661 Corrects: 2775\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1890, Loss 0.10594115406274796\n",
            "Train step - Step 1900, Loss 0.10554666817188263\n",
            "Train step - Step 1910, Loss 0.11125282198190689\n",
            "Train step - Step 1920, Loss 0.1065271869301796\n",
            "Train step - Step 1930, Loss 0.1052015945315361\n",
            "Train step - Step 1940, Loss 0.1070900708436966\n",
            "Train epoch - Accuracy: 0.41490593342981186 Loss: 0.10914533437807894 Corrects: 2867\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1950, Loss 0.10353125631809235\n",
            "Train step - Step 1960, Loss 0.10475434362888336\n",
            "Train step - Step 1970, Loss 0.10481033474206924\n",
            "Train step - Step 1980, Loss 0.10306191444396973\n",
            "Train step - Step 1990, Loss 0.11231887340545654\n",
            "Train epoch - Accuracy: 0.4136034732272069 Loss: 0.10904318046164409 Corrects: 2858\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2000, Loss 0.11126649379730225\n",
            "Train step - Step 2010, Loss 0.11040347069501877\n",
            "Train step - Step 2020, Loss 0.1060752421617508\n",
            "Train step - Step 2030, Loss 0.10729757696390152\n",
            "Train step - Step 2040, Loss 0.11145815998315811\n",
            "Train step - Step 2050, Loss 0.1143593117594719\n",
            "Train epoch - Accuracy: 0.4117221418234443 Loss: 0.10877090965097098 Corrects: 2845\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2060, Loss 0.11150465160608292\n",
            "Train step - Step 2070, Loss 0.10821395367383957\n",
            "Train step - Step 2080, Loss 0.11115238815546036\n",
            "Train step - Step 2090, Loss 0.11085646599531174\n",
            "Train step - Step 2100, Loss 0.10894143581390381\n",
            "Train epoch - Accuracy: 0.4199710564399421 Loss: 0.10875911150240863 Corrects: 2902\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2110, Loss 0.10740971565246582\n",
            "Train step - Step 2120, Loss 0.11012375354766846\n",
            "Train step - Step 2130, Loss 0.1123194471001625\n",
            "Train step - Step 2140, Loss 0.10707942396402359\n",
            "Train step - Step 2150, Loss 0.11062528938055038\n",
            "Train epoch - Accuracy: 0.42054992764109983 Loss: 0.10894448050498617 Corrects: 2906\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2160, Loss 0.10771413147449493\n",
            "Train step - Step 2170, Loss 0.11991439014673233\n",
            "Train step - Step 2180, Loss 0.10783066600561142\n",
            "Train step - Step 2190, Loss 0.11294998973608017\n",
            "Train step - Step 2200, Loss 0.10779903084039688\n",
            "Train step - Step 2210, Loss 0.11632449924945831\n",
            "Train epoch - Accuracy: 0.4309696092619392 Loss: 0.10885551737678034 Corrects: 2978\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2220, Loss 0.11107319593429565\n",
            "Train step - Step 2230, Loss 0.1098228469491005\n",
            "Train step - Step 2240, Loss 0.11266177892684937\n",
            "Train step - Step 2250, Loss 0.10748203098773956\n",
            "Train step - Step 2260, Loss 0.1050996333360672\n",
            "Train epoch - Accuracy: 0.4295224312590449 Loss: 0.10831964895797708 Corrects: 2968\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2270, Loss 0.10853938013315201\n",
            "Train step - Step 2280, Loss 0.10705020278692245\n",
            "Train step - Step 2290, Loss 0.11018174886703491\n",
            "Train step - Step 2300, Loss 0.1109001412987709\n",
            "Train step - Step 2310, Loss 0.10648321360349655\n",
            "Train step - Step 2320, Loss 0.10880694538354874\n",
            "Train epoch - Accuracy: 0.4248914616497829 Loss: 0.10859000256282376 Corrects: 2936\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2330, Loss 0.1040068045258522\n",
            "Train step - Step 2340, Loss 0.11039745807647705\n",
            "Train step - Step 2350, Loss 0.10842572897672653\n",
            "Train step - Step 2360, Loss 0.11005287617444992\n",
            "Train step - Step 2370, Loss 0.11254000663757324\n",
            "Train epoch - Accuracy: 0.43082489146164976 Loss: 0.10883189015597235 Corrects: 2977\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2380, Loss 0.11025303602218628\n",
            "Train step - Step 2390, Loss 0.10774703323841095\n",
            "Train step - Step 2400, Loss 0.10464074462652206\n",
            "Train step - Step 2410, Loss 0.10740718990564346\n",
            "Train step - Step 2420, Loss 0.11451790481805801\n",
            "Train epoch - Accuracy: 0.42995658465991315 Loss: 0.10831076953716458 Corrects: 2971\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2430, Loss 0.10952261835336685\n",
            "Train step - Step 2440, Loss 0.10931211709976196\n",
            "Train step - Step 2450, Loss 0.10760162025690079\n",
            "Train step - Step 2460, Loss 0.10530320554971695\n",
            "Train step - Step 2470, Loss 0.10403630882501602\n",
            "Train step - Step 2480, Loss 0.11036133766174316\n",
            "Train epoch - Accuracy: 0.4409551374819103 Loss: 0.10830740338641552 Corrects: 3047\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2490, Loss 0.10893507301807404\n",
            "Train step - Step 2500, Loss 0.10722947120666504\n",
            "Train step - Step 2510, Loss 0.10945906490087509\n",
            "Train step - Step 2520, Loss 0.10905923694372177\n",
            "Train step - Step 2530, Loss 0.10888027399778366\n",
            "Train epoch - Accuracy: 0.4402315484804631 Loss: 0.1083176534876292 Corrects: 3042\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2540, Loss 0.11360355466604233\n",
            "Train step - Step 2550, Loss 0.10532187670469284\n",
            "Train step - Step 2560, Loss 0.10508526861667633\n",
            "Train step - Step 2570, Loss 0.11947988718748093\n",
            "Train step - Step 2580, Loss 0.11363186687231064\n",
            "Train step - Step 2590, Loss 0.1054558977484703\n",
            "Train epoch - Accuracy: 0.4419681620839363 Loss: 0.10823938841843915 Corrects: 3054\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2600, Loss 0.10315728187561035\n",
            "Train step - Step 2610, Loss 0.10928789526224136\n",
            "Train step - Step 2620, Loss 0.10957520455121994\n",
            "Train step - Step 2630, Loss 0.10598862171173096\n",
            "Train step - Step 2640, Loss 0.09749634563922882\n",
            "Train epoch - Accuracy: 0.4492040520984081 Loss: 0.10789807808683853 Corrects: 3104\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2650, Loss 0.10539300739765167\n",
            "Train step - Step 2660, Loss 0.10121240466833115\n",
            "Train step - Step 2670, Loss 0.1064654141664505\n",
            "Train step - Step 2680, Loss 0.10785946995019913\n",
            "Train step - Step 2690, Loss 0.10855045169591904\n",
            "Train epoch - Accuracy: 0.44775687409551373 Loss: 0.10778220959665462 Corrects: 3094\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2700, Loss 0.11404644697904587\n",
            "Train step - Step 2710, Loss 0.10787781327962875\n",
            "Train step - Step 2720, Loss 0.11284613609313965\n",
            "Train step - Step 2730, Loss 0.11447751522064209\n",
            "Train step - Step 2740, Loss 0.10457050800323486\n",
            "Train step - Step 2750, Loss 0.10540292412042618\n",
            "Train epoch - Accuracy: 0.45180897250361796 Loss: 0.10790789050795407 Corrects: 3122\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2760, Loss 0.10687129944562912\n",
            "Train step - Step 2770, Loss 0.10919064283370972\n",
            "Train step - Step 2780, Loss 0.11385159939527512\n",
            "Train step - Step 2790, Loss 0.10481937974691391\n",
            "Train step - Step 2800, Loss 0.10321535915136337\n",
            "Train epoch - Accuracy: 0.4483357452966715 Loss: 0.10727206095298361 Corrects: 3098\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.1111454963684082\n",
            "Train step - Step 2820, Loss 0.10872495174407959\n",
            "Train step - Step 2830, Loss 0.1066020280122757\n",
            "Train step - Step 2840, Loss 0.10440094769001007\n",
            "Train step - Step 2850, Loss 0.1044958233833313\n",
            "Train step - Step 2860, Loss 0.1055370345711708\n",
            "Train epoch - Accuracy: 0.4487698986975398 Loss: 0.10757111412528282 Corrects: 3101\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2870, Loss 0.106661856174469\n",
            "Train step - Step 2880, Loss 0.1058851033449173\n",
            "Train step - Step 2890, Loss 0.11543821543455124\n",
            "Train step - Step 2900, Loss 0.10607268661260605\n",
            "Train step - Step 2910, Loss 0.10729475319385529\n",
            "Train epoch - Accuracy: 0.45557163531114325 Loss: 0.10762992240758087 Corrects: 3148\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.10743852704763412\n",
            "Train step - Step 2930, Loss 0.10356413573026657\n",
            "Train step - Step 2940, Loss 0.10491534322500229\n",
            "Train step - Step 2950, Loss 0.10885139554738998\n",
            "Train step - Step 2960, Loss 0.10779096931219101\n",
            "Train epoch - Accuracy: 0.4479015918958032 Loss: 0.10751303128154165 Corrects: 3095\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.10326772928237915\n",
            "Train step - Step 2980, Loss 0.10481622070074081\n",
            "Train step - Step 2990, Loss 0.1070701852440834\n",
            "Train step - Step 3000, Loss 0.10892193764448166\n",
            "Train step - Step 3010, Loss 0.10579414665699005\n",
            "Train step - Step 3020, Loss 0.11294833570718765\n",
            "Train epoch - Accuracy: 0.4539797395079595 Loss: 0.10725142111093366 Corrects: 3137\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.10801757872104645\n",
            "Train step - Step 3040, Loss 0.11148524284362793\n",
            "Train step - Step 3050, Loss 0.10401294380426407\n",
            "Train step - Step 3060, Loss 0.11264457553625107\n",
            "Train step - Step 3070, Loss 0.10624740272760391\n",
            "Train epoch - Accuracy: 0.45470332850940665 Loss: 0.1073606700697097 Corrects: 3142\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.11044790595769882\n",
            "Train step - Step 3090, Loss 0.10943951457738876\n",
            "Train step - Step 3100, Loss 0.10873037576675415\n",
            "Train step - Step 3110, Loss 0.10277865082025528\n",
            "Train step - Step 3120, Loss 0.11274080723524094\n",
            "Train step - Step 3130, Loss 0.10662104189395905\n",
            "Train epoch - Accuracy: 0.45470332850940665 Loss: 0.10751446538999698 Corrects: 3142\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.1099236011505127\n",
            "Train step - Step 3150, Loss 0.11399133503437042\n",
            "Train step - Step 3160, Loss 0.10541540384292603\n",
            "Train step - Step 3170, Loss 0.1068492904305458\n",
            "Train step - Step 3180, Loss 0.1084199920296669\n",
            "Train epoch - Accuracy: 0.45991316931982634 Loss: 0.10726926414519763 Corrects: 3178\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.10630112141370773\n",
            "Train step - Step 3200, Loss 0.10636565834283829\n",
            "Train step - Step 3210, Loss 0.10998298972845078\n",
            "Train step - Step 3220, Loss 0.10455924272537231\n",
            "Train step - Step 3230, Loss 0.10346678644418716\n",
            "Train epoch - Accuracy: 0.4541244573082489 Loss: 0.10782339807745345 Corrects: 3138\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3240, Loss 0.10675330460071564\n",
            "Train step - Step 3250, Loss 0.11241636425256729\n",
            "Train step - Step 3260, Loss 0.10814215987920761\n",
            "Train step - Step 3270, Loss 0.1096867099404335\n",
            "Train step - Step 3280, Loss 0.10584910213947296\n",
            "Train step - Step 3290, Loss 0.10295071452856064\n",
            "Train epoch - Accuracy: 0.45354558610709117 Loss: 0.10738723236210267 Corrects: 3134\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.10911338776350021\n",
            "Train step - Step 3310, Loss 0.10821957886219025\n",
            "Train step - Step 3320, Loss 0.11272753775119781\n",
            "Train step - Step 3330, Loss 0.10428237915039062\n",
            "Train step - Step 3340, Loss 0.10228051990270615\n",
            "Train epoch - Accuracy: 0.4565846599131693 Loss: 0.1072554809721955 Corrects: 3155\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3350, Loss 0.105851911008358\n",
            "Train step - Step 3360, Loss 0.10401220619678497\n",
            "Train step - Step 3370, Loss 0.1059749498963356\n",
            "Train step - Step 3380, Loss 0.1039453074336052\n",
            "Train step - Step 3390, Loss 0.1089467778801918\n",
            "Train step - Step 3400, Loss 0.11085090786218643\n",
            "Train epoch - Accuracy: 0.4548480463096961 Loss: 0.1073643955282123 Corrects: 3143\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3410, Loss 0.10408871620893478\n",
            "Train step - Step 3420, Loss 0.11319589614868164\n",
            "Train step - Step 3430, Loss 0.10307580232620239\n",
            "Train step - Step 3440, Loss 0.10657985508441925\n",
            "Train step - Step 3450, Loss 0.10392655432224274\n",
            "Train epoch - Accuracy: 0.45904486251808974 Loss: 0.10724895731714458 Corrects: 3172\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3460, Loss 0.10959501564502716\n",
            "Train step - Step 3470, Loss 0.11050980538129807\n",
            "Train step - Step 3480, Loss 0.10619913786649704\n",
            "Train step - Step 3490, Loss 0.10528912395238876\n",
            "Train step - Step 3500, Loss 0.10905641317367554\n",
            "Train epoch - Accuracy: 0.45267727930535456 Loss: 0.10751829433717189 Corrects: 3128\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3510, Loss 0.10699620097875595\n",
            "Train step - Step 3520, Loss 0.10938739776611328\n",
            "Train step - Step 3530, Loss 0.1052592545747757\n",
            "Train step - Step 3540, Loss 0.10684066265821457\n",
            "Train step - Step 3550, Loss 0.10743862390518188\n",
            "Train step - Step 3560, Loss 0.10817237943410873\n",
            "Train epoch - Accuracy: 0.4548480463096961 Loss: 0.1071051071008449 Corrects: 3143\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3570, Loss 0.1067626029253006\n",
            "Train step - Step 3580, Loss 0.10605865716934204\n",
            "Train step - Step 3590, Loss 0.1034398302435875\n",
            "Train step - Step 3600, Loss 0.10463196039199829\n",
            "Train step - Step 3610, Loss 0.1103852167725563\n",
            "Train epoch - Accuracy: 0.46367583212735164 Loss: 0.10711385254706385 Corrects: 3204\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3620, Loss 0.11610686033964157\n",
            "Train step - Step 3630, Loss 0.10438181459903717\n",
            "Train step - Step 3640, Loss 0.10472645610570908\n",
            "Train step - Step 3650, Loss 0.11115533113479614\n",
            "Train step - Step 3660, Loss 0.10930373519659042\n",
            "Train step - Step 3670, Loss 0.11275317519903183\n",
            "Train epoch - Accuracy: 0.46078147612156295 Loss: 0.10747062923594253 Corrects: 3184\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3680, Loss 0.10595567524433136\n",
            "Train step - Step 3690, Loss 0.10541880130767822\n",
            "Train step - Step 3700, Loss 0.10916761308908463\n",
            "Train step - Step 3710, Loss 0.10631784051656723\n",
            "Train step - Step 3720, Loss 0.10734996944665909\n",
            "Train epoch - Accuracy: 0.4593342981186686 Loss: 0.10713442157086 Corrects: 3174\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3730, Loss 0.1040586605668068\n",
            "Train step - Step 3740, Loss 0.10452532768249512\n",
            "Train step - Step 3750, Loss 0.1113550215959549\n",
            "Train step - Step 3760, Loss 0.10842668265104294\n",
            "Train step - Step 3770, Loss 0.11005140841007233\n",
            "Train epoch - Accuracy: 0.46150506512301015 Loss: 0.10723449138846308 Corrects: 3189\n",
            "Training finished in 393.83966064453125 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0, 79, 71, 63, 26, 14, 2, 73, 41, 56, 48, 99, 23, 66, 22, 6, 93, 17, 5, 1, 28, 87, 75, 55, 47, 39, 70, 42, 38, 29, 4]\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232d98850>\n",
            "Constructing exemplars of class 87\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [20762, 24931, 9880, 33460, 18171, 853, 45397, 43597, 26053, 39479, 43217, 49355, 34255, 30799, 38014, 24774, 20201, 24513, 36251, 2032, 21454, 34055, 10328, 34075, 23546]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223823c690>\n",
            "Constructing exemplars of class 75\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [32029, 11163, 23676, 23226, 27697, 37753, 42449, 36890, 31322, 48585, 1589, 4358, 96, 14503, 37476, 8879, 23467, 18600, 41614, 31890, 5110, 22887, 37294, 37005, 49259]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a5fbd0>\n",
            "Constructing exemplars of class 55\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [32965, 10517, 7753, 41524, 6551, 11085, 45587, 13034, 40380, 9630, 49332, 8751, 23991, 14118, 44470, 38423, 19579, 438, 43357, 32204, 13273, 31233, 43357, 29630, 21390]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22443a5050>\n",
            "Constructing exemplars of class 47\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [20277, 36111, 42412, 30852, 12824, 43979, 48131, 37911, 24080, 19613, 32268, 23074, 26618, 28400, 3659, 38194, 7166, 45938, 21846, 3281, 48326, 49096, 21998, 14039, 48495]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223802a090>\n",
            "Constructing exemplars of class 39\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [27868, 10953, 19357, 20919, 28163, 42909, 44285, 11046, 3745, 20430, 32839, 21409, 13219, 12135, 9660, 24427, 21575, 41736, 44085, 8683, 37881, 18749, 21409, 4519, 3524]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223156ec50>\n",
            "Constructing exemplars of class 70\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [14317, 12548, 28446, 48358, 31013, 4054, 18543, 126, 49805, 31345, 17917, 37948, 42512, 20367, 1731, 37331, 16392, 20944, 32729, 41538, 15271, 17639, 9492, 12548, 36353]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232ec3850>\n",
            "Constructing exemplars of class 42\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [42517, 2037, 33165, 6142, 14686, 48984, 44038, 7937, 4095, 26109, 48431, 20825, 48144, 48595, 15695, 24728, 5749, 4182, 6713, 36697, 16667, 18899, 45125, 31453, 45963]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232e8f490>\n",
            "Constructing exemplars of class 38\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [31276, 32737, 42757, 37908, 15513, 44953, 16866, 23861, 10214, 7788, 2650, 39910, 12004, 43523, 37752, 11868, 7670, 41816, 44284, 25638, 24077, 2675, 17462, 13858, 14901]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223295dcd0>\n",
            "Constructing exemplars of class 29\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [21217, 24325, 3396, 22691, 26854, 15187, 44731, 31686, 15060, 26458, 20116, 45196, 12047, 906, 35936, 21153, 36146, 41805, 30766, 4699, 1, 15740, 2446, 10682, 238]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238ac7e90>\n",
            "Constructing exemplars of class 4\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [21103, 916, 38786, 995, 40069, 27761, 21342, 45436, 34888, 45151, 30771, 2866, 47125, 7932, 22736, 28682, 24228, 34743, 22732, 15423, 5490, 2769, 37238, 17704, 30853]\n",
            "x train:  [-0.05520144 -0.0836652  -0.10323135 -0.02721385 -0.02918455 -0.1120626\n",
            " -0.12979832 -0.12500185 -0.00429738 -0.09824383 -0.10781135 -0.08598163\n",
            " -0.16633403 -0.06511641 -0.15195408 -0.12378561 -0.11994181 -0.102626\n",
            " -0.13085021 -0.15222587 -0.09312256 -0.07569142 -0.2164393  -0.06925848\n",
            " -0.06925847 -0.16110592 -0.03238026 -0.04341089 -0.07223613 -0.11191054\n",
            " -0.09977462 -0.08788153 -0.09841274 -0.00690635 -0.11196378 -0.15723044\n",
            " -0.05530297 -0.04475538 -0.1449207  -0.14093961 -0.09636339 -0.04226979\n",
            " -0.06441239 -0.06842959 -0.13234515 -0.07226862 -0.13750406 -0.00523175\n",
            " -0.16723022 -0.07739527 -0.10356999 -0.19809324 -0.09305768 -0.06129831\n",
            " -0.0341458  -0.06918787 -0.09491684 -0.09253453 -0.14487137 -0.18383802\n",
            " -0.06647409 -0.13855769 -0.11994819 -0.06819361 -0.08411522 -0.02877828\n",
            " -0.20634301 -0.16023658 -0.08718515 -0.17709658 -0.21633835 -0.16553575\n",
            " -0.05409345 -0.13465029 -0.06863572  0.00553011 -0.14270005 -0.13462523\n",
            " -0.06726891 -0.05804823]\n",
            "y_train:  [tensor([39]), tensor([2]), tensor([60]), tensor([45]), tensor([55]), tensor([96]), tensor([68]), tensor([45]), tensor([34]), tensor([87]), tensor([37]), tensor([47]), tensor([6]), tensor([95]), tensor([45]), tensor([25]), tensor([98]), tensor([97]), tensor([82]), tensor([96]), tensor([57]), tensor([94]), tensor([46]), tensor([25]), tensor([70]), tensor([72]), tensor([42]), tensor([38]), tensor([79]), tensor([5]), tensor([95]), tensor([97]), tensor([39]), tensor([26]), tensor([93]), tensor([82]), tensor([80]), tensor([14]), tensor([73]), tensor([56]), tensor([17]), tensor([53]), tensor([24]), tensor([27]), tensor([97]), tensor([95]), tensor([13]), tensor([53]), tensor([39]), tensor([23]), tensor([75]), tensor([22]), tensor([42]), tensor([30]), tensor([91]), tensor([58]), tensor([63]), tensor([56]), tensor([33]), tensor([70]), tensor([16]), tensor([7]), tensor([12]), tensor([37]), tensor([79]), tensor([69]), tensor([12]), tensor([33]), tensor([27]), tensor([6]), tensor([87]), tensor([95]), tensor([57]), tensor([10]), tensor([62]), tensor([7]), tensor([68]), tensor([32]), tensor([5]), tensor([42]), tensor([94]), tensor([53]), tensor([26]), tensor([12]), tensor([94]), tensor([78]), tensor([52]), tensor([13]), tensor([24]), tensor([68]), tensor([57]), tensor([6]), tensor([12]), tensor([55]), tensor([67]), tensor([33]), tensor([41]), tensor([52]), tensor([3]), tensor([66]), tensor([63]), tensor([30]), tensor([72]), tensor([16]), tensor([2]), tensor([47]), tensor([30]), tensor([6]), tensor([69]), tensor([70]), tensor([0]), tensor([36]), tensor([28]), tensor([94]), tensor([2]), tensor([70]), tensor([11]), tensor([33]), tensor([2]), tensor([10]), tensor([52]), tensor([42]), tensor([24]), tensor([25]), tensor([98]), tensor([60]), tensor([33]), tensor([82]), tensor([80]), tensor([7]), tensor([81]), tensor([49]), tensor([96]), tensor([37]), tensor([97]), tensor([45]), tensor([91]), tensor([70]), tensor([62]), tensor([36]), tensor([38]), tensor([96]), tensor([80]), tensor([28]), tensor([48]), tensor([93]), tensor([41]), tensor([97]), tensor([49]), tensor([95]), tensor([46]), tensor([29]), tensor([32]), tensor([82]), tensor([69]), tensor([50]), tensor([5]), tensor([70]), tensor([94]), tensor([23]), tensor([24]), tensor([57]), tensor([34]), tensor([72]), tensor([68]), tensor([58]), tensor([86]), tensor([35]), tensor([86]), tensor([79]), tensor([96]), tensor([6]), tensor([11]), tensor([22]), tensor([46]), tensor([24]), tensor([95]), tensor([59]), tensor([68]), tensor([50]), tensor([23]), tensor([87]), tensor([26]), tensor([36]), tensor([94]), tensor([35]), tensor([56]), tensor([41]), tensor([63]), tensor([77]), tensor([5]), tensor([50]), tensor([67]), tensor([33]), tensor([25]), tensor([39]), tensor([17]), tensor([47]), tensor([97]), tensor([56]), tensor([45]), tensor([80]), tensor([10]), tensor([99]), tensor([21]), tensor([57]), tensor([91]), tensor([88]), tensor([94]), tensor([92]), tensor([45]), tensor([66]), tensor([50]), tensor([71]), tensor([45]), tensor([86]), tensor([79]), tensor([39]), tensor([28]), tensor([27]), tensor([71]), tensor([69]), tensor([60]), tensor([17]), tensor([98]), tensor([23]), tensor([50]), tensor([53]), tensor([5]), tensor([30]), tensor([95]), tensor([11]), tensor([60]), tensor([65]), tensor([88]), tensor([92]), tensor([97]), tensor([52]), tensor([60]), tensor([28]), tensor([71]), tensor([26]), tensor([37]), tensor([36]), tensor([75]), tensor([42]), tensor([67]), tensor([47]), tensor([70]), tensor([81]), tensor([62]), tensor([78]), tensor([52]), tensor([38]), tensor([75]), tensor([50]), tensor([10]), tensor([1]), tensor([37]), tensor([66]), tensor([92]), tensor([72]), tensor([14]), tensor([13]), tensor([39]), tensor([72]), tensor([1]), tensor([16]), tensor([3]), tensor([75]), tensor([65]), tensor([45]), tensor([52]), tensor([97]), tensor([50]), tensor([21]), tensor([35]), tensor([58]), tensor([86]), tensor([14]), tensor([49]), tensor([58]), tensor([46]), tensor([34]), tensor([14]), tensor([56]), tensor([62]), tensor([63]), tensor([16]), tensor([91]), tensor([10]), tensor([33]), tensor([97]), tensor([75]), tensor([27]), tensor([36]), tensor([37]), tensor([69]), tensor([22]), tensor([26]), tensor([80]), tensor([46]), tensor([17]), tensor([66]), tensor([36]), tensor([77]), tensor([12]), tensor([33]), tensor([53]), tensor([0]), tensor([0]), tensor([75]), tensor([95]), tensor([33]), tensor([81]), tensor([5]), tensor([46]), tensor([21]), tensor([62]), tensor([75]), tensor([12]), tensor([80]), tensor([48]), tensor([55]), tensor([14]), tensor([33]), tensor([28]), tensor([12]), tensor([93]), tensor([42]), tensor([37]), tensor([24]), tensor([78]), tensor([1]), tensor([72]), tensor([87]), tensor([94]), tensor([73]), tensor([13]), tensor([72]), tensor([69]), tensor([65]), tensor([42]), tensor([1]), tensor([3]), tensor([13]), tensor([24]), tensor([30]), tensor([69]), tensor([58]), tensor([6]), tensor([17]), tensor([65]), tensor([30]), tensor([99]), tensor([22]), tensor([80]), tensor([26]), tensor([21]), tensor([6]), tensor([32]), tensor([36]), tensor([26]), tensor([26]), tensor([69]), tensor([23]), tensor([2]), tensor([21]), tensor([32]), tensor([67]), tensor([81]), tensor([70]), tensor([59]), tensor([4]), tensor([62]), tensor([46]), tensor([13]), tensor([4]), tensor([31]), tensor([93]), tensor([39]), tensor([12]), tensor([41]), tensor([95]), tensor([24]), tensor([59]), tensor([93]), tensor([25]), tensor([95]), tensor([77]), tensor([60]), tensor([10]), tensor([80]), tensor([63]), tensor([67]), tensor([77]), tensor([80]), tensor([86]), tensor([14]), tensor([22]), tensor([4]), tensor([23]), tensor([59]), tensor([55]), tensor([5]), tensor([1]), tensor([4]), tensor([2]), tensor([53]), tensor([87]), tensor([91]), tensor([50]), tensor([92]), tensor([63]), tensor([13]), tensor([11]), tensor([58]), tensor([23]), tensor([31]), tensor([17]), tensor([99]), tensor([71]), tensor([57]), tensor([32]), tensor([31]), tensor([28]), tensor([58]), tensor([0]), tensor([88]), tensor([42]), tensor([50]), tensor([94]), tensor([0]), tensor([88]), tensor([46]), tensor([75]), tensor([60]), tensor([39]), tensor([81]), tensor([1]), tensor([97]), tensor([63]), tensor([58]), tensor([92]), tensor([72]), tensor([1]), tensor([38]), tensor([72]), tensor([79]), tensor([32]), tensor([22]), tensor([5]), tensor([31]), tensor([3]), tensor([45]), tensor([66]), tensor([42]), tensor([31]), tensor([12]), tensor([97]), tensor([65]), tensor([14]), tensor([5]), tensor([68]), tensor([36]), tensor([1]), tensor([78]), tensor([94]), tensor([67]), tensor([53]), tensor([33]), tensor([36]), tensor([78]), tensor([60]), tensor([66]), tensor([41]), tensor([13]), tensor([95]), tensor([0]), tensor([69]), tensor([7]), tensor([96]), tensor([81]), tensor([47]), tensor([0]), tensor([63]), tensor([65]), tensor([32]), tensor([72]), tensor([73]), tensor([98]), tensor([26]), tensor([36]), tensor([28]), tensor([63]), tensor([67]), tensor([67]), tensor([39]), tensor([28]), tensor([88]), tensor([5]), tensor([59]), tensor([55]), tensor([68]), tensor([46]), tensor([71]), tensor([47]), tensor([21]), tensor([56]), tensor([13]), tensor([86]), tensor([22]), tensor([59]), tensor([56]), tensor([58]), tensor([38]), tensor([36]), tensor([11]), tensor([32]), tensor([56]), tensor([77]), tensor([88]), tensor([3]), tensor([91]), tensor([66]), tensor([59]), tensor([93]), tensor([65]), tensor([55]), tensor([77]), tensor([25]), tensor([17]), tensor([94]), tensor([75]), tensor([65]), tensor([14]), tensor([98]), tensor([53]), tensor([82]), tensor([3]), tensor([12]), tensor([13]), tensor([10]), tensor([27]), tensor([32]), tensor([82]), tensor([66]), tensor([49]), tensor([29]), tensor([53]), tensor([33]), tensor([24]), tensor([60]), tensor([21]), tensor([86]), tensor([97]), tensor([68]), tensor([67]), tensor([66]), tensor([80]), tensor([33]), tensor([16]), tensor([11]), tensor([25]), tensor([94]), tensor([49]), tensor([99]), tensor([41]), tensor([29]), tensor([26]), tensor([69]), tensor([34]), tensor([46]), tensor([79]), tensor([7]), tensor([87]), tensor([75]), tensor([21]), tensor([72]), tensor([98]), tensor([94]), tensor([67]), tensor([2]), tensor([97]), tensor([69]), tensor([45]), tensor([22]), tensor([41]), tensor([21]), tensor([93]), tensor([92]), tensor([56]), tensor([57]), tensor([7]), tensor([93]), tensor([52]), tensor([32]), tensor([69]), tensor([98]), tensor([30]), tensor([16]), tensor([48]), tensor([28]), tensor([28]), tensor([2]), tensor([11]), tensor([88]), tensor([27]), tensor([66]), tensor([98]), tensor([24]), tensor([78]), tensor([48]), tensor([52]), tensor([62]), tensor([79]), tensor([27]), tensor([72]), tensor([79]), tensor([80]), tensor([73]), tensor([77]), tensor([14]), tensor([35]), tensor([47]), tensor([42]), tensor([97]), tensor([67]), tensor([86]), tensor([16]), tensor([21]), tensor([86]), tensor([39]), tensor([62]), tensor([21]), tensor([45]), tensor([60]), tensor([52]), tensor([48]), tensor([46]), tensor([78]), tensor([36]), tensor([60]), tensor([92]), tensor([86]), tensor([96]), tensor([6]), tensor([22]), tensor([26]), tensor([80]), tensor([86]), tensor([91]), tensor([56]), tensor([59]), tensor([65]), tensor([70]), tensor([5]), tensor([50]), tensor([57]), tensor([62]), tensor([94]), tensor([22]), tensor([2]), tensor([1]), tensor([25]), tensor([14]), tensor([47]), tensor([7]), tensor([98]), tensor([50]), tensor([1]), tensor([21]), tensor([47]), tensor([49]), tensor([68]), tensor([32]), tensor([60]), tensor([94]), tensor([29]), tensor([60]), tensor([38]), tensor([71]), tensor([36]), tensor([26]), tensor([63]), tensor([94]), tensor([72]), tensor([71]), tensor([3]), tensor([73]), tensor([82]), tensor([79]), tensor([77]), tensor([65]), tensor([99]), tensor([77]), tensor([62]), tensor([4]), tensor([39]), tensor([14]), tensor([1]), tensor([67]), tensor([48]), tensor([79]), tensor([79]), tensor([35]), tensor([50]), tensor([82]), tensor([17]), tensor([23]), tensor([55]), tensor([93]), tensor([67]), tensor([63]), tensor([38]), tensor([12]), tensor([55]), tensor([26]), tensor([16]), tensor([95]), tensor([28]), tensor([48]), tensor([10]), tensor([48]), tensor([81]), tensor([80]), tensor([3]), tensor([75]), tensor([58]), tensor([65]), tensor([42]), tensor([62]), tensor([12]), tensor([45]), tensor([96]), tensor([53]), tensor([96]), tensor([92]), tensor([59]), tensor([23]), tensor([23]), tensor([73]), tensor([0]), tensor([58]), tensor([41]), tensor([75]), tensor([62]), tensor([31]), tensor([12]), tensor([4]), tensor([78]), tensor([99]), tensor([33]), tensor([30]), tensor([62]), tensor([27]), tensor([97]), tensor([39]), tensor([59]), tensor([65]), tensor([35]), tensor([1]), tensor([69]), tensor([86]), tensor([41]), tensor([98]), tensor([48]), tensor([13]), tensor([79]), tensor([30]), tensor([91]), tensor([96]), tensor([78]), tensor([96]), tensor([5]), tensor([11]), tensor([82]), tensor([7]), tensor([49]), tensor([12]), tensor([33]), tensor([26]), tensor([11]), tensor([48]), tensor([5]), tensor([81]), tensor([53]), tensor([3]), tensor([52]), tensor([7]), tensor([93]), tensor([98]), tensor([69]), tensor([69]), tensor([57]), tensor([68]), tensor([69]), tensor([47]), tensor([72]), tensor([7]), tensor([6]), tensor([57]), tensor([78]), tensor([49]), tensor([11]), tensor([70]), tensor([82]), tensor([5]), tensor([66]), tensor([80]), tensor([92]), tensor([33]), tensor([91]), tensor([26]), tensor([63]), tensor([87]), tensor([47]), tensor([11]), tensor([49]), tensor([23]), tensor([82]), tensor([91]), tensor([35]), tensor([22]), tensor([34]), tensor([17]), tensor([17]), tensor([63]), tensor([70]), tensor([30]), tensor([57]), tensor([88]), tensor([24]), tensor([92]), tensor([41]), tensor([35]), tensor([26]), tensor([73]), tensor([22]), tensor([34]), tensor([25]), tensor([5]), tensor([31]), tensor([13]), tensor([29]), tensor([22]), tensor([38]), tensor([57]), tensor([25]), tensor([36]), tensor([25]), tensor([50]), tensor([36]), tensor([99]), tensor([5]), tensor([87]), tensor([30]), tensor([77]), tensor([60]), tensor([2]), tensor([59]), tensor([3]), tensor([12]), tensor([31]), tensor([5]), tensor([13]), tensor([95]), tensor([79]), tensor([38]), tensor([36]), tensor([58]), tensor([56]), tensor([92]), tensor([81]), tensor([36]), tensor([42]), tensor([47]), tensor([96]), tensor([97]), tensor([27]), tensor([68]), tensor([65]), tensor([16]), tensor([87]), tensor([70]), tensor([6]), tensor([22]), tensor([87]), tensor([80]), tensor([68]), tensor([55]), tensor([60]), tensor([66]), tensor([91]), tensor([88]), tensor([95]), tensor([49]), tensor([34]), tensor([63]), tensor([98]), tensor([87]), tensor([57]), tensor([41]), tensor([4]), tensor([21]), tensor([2]), tensor([75]), tensor([52]), tensor([21]), tensor([67]), tensor([91]), tensor([80]), tensor([57]), tensor([73]), tensor([7]), tensor([94]), tensor([96]), tensor([71]), tensor([28]), tensor([2]), tensor([86]), tensor([77]), tensor([25]), tensor([87]), tensor([45]), tensor([68]), tensor([92]), tensor([88]), tensor([57]), tensor([62]), tensor([62]), tensor([81]), tensor([22]), tensor([81]), tensor([41]), tensor([93]), tensor([30]), tensor([1]), tensor([80]), tensor([33]), tensor([87]), tensor([86]), tensor([26]), tensor([29]), tensor([60]), tensor([46]), tensor([0]), tensor([31]), tensor([88]), tensor([70]), tensor([3]), tensor([80]), tensor([16]), tensor([38]), tensor([99]), tensor([63]), tensor([49]), tensor([78]), tensor([23]), tensor([75]), tensor([7]), tensor([63]), tensor([0]), tensor([27]), tensor([77]), tensor([0]), tensor([21]), tensor([38]), tensor([35]), tensor([68]), tensor([60]), tensor([78]), tensor([66]), tensor([93]), tensor([48]), tensor([41]), tensor([77]), tensor([34]), tensor([48]), tensor([26]), tensor([32]), tensor([34]), tensor([39]), tensor([21]), tensor([26]), tensor([65]), tensor([97]), tensor([66]), tensor([50]), tensor([47]), tensor([98]), tensor([78]), tensor([92]), tensor([46]), tensor([42]), tensor([86]), tensor([27]), tensor([87]), tensor([38]), tensor([52]), tensor([17]), tensor([12]), tensor([62]), tensor([31]), tensor([92]), tensor([47]), tensor([25]), tensor([48]), tensor([79]), tensor([12]), tensor([50]), tensor([88]), tensor([25]), tensor([82]), tensor([22]), tensor([53]), tensor([73]), tensor([0]), tensor([58]), tensor([11]), tensor([79]), tensor([50]), tensor([37]), tensor([17]), tensor([13]), tensor([86]), tensor([32]), tensor([56]), tensor([29]), tensor([75]), tensor([93]), tensor([58]), tensor([21]), tensor([77]), tensor([28]), tensor([58]), tensor([27]), tensor([58]), tensor([42]), tensor([94]), tensor([56]), tensor([86]), tensor([39]), tensor([7]), tensor([78]), tensor([22]), tensor([78]), tensor([92]), tensor([71]), tensor([49]), tensor([46]), tensor([93]), tensor([35]), tensor([91]), tensor([28]), tensor([14]), tensor([52]), tensor([98]), tensor([14]), tensor([39]), tensor([27]), tensor([48]), tensor([35]), tensor([41]), tensor([12]), tensor([55]), tensor([59]), tensor([17]), tensor([21]), tensor([55]), tensor([32]), tensor([39]), tensor([16]), tensor([38]), tensor([37]), tensor([57]), tensor([86]), tensor([33]), tensor([23]), tensor([33]), tensor([55]), tensor([71]), tensor([71]), tensor([62]), tensor([59]), tensor([28]), tensor([17]), tensor([70]), tensor([7]), tensor([70]), tensor([91]), tensor([63]), tensor([80]), tensor([24]), tensor([59]), tensor([46]), tensor([66]), tensor([77]), tensor([87]), tensor([24]), tensor([14]), tensor([13]), tensor([2]), tensor([62]), tensor([12]), tensor([80]), tensor([25]), tensor([92]), tensor([69]), tensor([48]), tensor([93]), tensor([73]), tensor([57]), tensor([59]), tensor([11]), tensor([81]), tensor([1]), tensor([91]), tensor([32]), tensor([62]), tensor([10]), tensor([2]), tensor([48]), tensor([66]), tensor([91]), tensor([0]), tensor([42]), tensor([53]), tensor([37]), tensor([27]), tensor([4]), tensor([33]), tensor([49]), tensor([27]), tensor([16]), tensor([30]), tensor([25]), tensor([24]), tensor([32]), tensor([48]), tensor([86]), tensor([2]), tensor([46]), tensor([10]), tensor([88]), tensor([92]), tensor([97]), tensor([75]), tensor([7]), tensor([30]), tensor([0]), tensor([81]), tensor([35]), tensor([50]), tensor([10]), tensor([69]), tensor([88]), tensor([94]), tensor([5]), tensor([53]), tensor([0]), tensor([34]), tensor([42]), tensor([0]), tensor([39]), tensor([47]), tensor([3]), tensor([3]), tensor([57]), tensor([13]), tensor([50]), tensor([41]), tensor([13]), tensor([30]), tensor([60]), tensor([17]), tensor([35]), tensor([55]), tensor([78]), tensor([12]), tensor([38]), tensor([38]), tensor([96]), tensor([4]), tensor([41]), tensor([34]), tensor([7]), tensor([34]), tensor([81]), tensor([13]), tensor([98]), tensor([17]), tensor([62]), tensor([7]), tensor([73]), tensor([47]), tensor([37]), tensor([77]), tensor([36]), tensor([29]), tensor([49]), tensor([68]), tensor([13]), tensor([52]), tensor([13]), tensor([98]), tensor([88]), tensor([77]), tensor([1]), tensor([70]), tensor([46]), tensor([62]), tensor([52]), tensor([3]), tensor([60]), tensor([98]), tensor([70]), tensor([71]), tensor([42]), tensor([55]), tensor([22]), tensor([81]), tensor([96]), tensor([22]), tensor([10]), tensor([32]), tensor([38]), tensor([63]), tensor([37]), tensor([65]), tensor([4]), tensor([60]), tensor([87]), tensor([23]), tensor([4]), tensor([66]), tensor([4]), tensor([82]), tensor([91]), tensor([82]), tensor([60]), tensor([96]), tensor([48]), tensor([66]), tensor([55]), tensor([47]), tensor([86]), tensor([71]), tensor([46]), tensor([72]), tensor([32]), tensor([93]), tensor([24]), tensor([80]), tensor([11]), tensor([16]), tensor([88]), tensor([93]), tensor([55]), tensor([39]), tensor([37]), tensor([79]), tensor([96]), tensor([92]), tensor([1]), tensor([95]), tensor([25]), tensor([3]), tensor([47]), tensor([31]), tensor([56]), tensor([93]), tensor([17]), tensor([24]), tensor([58]), tensor([87]), tensor([53]), tensor([34]), tensor([55]), tensor([11]), tensor([38]), tensor([95]), tensor([52]), tensor([13]), tensor([50]), tensor([3]), tensor([58]), tensor([59]), tensor([52]), tensor([23]), tensor([56]), tensor([95]), tensor([32]), tensor([68]), tensor([80]), tensor([97]), tensor([86]), tensor([93]), tensor([81]), tensor([99]), tensor([81]), tensor([27]), tensor([47]), tensor([97]), tensor([14]), tensor([10]), tensor([59]), tensor([2]), tensor([14]), tensor([24]), tensor([23]), tensor([21]), tensor([81]), tensor([48]), tensor([27]), tensor([53]), tensor([32]), tensor([73]), tensor([6]), tensor([11]), tensor([77]), tensor([10]), tensor([96]), tensor([73]), tensor([82]), tensor([36]), tensor([3]), tensor([99]), tensor([2]), tensor([11]), tensor([92]), tensor([70]), tensor([14]), tensor([81]), tensor([56]), tensor([26]), tensor([10]), tensor([55]), tensor([53]), tensor([45]), tensor([16]), tensor([65]), tensor([63]), tensor([58]), tensor([14]), tensor([77]), tensor([66]), tensor([22]), tensor([3]), tensor([99]), tensor([92]), tensor([95]), tensor([35]), tensor([21]), tensor([22]), tensor([70]), tensor([55]), tensor([38]), tensor([93]), tensor([91]), tensor([14]), tensor([99]), tensor([26]), tensor([96]), tensor([67]), tensor([71]), tensor([65]), tensor([28]), tensor([48]), tensor([94]), tensor([6]), tensor([78]), tensor([67]), tensor([16]), tensor([59]), tensor([95]), tensor([97]), tensor([4]), tensor([69]), tensor([87]), tensor([25]), tensor([25]), tensor([21]), tensor([5]), tensor([34]), tensor([36]), tensor([72]), tensor([39]), tensor([91]), tensor([4]), tensor([26]), tensor([2]), tensor([88]), tensor([81]), tensor([2]), tensor([32]), tensor([56]), tensor([3]), tensor([28]), tensor([24]), tensor([79]), tensor([11]), tensor([88]), tensor([87]), tensor([75]), tensor([30]), tensor([86]), tensor([34]), tensor([88]), tensor([82]), tensor([67]), tensor([78]), tensor([58]), tensor([34]), tensor([93]), tensor([25]), tensor([6]), tensor([88]), tensor([78]), tensor([53]), tensor([57]), tensor([56]), tensor([75]), tensor([31]), tensor([56]), tensor([66]), tensor([28]), tensor([27]), tensor([24]), tensor([88]), tensor([69]), tensor([62]), tensor([46]), tensor([48]), tensor([34]), tensor([49]), tensor([10]), tensor([25]), tensor([6]), tensor([47]), tensor([31]), tensor([97]), tensor([11]), tensor([22]), tensor([70]), tensor([68]), tensor([60]), tensor([79]), tensor([49]), tensor([53]), tensor([72]), tensor([62]), tensor([14]), tensor([55]), tensor([33]), tensor([59]), tensor([4]), tensor([99]), tensor([75]), tensor([2]), tensor([7]), tensor([7]), tensor([68]), tensor([6]), tensor([22]), tensor([56]), tensor([53]), tensor([92]), tensor([34]), tensor([94]), tensor([30]), tensor([17]), tensor([17]), tensor([25]), tensor([6]), tensor([24]), tensor([14]), tensor([21]), tensor([80]), tensor([29]), tensor([2]), tensor([70]), tensor([23]), tensor([23]), tensor([88]), tensor([47]), tensor([30]), tensor([71]), tensor([65]), tensor([16]), tensor([37]), tensor([35]), tensor([86]), tensor([33]), tensor([35]), tensor([68]), tensor([30]), tensor([95]), tensor([68]), tensor([29]), tensor([63]), tensor([56]), tensor([12]), tensor([73]), tensor([12]), tensor([45]), tensor([99]), tensor([93]), tensor([17]), tensor([45]), tensor([93]), tensor([52]), tensor([35]), tensor([46]), tensor([34]), tensor([29]), tensor([95]), tensor([41]), tensor([71]), tensor([11]), tensor([52]), tensor([69]), tensor([99]), tensor([75]), tensor([78]), tensor([63]), tensor([35]), tensor([47]), tensor([27]), tensor([3]), tensor([31]), tensor([67]), tensor([52]), tensor([99]), tensor([25]), tensor([35]), tensor([91]), tensor([69]), tensor([35]), tensor([28]), tensor([95]), tensor([82]), tensor([17]), tensor([66]), tensor([68]), tensor([79]), tensor([41]), tensor([13]), tensor([88]), tensor([75]), tensor([35]), tensor([34]), tensor([38]), tensor([77]), tensor([4]), tensor([14]), tensor([70]), tensor([6]), tensor([93]), tensor([55]), tensor([10]), tensor([6]), tensor([36]), tensor([80]), tensor([50]), tensor([87]), tensor([77]), tensor([14]), tensor([59]), tensor([7]), tensor([47]), tensor([42]), tensor([52]), tensor([45]), tensor([59]), tensor([73]), tensor([63]), tensor([7]), tensor([6]), tensor([34]), tensor([55]), tensor([37]), tensor([32]), tensor([99]), tensor([4]), tensor([12]), tensor([58]), tensor([0]), tensor([71]), tensor([39]), tensor([2]), tensor([62]), tensor([4]), tensor([39]), tensor([49]), tensor([72]), tensor([53]), tensor([1]), tensor([70]), tensor([79]), tensor([69]), tensor([0]), tensor([28]), tensor([87]), tensor([67]), tensor([36]), tensor([1]), tensor([2]), tensor([86]), tensor([17]), tensor([16]), tensor([88]), tensor([95]), tensor([67]), tensor([73]), tensor([35]), tensor([65]), tensor([23]), tensor([66]), tensor([4]), tensor([75]), tensor([36]), tensor([71]), tensor([58]), tensor([67]), tensor([34]), tensor([72]), tensor([4]), tensor([29]), tensor([41]), tensor([38]), tensor([79]), tensor([1]), tensor([49]), tensor([33]), tensor([98]), tensor([46]), tensor([47]), tensor([3]), tensor([30]), tensor([5]), tensor([82]), tensor([3]), tensor([1]), tensor([23]), tensor([73]), tensor([91]), tensor([49]), tensor([58]), tensor([45]), tensor([99]), tensor([57]), tensor([28]), tensor([28]), tensor([31]), tensor([82]), tensor([30]), tensor([16]), tensor([2]), tensor([78]), tensor([75]), tensor([71]), tensor([16]), tensor([82]), tensor([53]), tensor([27]), tensor([26]), tensor([56]), tensor([93]), tensor([94]), tensor([10]), tensor([50]), tensor([79]), tensor([30]), tensor([11]), tensor([37]), tensor([0]), tensor([59]), tensor([16]), tensor([37]), tensor([57]), tensor([16]), tensor([67]), tensor([28]), tensor([29]), tensor([10]), tensor([77]), tensor([29]), tensor([3]), tensor([38]), tensor([4]), tensor([23]), tensor([48]), tensor([96]), tensor([57]), tensor([25]), tensor([73]), tensor([3]), tensor([7]), tensor([60]), tensor([24]), tensor([52]), tensor([27]), tensor([5]), tensor([97]), tensor([29]), tensor([46]), tensor([29]), tensor([66]), tensor([96]), tensor([10]), tensor([48]), tensor([82]), tensor([16]), tensor([71]), tensor([49]), tensor([33]), tensor([45]), tensor([28]), tensor([37]), tensor([39]), tensor([60]), tensor([72]), tensor([50]), tensor([65]), tensor([12]), tensor([10]), tensor([21]), tensor([96]), tensor([23]), tensor([33]), tensor([99]), tensor([65]), tensor([81]), tensor([95]), tensor([56]), tensor([4]), tensor([96]), tensor([7]), tensor([57]), tensor([68]), tensor([13]), tensor([63]), tensor([91]), tensor([98]), tensor([14]), tensor([81]), tensor([77]), tensor([24]), tensor([94]), tensor([94]), tensor([29]), tensor([30]), tensor([10]), tensor([59]), tensor([45]), tensor([81]), tensor([11]), tensor([65]), tensor([96]), tensor([99]), tensor([50]), tensor([6]), tensor([53]), tensor([6]), tensor([31]), tensor([77]), tensor([60]), tensor([31]), tensor([98]), tensor([46]), tensor([41]), tensor([29]), tensor([27]), tensor([97]), tensor([79]), tensor([91]), tensor([42]), tensor([25]), tensor([39]), tensor([55]), tensor([99]), tensor([37]), tensor([2]), tensor([34]), tensor([24]), tensor([13]), tensor([32]), tensor([49]), tensor([41]), tensor([69]), tensor([94]), tensor([65]), tensor([10]), tensor([30]), tensor([1]), tensor([92]), tensor([72]), tensor([11]), tensor([27]), tensor([73]), tensor([11]), tensor([10]), tensor([0]), tensor([45]), tensor([53]), tensor([77]), tensor([31]), tensor([72]), tensor([22]), tensor([41]), tensor([31]), tensor([36]), tensor([42]), tensor([58]), tensor([95]), tensor([45]), tensor([4]), tensor([56]), tensor([98]), tensor([6]), tensor([92]), tensor([30]), tensor([57]), tensor([29]), tensor([82]), tensor([53]), tensor([32]), tensor([59]), tensor([81]), tensor([68]), tensor([71]), tensor([67]), tensor([31]), tensor([37]), tensor([48]), tensor([1]), tensor([73]), tensor([31]), tensor([79]), tensor([71]), tensor([14]), tensor([71]), tensor([0]), tensor([41]), tensor([67]), tensor([13]), tensor([49]), tensor([41]), tensor([46]), tensor([91]), tensor([38]), tensor([35]), tensor([88]), tensor([72]), tensor([39]), tensor([23]), tensor([45]), tensor([99]), tensor([52]), tensor([0]), tensor([73]), tensor([27]), tensor([55]), tensor([36]), tensor([5]), tensor([38]), tensor([11]), tensor([1]), tensor([26]), tensor([29]), tensor([68]), tensor([31]), tensor([31]), tensor([37]), tensor([92]), tensor([99]), tensor([99]), tensor([58]), tensor([0]), tensor([37]), tensor([32]), tensor([70]), tensor([37]), tensor([0]), tensor([23]), tensor([23]), tensor([82]), tensor([67]), tensor([29]), tensor([65]), tensor([98]), tensor([68]), tensor([87]), tensor([7]), tensor([98]), tensor([5]), tensor([50]), tensor([24]), tensor([17]), tensor([59]), tensor([29]), tensor([87]), tensor([34]), tensor([21]), tensor([10]), tensor([4]), tensor([63]), tensor([96]), tensor([31]), tensor([46]), tensor([45]), tensor([41]), tensor([98]), tensor([48]), tensor([71]), tensor([26]), tensor([72]), tensor([78]), tensor([79]), tensor([55]), tensor([82]), tensor([97]), tensor([73]), tensor([49]), tensor([69]), tensor([31]), tensor([0]), tensor([66]), tensor([42]), tensor([27]), tensor([50]), tensor([12]), tensor([35]), tensor([78]), tensor([66]), tensor([16]), tensor([73]), tensor([87]), tensor([73]), tensor([75]), tensor([86]), tensor([52]), tensor([1]), tensor([45]), tensor([7]), tensor([42]), tensor([78]), tensor([92]), tensor([6]), tensor([70]), tensor([16]), tensor([28]), tensor([91]), tensor([17]), tensor([49]), tensor([37]), tensor([73]), tensor([5]), tensor([24]), tensor([3]), tensor([5]), tensor([38]), tensor([17]), tensor([29]), tensor([72]), tensor([42]), tensor([34]), tensor([47]), tensor([35]), tensor([37]), tensor([62]), tensor([80]), tensor([87]), tensor([21]), tensor([75]), tensor([99]), tensor([81]), tensor([38]), tensor([29]), tensor([63]), tensor([82]), tensor([57]), tensor([52]), tensor([65]), tensor([6]), tensor([98]), tensor([71]), tensor([42]), tensor([78]), tensor([39]), tensor([56]), tensor([42]), tensor([4]), tensor([16]), tensor([49]), tensor([1]), tensor([22]), tensor([6]), tensor([29])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.44 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.436\n",
            "TEST ALL:  0.441875\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  9000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 81, 97, 2, 10, 18, 26, 34, 42, 50, 58, 66, 82, 98, 3, 11, 27, 35, 51, 59, 67, 89, 73, 87, 65, 8, 16, 24, 32, 48, 56, 64, 72, 80, 88, 96, 1, 9, 17, 25, 33, 41, 49, 57, 75, 83, 91, 99, 14, 22, 30, 38, 46, 62, 70, 78, 86, 94, 7, 23, 31, 39, 47, 55, 63, 71, 79, 6, 93, 85, 84, 4, 12, 20, 28, 36, 52, 60, 68, 92, 77, 5, 13, 21, 29, 37, 45, 53, 69, 0]\n",
            "TRAIN_SET CLASSES:  [83, 51, 18, 89, 85, 9, 84, 64, 20, 8]\n",
            "VALIDATION CLASSES:  [20, 51, 89, 85, 84, 83, 18, 9, 8, 64]\n",
            "GROUP:  9\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [83, 51, 18, 89, 85, 9, 84, 64, 20, 8]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  90\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.19942538440227509\n",
            "Train step - Step 10, Loss 0.13514085114002228\n",
            "Train step - Step 20, Loss 0.12644003331661224\n",
            "Train step - Step 30, Loss 0.1262558251619339\n",
            "Train step - Step 40, Loss 0.11731797456741333\n",
            "Train step - Step 50, Loss 0.12036912888288498\n",
            "Train epoch - Accuracy: 0.13798561151079136 Loss: 0.13427696183859872 Corrects: 959\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.11954716593027115\n",
            "Train step - Step 70, Loss 0.11889682710170746\n",
            "Train step - Step 80, Loss 0.1151750385761261\n",
            "Train step - Step 90, Loss 0.12264082580804825\n",
            "Train step - Step 100, Loss 0.11317749321460724\n",
            "Train epoch - Accuracy: 0.16618705035971224 Loss: 0.11875045987985117 Corrects: 1155\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11756113171577454\n",
            "Train step - Step 120, Loss 0.11730414628982544\n",
            "Train step - Step 130, Loss 0.11552576720714569\n",
            "Train step - Step 140, Loss 0.11537028849124908\n",
            "Train step - Step 150, Loss 0.11485763639211655\n",
            "Train step - Step 160, Loss 0.11286706477403641\n",
            "Train epoch - Accuracy: 0.18129496402877698 Loss: 0.1171990903097091 Corrects: 1260\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.1216273233294487\n",
            "Train step - Step 180, Loss 0.1142764613032341\n",
            "Train step - Step 190, Loss 0.1254003942012787\n",
            "Train step - Step 200, Loss 0.1140199825167656\n",
            "Train step - Step 210, Loss 0.11429524421691895\n",
            "Train epoch - Accuracy: 0.20676258992805754 Loss: 0.11578718427059462 Corrects: 1437\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11654850095510483\n",
            "Train step - Step 230, Loss 0.11354200541973114\n",
            "Train step - Step 240, Loss 0.11923739314079285\n",
            "Train step - Step 250, Loss 0.11700757592916489\n",
            "Train step - Step 260, Loss 0.11001159250736237\n",
            "Train step - Step 270, Loss 0.11452047526836395\n",
            "Train epoch - Accuracy: 0.23093525179856114 Loss: 0.11525057683316924 Corrects: 1605\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.11056678742170334\n",
            "Train step - Step 290, Loss 0.11832010746002197\n",
            "Train step - Step 300, Loss 0.116853728890419\n",
            "Train step - Step 310, Loss 0.11543288826942444\n",
            "Train step - Step 320, Loss 0.1121039018034935\n",
            "Train epoch - Accuracy: 0.239568345323741 Loss: 0.11485367940484191 Corrects: 1665\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.10666099190711975\n",
            "Train step - Step 340, Loss 0.112849660217762\n",
            "Train step - Step 350, Loss 0.11724250018596649\n",
            "Train step - Step 360, Loss 0.11462025344371796\n",
            "Train step - Step 370, Loss 0.11669430136680603\n",
            "Train step - Step 380, Loss 0.11310174316167831\n",
            "Train epoch - Accuracy: 0.2568345323741007 Loss: 0.1143492324210757 Corrects: 1785\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.11536680907011032\n",
            "Train step - Step 400, Loss 0.112955242395401\n",
            "Train step - Step 410, Loss 0.11194909363985062\n",
            "Train step - Step 420, Loss 0.11770566552877426\n",
            "Train step - Step 430, Loss 0.11373474448919296\n",
            "Train epoch - Accuracy: 0.2745323741007194 Loss: 0.11376347850123755 Corrects: 1908\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.11306513100862503\n",
            "Train step - Step 450, Loss 0.11350329220294952\n",
            "Train step - Step 460, Loss 0.11620602011680603\n",
            "Train step - Step 470, Loss 0.11100805550813675\n",
            "Train step - Step 480, Loss 0.1117439717054367\n",
            "Train step - Step 490, Loss 0.10749253630638123\n",
            "Train epoch - Accuracy: 0.2896402877697842 Loss: 0.11368241737643592 Corrects: 2013\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.11623327434062958\n",
            "Train step - Step 510, Loss 0.11686524748802185\n",
            "Train step - Step 520, Loss 0.11276733130216599\n",
            "Train step - Step 530, Loss 0.11707252264022827\n",
            "Train step - Step 540, Loss 0.1120595932006836\n",
            "Train epoch - Accuracy: 0.29884892086330933 Loss: 0.11344423043642113 Corrects: 2077\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.11201664060354233\n",
            "Train step - Step 560, Loss 0.10931085050106049\n",
            "Train step - Step 570, Loss 0.11441201716661453\n",
            "Train step - Step 580, Loss 0.11249684542417526\n",
            "Train step - Step 590, Loss 0.11337482184171677\n",
            "Train step - Step 600, Loss 0.11615990847349167\n",
            "Train epoch - Accuracy: 0.31107913669064746 Loss: 0.11340075375579244 Corrects: 2162\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.11100871115922928\n",
            "Train step - Step 620, Loss 0.11423536390066147\n",
            "Train step - Step 630, Loss 0.11838186532258987\n",
            "Train step - Step 640, Loss 0.11230996996164322\n",
            "Train step - Step 650, Loss 0.11297067999839783\n",
            "Train epoch - Accuracy: 0.32258992805755393 Loss: 0.112846786167553 Corrects: 2242\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10740680992603302\n",
            "Train step - Step 670, Loss 0.11527968943119049\n",
            "Train step - Step 680, Loss 0.12042137235403061\n",
            "Train step - Step 690, Loss 0.11054392904043198\n",
            "Train step - Step 700, Loss 0.1095813438296318\n",
            "Train step - Step 710, Loss 0.1049724817276001\n",
            "Train epoch - Accuracy: 0.3254676258992806 Loss: 0.11275973085233633 Corrects: 2262\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.1136636957526207\n",
            "Train step - Step 730, Loss 0.11523266136646271\n",
            "Train step - Step 740, Loss 0.1136997863650322\n",
            "Train step - Step 750, Loss 0.11923065781593323\n",
            "Train step - Step 760, Loss 0.11311086267232895\n",
            "Train epoch - Accuracy: 0.33482014388489206 Loss: 0.1126469631139323 Corrects: 2327\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.11281684786081314\n",
            "Train step - Step 780, Loss 0.11650063842535019\n",
            "Train step - Step 790, Loss 0.11473600566387177\n",
            "Train step - Step 800, Loss 0.11251138150691986\n",
            "Train step - Step 810, Loss 0.11765903234481812\n",
            "Train step - Step 820, Loss 0.11238054186105728\n",
            "Train epoch - Accuracy: 0.3454676258992806 Loss: 0.11262122858342508 Corrects: 2401\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.11919401586055756\n",
            "Train step - Step 840, Loss 0.11357080191373825\n",
            "Train step - Step 850, Loss 0.11343303322792053\n",
            "Train step - Step 860, Loss 0.1116923913359642\n",
            "Train step - Step 870, Loss 0.1131042093038559\n",
            "Train epoch - Accuracy: 0.35122302158273383 Loss: 0.11207550795601426 Corrects: 2441\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.11052924394607544\n",
            "Train step - Step 890, Loss 0.10482358932495117\n",
            "Train step - Step 900, Loss 0.10636837780475616\n",
            "Train step - Step 910, Loss 0.11081810295581818\n",
            "Train step - Step 920, Loss 0.11191578209400177\n",
            "Train step - Step 930, Loss 0.10955432802438736\n",
            "Train epoch - Accuracy: 0.36071942446043165 Loss: 0.11235991797644457 Corrects: 2507\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.11238051950931549\n",
            "Train step - Step 950, Loss 0.11739576607942581\n",
            "Train step - Step 960, Loss 0.11427891254425049\n",
            "Train step - Step 970, Loss 0.10874515026807785\n",
            "Train step - Step 980, Loss 0.11270540207624435\n",
            "Train epoch - Accuracy: 0.36431654676258995 Loss: 0.11205563925367465 Corrects: 2532\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.11111820489168167\n",
            "Train step - Step 1000, Loss 0.11154694110155106\n",
            "Train step - Step 1010, Loss 0.11184340715408325\n",
            "Train step - Step 1020, Loss 0.10627755522727966\n",
            "Train step - Step 1030, Loss 0.11793708801269531\n",
            "Train step - Step 1040, Loss 0.11133959144353867\n",
            "Train epoch - Accuracy: 0.3697841726618705 Loss: 0.11179144210309433 Corrects: 2570\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.11280340701341629\n",
            "Train step - Step 1060, Loss 0.11264730989933014\n",
            "Train step - Step 1070, Loss 0.1092832162976265\n",
            "Train step - Step 1080, Loss 0.10720846801996231\n",
            "Train step - Step 1090, Loss 0.11332501471042633\n",
            "Train epoch - Accuracy: 0.38244604316546765 Loss: 0.11147232895703625 Corrects: 2658\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.11254601180553436\n",
            "Train step - Step 1110, Loss 0.10876584053039551\n",
            "Train step - Step 1120, Loss 0.111696258187294\n",
            "Train step - Step 1130, Loss 0.11131282895803452\n",
            "Train step - Step 1140, Loss 0.11205687373876572\n",
            "Train step - Step 1150, Loss 0.11034910380840302\n",
            "Train epoch - Accuracy: 0.38446043165467625 Loss: 0.11138639373744992 Corrects: 2672\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10955408215522766\n",
            "Train step - Step 1170, Loss 0.1127733662724495\n",
            "Train step - Step 1180, Loss 0.10707147419452667\n",
            "Train step - Step 1190, Loss 0.11030612885951996\n",
            "Train step - Step 1200, Loss 0.11347579956054688\n",
            "Train epoch - Accuracy: 0.3814388489208633 Loss: 0.1115611317839554 Corrects: 2651\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.10884632915258408\n",
            "Train step - Step 1220, Loss 0.11366831511259079\n",
            "Train step - Step 1230, Loss 0.11168120056390762\n",
            "Train step - Step 1240, Loss 0.11466444283723831\n",
            "Train step - Step 1250, Loss 0.11257053166627884\n",
            "Train step - Step 1260, Loss 0.10899703949689865\n",
            "Train epoch - Accuracy: 0.3893525179856115 Loss: 0.11159475109774432 Corrects: 2706\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.11124502867460251\n",
            "Train step - Step 1280, Loss 0.11553194373846054\n",
            "Train step - Step 1290, Loss 0.11085567623376846\n",
            "Train step - Step 1300, Loss 0.11326587945222855\n",
            "Train step - Step 1310, Loss 0.10981163382530212\n",
            "Train epoch - Accuracy: 0.39741007194244604 Loss: 0.11160456498106607 Corrects: 2762\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.10753488540649414\n",
            "Train step - Step 1330, Loss 0.11150896549224854\n",
            "Train step - Step 1340, Loss 0.11015025526285172\n",
            "Train step - Step 1350, Loss 0.11132258921861649\n",
            "Train step - Step 1360, Loss 0.10968342423439026\n",
            "Train step - Step 1370, Loss 0.11144110560417175\n",
            "Train epoch - Accuracy: 0.41050359712230217 Loss: 0.11125937593069007 Corrects: 2853\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.11515199393033981\n",
            "Train step - Step 1390, Loss 0.11044837534427643\n",
            "Train step - Step 1400, Loss 0.10305175185203552\n",
            "Train step - Step 1410, Loss 0.10848773270845413\n",
            "Train step - Step 1420, Loss 0.11145687103271484\n",
            "Train epoch - Accuracy: 0.41007194244604317 Loss: 0.11109499747590196 Corrects: 2850\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.1070035994052887\n",
            "Train step - Step 1440, Loss 0.11277913302183151\n",
            "Train step - Step 1450, Loss 0.11234478652477264\n",
            "Train step - Step 1460, Loss 0.10928717255592346\n",
            "Train step - Step 1470, Loss 0.11461663991212845\n",
            "Train step - Step 1480, Loss 0.11687944829463959\n",
            "Train epoch - Accuracy: 0.4181294964028777 Loss: 0.11123203638860647 Corrects: 2906\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.11128254234790802\n",
            "Train step - Step 1500, Loss 0.11456124484539032\n",
            "Train step - Step 1510, Loss 0.11715060472488403\n",
            "Train step - Step 1520, Loss 0.10762407630681992\n",
            "Train step - Step 1530, Loss 0.111262746155262\n",
            "Train epoch - Accuracy: 0.4153956834532374 Loss: 0.11090628285845407 Corrects: 2887\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.11120828986167908\n",
            "Train step - Step 1550, Loss 0.11126578599214554\n",
            "Train step - Step 1560, Loss 0.11340449750423431\n",
            "Train step - Step 1570, Loss 0.11019483208656311\n",
            "Train step - Step 1580, Loss 0.11286943405866623\n",
            "Train step - Step 1590, Loss 0.1149769052863121\n",
            "Train epoch - Accuracy: 0.4162589928057554 Loss: 0.1111660575116281 Corrects: 2893\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.11194290965795517\n",
            "Train step - Step 1610, Loss 0.11284130066633224\n",
            "Train step - Step 1620, Loss 0.1077054813504219\n",
            "Train step - Step 1630, Loss 0.11191894114017487\n",
            "Train step - Step 1640, Loss 0.10993587225675583\n",
            "Train epoch - Accuracy: 0.4312230215827338 Loss: 0.11079684528086683 Corrects: 2997\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.1080731526017189\n",
            "Train step - Step 1660, Loss 0.10984128713607788\n",
            "Train step - Step 1670, Loss 0.11096865683794022\n",
            "Train step - Step 1680, Loss 0.11229459941387177\n",
            "Train step - Step 1690, Loss 0.11068448424339294\n",
            "Train step - Step 1700, Loss 0.11012537777423859\n",
            "Train epoch - Accuracy: 0.4300719424460432 Loss: 0.11065561116384945 Corrects: 2989\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.10809464007616043\n",
            "Train step - Step 1720, Loss 0.10567010194063187\n",
            "Train step - Step 1730, Loss 0.1107209101319313\n",
            "Train step - Step 1740, Loss 0.11146298050880432\n",
            "Train step - Step 1750, Loss 0.1132354587316513\n",
            "Train epoch - Accuracy: 0.441294964028777 Loss: 0.11079422479696412 Corrects: 3067\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.10851992666721344\n",
            "Train step - Step 1770, Loss 0.11201462149620056\n",
            "Train step - Step 1780, Loss 0.11066582053899765\n",
            "Train step - Step 1790, Loss 0.10723243653774261\n",
            "Train step - Step 1800, Loss 0.11054770648479462\n",
            "Train step - Step 1810, Loss 0.11226419359445572\n",
            "Train epoch - Accuracy: 0.4322302158273381 Loss: 0.11074140697288856 Corrects: 3004\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.10644669085741043\n",
            "Train step - Step 1830, Loss 0.11139556020498276\n",
            "Train step - Step 1840, Loss 0.10414527356624603\n",
            "Train step - Step 1850, Loss 0.10871492326259613\n",
            "Train step - Step 1860, Loss 0.11148341000080109\n",
            "Train epoch - Accuracy: 0.44402877697841725 Loss: 0.11070941352158141 Corrects: 3086\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.11019007861614227\n",
            "Train step - Step 1880, Loss 0.10894083976745605\n",
            "Train step - Step 1890, Loss 0.11385776102542877\n",
            "Train step - Step 1900, Loss 0.11300301551818848\n",
            "Train step - Step 1910, Loss 0.11296828091144562\n",
            "Train step - Step 1920, Loss 0.10726422816514969\n",
            "Train epoch - Accuracy: 0.4470503597122302 Loss: 0.11047847422764456 Corrects: 3107\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.10640314221382141\n",
            "Train step - Step 1940, Loss 0.10740918666124344\n",
            "Train step - Step 1950, Loss 0.11404767632484436\n",
            "Train step - Step 1960, Loss 0.10807978361845016\n",
            "Train step - Step 1970, Loss 0.1120174452662468\n",
            "Train epoch - Accuracy: 0.45884892086330936 Loss: 0.11021171030380743 Corrects: 3189\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.11276271939277649\n",
            "Train step - Step 1990, Loss 0.1098063588142395\n",
            "Train step - Step 2000, Loss 0.09906284511089325\n",
            "Train step - Step 2010, Loss 0.11249485611915588\n",
            "Train step - Step 2020, Loss 0.11432306468486786\n",
            "Train step - Step 2030, Loss 0.11648441106081009\n",
            "Train epoch - Accuracy: 0.4571223021582734 Loss: 0.1105662069625134 Corrects: 3177\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.11344647407531738\n",
            "Train step - Step 2050, Loss 0.11283976584672928\n",
            "Train step - Step 2060, Loss 0.1111178919672966\n",
            "Train step - Step 2070, Loss 0.11041387170553207\n",
            "Train step - Step 2080, Loss 0.11094225943088531\n",
            "Train epoch - Accuracy: 0.4566906474820144 Loss: 0.11020371581367451 Corrects: 3174\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.11312046647071838\n",
            "Train step - Step 2100, Loss 0.10876409709453583\n",
            "Train step - Step 2110, Loss 0.10700599104166031\n",
            "Train step - Step 2120, Loss 0.11417405307292938\n",
            "Train step - Step 2130, Loss 0.10538226366043091\n",
            "Train step - Step 2140, Loss 0.10739479213953018\n",
            "Train epoch - Accuracy: 0.4623021582733813 Loss: 0.10991667085628716 Corrects: 3213\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.1111285537481308\n",
            "Train step - Step 2160, Loss 0.11043570935726166\n",
            "Train step - Step 2170, Loss 0.1132730096578598\n",
            "Train step - Step 2180, Loss 0.10797766596078873\n",
            "Train step - Step 2190, Loss 0.1075136736035347\n",
            "Train epoch - Accuracy: 0.4646043165467626 Loss: 0.11013465460041444 Corrects: 3229\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.10778594762086868\n",
            "Train step - Step 2210, Loss 0.11285849660634995\n",
            "Train step - Step 2220, Loss 0.11016436666250229\n",
            "Train step - Step 2230, Loss 0.10984543710947037\n",
            "Train step - Step 2240, Loss 0.11526627093553543\n",
            "Train step - Step 2250, Loss 0.11246607452630997\n",
            "Train epoch - Accuracy: 0.46446043165467626 Loss: 0.10992793844758178 Corrects: 3228\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.11123178899288177\n",
            "Train step - Step 2270, Loss 0.11217700690031052\n",
            "Train step - Step 2280, Loss 0.10743870586156845\n",
            "Train step - Step 2290, Loss 0.11287301778793335\n",
            "Train step - Step 2300, Loss 0.11007250100374222\n",
            "Train epoch - Accuracy: 0.47784172661870505 Loss: 0.10995096202805746 Corrects: 3321\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.11518025398254395\n",
            "Train step - Step 2320, Loss 0.10609492659568787\n",
            "Train step - Step 2330, Loss 0.1085001677274704\n",
            "Train step - Step 2340, Loss 0.1124579980969429\n",
            "Train step - Step 2350, Loss 0.10852067172527313\n",
            "Train step - Step 2360, Loss 0.11443451792001724\n",
            "Train epoch - Accuracy: 0.46964028776978417 Loss: 0.11011540832922613 Corrects: 3264\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.10985345393419266\n",
            "Train step - Step 2380, Loss 0.11144489794969559\n",
            "Train step - Step 2390, Loss 0.11819238215684891\n",
            "Train step - Step 2400, Loss 0.11074376851320267\n",
            "Train step - Step 2410, Loss 0.11191879212856293\n",
            "Train epoch - Accuracy: 0.47870503597122305 Loss: 0.1100232518383925 Corrects: 3327\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.10789848119020462\n",
            "Train step - Step 2430, Loss 0.10747861862182617\n",
            "Train step - Step 2440, Loss 0.1044626384973526\n",
            "Train step - Step 2450, Loss 0.10420090705156326\n",
            "Train step - Step 2460, Loss 0.11318976432085037\n",
            "Train step - Step 2470, Loss 0.10898756980895996\n",
            "Train epoch - Accuracy: 0.47899280575539566 Loss: 0.10999306189070503 Corrects: 3329\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.10656280815601349\n",
            "Train step - Step 2490, Loss 0.11154776811599731\n",
            "Train step - Step 2500, Loss 0.10699678957462311\n",
            "Train step - Step 2510, Loss 0.10411857068538666\n",
            "Train step - Step 2520, Loss 0.10885108262300491\n",
            "Train epoch - Accuracy: 0.48345323741007196 Loss: 0.10958167232197823 Corrects: 3360\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.11255710572004318\n",
            "Train step - Step 2540, Loss 0.11555091291666031\n",
            "Train step - Step 2550, Loss 0.10906470566987991\n",
            "Train step - Step 2560, Loss 0.10887506604194641\n",
            "Train step - Step 2570, Loss 0.11346904188394547\n",
            "Train step - Step 2580, Loss 0.11148615926504135\n",
            "Train epoch - Accuracy: 0.48503597122302156 Loss: 0.10982423549504589 Corrects: 3371\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.10993651300668716\n",
            "Train step - Step 2600, Loss 0.10985113680362701\n",
            "Train step - Step 2610, Loss 0.10834859311580658\n",
            "Train step - Step 2620, Loss 0.10697795450687408\n",
            "Train step - Step 2630, Loss 0.1103862076997757\n",
            "Train epoch - Accuracy: 0.48733812949640287 Loss: 0.10975674594692189 Corrects: 3387\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.11245259642601013\n",
            "Train step - Step 2650, Loss 0.11202692985534668\n",
            "Train step - Step 2660, Loss 0.11235354095697403\n",
            "Train step - Step 2670, Loss 0.1054014042019844\n",
            "Train step - Step 2680, Loss 0.10780233144760132\n",
            "Train step - Step 2690, Loss 0.10777811706066132\n",
            "Train epoch - Accuracy: 0.48589928057553955 Loss: 0.10947825575904023 Corrects: 3377\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.11478321254253387\n",
            "Train step - Step 2710, Loss 0.11138267815113068\n",
            "Train step - Step 2720, Loss 0.10327067971229553\n",
            "Train step - Step 2730, Loss 0.11553207039833069\n",
            "Train step - Step 2740, Loss 0.11064747720956802\n",
            "Train epoch - Accuracy: 0.4925179856115108 Loss: 0.10923080480141606 Corrects: 3423\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.10803645849227905\n",
            "Train step - Step 2760, Loss 0.1095532476902008\n",
            "Train step - Step 2770, Loss 0.11082753539085388\n",
            "Train step - Step 2780, Loss 0.10915699601173401\n",
            "Train step - Step 2790, Loss 0.10794468969106674\n",
            "Train step - Step 2800, Loss 0.10739375650882721\n",
            "Train epoch - Accuracy: 0.5064748201438849 Loss: 0.10879517895926674 Corrects: 3520\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.11303187906742096\n",
            "Train step - Step 2820, Loss 0.11210775375366211\n",
            "Train step - Step 2830, Loss 0.11195001006126404\n",
            "Train step - Step 2840, Loss 0.1091427430510521\n",
            "Train step - Step 2850, Loss 0.10330520570278168\n",
            "Train epoch - Accuracy: 0.49798561151079135 Loss: 0.10895825965799016 Corrects: 3461\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.11089502274990082\n",
            "Train step - Step 2870, Loss 0.10892418771982193\n",
            "Train step - Step 2880, Loss 0.10656187683343887\n",
            "Train step - Step 2890, Loss 0.10784585773944855\n",
            "Train step - Step 2900, Loss 0.11091858893632889\n",
            "Train step - Step 2910, Loss 0.11401791125535965\n",
            "Train epoch - Accuracy: 0.5041726618705036 Loss: 0.10892918633685696 Corrects: 3504\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.10582082718610764\n",
            "Train step - Step 2930, Loss 0.10994654148817062\n",
            "Train step - Step 2940, Loss 0.10541077703237534\n",
            "Train step - Step 2950, Loss 0.10537267476320267\n",
            "Train step - Step 2960, Loss 0.11198296397924423\n",
            "Train epoch - Accuracy: 0.5053237410071942 Loss: 0.10881994242076394 Corrects: 3512\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.11389835178852081\n",
            "Train step - Step 2980, Loss 0.10447391867637634\n",
            "Train step - Step 2990, Loss 0.11201303452253342\n",
            "Train step - Step 3000, Loss 0.1058720275759697\n",
            "Train step - Step 3010, Loss 0.10759922116994858\n",
            "Train step - Step 3020, Loss 0.11055205017328262\n",
            "Train epoch - Accuracy: 0.5001438848920863 Loss: 0.10904232826807517 Corrects: 3476\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.10850048065185547\n",
            "Train step - Step 3040, Loss 0.11058203876018524\n",
            "Train step - Step 3050, Loss 0.10442548245191574\n",
            "Train step - Step 3060, Loss 0.107340008020401\n",
            "Train step - Step 3070, Loss 0.10866621136665344\n",
            "Train epoch - Accuracy: 0.5024460431654676 Loss: 0.1090829867060236 Corrects: 3492\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.11722080409526825\n",
            "Train step - Step 3090, Loss 0.11135861277580261\n",
            "Train step - Step 3100, Loss 0.1118389293551445\n",
            "Train step - Step 3110, Loss 0.10675091296434402\n",
            "Train step - Step 3120, Loss 0.10600626468658447\n",
            "Train step - Step 3130, Loss 0.10650550574064255\n",
            "Train epoch - Accuracy: 0.5064748201438849 Loss: 0.109238086565793 Corrects: 3520\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.10684335976839066\n",
            "Train step - Step 3150, Loss 0.11048531532287598\n",
            "Train step - Step 3160, Loss 0.10643172264099121\n",
            "Train step - Step 3170, Loss 0.10603352636098862\n",
            "Train step - Step 3180, Loss 0.11114582419395447\n",
            "Train epoch - Accuracy: 0.5044604316546762 Loss: 0.10907553566445549 Corrects: 3506\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.10778507590293884\n",
            "Train step - Step 3200, Loss 0.10976175218820572\n",
            "Train step - Step 3210, Loss 0.11332609504461288\n",
            "Train step - Step 3220, Loss 0.10442261397838593\n",
            "Train step - Step 3230, Loss 0.10723082721233368\n",
            "Train step - Step 3240, Loss 0.10427755117416382\n",
            "Train epoch - Accuracy: 0.49366906474820144 Loss: 0.10929572949092164 Corrects: 3431\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.10524629056453705\n",
            "Train step - Step 3260, Loss 0.10819964855909348\n",
            "Train step - Step 3270, Loss 0.10620851069688797\n",
            "Train step - Step 3280, Loss 0.11090235412120819\n",
            "Train step - Step 3290, Loss 0.10308661311864853\n",
            "Train epoch - Accuracy: 0.5035971223021583 Loss: 0.10893354634158045 Corrects: 3500\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.11024394631385803\n",
            "Train step - Step 3310, Loss 0.10673773288726807\n",
            "Train step - Step 3320, Loss 0.1092933863401413\n",
            "Train step - Step 3330, Loss 0.10927380621433258\n",
            "Train step - Step 3340, Loss 0.10730932652950287\n",
            "Train step - Step 3350, Loss 0.10309305787086487\n",
            "Train epoch - Accuracy: 0.5079136690647482 Loss: 0.10902472330297497 Corrects: 3530\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.11129717528820038\n",
            "Train step - Step 3370, Loss 0.10978084057569504\n",
            "Train step - Step 3380, Loss 0.11048128455877304\n",
            "Train step - Step 3390, Loss 0.10566554963588715\n",
            "Train step - Step 3400, Loss 0.11031026393175125\n",
            "Train epoch - Accuracy: 0.5050359712230216 Loss: 0.10893415376007985 Corrects: 3510\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.10509617626667023\n",
            "Train step - Step 3420, Loss 0.10180573910474777\n",
            "Train step - Step 3430, Loss 0.1096348911523819\n",
            "Train step - Step 3440, Loss 0.11126618087291718\n",
            "Train step - Step 3450, Loss 0.11009401082992554\n",
            "Train step - Step 3460, Loss 0.11095483601093292\n",
            "Train epoch - Accuracy: 0.5037410071942446 Loss: 0.1087986209366819 Corrects: 3501\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.10735827684402466\n",
            "Train step - Step 3480, Loss 0.10729442536830902\n",
            "Train step - Step 3490, Loss 0.10839799791574478\n",
            "Train step - Step 3500, Loss 0.11079046875238419\n",
            "Train step - Step 3510, Loss 0.10532907396554947\n",
            "Train epoch - Accuracy: 0.5074820143884892 Loss: 0.10872635985878731 Corrects: 3527\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.1121063232421875\n",
            "Train step - Step 3530, Loss 0.1089276447892189\n",
            "Train step - Step 3540, Loss 0.11132317036390305\n",
            "Train step - Step 3550, Loss 0.11112483590841293\n",
            "Train step - Step 3560, Loss 0.10539624840021133\n",
            "Train step - Step 3570, Loss 0.11309166252613068\n",
            "Train epoch - Accuracy: 0.5086330935251798 Loss: 0.10875009624434889 Corrects: 3535\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.10463821887969971\n",
            "Train step - Step 3590, Loss 0.1083456501364708\n",
            "Train step - Step 3600, Loss 0.11208462715148926\n",
            "Train step - Step 3610, Loss 0.10617833584547043\n",
            "Train step - Step 3620, Loss 0.10648415982723236\n",
            "Train epoch - Accuracy: 0.5054676258992806 Loss: 0.10883055514140094 Corrects: 3513\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.1134973093867302\n",
            "Train step - Step 3640, Loss 0.1046425849199295\n",
            "Train step - Step 3650, Loss 0.10849392414093018\n",
            "Train step - Step 3660, Loss 0.10880487412214279\n",
            "Train step - Step 3670, Loss 0.11140649765729904\n",
            "Train step - Step 3680, Loss 0.11175882816314697\n",
            "Train epoch - Accuracy: 0.5048920863309353 Loss: 0.10882648020982742 Corrects: 3509\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.11110129952430725\n",
            "Train step - Step 3700, Loss 0.10738219320774078\n",
            "Train step - Step 3710, Loss 0.10985168069601059\n",
            "Train step - Step 3720, Loss 0.10823574662208557\n",
            "Train step - Step 3730, Loss 0.11040154099464417\n",
            "Train epoch - Accuracy: 0.5066187050359712 Loss: 0.10888175349441363 Corrects: 3521\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.10949043929576874\n",
            "Train step - Step 3750, Loss 0.1051686555147171\n",
            "Train step - Step 3760, Loss 0.10337143391370773\n",
            "Train step - Step 3770, Loss 0.10748476535081863\n",
            "Train step - Step 3780, Loss 0.11219697445631027\n",
            "Train step - Step 3790, Loss 0.1050342246890068\n",
            "Train epoch - Accuracy: 0.5046043165467626 Loss: 0.10853031008363628 Corrects: 3507\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.10541073977947235\n",
            "Train step - Step 3810, Loss 0.11422277241945267\n",
            "Train step - Step 3820, Loss 0.10719963163137436\n",
            "Train step - Step 3830, Loss 0.10253601521253586\n",
            "Train step - Step 3840, Loss 0.11046653240919113\n",
            "Train epoch - Accuracy: 0.5063309352517985 Loss: 0.10856530608461915 Corrects: 3519\n",
            "Training finished in 401.8233149051666 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0, 79, 71, 63, 26, 14, 2, 73, 41, 56, 48, 99, 23, 66, 22, 6, 93, 17, 5, 1, 28, 87, 75, 55, 47, 39, 70, 42, 38, 29, 4, 83, 51, 18, 89, 85, 9, 84, 64, 20, 8]\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238225590>\n",
            "Constructing exemplars of class 83\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [26992, 39204, 30835, 34520, 49226, 32336, 43773, 49484, 4849, 49689, 48391, 49905, 311, 28036, 14953, 1857, 5618, 4408, 4991, 28652, 23548, 37021]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22413d9350>\n",
            "Constructing exemplars of class 51\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [4839, 26436, 19938, 8857, 10522, 47680, 2926, 7525, 48532, 12540, 1994, 713, 25056, 17143, 33969, 43362, 4778, 21189, 2882, 40224, 26797, 2381]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232ec3850>\n",
            "Constructing exemplars of class 18\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [44655, 46884, 9654, 437, 32203, 18847, 46329, 22976, 89, 41094, 2435, 1242, 11061, 2554, 10359, 20584, 21942, 14633, 4384, 38338, 32775, 8405]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238225590>\n",
            "Constructing exemplars of class 89\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [4633, 46031, 47744, 28360, 42165, 16618, 15526, 36363, 9168, 43324, 20340, 21688, 35994, 26335, 13415, 41994, 24089, 29224, 43373, 42624, 34007, 26716]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a7e090>\n",
            "Constructing exemplars of class 85\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [34165, 7911, 13562, 25446, 43576, 43692, 40029, 803, 49501, 32231, 12601, 5799, 6105, 27580, 13562, 10604, 16094, 36380, 11452, 46988, 46360, 40390]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232935650>\n",
            "Constructing exemplars of class 9\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [19148, 40902, 27408, 36125, 47217, 45314, 31463, 35341, 46470, 37927, 9131, 29221, 22462, 26685, 49020, 4477, 31474, 4425, 31777, 10860, 17401, 22513]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232f25850>\n",
            "Constructing exemplars of class 84\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [42878, 46177, 43425, 14973, 45186, 3808, 15662, 28012, 34961, 20195, 21532, 23425, 25895, 9022, 7949, 45212, 37019, 32817, 39707, 611, 40602, 31447]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223802bd50>\n",
            "Constructing exemplars of class 64\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [35131, 5876, 47777, 11339, 7605, 3583, 1820, 28199, 20867, 34735, 38038, 37690, 19318, 36455, 34476, 12388, 4665, 44105, 10418, 29199, 4137, 2134]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223823c3d0>\n",
            "Constructing exemplars of class 20\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [25504, 45876, 41499, 19315, 32522, 49630, 48234, 37538, 33814, 5231, 37657, 24540, 37898, 34447, 7813, 26019, 10329, 36557, 19222, 15143, 1827, 44914]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22318f5750>\n",
            "Constructing exemplars of class 8\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [5757, 45769, 1023, 16, 42539, 35210, 45233, 13295, 43052, 43343, 11388, 41735, 3954, 22403, 28530, 9066, 9094, 33226, 22948, 1555, 37517, 8222]\n",
            "x train:  [-0.09352785 -0.05136676 -0.04969806 -0.05857512 -0.05539732 -0.13016284\n",
            " -0.06102295 -0.08115377 -0.00628711 -0.04326263 -0.16178523 -0.10003998\n",
            " -0.05314515 -0.0704823  -0.07987405 -0.09192872 -0.06082618 -0.09903963\n",
            " -0.17164305 -0.11467971 -0.01610225 -0.08194405 -0.07670053 -0.08898933\n",
            "  0.00143447 -0.10025608 -0.12557998 -0.08642658 -0.08870966 -0.1502889\n",
            " -0.0842734  -0.06207918 -0.06425309 -0.14676146 -0.06749631 -0.07467256\n",
            " -0.09115262  0.05204477 -0.14654665 -0.13180998 -0.1246314  -0.12980564\n",
            " -0.085691    0.03679985 -0.0724887  -0.12419569 -0.1173562  -0.09047754\n",
            " -0.10537238 -0.03525478 -0.11926162 -0.13276824 -0.17137156 -0.06704759\n",
            " -0.05119825 -0.07478905 -0.1407187  -0.11746655 -0.1142459  -0.11001165\n",
            " -0.07796169 -0.12239216 -0.13335155 -0.09141608 -0.04167837 -0.16745898\n",
            " -0.14706273 -0.14847566 -0.04259048 -0.12654921 -0.12332562 -0.14890416\n",
            " -0.12440994 -0.10057245 -0.09354613  0.02922344 -0.16116078 -0.14937773\n",
            " -0.0837955  -0.10856777  0.05189877 -0.08517921 -0.12563795 -0.11871632\n",
            " -0.18916553 -0.09354958 -0.09550085 -0.08747629 -0.16797033 -0.13474862]\n",
            "y_train:  [tensor([92]), tensor([77]), tensor([25]), tensor([70]), tensor([70]), tensor([89]), tensor([23]), tensor([28]), tensor([4]), tensor([23]), tensor([75]), tensor([93]), tensor([18]), tensor([23]), tensor([3]), tensor([31]), tensor([99]), tensor([99]), tensor([45]), tensor([28]), tensor([59]), tensor([14]), tensor([92]), tensor([64]), tensor([47]), tensor([8]), tensor([32]), tensor([42]), tensor([25]), tensor([66]), tensor([21]), tensor([41]), tensor([99]), tensor([45]), tensor([49]), tensor([3]), tensor([24]), tensor([48]), tensor([4]), tensor([21]), tensor([42]), tensor([42]), tensor([93]), tensor([41]), tensor([46]), tensor([4]), tensor([9]), tensor([49]), tensor([94]), tensor([96]), tensor([93]), tensor([60]), tensor([8]), tensor([84]), tensor([30]), tensor([22]), tensor([27]), tensor([86]), tensor([39]), tensor([86]), tensor([31]), tensor([85]), tensor([66]), tensor([66]), tensor([47]), tensor([32]), tensor([79]), tensor([25]), tensor([1]), tensor([23]), tensor([48]), tensor([93]), tensor([53]), tensor([58]), tensor([73]), tensor([37]), tensor([71]), tensor([67]), tensor([65]), tensor([5]), tensor([8]), tensor([46]), tensor([6]), tensor([11]), tensor([39]), tensor([52]), tensor([50]), tensor([88]), tensor([73]), tensor([36]), tensor([68]), tensor([12]), tensor([0]), tensor([95]), tensor([14]), tensor([80]), tensor([32]), tensor([51]), tensor([29]), tensor([85]), tensor([83]), tensor([10]), tensor([52]), tensor([4]), tensor([47]), tensor([1]), tensor([36]), tensor([33]), tensor([26]), tensor([93]), tensor([53]), tensor([81]), tensor([2]), tensor([20]), tensor([35]), tensor([56]), tensor([72]), tensor([31]), tensor([45]), tensor([51]), tensor([24]), tensor([99]), tensor([3]), tensor([30]), tensor([72]), tensor([82]), tensor([97]), tensor([18]), tensor([26]), tensor([4]), tensor([37]), tensor([33]), tensor([28]), tensor([60]), tensor([72]), tensor([91]), tensor([1]), tensor([30]), tensor([50]), tensor([85]), tensor([79]), tensor([84]), tensor([59]), tensor([10]), tensor([91]), tensor([17]), tensor([84]), tensor([41]), tensor([16]), tensor([11]), tensor([98]), tensor([3]), tensor([57]), tensor([81]), tensor([46]), tensor([25]), tensor([55]), tensor([94]), tensor([64]), tensor([34]), tensor([55]), tensor([46]), tensor([98]), tensor([73]), tensor([18]), tensor([14]), tensor([59]), tensor([58]), tensor([99]), tensor([49]), tensor([21]), tensor([94]), tensor([18]), tensor([2]), tensor([98]), tensor([23]), tensor([47]), tensor([31]), tensor([3]), tensor([25]), tensor([80]), tensor([96]), tensor([12]), tensor([86]), tensor([85]), tensor([24]), tensor([46]), tensor([38]), tensor([84]), tensor([39]), tensor([91]), tensor([82]), tensor([83]), tensor([25]), tensor([28]), tensor([81]), tensor([46]), tensor([27]), tensor([22]), tensor([85]), tensor([31]), tensor([94]), tensor([41]), tensor([48]), tensor([67]), tensor([67]), tensor([81]), tensor([11]), tensor([83]), tensor([13]), tensor([18]), tensor([85]), tensor([22]), tensor([33]), tensor([5]), tensor([14]), tensor([65]), tensor([98]), tensor([6]), tensor([57]), tensor([38]), tensor([16]), tensor([77]), tensor([5]), tensor([56]), tensor([95]), tensor([12]), tensor([39]), tensor([2]), tensor([38]), tensor([70]), tensor([38]), tensor([67]), tensor([28]), tensor([95]), tensor([62]), tensor([82]), tensor([11]), tensor([82]), tensor([51]), tensor([99]), tensor([99]), tensor([70]), tensor([17]), tensor([94]), tensor([34]), tensor([66]), tensor([52]), tensor([12]), tensor([59]), tensor([8]), tensor([81]), tensor([56]), tensor([98]), tensor([63]), tensor([86]), tensor([36]), tensor([66]), tensor([31]), tensor([62]), tensor([51]), tensor([45]), tensor([42]), tensor([60]), tensor([32]), tensor([80]), tensor([58]), tensor([33]), tensor([16]), tensor([20]), tensor([78]), tensor([47]), tensor([49]), tensor([8]), tensor([32]), tensor([95]), tensor([69]), tensor([0]), tensor([33]), tensor([12]), tensor([83]), tensor([94]), tensor([75]), tensor([62]), tensor([48]), tensor([70]), tensor([63]), tensor([29]), tensor([8]), tensor([62]), tensor([65]), tensor([97]), tensor([16]), tensor([92]), tensor([70]), tensor([66]), tensor([80]), tensor([29]), tensor([97]), tensor([98]), tensor([60]), tensor([82]), tensor([31]), tensor([23]), tensor([35]), tensor([71]), tensor([66]), tensor([5]), tensor([42]), tensor([52]), tensor([18]), tensor([95]), tensor([95]), tensor([79]), tensor([82]), tensor([17]), tensor([29]), tensor([81]), tensor([63]), tensor([82]), tensor([65]), tensor([26]), tensor([38]), tensor([16]), tensor([62]), tensor([47]), tensor([25]), tensor([47]), tensor([45]), tensor([78]), tensor([91]), tensor([99]), tensor([55]), tensor([62]), tensor([26]), tensor([95]), tensor([8]), tensor([51]), tensor([71]), tensor([42]), tensor([21]), tensor([2]), tensor([1]), tensor([30]), tensor([93]), tensor([87]), tensor([3]), tensor([68]), tensor([65]), tensor([77]), tensor([88]), tensor([34]), tensor([94]), tensor([12]), tensor([41]), tensor([63]), tensor([72]), tensor([32]), tensor([69]), tensor([69]), tensor([86]), tensor([47]), tensor([70]), tensor([56]), tensor([80]), tensor([21]), tensor([27]), tensor([97]), tensor([20]), tensor([1]), tensor([97]), tensor([53]), tensor([24]), tensor([45]), tensor([92]), tensor([0]), tensor([88]), tensor([92]), tensor([78]), tensor([33]), tensor([85]), tensor([79]), tensor([7]), tensor([14]), tensor([80]), tensor([94]), tensor([71]), tensor([33]), tensor([81]), tensor([30]), tensor([36]), tensor([53]), tensor([5]), tensor([97]), tensor([39]), tensor([86]), tensor([25]), tensor([86]), tensor([12]), tensor([96]), tensor([7]), tensor([82]), tensor([10]), tensor([58]), tensor([87]), tensor([33]), tensor([66]), tensor([6]), tensor([36]), tensor([45]), tensor([99]), tensor([30]), tensor([11]), tensor([0]), tensor([17]), tensor([47]), tensor([63]), tensor([49]), tensor([25]), tensor([55]), tensor([88]), tensor([66]), tensor([77]), tensor([98]), tensor([31]), tensor([92]), tensor([85]), tensor([27]), tensor([36]), tensor([98]), tensor([79]), tensor([81]), tensor([66]), tensor([84]), tensor([98]), tensor([17]), tensor([2]), tensor([55]), tensor([16]), tensor([2]), tensor([42]), tensor([30]), tensor([89]), tensor([0]), tensor([72]), tensor([98]), tensor([8]), tensor([10]), tensor([87]), tensor([73]), tensor([23]), tensor([16]), tensor([10]), tensor([70]), tensor([31]), tensor([12]), tensor([88]), tensor([34]), tensor([60]), tensor([75]), tensor([87]), tensor([6]), tensor([37]), tensor([50]), tensor([41]), tensor([51]), tensor([24]), tensor([7]), tensor([64]), tensor([21]), tensor([1]), tensor([23]), tensor([79]), tensor([58]), tensor([83]), tensor([52]), tensor([32]), tensor([89]), tensor([1]), tensor([67]), tensor([95]), tensor([25]), tensor([65]), tensor([50]), tensor([53]), tensor([68]), tensor([23]), tensor([86]), tensor([95]), tensor([94]), tensor([11]), tensor([38]), tensor([48]), tensor([32]), tensor([60]), tensor([68]), tensor([93]), tensor([20]), tensor([13]), tensor([35]), tensor([49]), tensor([88]), tensor([38]), tensor([35]), tensor([75]), tensor([8]), tensor([18]), tensor([34]), tensor([97]), tensor([82]), tensor([9]), tensor([87]), tensor([21]), tensor([13]), tensor([45]), tensor([10]), tensor([84]), tensor([67]), tensor([94]), tensor([10]), tensor([67]), tensor([34]), tensor([45]), tensor([75]), tensor([5]), tensor([99]), tensor([84]), tensor([31]), tensor([64]), tensor([96]), tensor([41]), tensor([79]), tensor([93]), tensor([71]), tensor([47]), tensor([48]), tensor([93]), tensor([72]), tensor([80]), tensor([72]), tensor([70]), tensor([88]), tensor([41]), tensor([98]), tensor([45]), tensor([16]), tensor([37]), tensor([50]), tensor([26]), tensor([30]), tensor([42]), tensor([79]), tensor([92]), tensor([87]), tensor([66]), tensor([78]), tensor([8]), tensor([0]), tensor([98]), tensor([49]), tensor([72]), tensor([53]), tensor([2]), tensor([67]), tensor([71]), tensor([77]), tensor([46]), tensor([84]), tensor([91]), tensor([83]), tensor([64]), tensor([72]), tensor([68]), tensor([38]), tensor([87]), tensor([46]), tensor([91]), tensor([86]), tensor([9]), tensor([60]), tensor([37]), tensor([11]), tensor([4]), tensor([22]), tensor([73]), tensor([89]), tensor([32]), tensor([56]), tensor([1]), tensor([80]), tensor([72]), tensor([10]), tensor([51]), tensor([23]), tensor([98]), tensor([53]), tensor([55]), tensor([71]), tensor([46]), tensor([65]), tensor([95]), tensor([67]), tensor([63]), tensor([29]), tensor([27]), tensor([1]), tensor([87]), tensor([49]), tensor([12]), tensor([20]), tensor([12]), tensor([5]), tensor([75]), tensor([25]), tensor([38]), tensor([48]), tensor([31]), tensor([33]), tensor([92]), tensor([9]), tensor([89]), tensor([34]), tensor([51]), tensor([29]), tensor([11]), tensor([70]), tensor([20]), tensor([55]), tensor([86]), tensor([60]), tensor([46]), tensor([45]), tensor([65]), tensor([87]), tensor([59]), tensor([69]), tensor([83]), tensor([11]), tensor([20]), tensor([25]), tensor([85]), tensor([38]), tensor([7]), tensor([34]), tensor([92]), tensor([35]), tensor([2]), tensor([38]), tensor([53]), tensor([5]), tensor([81]), tensor([78]), tensor([7]), tensor([96]), tensor([70]), tensor([28]), tensor([80]), tensor([63]), tensor([72]), tensor([57]), tensor([45]), tensor([60]), tensor([33]), tensor([4]), tensor([5]), tensor([96]), tensor([87]), tensor([83]), tensor([46]), tensor([37]), tensor([78]), tensor([27]), tensor([78]), tensor([41]), tensor([77]), tensor([47]), tensor([71]), tensor([36]), tensor([71]), tensor([81]), tensor([69]), tensor([63]), tensor([77]), tensor([97]), tensor([62]), tensor([39]), tensor([22]), tensor([65]), tensor([92]), tensor([68]), tensor([41]), tensor([57]), tensor([21]), tensor([0]), tensor([92]), tensor([96]), tensor([30]), tensor([51]), tensor([21]), tensor([63]), tensor([92]), tensor([42]), tensor([72]), tensor([34]), tensor([53]), tensor([31]), tensor([11]), tensor([39]), tensor([57]), tensor([9]), tensor([80]), tensor([34]), tensor([2]), tensor([5]), tensor([46]), tensor([4]), tensor([23]), tensor([56]), tensor([4]), tensor([56]), tensor([22]), tensor([29]), tensor([18]), tensor([26]), tensor([20]), tensor([16]), tensor([0]), tensor([25]), tensor([7]), tensor([99]), tensor([98]), tensor([37]), tensor([57]), tensor([98]), tensor([99]), tensor([87]), tensor([13]), tensor([38]), tensor([50]), tensor([29]), tensor([69]), tensor([91]), tensor([80]), tensor([57]), tensor([69]), tensor([30]), tensor([6]), tensor([23]), tensor([46]), tensor([26]), tensor([93]), tensor([94]), tensor([60]), tensor([94]), tensor([62]), tensor([56]), tensor([32]), tensor([42]), tensor([48]), tensor([31]), tensor([62]), tensor([45]), tensor([39]), tensor([24]), tensor([35]), tensor([23]), tensor([28]), tensor([13]), tensor([52]), tensor([57]), tensor([24]), tensor([80]), tensor([41]), tensor([51]), tensor([1]), tensor([20]), tensor([88]), tensor([51]), tensor([72]), tensor([58]), tensor([88]), tensor([18]), tensor([97]), tensor([23]), tensor([18]), tensor([27]), tensor([77]), tensor([82]), tensor([47]), tensor([6]), tensor([28]), tensor([50]), tensor([25]), tensor([5]), tensor([2]), tensor([37]), tensor([8]), tensor([0]), tensor([96]), tensor([85]), tensor([67]), tensor([59]), tensor([98]), tensor([11]), tensor([0]), tensor([60]), tensor([11]), tensor([7]), tensor([24]), tensor([99]), tensor([53]), tensor([49]), tensor([33]), tensor([89]), tensor([52]), tensor([81]), tensor([72]), tensor([95]), tensor([56]), tensor([69]), tensor([21]), tensor([22]), tensor([60]), tensor([5]), tensor([5]), tensor([10]), tensor([86]), tensor([83]), tensor([16]), tensor([58]), tensor([28]), tensor([3]), tensor([59]), tensor([55]), tensor([2]), tensor([59]), tensor([75]), tensor([22]), tensor([88]), tensor([42]), tensor([58]), tensor([27]), tensor([68]), tensor([94]), tensor([87]), tensor([66]), tensor([59]), tensor([63]), tensor([39]), tensor([14]), tensor([0]), tensor([11]), tensor([16]), tensor([48]), tensor([14]), tensor([36]), tensor([7]), tensor([58]), tensor([79]), tensor([67]), tensor([27]), tensor([29]), tensor([45]), tensor([81]), tensor([71]), tensor([7]), tensor([10]), tensor([37]), tensor([55]), tensor([87]), tensor([95]), tensor([26]), tensor([82]), tensor([30]), tensor([91]), tensor([73]), tensor([78]), tensor([65]), tensor([87]), tensor([51]), tensor([69]), tensor([86]), tensor([24]), tensor([22]), tensor([73]), tensor([48]), tensor([73]), tensor([68]), tensor([14]), tensor([14]), tensor([56]), tensor([12]), tensor([62]), tensor([68]), tensor([55]), tensor([89]), tensor([45]), tensor([6]), tensor([48]), tensor([53]), tensor([10]), tensor([55]), tensor([80]), tensor([51]), tensor([86]), tensor([20]), tensor([24]), tensor([4]), tensor([51]), tensor([64]), tensor([73]), tensor([1]), tensor([18]), tensor([26]), tensor([31]), tensor([84]), tensor([39]), tensor([1]), tensor([41]), tensor([68]), tensor([2]), tensor([46]), tensor([48]), tensor([31]), tensor([42]), tensor([85]), tensor([64]), tensor([26]), tensor([93]), tensor([50]), tensor([3]), tensor([8]), tensor([67]), tensor([97]), tensor([18]), tensor([63]), tensor([49]), tensor([91]), tensor([12]), tensor([8]), tensor([89]), tensor([12]), tensor([58]), tensor([88]), tensor([34]), tensor([86]), tensor([66]), tensor([78]), tensor([58]), tensor([10]), tensor([36]), tensor([97]), tensor([59]), tensor([18]), tensor([86]), tensor([53]), tensor([28]), tensor([3]), tensor([27]), tensor([62]), tensor([56]), tensor([23]), tensor([42]), tensor([29]), tensor([33]), tensor([97]), tensor([28]), tensor([73]), tensor([96]), tensor([79]), tensor([67]), tensor([99]), tensor([85]), tensor([47]), tensor([65]), tensor([50]), tensor([23]), tensor([64]), tensor([36]), tensor([60]), tensor([63]), tensor([67]), tensor([79]), tensor([62]), tensor([89]), tensor([22]), tensor([24]), tensor([28]), tensor([10]), tensor([84]), tensor([67]), tensor([18]), tensor([88]), tensor([35]), tensor([97]), tensor([16]), tensor([26]), tensor([81]), tensor([96]), tensor([58]), tensor([22]), tensor([20]), tensor([80]), tensor([60]), tensor([59]), tensor([81]), tensor([27]), tensor([84]), tensor([6]), tensor([27]), tensor([39]), tensor([48]), tensor([82]), tensor([98]), tensor([8]), tensor([27]), tensor([6]), tensor([65]), tensor([39]), tensor([99]), tensor([53]), tensor([73]), tensor([4]), tensor([50]), tensor([4]), tensor([17]), tensor([81]), tensor([45]), tensor([77]), tensor([22]), tensor([41]), tensor([24]), tensor([37]), tensor([58]), tensor([24]), tensor([13]), tensor([93]), tensor([93]), tensor([89]), tensor([50]), tensor([57]), tensor([68]), tensor([99]), tensor([42]), tensor([69]), tensor([71]), tensor([77]), tensor([18]), tensor([27]), tensor([41]), tensor([78]), tensor([28]), tensor([29]), tensor([9]), tensor([25]), tensor([79]), tensor([98]), tensor([1]), tensor([24]), tensor([68]), tensor([60]), tensor([96]), tensor([29]), tensor([56]), tensor([6]), tensor([31]), tensor([94]), tensor([69]), tensor([34]), tensor([73]), tensor([91]), tensor([12]), tensor([55]), tensor([32]), tensor([32]), tensor([88]), tensor([21]), tensor([79]), tensor([62]), tensor([0]), tensor([89]), tensor([59]), tensor([92]), tensor([65]), tensor([21]), tensor([85]), tensor([30]), tensor([99]), tensor([38]), tensor([9]), tensor([96]), tensor([46]), tensor([66]), tensor([53]), tensor([50]), tensor([82]), tensor([14]), tensor([84]), tensor([9]), tensor([18]), tensor([50]), tensor([3]), tensor([52]), tensor([0]), tensor([17]), tensor([36]), tensor([70]), tensor([38]), tensor([97]), tensor([55]), tensor([50]), tensor([14]), tensor([89]), tensor([11]), tensor([77]), tensor([82]), tensor([21]), tensor([98]), tensor([64]), tensor([42]), tensor([33]), tensor([42]), tensor([1]), tensor([56]), tensor([96]), tensor([10]), tensor([39]), tensor([3]), tensor([36]), tensor([56]), tensor([78]), tensor([91]), tensor([29]), tensor([57]), tensor([7]), tensor([20]), tensor([87]), tensor([21]), tensor([78]), tensor([72]), tensor([55]), tensor([1]), tensor([30]), tensor([92]), tensor([18]), tensor([25]), tensor([97]), tensor([28]), tensor([38]), tensor([52]), tensor([96]), tensor([35]), tensor([7]), tensor([65]), tensor([91]), tensor([2]), tensor([53]), tensor([57]), tensor([35]), tensor([70]), tensor([3]), tensor([93]), tensor([26]), tensor([99]), tensor([12]), tensor([16]), tensor([36]), tensor([6]), tensor([87]), tensor([42]), tensor([35]), tensor([82]), tensor([36]), tensor([13]), tensor([23]), tensor([29]), tensor([21]), tensor([97]), tensor([91]), tensor([57]), tensor([42]), tensor([9]), tensor([71]), tensor([39]), tensor([6]), tensor([83]), tensor([9]), tensor([64]), tensor([73]), tensor([14]), tensor([60]), tensor([68]), tensor([64]), tensor([47]), tensor([79]), tensor([16]), tensor([62]), tensor([89]), tensor([99]), tensor([78]), tensor([83]), tensor([22]), tensor([25]), tensor([9]), tensor([72]), tensor([37]), tensor([9]), tensor([57]), tensor([11]), tensor([5]), tensor([52]), tensor([13]), tensor([77]), tensor([91]), tensor([63]), tensor([3]), tensor([97]), tensor([59]), tensor([2]), tensor([22]), tensor([22]), tensor([13]), tensor([32]), tensor([49]), tensor([28]), tensor([27]), tensor([34]), tensor([48]), tensor([94]), tensor([63]), tensor([6]), tensor([27]), tensor([79]), tensor([23]), tensor([17]), tensor([22]), tensor([24]), tensor([86]), tensor([1]), tensor([52]), tensor([13]), tensor([35]), tensor([79]), tensor([77]), tensor([88]), tensor([65]), tensor([55]), tensor([66]), tensor([34]), tensor([86]), tensor([51]), tensor([5]), tensor([23]), tensor([34]), tensor([92]), tensor([42]), tensor([82]), tensor([9]), tensor([52]), tensor([58]), tensor([70]), tensor([66]), tensor([9]), tensor([86]), tensor([52]), tensor([45]), tensor([85]), tensor([93]), tensor([45]), tensor([97]), tensor([23]), tensor([5]), tensor([96]), tensor([88]), tensor([70]), tensor([51]), tensor([17]), tensor([2]), tensor([3]), tensor([28]), tensor([21]), tensor([92]), tensor([56]), tensor([55]), tensor([85]), tensor([87]), tensor([52]), tensor([29]), tensor([86]), tensor([29]), tensor([36]), tensor([7]), tensor([22]), tensor([71]), tensor([62]), tensor([17]), tensor([62]), tensor([38]), tensor([91]), tensor([77]), tensor([16]), tensor([18]), tensor([50]), tensor([78]), tensor([58]), tensor([5]), tensor([35]), tensor([1]), tensor([79]), tensor([97]), tensor([89]), tensor([67]), tensor([58]), tensor([73]), tensor([84]), tensor([92]), tensor([32]), tensor([47]), tensor([68]), tensor([17]), tensor([79]), tensor([33]), tensor([16]), tensor([3]), tensor([96]), tensor([50]), tensor([69]), tensor([10]), tensor([7]), tensor([91]), tensor([72]), tensor([98]), tensor([52]), tensor([11]), tensor([45]), tensor([83]), tensor([10]), tensor([3]), tensor([87]), tensor([65]), tensor([39]), tensor([9]), tensor([26]), tensor([75]), tensor([79]), tensor([66]), tensor([26]), tensor([80]), tensor([48]), tensor([64]), tensor([39]), tensor([94]), tensor([13]), tensor([2]), tensor([41]), tensor([13]), tensor([69]), tensor([97]), tensor([28]), tensor([34]), tensor([67]), tensor([27]), tensor([87]), tensor([36]), tensor([10]), tensor([7]), tensor([78]), tensor([35]), tensor([89]), tensor([7]), tensor([75]), tensor([56]), tensor([35]), tensor([65]), tensor([32]), tensor([89]), tensor([37]), tensor([75]), tensor([45]), tensor([13]), tensor([73]), tensor([75]), tensor([35]), tensor([56]), tensor([93]), tensor([18]), tensor([3]), tensor([89]), tensor([10]), tensor([60]), tensor([86]), tensor([5]), tensor([24]), tensor([14]), tensor([71]), tensor([91]), tensor([57]), tensor([11]), tensor([77]), tensor([69]), tensor([75]), tensor([21]), tensor([91]), tensor([56]), tensor([70]), tensor([98]), tensor([0]), tensor([37]), tensor([62]), tensor([10]), tensor([68]), tensor([7]), tensor([71]), tensor([81]), tensor([20]), tensor([8]), tensor([85]), tensor([65]), tensor([69]), tensor([70]), tensor([63]), tensor([30]), tensor([73]), tensor([24]), tensor([75]), tensor([34]), tensor([73]), tensor([37]), tensor([71]), tensor([41]), tensor([9]), tensor([88]), tensor([78]), tensor([78]), tensor([68]), tensor([55]), tensor([24]), tensor([59]), tensor([88]), tensor([31]), tensor([80]), tensor([41]), tensor([31]), tensor([91]), tensor([67]), tensor([6]), tensor([30]), tensor([41]), tensor([13]), tensor([33]), tensor([4]), tensor([4]), tensor([95]), tensor([32]), tensor([41]), tensor([14]), tensor([63]), tensor([77]), tensor([7]), tensor([8]), tensor([77]), tensor([22]), tensor([84]), tensor([22]), tensor([53]), tensor([25]), tensor([53]), tensor([67]), tensor([37]), tensor([26]), tensor([39]), tensor([17]), tensor([8]), tensor([42]), tensor([37]), tensor([51]), tensor([8]), tensor([64]), tensor([12]), tensor([59]), tensor([37]), tensor([83]), tensor([24]), tensor([62]), tensor([95]), tensor([20]), tensor([11]), tensor([33]), tensor([34]), tensor([25]), tensor([9]), tensor([22]), tensor([78]), tensor([84]), tensor([70]), tensor([49]), tensor([94]), tensor([26]), tensor([72]), tensor([27]), tensor([96]), tensor([51]), tensor([7]), tensor([29]), tensor([64]), tensor([1]), tensor([66]), tensor([28]), tensor([82]), tensor([99]), tensor([24]), tensor([62]), tensor([36]), tensor([75]), tensor([5]), tensor([52]), tensor([20]), tensor([56]), tensor([85]), tensor([64]), tensor([91]), tensor([30]), tensor([39]), tensor([47]), tensor([47]), tensor([94]), tensor([59]), tensor([65]), tensor([91]), tensor([48]), tensor([92]), tensor([75]), tensor([80]), tensor([21]), tensor([0]), tensor([30]), tensor([57]), tensor([97]), tensor([27]), tensor([94]), tensor([88]), tensor([63]), tensor([39]), tensor([84]), tensor([14]), tensor([20]), tensor([59]), tensor([82]), tensor([73]), tensor([92]), tensor([36]), tensor([13]), tensor([83]), tensor([7]), tensor([37]), tensor([51]), tensor([10]), tensor([71]), tensor([66]), tensor([17]), tensor([83]), tensor([47]), tensor([52]), tensor([9]), tensor([88]), tensor([11]), tensor([62]), tensor([81]), tensor([3]), tensor([13]), tensor([35]), tensor([14]), tensor([84]), tensor([56]), tensor([93]), tensor([50]), tensor([83]), tensor([16]), tensor([83]), tensor([84]), tensor([92]), tensor([55]), tensor([37]), tensor([22]), tensor([49]), tensor([95]), tensor([84]), tensor([49]), tensor([58]), tensor([50]), tensor([47]), tensor([0]), tensor([33]), tensor([57]), tensor([6]), tensor([89]), tensor([59]), tensor([4]), tensor([32]), tensor([26]), tensor([3]), tensor([11]), tensor([29]), tensor([2]), tensor([87]), tensor([13]), tensor([30]), tensor([21]), tensor([33]), tensor([0]), tensor([4]), tensor([95]), tensor([77]), tensor([38]), tensor([53]), tensor([38]), tensor([80]), tensor([71]), tensor([73]), tensor([82]), tensor([67]), tensor([12]), tensor([63]), tensor([30]), tensor([71]), tensor([26]), tensor([49]), tensor([52]), tensor([1]), tensor([68]), tensor([71]), tensor([8]), tensor([51]), tensor([39]), tensor([83]), tensor([2]), tensor([92]), tensor([87]), tensor([13]), tensor([36]), tensor([8]), tensor([80]), tensor([49]), tensor([46]), tensor([70]), tensor([72]), tensor([69]), tensor([36]), tensor([31]), tensor([48]), tensor([96]), tensor([4]), tensor([25]), tensor([6]), tensor([93]), tensor([84]), tensor([29]), tensor([48]), tensor([83]), tensor([73]), tensor([55]), tensor([48]), tensor([64]), tensor([13]), tensor([77]), tensor([36]), tensor([93]), tensor([45]), tensor([42]), tensor([0]), tensor([9]), tensor([85]), tensor([55]), tensor([89]), tensor([26]), tensor([32]), tensor([17]), tensor([75]), tensor([63]), tensor([6]), tensor([75]), tensor([59]), tensor([58]), tensor([46]), tensor([17]), tensor([17]), tensor([81]), tensor([18]), tensor([41]), tensor([30]), tensor([41]), tensor([85]), tensor([12]), tensor([29]), tensor([24]), tensor([70]), tensor([47]), tensor([57]), tensor([36]), tensor([10]), tensor([56]), tensor([35]), tensor([51]), tensor([95]), tensor([1]), tensor([45]), tensor([31]), tensor([89]), tensor([12]), tensor([78]), tensor([65]), tensor([6]), tensor([13]), tensor([60]), tensor([59]), tensor([22]), tensor([96]), tensor([72]), tensor([38]), tensor([39]), tensor([17]), tensor([60]), tensor([77]), tensor([21]), tensor([46]), tensor([66]), tensor([35]), tensor([71]), tensor([75]), tensor([32]), tensor([49]), tensor([0]), tensor([7]), tensor([58]), tensor([24]), tensor([41]), tensor([2]), tensor([49]), tensor([11]), tensor([75]), tensor([51]), tensor([69]), tensor([23]), tensor([64]), tensor([20]), tensor([53]), tensor([6]), tensor([4]), tensor([59]), tensor([64]), tensor([34]), tensor([25]), tensor([58]), tensor([6]), tensor([80]), tensor([38]), tensor([27]), tensor([65]), tensor([62]), tensor([4]), tensor([88]), tensor([91]), tensor([0]), tensor([32]), tensor([14]), tensor([49]), tensor([64]), tensor([57]), tensor([66]), tensor([25]), tensor([20]), tensor([55]), tensor([99]), tensor([49]), tensor([93]), tensor([81]), tensor([34]), tensor([6]), tensor([49]), tensor([35]), tensor([35]), tensor([79]), tensor([69]), tensor([60]), tensor([28]), tensor([65]), tensor([33]), tensor([13]), tensor([23]), tensor([20]), tensor([38]), tensor([17]), tensor([77]), tensor([52]), tensor([28]), tensor([84]), tensor([83]), tensor([11]), tensor([29]), tensor([47]), tensor([27]), tensor([17]), tensor([78]), tensor([59]), tensor([75]), tensor([4]), tensor([97]), tensor([47]), tensor([3]), tensor([57]), tensor([3]), tensor([30]), tensor([78]), tensor([14]), tensor([27]), tensor([58]), tensor([82]), tensor([93]), tensor([13]), tensor([75]), tensor([70]), tensor([26]), tensor([52]), tensor([9]), tensor([49]), tensor([83]), tensor([73]), tensor([52]), tensor([9]), tensor([33]), tensor([70]), tensor([18]), tensor([16]), tensor([4]), tensor([0]), tensor([82]), tensor([33]), tensor([67]), tensor([69]), tensor([26]), tensor([75]), tensor([63]), tensor([72]), tensor([48]), tensor([84]), tensor([2]), tensor([67]), tensor([7]), tensor([66]), tensor([14]), tensor([86]), tensor([50]), tensor([1]), tensor([8]), tensor([37]), tensor([95]), tensor([93]), tensor([46]), tensor([79]), tensor([53]), tensor([46]), tensor([17]), tensor([60]), tensor([71]), tensor([46]), tensor([50]), tensor([57]), tensor([57]), tensor([88]), tensor([16]), tensor([88]), tensor([48]), tensor([86]), tensor([7]), tensor([5]), tensor([62]), tensor([5]), tensor([12]), tensor([35]), tensor([17]), tensor([85]), tensor([37]), tensor([5]), tensor([34]), tensor([53]), tensor([3]), tensor([9]), tensor([81]), tensor([21]), tensor([87]), tensor([77]), tensor([94]), tensor([89]), tensor([50]), tensor([2]), tensor([89]), tensor([80]), tensor([42]), tensor([28]), tensor([85]), tensor([85]), tensor([32]), tensor([80]), tensor([48]), tensor([32]), tensor([95]), tensor([16]), tensor([0]), tensor([1]), tensor([34]), tensor([72]), tensor([96]), tensor([14]), tensor([98]), tensor([35]), tensor([96]), tensor([64]), tensor([8]), tensor([81]), tensor([68]), tensor([46]), tensor([68]), tensor([31]), tensor([52]), tensor([68]), tensor([38]), tensor([55]), tensor([96]), tensor([30]), tensor([33]), tensor([94]), tensor([69]), tensor([14]), tensor([63]), tensor([95]), tensor([95]), tensor([2]), tensor([78]), tensor([81]), tensor([31]), tensor([69]), tensor([82]), tensor([68]), tensor([12]), tensor([12]), tensor([17]), tensor([26]), tensor([56]), tensor([16]), tensor([10]), tensor([58]), tensor([20]), tensor([60]), tensor([63]), tensor([94]), tensor([50]), tensor([16]), tensor([73]), tensor([79]), tensor([53]), tensor([28]), tensor([20]), tensor([21]), tensor([83]), tensor([20]), tensor([57]), tensor([95]), tensor([69]), tensor([99]), tensor([64]), tensor([92]), tensor([4]), tensor([29]), tensor([37]), tensor([14]), tensor([68]), tensor([39]), tensor([18]), tensor([64]), tensor([6])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.46 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.443\n",
            "TEST ALL:  0.4142222222222222\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  10000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 82, 2, 10, 18, 26, 34, 42, 50, 58, 66, 74, 90, 87, 98, 3, 11, 19, 27, 35, 43, 51, 59, 67, 97, 89, 81, 73, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 1, 9, 17, 25, 33, 41, 49, 57, 65, 75, 83, 91, 93, 14, 22, 30, 38, 46, 54, 62, 70, 78, 86, 94, 7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 6, 85, 99, 77, 4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92, 5, 13, 21, 29, 37, 45, 53, 61, 69, 0]\n",
            "TRAIN_SET CLASSES:  [43, 19, 15, 90, 74, 54, 61, 76, 44, 40]\n",
            "VALIDATION CLASSES:  [61, 54, 44, 43, 40, 90, 19, 15, 76, 74]\n",
            "GROUP:  10\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [43, 19, 15, 90, 74, 54, 61, 76, 44, 40]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "num classes till now:  100\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.20515647530555725\n",
            "Train step - Step 10, Loss 0.13973425328731537\n",
            "Train step - Step 20, Loss 0.13115456700325012\n",
            "Train step - Step 30, Loss 0.12848767638206482\n",
            "Train step - Step 40, Loss 0.12299854308366776\n",
            "Train step - Step 50, Loss 0.12956057488918304\n",
            "Train epoch - Accuracy: 0.13924963924963926 Loss: 0.13828148774340382 Corrects: 965\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12331976741552353\n",
            "Train step - Step 70, Loss 0.1187295913696289\n",
            "Train step - Step 80, Loss 0.11812479048967361\n",
            "Train step - Step 90, Loss 0.11942512542009354\n",
            "Train step - Step 100, Loss 0.119901143014431\n",
            "Train epoch - Accuracy: 0.15367965367965367 Loss: 0.12223051423462267 Corrects: 1065\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11695580929517746\n",
            "Train step - Step 120, Loss 0.12129579484462738\n",
            "Train step - Step 130, Loss 0.12145427614450455\n",
            "Train step - Step 140, Loss 0.11733583360910416\n",
            "Train step - Step 150, Loss 0.119474858045578\n",
            "Train step - Step 160, Loss 0.11809077858924866\n",
            "Train epoch - Accuracy: 0.1717171717171717 Loss: 0.12039049495264222 Corrects: 1190\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.1199551522731781\n",
            "Train step - Step 180, Loss 0.12079092115163803\n",
            "Train step - Step 190, Loss 0.1189141497015953\n",
            "Train step - Step 200, Loss 0.11680307239294052\n",
            "Train step - Step 210, Loss 0.11820166558027267\n",
            "Train epoch - Accuracy: 0.19336219336219337 Loss: 0.11929318951106863 Corrects: 1340\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.12057662755250931\n",
            "Train step - Step 230, Loss 0.12109102308750153\n",
            "Train step - Step 240, Loss 0.11602922528982162\n",
            "Train step - Step 250, Loss 0.11512701958417892\n",
            "Train step - Step 260, Loss 0.12291865795850754\n",
            "Train step - Step 270, Loss 0.11736612021923065\n",
            "Train epoch - Accuracy: 0.2222222222222222 Loss: 0.11877823774561738 Corrects: 1540\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1148950383067131\n",
            "Train step - Step 290, Loss 0.11641759425401688\n",
            "Train step - Step 300, Loss 0.12453174591064453\n",
            "Train step - Step 310, Loss 0.11912957578897476\n",
            "Train step - Step 320, Loss 0.12025173008441925\n",
            "Train epoch - Accuracy: 0.24733044733044732 Loss: 0.11817513107393383 Corrects: 1714\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.1206008791923523\n",
            "Train step - Step 340, Loss 0.11408208310604095\n",
            "Train step - Step 350, Loss 0.12369592487812042\n",
            "Train step - Step 360, Loss 0.12381793558597565\n",
            "Train step - Step 370, Loss 0.11948974430561066\n",
            "Train step - Step 380, Loss 0.11832679808139801\n",
            "Train epoch - Accuracy: 0.2623376623376623 Loss: 0.11798701340501959 Corrects: 1818\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.1181098073720932\n",
            "Train step - Step 400, Loss 0.12044373154640198\n",
            "Train step - Step 410, Loss 0.11805488169193268\n",
            "Train step - Step 420, Loss 0.11970300227403641\n",
            "Train step - Step 430, Loss 0.11126331984996796\n",
            "Train epoch - Accuracy: 0.2813852813852814 Loss: 0.11775882789840945 Corrects: 1950\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.11729207634925842\n",
            "Train step - Step 450, Loss 0.11614049971103668\n",
            "Train step - Step 460, Loss 0.1175064817070961\n",
            "Train step - Step 470, Loss 0.1179024875164032\n",
            "Train step - Step 480, Loss 0.11482397466897964\n",
            "Train step - Step 490, Loss 0.11403806507587433\n",
            "Train epoch - Accuracy: 0.28773448773448773 Loss: 0.11742301561357656 Corrects: 1994\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.11985864490270615\n",
            "Train step - Step 510, Loss 0.11544673889875412\n",
            "Train step - Step 520, Loss 0.1193414181470871\n",
            "Train step - Step 530, Loss 0.12025269865989685\n",
            "Train step - Step 540, Loss 0.11816059052944183\n",
            "Train epoch - Accuracy: 0.298989898989899 Loss: 0.11735829816752182 Corrects: 2072\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.11776319146156311\n",
            "Train step - Step 560, Loss 0.1173877865076065\n",
            "Train step - Step 570, Loss 0.11418984830379486\n",
            "Train step - Step 580, Loss 0.11465027183294296\n",
            "Train step - Step 590, Loss 0.11830779910087585\n",
            "Train step - Step 600, Loss 0.1227138489484787\n",
            "Train epoch - Accuracy: 0.30966810966810965 Loss: 0.11688897553159389 Corrects: 2146\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.11697197705507278\n",
            "Train step - Step 620, Loss 0.11220494657754898\n",
            "Train step - Step 630, Loss 0.11762423068284988\n",
            "Train step - Step 640, Loss 0.11773614585399628\n",
            "Train step - Step 650, Loss 0.12386830896139145\n",
            "Train epoch - Accuracy: 0.3202020202020202 Loss: 0.11716341900318043 Corrects: 2219\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.12031761556863785\n",
            "Train step - Step 670, Loss 0.11219771951436996\n",
            "Train step - Step 680, Loss 0.11989694088697433\n",
            "Train step - Step 690, Loss 0.11805829405784607\n",
            "Train step - Step 700, Loss 0.11529523879289627\n",
            "Train step - Step 710, Loss 0.11920066922903061\n",
            "Train epoch - Accuracy: 0.31861471861471863 Loss: 0.11665843845617892 Corrects: 2208\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.11757568269968033\n",
            "Train step - Step 730, Loss 0.11449664831161499\n",
            "Train step - Step 740, Loss 0.116350457072258\n",
            "Train step - Step 750, Loss 0.11911731213331223\n",
            "Train step - Step 760, Loss 0.12102480977773666\n",
            "Train epoch - Accuracy: 0.3443001443001443 Loss: 0.11689491145323537 Corrects: 2386\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.11427862197160721\n",
            "Train step - Step 780, Loss 0.11574437469244003\n",
            "Train step - Step 790, Loss 0.11634501814842224\n",
            "Train step - Step 800, Loss 0.11730712652206421\n",
            "Train step - Step 810, Loss 0.11726737767457962\n",
            "Train step - Step 820, Loss 0.11582225561141968\n",
            "Train epoch - Accuracy: 0.3500721500721501 Loss: 0.11662793467399189 Corrects: 2426\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.12019597738981247\n",
            "Train step - Step 840, Loss 0.11323960870504379\n",
            "Train step - Step 850, Loss 0.1200643926858902\n",
            "Train step - Step 860, Loss 0.11219167709350586\n",
            "Train step - Step 870, Loss 0.11622937768697739\n",
            "Train epoch - Accuracy: 0.3528138528138528 Loss: 0.11614584294705507 Corrects: 2445\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.11781468987464905\n",
            "Train step - Step 890, Loss 0.11744517087936401\n",
            "Train step - Step 900, Loss 0.11109040677547455\n",
            "Train step - Step 910, Loss 0.11027180403470993\n",
            "Train step - Step 920, Loss 0.11367997527122498\n",
            "Train step - Step 930, Loss 0.11437284201383591\n",
            "Train epoch - Accuracy: 0.35223665223665224 Loss: 0.11644562101596362 Corrects: 2441\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.11497344821691513\n",
            "Train step - Step 950, Loss 0.11598257720470428\n",
            "Train step - Step 960, Loss 0.11893834918737411\n",
            "Train step - Step 970, Loss 0.11737676709890366\n",
            "Train step - Step 980, Loss 0.11421018093824387\n",
            "Train epoch - Accuracy: 0.3595959595959596 Loss: 0.1162454421859111 Corrects: 2492\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.1179003193974495\n",
            "Train step - Step 1000, Loss 0.11486894637346268\n",
            "Train step - Step 1010, Loss 0.11565852910280228\n",
            "Train step - Step 1020, Loss 0.11134208738803864\n",
            "Train step - Step 1030, Loss 0.11387208849191666\n",
            "Train step - Step 1040, Loss 0.11274600028991699\n",
            "Train epoch - Accuracy: 0.366955266955267 Loss: 0.11633400416907466 Corrects: 2543\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.11929309368133545\n",
            "Train step - Step 1060, Loss 0.12088174372911453\n",
            "Train step - Step 1070, Loss 0.12054216116666794\n",
            "Train step - Step 1080, Loss 0.11283393949270248\n",
            "Train step - Step 1090, Loss 0.11828723549842834\n",
            "Train epoch - Accuracy: 0.37777777777777777 Loss: 0.11633153153747364 Corrects: 2618\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.11202960461378098\n",
            "Train step - Step 1110, Loss 0.11347892135381699\n",
            "Train step - Step 1120, Loss 0.11221760511398315\n",
            "Train step - Step 1130, Loss 0.11525694280862808\n",
            "Train step - Step 1140, Loss 0.12299496680498123\n",
            "Train step - Step 1150, Loss 0.11913187801837921\n",
            "Train epoch - Accuracy: 0.37777777777777777 Loss: 0.11612291252200221 Corrects: 2618\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.11754342168569565\n",
            "Train step - Step 1170, Loss 0.11346706002950668\n",
            "Train step - Step 1180, Loss 0.11285319924354553\n",
            "Train step - Step 1190, Loss 0.1192445382475853\n",
            "Train step - Step 1200, Loss 0.1146794930100441\n",
            "Train epoch - Accuracy: 0.3759018759018759 Loss: 0.11600268177612863 Corrects: 2605\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.12150537222623825\n",
            "Train step - Step 1220, Loss 0.1174275353550911\n",
            "Train step - Step 1230, Loss 0.1155790314078331\n",
            "Train step - Step 1240, Loss 0.11303365230560303\n",
            "Train step - Step 1250, Loss 0.11980033665895462\n",
            "Train step - Step 1260, Loss 0.12060736119747162\n",
            "Train epoch - Accuracy: 0.3904761904761905 Loss: 0.11595651987741176 Corrects: 2706\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.11882142722606659\n",
            "Train step - Step 1280, Loss 0.11224672198295593\n",
            "Train step - Step 1290, Loss 0.11533067375421524\n",
            "Train step - Step 1300, Loss 0.11538612097501755\n",
            "Train step - Step 1310, Loss 0.11366469413042068\n",
            "Train epoch - Accuracy: 0.3926406926406926 Loss: 0.11605893862883938 Corrects: 2721\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.11674235761165619\n",
            "Train step - Step 1330, Loss 0.11808092147111893\n",
            "Train step - Step 1340, Loss 0.11691871285438538\n",
            "Train step - Step 1350, Loss 0.11860507726669312\n",
            "Train step - Step 1360, Loss 0.11800242215394974\n",
            "Train step - Step 1370, Loss 0.11712589859962463\n",
            "Train epoch - Accuracy: 0.3927849927849928 Loss: 0.11609377687713629 Corrects: 2722\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.10996823757886887\n",
            "Train step - Step 1390, Loss 0.11541585624217987\n",
            "Train step - Step 1400, Loss 0.11627250164747238\n",
            "Train step - Step 1410, Loss 0.11855892091989517\n",
            "Train step - Step 1420, Loss 0.11455516517162323\n",
            "Train epoch - Accuracy: 0.39826839826839827 Loss: 0.1155013294554548 Corrects: 2760\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.12235084176063538\n",
            "Train step - Step 1440, Loss 0.11563364416360855\n",
            "Train step - Step 1450, Loss 0.11434682458639145\n",
            "Train step - Step 1460, Loss 0.1184031069278717\n",
            "Train step - Step 1470, Loss 0.11795402318239212\n",
            "Train step - Step 1480, Loss 0.11582902818918228\n",
            "Train epoch - Accuracy: 0.40663780663780663 Loss: 0.11597850874259874 Corrects: 2818\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.11553803831338882\n",
            "Train step - Step 1500, Loss 0.1162339448928833\n",
            "Train step - Step 1510, Loss 0.11170360445976257\n",
            "Train step - Step 1520, Loss 0.11330676823854446\n",
            "Train step - Step 1530, Loss 0.11194053292274475\n",
            "Train epoch - Accuracy: 0.4090909090909091 Loss: 0.11564902486051144 Corrects: 2835\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.11543828994035721\n",
            "Train step - Step 1550, Loss 0.11552716046571732\n",
            "Train step - Step 1560, Loss 0.11716022342443466\n",
            "Train step - Step 1570, Loss 0.11805038154125214\n",
            "Train step - Step 1580, Loss 0.12147002667188644\n",
            "Train step - Step 1590, Loss 0.11206980794668198\n",
            "Train epoch - Accuracy: 0.4111111111111111 Loss: 0.1156536946307013 Corrects: 2849\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.11186164617538452\n",
            "Train step - Step 1610, Loss 0.11573861539363861\n",
            "Train step - Step 1620, Loss 0.11312414705753326\n",
            "Train step - Step 1630, Loss 0.11707967519760132\n",
            "Train step - Step 1640, Loss 0.11159928888082504\n",
            "Train epoch - Accuracy: 0.4113997113997114 Loss: 0.11564302356513204 Corrects: 2851\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.11491338163614273\n",
            "Train step - Step 1660, Loss 0.11319974809885025\n",
            "Train step - Step 1670, Loss 0.114434152841568\n",
            "Train step - Step 1680, Loss 0.11836066842079163\n",
            "Train step - Step 1690, Loss 0.11783530563116074\n",
            "Train step - Step 1700, Loss 0.11912312358617783\n",
            "Train epoch - Accuracy: 0.4171717171717172 Loss: 0.11549160992256319 Corrects: 2891\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.11535317450761795\n",
            "Train step - Step 1720, Loss 0.11972539871931076\n",
            "Train step - Step 1730, Loss 0.1143476590514183\n",
            "Train step - Step 1740, Loss 0.11446858942508698\n",
            "Train step - Step 1750, Loss 0.11688948422670364\n",
            "Train epoch - Accuracy: 0.4261183261183261 Loss: 0.11576156807797296 Corrects: 2953\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.11579287052154541\n",
            "Train step - Step 1770, Loss 0.12329939752817154\n",
            "Train step - Step 1780, Loss 0.11917193979024887\n",
            "Train step - Step 1790, Loss 0.11375865340232849\n",
            "Train step - Step 1800, Loss 0.11735055595636368\n",
            "Train step - Step 1810, Loss 0.10998492687940598\n",
            "Train epoch - Accuracy: 0.4196248196248196 Loss: 0.11522437802382877 Corrects: 2908\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.11271171271800995\n",
            "Train step - Step 1830, Loss 0.11292250454425812\n",
            "Train step - Step 1840, Loss 0.11642467230558395\n",
            "Train step - Step 1850, Loss 0.11132734268903732\n",
            "Train step - Step 1860, Loss 0.1170365959405899\n",
            "Train epoch - Accuracy: 0.43477633477633476 Loss: 0.11552890028702405 Corrects: 3013\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.11969444900751114\n",
            "Train step - Step 1880, Loss 0.11261510848999023\n",
            "Train step - Step 1890, Loss 0.1173606738448143\n",
            "Train step - Step 1900, Loss 0.1118399053812027\n",
            "Train step - Step 1910, Loss 0.11666545271873474\n",
            "Train step - Step 1920, Loss 0.11461003869771957\n",
            "Train epoch - Accuracy: 0.42424242424242425 Loss: 0.11546701339072135 Corrects: 2940\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.11429120600223541\n",
            "Train step - Step 1940, Loss 0.11565076559782028\n",
            "Train step - Step 1950, Loss 0.11289569735527039\n",
            "Train step - Step 1960, Loss 0.11573562026023865\n",
            "Train step - Step 1970, Loss 0.1132044568657875\n",
            "Train epoch - Accuracy: 0.4341991341991342 Loss: 0.11531227236831343 Corrects: 3009\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.11373872309923172\n",
            "Train step - Step 1990, Loss 0.11797172576189041\n",
            "Train step - Step 2000, Loss 0.11990701407194138\n",
            "Train step - Step 2010, Loss 0.11003230512142181\n",
            "Train step - Step 2020, Loss 0.11566244810819626\n",
            "Train step - Step 2030, Loss 0.11355193704366684\n",
            "Train epoch - Accuracy: 0.43593073593073595 Loss: 0.1151630892618566 Corrects: 3021\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.11760410666465759\n",
            "Train step - Step 2050, Loss 0.11216526478528976\n",
            "Train step - Step 2060, Loss 0.11886508762836456\n",
            "Train step - Step 2070, Loss 0.11158519238233566\n",
            "Train step - Step 2080, Loss 0.11340958625078201\n",
            "Train epoch - Accuracy: 0.43564213564213566 Loss: 0.11527303072797272 Corrects: 3019\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.11323156207799911\n",
            "Train step - Step 2100, Loss 0.11451224982738495\n",
            "Train step - Step 2110, Loss 0.1205471009016037\n",
            "Train step - Step 2120, Loss 0.11720018088817596\n",
            "Train step - Step 2130, Loss 0.11684074252843857\n",
            "Train step - Step 2140, Loss 0.11833017319440842\n",
            "Train epoch - Accuracy: 0.4393939393939394 Loss: 0.1152923686808838 Corrects: 3045\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.11640428006649017\n",
            "Train step - Step 2160, Loss 0.11803880333900452\n",
            "Train step - Step 2170, Loss 0.11094491928815842\n",
            "Train step - Step 2180, Loss 0.11625707149505615\n",
            "Train step - Step 2190, Loss 0.11421290040016174\n",
            "Train epoch - Accuracy: 0.4458874458874459 Loss: 0.11494445176136614 Corrects: 3090\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.11336293816566467\n",
            "Train step - Step 2210, Loss 0.11529647558927536\n",
            "Train step - Step 2220, Loss 0.1143600270152092\n",
            "Train step - Step 2230, Loss 0.11472313851118088\n",
            "Train step - Step 2240, Loss 0.11605554074048996\n",
            "Train step - Step 2250, Loss 0.11697307229042053\n",
            "Train epoch - Accuracy: 0.44545454545454544 Loss: 0.11497996310565035 Corrects: 3087\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.11667134612798691\n",
            "Train step - Step 2270, Loss 0.11579078435897827\n",
            "Train step - Step 2280, Loss 0.117215096950531\n",
            "Train step - Step 2290, Loss 0.11175279319286346\n",
            "Train step - Step 2300, Loss 0.1129140630364418\n",
            "Train epoch - Accuracy: 0.44516594516594515 Loss: 0.11504706190348016 Corrects: 3085\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.1144077479839325\n",
            "Train step - Step 2320, Loss 0.11332406103610992\n",
            "Train step - Step 2330, Loss 0.1146463006734848\n",
            "Train step - Step 2340, Loss 0.11135588586330414\n",
            "Train step - Step 2350, Loss 0.11328194290399551\n",
            "Train step - Step 2360, Loss 0.11536908894777298\n",
            "Train epoch - Accuracy: 0.44617604617604617 Loss: 0.11525930981185357 Corrects: 3092\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.11484795063734055\n",
            "Train step - Step 2380, Loss 0.1133682057261467\n",
            "Train step - Step 2390, Loss 0.11672420054674149\n",
            "Train step - Step 2400, Loss 0.10827378183603287\n",
            "Train step - Step 2410, Loss 0.11854410916566849\n",
            "Train epoch - Accuracy: 0.4505050505050505 Loss: 0.11482931727712804 Corrects: 3122\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.11735206097364426\n",
            "Train step - Step 2430, Loss 0.1161511018872261\n",
            "Train step - Step 2440, Loss 0.11029649525880814\n",
            "Train step - Step 2450, Loss 0.11898083239793777\n",
            "Train step - Step 2460, Loss 0.11564517766237259\n",
            "Train step - Step 2470, Loss 0.11426476389169693\n",
            "Train epoch - Accuracy: 0.4479076479076479 Loss: 0.11507369759794954 Corrects: 3104\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.11764376610517502\n",
            "Train step - Step 2490, Loss 0.11776076257228851\n",
            "Train step - Step 2500, Loss 0.11441193521022797\n",
            "Train step - Step 2510, Loss 0.11677360534667969\n",
            "Train step - Step 2520, Loss 0.11470348387956619\n",
            "Train epoch - Accuracy: 0.4572871572871573 Loss: 0.11467200711175993 Corrects: 3169\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.1110173687338829\n",
            "Train step - Step 2540, Loss 0.11488892883062363\n",
            "Train step - Step 2550, Loss 0.11423870176076889\n",
            "Train step - Step 2560, Loss 0.10907470434904099\n",
            "Train step - Step 2570, Loss 0.11821319907903671\n",
            "Train step - Step 2580, Loss 0.1168350875377655\n",
            "Train epoch - Accuracy: 0.46002886002886 Loss: 0.11463489120699799 Corrects: 3188\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.1141110360622406\n",
            "Train step - Step 2600, Loss 0.11389579623937607\n",
            "Train step - Step 2610, Loss 0.11831745505332947\n",
            "Train step - Step 2620, Loss 0.1111767366528511\n",
            "Train step - Step 2630, Loss 0.11287263035774231\n",
            "Train epoch - Accuracy: 0.4595959595959596 Loss: 0.1148412225566385 Corrects: 3185\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.11441829055547714\n",
            "Train step - Step 2650, Loss 0.11873365938663483\n",
            "Train step - Step 2660, Loss 0.11675702780485153\n",
            "Train step - Step 2670, Loss 0.11346647143363953\n",
            "Train step - Step 2680, Loss 0.1160617396235466\n",
            "Train step - Step 2690, Loss 0.11465464532375336\n",
            "Train epoch - Accuracy: 0.4691197691197691 Loss: 0.1150537829077433 Corrects: 3251\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.1135544627904892\n",
            "Train step - Step 2710, Loss 0.11586343497037888\n",
            "Train step - Step 2720, Loss 0.11646459251642227\n",
            "Train step - Step 2730, Loss 0.1140405610203743\n",
            "Train step - Step 2740, Loss 0.1163986548781395\n",
            "Train epoch - Accuracy: 0.47243867243867244 Loss: 0.11417686301908452 Corrects: 3274\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.11391141265630722\n",
            "Train step - Step 2760, Loss 0.11564429849386215\n",
            "Train step - Step 2770, Loss 0.11147750914096832\n",
            "Train step - Step 2780, Loss 0.1120099201798439\n",
            "Train step - Step 2790, Loss 0.11414685100317001\n",
            "Train step - Step 2800, Loss 0.11197071522474289\n",
            "Train epoch - Accuracy: 0.4733044733044733 Loss: 0.11414041329943944 Corrects: 3280\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.11483026295900345\n",
            "Train step - Step 2820, Loss 0.11439813673496246\n",
            "Train step - Step 2830, Loss 0.11499595642089844\n",
            "Train step - Step 2840, Loss 0.11171675473451614\n",
            "Train step - Step 2850, Loss 0.11439710110425949\n",
            "Train epoch - Accuracy: 0.46998556998557 Loss: 0.11458170489229337 Corrects: 3257\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.11648529022932053\n",
            "Train step - Step 2870, Loss 0.11459732055664062\n",
            "Train step - Step 2880, Loss 0.11323688179254532\n",
            "Train step - Step 2890, Loss 0.10811412334442139\n",
            "Train step - Step 2900, Loss 0.11242149025201797\n",
            "Train step - Step 2910, Loss 0.11857146769762039\n",
            "Train epoch - Accuracy: 0.46854256854256854 Loss: 0.11440623860381555 Corrects: 3247\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.11057431995868683\n",
            "Train step - Step 2930, Loss 0.11020534485578537\n",
            "Train step - Step 2940, Loss 0.1164248138666153\n",
            "Train step - Step 2950, Loss 0.11141300946474075\n",
            "Train step - Step 2960, Loss 0.11681662499904633\n",
            "Train epoch - Accuracy: 0.4727272727272727 Loss: 0.11443514775748205 Corrects: 3276\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.11509692668914795\n",
            "Train step - Step 2980, Loss 0.11343797296285629\n",
            "Train step - Step 2990, Loss 0.11060883104801178\n",
            "Train step - Step 3000, Loss 0.11516328901052475\n",
            "Train step - Step 3010, Loss 0.11420615762472153\n",
            "Train step - Step 3020, Loss 0.10841840505599976\n",
            "Train epoch - Accuracy: 0.4702741702741703 Loss: 0.11436558324107188 Corrects: 3259\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.10927579551935196\n",
            "Train step - Step 3040, Loss 0.11513537913560867\n",
            "Train step - Step 3050, Loss 0.11210725456476212\n",
            "Train step - Step 3060, Loss 0.11397016048431396\n",
            "Train step - Step 3070, Loss 0.11575730890035629\n",
            "Train epoch - Accuracy: 0.4715728715728716 Loss: 0.11427905876485128 Corrects: 3268\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.11380397528409958\n",
            "Train step - Step 3090, Loss 0.1156754121184349\n",
            "Train step - Step 3100, Loss 0.1129990816116333\n",
            "Train step - Step 3110, Loss 0.11894714087247849\n",
            "Train step - Step 3120, Loss 0.11315218359231949\n",
            "Train step - Step 3130, Loss 0.1179957166314125\n",
            "Train epoch - Accuracy: 0.4733044733044733 Loss: 0.11437968716218874 Corrects: 3280\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.11645985394716263\n",
            "Train step - Step 3150, Loss 0.11752095818519592\n",
            "Train step - Step 3160, Loss 0.11421556025743484\n",
            "Train step - Step 3170, Loss 0.11103449761867523\n",
            "Train step - Step 3180, Loss 0.11596159636974335\n",
            "Train epoch - Accuracy: 0.4701298701298701 Loss: 0.11437089871319514 Corrects: 3258\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.11118876934051514\n",
            "Train step - Step 3200, Loss 0.11453399062156677\n",
            "Train step - Step 3210, Loss 0.11405282467603683\n",
            "Train step - Step 3220, Loss 0.10750210285186768\n",
            "Train step - Step 3230, Loss 0.11600560694932938\n",
            "Train step - Step 3240, Loss 0.11145826429128647\n",
            "Train epoch - Accuracy: 0.47056277056277057 Loss: 0.11431617684995629 Corrects: 3261\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.11883079260587692\n",
            "Train step - Step 3260, Loss 0.10886847972869873\n",
            "Train step - Step 3270, Loss 0.11266481876373291\n",
            "Train step - Step 3280, Loss 0.11313356459140778\n",
            "Train step - Step 3290, Loss 0.10866374522447586\n",
            "Train epoch - Accuracy: 0.4683982683982684 Loss: 0.11452383145066394 Corrects: 3246\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.11128439009189606\n",
            "Train step - Step 3310, Loss 0.11235146969556808\n",
            "Train step - Step 3320, Loss 0.11262435466051102\n",
            "Train step - Step 3330, Loss 0.11433707922697067\n",
            "Train step - Step 3340, Loss 0.11043090373277664\n",
            "Train step - Step 3350, Loss 0.11000490188598633\n",
            "Train epoch - Accuracy: 0.4717171717171717 Loss: 0.11383334122393898 Corrects: 3269\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.11199995875358582\n",
            "Train step - Step 3370, Loss 0.11839363723993301\n",
            "Train step - Step 3380, Loss 0.1122889295220375\n",
            "Train step - Step 3390, Loss 0.11215192824602127\n",
            "Train step - Step 3400, Loss 0.11764860153198242\n",
            "Train epoch - Accuracy: 0.48095238095238096 Loss: 0.11452389108971256 Corrects: 3333\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.11151646077632904\n",
            "Train step - Step 3420, Loss 0.11012792587280273\n",
            "Train step - Step 3430, Loss 0.115610271692276\n",
            "Train step - Step 3440, Loss 0.11983852833509445\n",
            "Train step - Step 3450, Loss 0.11072488874197006\n",
            "Train step - Step 3460, Loss 0.10993586480617523\n",
            "Train epoch - Accuracy: 0.47965367965367967 Loss: 0.11419396837810417 Corrects: 3324\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.11552832275629044\n",
            "Train step - Step 3480, Loss 0.11544854193925858\n",
            "Train step - Step 3490, Loss 0.11276134103536606\n",
            "Train step - Step 3500, Loss 0.11668416857719421\n",
            "Train step - Step 3510, Loss 0.11272110790014267\n",
            "Train epoch - Accuracy: 0.481962481962482 Loss: 0.11417821748303128 Corrects: 3340\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.11430332809686661\n",
            "Train step - Step 3530, Loss 0.11454149335622787\n",
            "Train step - Step 3540, Loss 0.1065295934677124\n",
            "Train step - Step 3550, Loss 0.11553466320037842\n",
            "Train step - Step 3560, Loss 0.11106251180171967\n",
            "Train step - Step 3570, Loss 0.11465189605951309\n",
            "Train epoch - Accuracy: 0.4790764790764791 Loss: 0.11398669959920825 Corrects: 3320\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.112025685608387\n",
            "Train step - Step 3590, Loss 0.11458180844783783\n",
            "Train step - Step 3600, Loss 0.11411602795124054\n",
            "Train step - Step 3610, Loss 0.1058979406952858\n",
            "Train step - Step 3620, Loss 0.11349192261695862\n",
            "Train epoch - Accuracy: 0.4767676767676768 Loss: 0.11419581016592821 Corrects: 3304\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.11490560322999954\n",
            "Train step - Step 3640, Loss 0.11841193586587906\n",
            "Train step - Step 3650, Loss 0.11169448494911194\n",
            "Train step - Step 3660, Loss 0.11777128279209137\n",
            "Train step - Step 3670, Loss 0.11488260328769684\n",
            "Train step - Step 3680, Loss 0.11902904510498047\n",
            "Train epoch - Accuracy: 0.4767676767676768 Loss: 0.11413901000139862 Corrects: 3304\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.11766919493675232\n",
            "Train step - Step 3700, Loss 0.11005038768053055\n",
            "Train step - Step 3710, Loss 0.11462552845478058\n",
            "Train step - Step 3720, Loss 0.11450164765119553\n",
            "Train step - Step 3730, Loss 0.11651621758937836\n",
            "Train epoch - Accuracy: 0.47748917748917746 Loss: 0.11430842065793956 Corrects: 3309\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.11407773941755295\n",
            "Train step - Step 3750, Loss 0.11708127707242966\n",
            "Train step - Step 3760, Loss 0.11147367209196091\n",
            "Train step - Step 3770, Loss 0.11585172265768051\n",
            "Train step - Step 3780, Loss 0.11272601038217545\n",
            "Train step - Step 3790, Loss 0.11291061341762543\n",
            "Train epoch - Accuracy: 0.4704184704184704 Loss: 0.1142318940523899 Corrects: 3260\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.108919158577919\n",
            "Train step - Step 3810, Loss 0.10937093198299408\n",
            "Train step - Step 3820, Loss 0.11579043418169022\n",
            "Train step - Step 3830, Loss 0.11309321969747543\n",
            "Train step - Step 3840, Loss 0.1103334054350853\n",
            "Train epoch - Accuracy: 0.4782106782106782 Loss: 0.11416355182254125 Corrects: 3314\n",
            "Training finished in 404.2914321422577 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60, 35, 3, 94, 10, 45, 37, 80, 72, 36, 24, 31, 11, 98, 77, 53, 33, 96, 92, 52, 12, 91, 67, 7, 62, 46, 65, 21, 32, 16, 0, 79, 71, 63, 26, 14, 2, 73, 41, 56, 48, 99, 23, 66, 22, 6, 93, 17, 5, 1, 28, 87, 75, 55, 47, 39, 70, 42, 38, 29, 4, 83, 51, 18, 89, 85, 9, 84, 64, 20, 8, 43, 19, 15, 90, 74, 54, 61, 76, 44, 40]\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232ed8050>\n",
            "Constructing exemplars of class 43\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [31008, 27402, 45916, 13088, 26807, 25860, 30952, 14912, 31582, 42868, 14937, 37241, 3030, 28984, 49892, 28617, 31262, 33510, 39562, 2388]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2241476e50>\n",
            "Constructing exemplars of class 19\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [21314, 18443, 48684, 47129, 34860, 18019, 41450, 28223, 9482, 16162, 19008, 20489, 38116, 18134, 28927, 10093, 26037, 30909, 7698, 44475]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22414762d0>\n",
            "Constructing exemplars of class 15\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [4488, 31377, 3244, 19608, 45908, 28315, 13598, 25799, 11715, 7123, 17164, 37178, 18148, 35927, 10697, 21220, 33158, 2724, 15355, 19604]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a87e50>\n",
            "Constructing exemplars of class 90\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [39115, 1607, 39041, 44403, 32236, 29806, 15048, 39540, 26155, 1034, 41330, 32407, 18880, 31363, 26536, 10871, 37973, 7392, 3843, 1095]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a877d0>\n",
            "Constructing exemplars of class 74\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [21759, 49523, 28050, 44615, 48493, 11865, 13556, 1075, 38112, 20463, 16149, 45905, 29340, 24924, 9763, 47070, 1583, 33044, 22501, 6885]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238035ad0>\n",
            "Constructing exemplars of class 54\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [12166, 15609, 20153, 31725, 5071, 49847, 49741, 47082, 6048, 49087, 29521, 40111, 22148, 14325, 47633, 13587, 2642, 47082, 24730, 31630]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2232f25850>\n",
            "Constructing exemplars of class 61\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [44699, 31406, 41996, 46714, 11401, 16119, 49846, 21261, 38599, 37139, 33306, 15565, 24156, 8865, 39247, 48165, 7575, 20306, 24977, 11946]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f223822c790>\n",
            "Constructing exemplars of class 76\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [5101, 28994, 4629, 11120, 44302, 2885, 18783, 34295, 6467, 10228, 16345, 13182, 20353, 35830, 32760, 48950, 6811, 18783, 30768, 21990]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f2238a7da90>\n",
            "Constructing exemplars of class 44\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [43555, 17570, 32508, 980, 2576, 1393, 10431, 49674, 39668, 48771, 2506, 37316, 19866, 15720, 12980, 40010, 36742, 10606, 18521, 661]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f22318d2850>\n",
            "Constructing exemplars of class 40\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [4717, 5156, 17538, 6843, 15961, 48663, 933, 19450, 46868, 32561, 14963, 24708, 4256, 6857, 37567, 16977, 24803, 1282, 44399, 24645]\n",
            "x train:  [-0.08496322 -0.07788854 -0.046113   -0.07923395 -0.04362911 -0.13332394\n",
            " -0.08848953 -0.09046408 -0.04019038  0.01666227 -0.12081163 -0.11035814\n",
            " -0.04287349 -0.07399985 -0.095273   -0.14041409 -0.10174412 -0.08174222\n",
            " -0.11468966 -0.12497511 -0.00806111 -0.11015553 -0.0660091  -0.0511431\n",
            " -0.06363802 -0.13148184 -0.10468461 -0.10428838 -0.04916965 -0.06203615\n",
            " -0.11129855 -0.01797907 -0.01567997 -0.09317721 -0.03820258 -0.11543341\n",
            " -0.12162873 -0.11939751 -0.12605608 -0.12055225 -0.09531885 -0.09854062\n",
            " -0.09122235 -0.10358276 -0.02497499 -0.10807543 -0.13844107 -0.06017898\n",
            " -0.09491657 -0.04132134 -0.11194368 -0.10152763 -0.09306684 -0.06513982\n",
            " -0.1146904  -0.0097174  -0.12202356 -0.12605217 -0.10635399 -0.18038246\n",
            " -0.13104962 -0.12183479 -0.15104206 -0.07464983 -0.0844496  -0.07215464\n",
            " -0.17880975 -0.02217012 -0.10435662 -0.04914587 -0.08183428 -0.15717053\n",
            " -0.10722704 -0.08447871 -0.09671955 -0.07894989 -0.11027104 -0.1013234\n",
            " -0.07671177 -0.07828113 -0.05364253 -0.0942109  -0.12825455 -0.12780488\n",
            " -0.10752824 -0.09702511 -0.03437965 -0.10617768 -0.04584351 -0.17069468\n",
            " -0.09792249 -0.11123469 -0.07254475 -0.13060096 -0.11944653 -0.09955826\n",
            " -0.10184825 -0.21622214 -0.07354517 -0.0385037 ]\n",
            "y_train:  [tensor([46]), tensor([88]), tensor([9]), tensor([13]), tensor([6]), tensor([1]), tensor([1]), tensor([43]), tensor([36]), tensor([67]), tensor([87]), tensor([38]), tensor([4]), tensor([71]), tensor([98]), tensor([14]), tensor([60]), tensor([20]), tensor([41]), tensor([29]), tensor([83]), tensor([90]), tensor([57]), tensor([31]), tensor([22]), tensor([78]), tensor([17]), tensor([92]), tensor([47]), tensor([25]), tensor([13]), tensor([77]), tensor([15]), tensor([57]), tensor([40]), tensor([70]), tensor([61]), tensor([33]), tensor([16]), tensor([72]), tensor([61]), tensor([38]), tensor([54]), tensor([17]), tensor([52]), tensor([28]), tensor([44]), tensor([75]), tensor([92]), tensor([41]), tensor([67]), tensor([91]), tensor([75]), tensor([36]), tensor([78]), tensor([80]), tensor([69]), tensor([58]), tensor([53]), tensor([62]), tensor([63]), tensor([5]), tensor([37]), tensor([94]), tensor([92]), tensor([33]), tensor([29]), tensor([74]), tensor([98]), tensor([80]), tensor([13]), tensor([67]), tensor([91]), tensor([23]), tensor([68]), tensor([33]), tensor([20]), tensor([43]), tensor([78]), tensor([70]), tensor([81]), tensor([38]), tensor([48]), tensor([71]), tensor([52]), tensor([1]), tensor([69]), tensor([23]), tensor([15]), tensor([50]), tensor([34]), tensor([83]), tensor([78]), tensor([66]), tensor([23]), tensor([30]), tensor([88]), tensor([15]), tensor([95]), tensor([76]), tensor([76]), tensor([7]), tensor([12]), tensor([88]), tensor([78]), tensor([77]), tensor([82]), tensor([86]), tensor([75]), tensor([50]), tensor([70]), tensor([66]), tensor([78]), tensor([25]), tensor([52]), tensor([56]), tensor([68]), tensor([85]), tensor([93]), tensor([3]), tensor([17]), tensor([83]), tensor([62]), tensor([17]), tensor([37]), tensor([66]), tensor([62]), tensor([10]), tensor([39]), tensor([61]), tensor([65]), tensor([73]), tensor([5]), tensor([75]), tensor([39]), tensor([43]), tensor([33]), tensor([28]), tensor([81]), tensor([54]), tensor([88]), tensor([12]), tensor([91]), tensor([11]), tensor([56]), tensor([38]), tensor([48]), tensor([34]), tensor([3]), tensor([85]), tensor([22]), tensor([73]), tensor([21]), tensor([90]), tensor([72]), tensor([22]), tensor([95]), tensor([38]), tensor([51]), tensor([87]), tensor([3]), tensor([51]), tensor([25]), tensor([52]), tensor([25]), tensor([42]), tensor([22]), tensor([74]), tensor([9]), tensor([73]), tensor([69]), tensor([23]), tensor([46]), tensor([68]), tensor([21]), tensor([85]), tensor([23]), tensor([37]), tensor([27]), tensor([77]), tensor([88]), tensor([23]), tensor([92]), tensor([67]), tensor([81]), tensor([26]), tensor([47]), tensor([61]), tensor([42]), tensor([41]), tensor([62]), tensor([3]), tensor([58]), tensor([80]), tensor([37]), tensor([9]), tensor([4]), tensor([62]), tensor([17]), tensor([77]), tensor([42]), tensor([92]), tensor([7]), tensor([21]), tensor([3]), tensor([55]), tensor([43]), tensor([5]), tensor([71]), tensor([26]), tensor([19]), tensor([80]), tensor([82]), tensor([84]), tensor([87]), tensor([97]), tensor([81]), tensor([92]), tensor([83]), tensor([82]), tensor([62]), tensor([50]), tensor([56]), tensor([55]), tensor([55]), tensor([87]), tensor([24]), tensor([87]), tensor([96]), tensor([67]), tensor([8]), tensor([63]), tensor([40]), tensor([49]), tensor([73]), tensor([19]), tensor([10]), tensor([42]), tensor([63]), tensor([31]), tensor([27]), tensor([31]), tensor([28]), tensor([32]), tensor([68]), tensor([51]), tensor([9]), tensor([59]), tensor([67]), tensor([18]), tensor([48]), tensor([23]), tensor([47]), tensor([21]), tensor([64]), tensor([82]), tensor([90]), tensor([45]), tensor([77]), tensor([89]), tensor([73]), tensor([96]), tensor([37]), tensor([31]), tensor([44]), tensor([84]), tensor([45]), tensor([89]), tensor([93]), tensor([65]), tensor([64]), tensor([35]), tensor([71]), tensor([56]), tensor([93]), tensor([67]), tensor([38]), tensor([22]), tensor([64]), tensor([47]), tensor([78]), tensor([84]), tensor([27]), tensor([15]), tensor([66]), tensor([13]), tensor([29]), tensor([34]), tensor([19]), tensor([29]), tensor([84]), tensor([95]), tensor([4]), tensor([71]), tensor([59]), tensor([12]), tensor([15]), tensor([53]), tensor([63]), tensor([45]), tensor([61]), tensor([7]), tensor([16]), tensor([12]), tensor([45]), tensor([0]), tensor([36]), tensor([29]), tensor([88]), tensor([29]), tensor([89]), tensor([54]), tensor([37]), tensor([68]), tensor([92]), tensor([20]), tensor([52]), tensor([11]), tensor([29]), tensor([56]), tensor([23]), tensor([84]), tensor([84]), tensor([1]), tensor([83]), tensor([35]), tensor([66]), tensor([59]), tensor([37]), tensor([91]), tensor([43]), tensor([58]), tensor([10]), tensor([38]), tensor([2]), tensor([21]), tensor([23]), tensor([27]), tensor([66]), tensor([22]), tensor([80]), tensor([0]), tensor([71]), tensor([65]), tensor([44]), tensor([1]), tensor([39]), tensor([14]), tensor([56]), tensor([18]), tensor([70]), tensor([94]), tensor([54]), tensor([60]), tensor([62]), tensor([39]), tensor([23]), tensor([33]), tensor([14]), tensor([4]), tensor([78]), tensor([8]), tensor([63]), tensor([80]), tensor([77]), tensor([34]), tensor([93]), tensor([10]), tensor([14]), tensor([41]), tensor([23]), tensor([1]), tensor([90]), tensor([23]), tensor([34]), tensor([79]), tensor([54]), tensor([9]), tensor([36]), tensor([94]), tensor([80]), tensor([64]), tensor([65]), tensor([90]), tensor([29]), tensor([48]), tensor([16]), tensor([43]), tensor([50]), tensor([95]), tensor([5]), tensor([88]), tensor([90]), tensor([68]), tensor([7]), tensor([39]), tensor([13]), tensor([21]), tensor([92]), tensor([9]), tensor([81]), tensor([31]), tensor([11]), tensor([50]), tensor([30]), tensor([6]), tensor([26]), tensor([80]), tensor([84]), tensor([67]), tensor([28]), tensor([84]), tensor([68]), tensor([48]), tensor([60]), tensor([81]), tensor([7]), tensor([95]), tensor([49]), tensor([86]), tensor([74]), tensor([15]), tensor([89]), tensor([46]), tensor([33]), tensor([37]), tensor([98]), tensor([21]), tensor([98]), tensor([7]), tensor([83]), tensor([98]), tensor([57]), tensor([89]), tensor([21]), tensor([44]), tensor([77]), tensor([46]), tensor([65]), tensor([33]), tensor([54]), tensor([80]), tensor([45]), tensor([86]), tensor([9]), tensor([32]), tensor([14]), tensor([59]), tensor([8]), tensor([70]), tensor([41]), tensor([8]), tensor([52]), tensor([79]), tensor([43]), tensor([6]), tensor([89]), tensor([51]), tensor([67]), tensor([62]), tensor([21]), tensor([59]), tensor([61]), tensor([10]), tensor([61]), tensor([75]), tensor([44]), tensor([62]), tensor([19]), tensor([83]), tensor([8]), tensor([64]), tensor([53]), tensor([19]), tensor([34]), tensor([61]), tensor([2]), tensor([65]), tensor([54]), tensor([89]), tensor([86]), tensor([88]), tensor([96]), tensor([16]), tensor([91]), tensor([0]), tensor([11]), tensor([0]), tensor([0]), tensor([97]), tensor([77]), tensor([81]), tensor([60]), tensor([9]), tensor([84]), tensor([61]), tensor([97]), tensor([45]), tensor([63]), tensor([14]), tensor([94]), tensor([30]), tensor([89]), tensor([5]), tensor([71]), tensor([47]), tensor([70]), tensor([45]), tensor([38]), tensor([35]), tensor([75]), tensor([26]), tensor([54]), tensor([11]), tensor([20]), tensor([38]), tensor([8]), tensor([72]), tensor([34]), tensor([44]), tensor([27]), tensor([39]), tensor([95]), tensor([25]), tensor([93]), tensor([40]), tensor([52]), tensor([88]), tensor([7]), tensor([9]), tensor([85]), tensor([90]), tensor([53]), tensor([16]), tensor([12]), tensor([99]), tensor([98]), tensor([82]), tensor([22]), tensor([35]), tensor([3]), tensor([68]), tensor([66]), tensor([65]), tensor([34]), tensor([55]), tensor([1]), tensor([72]), tensor([73]), tensor([49]), tensor([69]), tensor([69]), tensor([54]), tensor([1]), tensor([78]), tensor([52]), tensor([46]), tensor([33]), tensor([25]), tensor([43]), tensor([63]), tensor([55]), tensor([69]), tensor([22]), tensor([49]), tensor([58]), tensor([42]), tensor([48]), tensor([94]), tensor([11]), tensor([36]), tensor([93]), tensor([74]), tensor([26]), tensor([4]), tensor([17]), tensor([7]), tensor([67]), tensor([82]), tensor([44]), tensor([42]), tensor([9]), tensor([44]), tensor([60]), tensor([97]), tensor([31]), tensor([2]), tensor([42]), tensor([82]), tensor([78]), tensor([19]), tensor([2]), tensor([31]), tensor([10]), tensor([15]), tensor([13]), tensor([61]), tensor([14]), tensor([3]), tensor([52]), tensor([7]), tensor([30]), tensor([41]), tensor([59]), tensor([12]), tensor([88]), tensor([99]), tensor([43]), tensor([27]), tensor([46]), tensor([69]), tensor([94]), tensor([17]), tensor([68]), tensor([1]), tensor([59]), tensor([2]), tensor([71]), tensor([56]), tensor([13]), tensor([56]), tensor([37]), tensor([82]), tensor([22]), tensor([64]), tensor([68]), tensor([5]), tensor([97]), tensor([52]), tensor([47]), tensor([21]), tensor([12]), tensor([98]), tensor([33]), tensor([62]), tensor([87]), tensor([4]), tensor([43]), tensor([15]), tensor([95]), tensor([86]), tensor([11]), tensor([87]), tensor([51]), tensor([85]), tensor([85]), tensor([63]), tensor([28]), tensor([59]), tensor([36]), tensor([3]), tensor([70]), tensor([31]), tensor([57]), tensor([65]), tensor([4]), tensor([87]), tensor([22]), tensor([40]), tensor([31]), tensor([18]), tensor([5]), tensor([98]), tensor([54]), tensor([83]), tensor([32]), tensor([77]), tensor([36]), tensor([12]), tensor([28]), tensor([86]), tensor([78]), tensor([26]), tensor([75]), tensor([6]), tensor([99]), tensor([27]), tensor([63]), tensor([18]), tensor([24]), tensor([65]), tensor([76]), tensor([72]), tensor([22]), tensor([64]), tensor([34]), tensor([50]), tensor([12]), tensor([88]), tensor([53]), tensor([14]), tensor([23]), tensor([56]), tensor([92]), tensor([98]), tensor([17]), tensor([12]), tensor([20]), tensor([96]), tensor([75]), tensor([4]), tensor([90]), tensor([40]), tensor([30]), tensor([34]), tensor([41]), tensor([53]), tensor([79]), tensor([45]), tensor([18]), tensor([95]), tensor([49]), tensor([80]), tensor([28]), tensor([37]), tensor([42]), tensor([2]), tensor([37]), tensor([56]), tensor([17]), tensor([17]), tensor([84]), tensor([1]), tensor([81]), tensor([24]), tensor([57]), tensor([32]), tensor([88]), tensor([39]), tensor([72]), tensor([86]), tensor([40]), tensor([41]), tensor([34]), tensor([71]), tensor([65]), tensor([36]), tensor([85]), tensor([25]), tensor([62]), tensor([80]), tensor([2]), tensor([65]), tensor([93]), tensor([74]), tensor([20]), tensor([88]), tensor([5]), tensor([70]), tensor([35]), tensor([4]), tensor([76]), tensor([69]), tensor([40]), tensor([68]), tensor([93]), tensor([43]), tensor([61]), tensor([13]), tensor([92]), tensor([52]), tensor([96]), tensor([9]), tensor([57]), tensor([39]), tensor([31]), tensor([49]), tensor([27]), tensor([66]), tensor([59]), tensor([30]), tensor([80]), tensor([72]), tensor([87]), tensor([38]), tensor([36]), tensor([40]), tensor([49]), tensor([22]), tensor([46]), tensor([68]), tensor([14]), tensor([96]), tensor([54]), tensor([2]), tensor([70]), tensor([95]), tensor([44]), tensor([21]), tensor([75]), tensor([15]), tensor([88]), tensor([53]), tensor([4]), tensor([48]), tensor([18]), tensor([95]), tensor([15]), tensor([79]), tensor([55]), tensor([31]), tensor([1]), tensor([76]), tensor([90]), tensor([71]), tensor([91]), tensor([27]), tensor([2]), tensor([0]), tensor([33]), tensor([60]), tensor([47]), tensor([22]), tensor([49]), tensor([58]), tensor([99]), tensor([3]), tensor([72]), tensor([72]), tensor([95]), tensor([6]), tensor([93]), tensor([47]), tensor([13]), tensor([23]), tensor([73]), tensor([50]), tensor([57]), tensor([25]), tensor([10]), tensor([83]), tensor([94]), tensor([92]), tensor([70]), tensor([74]), tensor([4]), tensor([70]), tensor([26]), tensor([93]), tensor([29]), tensor([41]), tensor([73]), tensor([72]), tensor([2]), tensor([36]), tensor([0]), tensor([77]), tensor([41]), tensor([60]), tensor([51]), tensor([32]), tensor([21]), tensor([44]), tensor([99]), tensor([64]), tensor([73]), tensor([91]), tensor([67]), tensor([14]), tensor([89]), tensor([51]), tensor([32]), tensor([25]), tensor([66]), tensor([62]), tensor([44]), tensor([16]), tensor([55]), tensor([30]), tensor([48]), tensor([96]), tensor([24]), tensor([86]), tensor([84]), tensor([3]), tensor([26]), tensor([89]), tensor([2]), tensor([58]), tensor([78]), tensor([63]), tensor([0]), tensor([95]), tensor([52]), tensor([83]), tensor([99]), tensor([82]), tensor([51]), tensor([0]), tensor([69]), tensor([86]), tensor([72]), tensor([12]), tensor([6]), tensor([72]), tensor([28]), tensor([61]), tensor([98]), tensor([10]), tensor([52]), tensor([82]), tensor([93]), tensor([30]), tensor([84]), tensor([58]), tensor([66]), tensor([57]), tensor([80]), tensor([46]), tensor([13]), tensor([28]), tensor([59]), tensor([63]), tensor([35]), tensor([35]), tensor([27]), tensor([50]), tensor([12]), tensor([13]), tensor([68]), tensor([36]), tensor([24]), tensor([91]), tensor([64]), tensor([85]), tensor([2]), tensor([53]), tensor([36]), tensor([96]), tensor([27]), tensor([89]), tensor([91]), tensor([12]), tensor([42]), tensor([5]), tensor([45]), tensor([18]), tensor([34]), tensor([10]), tensor([40]), tensor([14]), tensor([19]), tensor([32]), tensor([15]), tensor([81]), tensor([24]), tensor([8]), tensor([74]), tensor([41]), tensor([6]), tensor([43]), tensor([34]), tensor([1]), tensor([71]), tensor([79]), tensor([94]), tensor([80]), tensor([3]), tensor([58]), tensor([11]), tensor([81]), tensor([47]), tensor([26]), tensor([0]), tensor([50]), tensor([39]), tensor([17]), tensor([37]), tensor([88]), tensor([22]), tensor([41]), tensor([14]), tensor([57]), tensor([1]), tensor([96]), tensor([88]), tensor([20]), tensor([29]), tensor([87]), tensor([24]), tensor([36]), tensor([5]), tensor([2]), tensor([40]), tensor([55]), tensor([38]), tensor([33]), tensor([72]), tensor([95]), tensor([6]), tensor([27]), tensor([8]), tensor([20]), tensor([21]), tensor([44]), tensor([65]), tensor([20]), tensor([36]), tensor([32]), tensor([8]), tensor([86]), tensor([47]), tensor([27]), tensor([91]), tensor([24]), tensor([30]), tensor([8]), tensor([10]), tensor([43]), tensor([98]), tensor([63]), tensor([6]), tensor([71]), tensor([86]), tensor([99]), tensor([13]), tensor([75]), tensor([74]), tensor([59]), tensor([66]), tensor([47]), tensor([70]), tensor([10]), tensor([71]), tensor([55]), tensor([47]), tensor([88]), tensor([94]), tensor([87]), tensor([7]), tensor([82]), tensor([39]), tensor([92]), tensor([11]), tensor([92]), tensor([49]), tensor([90]), tensor([68]), tensor([64]), tensor([57]), tensor([97]), tensor([58]), tensor([96]), tensor([25]), tensor([17]), tensor([55]), tensor([30]), tensor([7]), tensor([13]), tensor([3]), tensor([49]), tensor([60]), tensor([39]), tensor([49]), tensor([0]), tensor([66]), tensor([91]), tensor([75]), tensor([0]), tensor([39]), tensor([41]), tensor([22]), tensor([18]), tensor([30]), tensor([14]), tensor([99]), tensor([98]), tensor([54]), tensor([37]), tensor([27]), tensor([20]), tensor([89]), tensor([58]), tensor([1]), tensor([36]), tensor([98]), tensor([79]), tensor([99]), tensor([43]), tensor([85]), tensor([55]), tensor([26]), tensor([76]), tensor([21]), tensor([39]), tensor([86]), tensor([47]), tensor([50]), tensor([65]), tensor([16]), tensor([35]), tensor([15]), tensor([82]), tensor([41]), tensor([22]), tensor([25]), tensor([45]), tensor([51]), tensor([57]), tensor([63]), tensor([99]), tensor([47]), tensor([14]), tensor([48]), tensor([54]), tensor([85]), tensor([23]), tensor([60]), tensor([83]), tensor([48]), tensor([79]), tensor([57]), tensor([9]), tensor([38]), tensor([24]), tensor([50]), tensor([44]), tensor([69]), tensor([8]), tensor([25]), tensor([94]), tensor([53]), tensor([81]), tensor([43]), tensor([10]), tensor([43]), tensor([77]), tensor([41]), tensor([69]), tensor([60]), tensor([46]), tensor([6]), tensor([4]), tensor([23]), tensor([74]), tensor([68]), tensor([66]), tensor([25]), tensor([62]), tensor([34]), tensor([93]), tensor([65]), tensor([99]), tensor([17]), tensor([54]), tensor([75]), tensor([79]), tensor([48]), tensor([27]), tensor([59]), tensor([64]), tensor([69]), tensor([98]), tensor([69]), tensor([64]), tensor([7]), tensor([91]), tensor([28]), tensor([40]), tensor([32]), tensor([29]), tensor([13]), tensor([55]), tensor([67]), tensor([42]), tensor([51]), tensor([31]), tensor([19]), tensor([11]), tensor([61]), tensor([20]), tensor([91]), tensor([74]), tensor([41]), tensor([53]), tensor([24]), tensor([79]), tensor([86]), tensor([72]), tensor([37]), tensor([79]), tensor([62]), tensor([19]), tensor([8]), tensor([51]), tensor([53]), tensor([5]), tensor([53]), tensor([20]), tensor([37]), tensor([98]), tensor([87]), tensor([38]), tensor([32]), tensor([90]), tensor([51]), tensor([70]), tensor([62]), tensor([38]), tensor([83]), tensor([59]), tensor([51]), tensor([79]), tensor([66]), tensor([32]), tensor([3]), tensor([78]), tensor([21]), tensor([26]), tensor([10]), tensor([32]), tensor([96]), tensor([76]), tensor([73]), tensor([77]), tensor([75]), tensor([33]), tensor([1]), tensor([31]), tensor([16]), tensor([48]), tensor([8]), tensor([21]), tensor([82]), tensor([22]), tensor([34]), tensor([17]), tensor([76]), tensor([35]), tensor([59]), tensor([75]), tensor([6]), tensor([85]), tensor([56]), tensor([72]), tensor([87]), tensor([67]), tensor([72]), tensor([52]), tensor([76]), tensor([40]), tensor([49]), tensor([73]), tensor([52]), tensor([15]), tensor([56]), tensor([75]), tensor([58]), tensor([81]), tensor([97]), tensor([80]), tensor([40]), tensor([85]), tensor([16]), tensor([97]), tensor([57]), tensor([8]), tensor([71]), tensor([44]), tensor([33]), tensor([47]), tensor([89]), tensor([28]), tensor([93]), tensor([74]), tensor([59]), tensor([72]), tensor([95]), tensor([26]), tensor([91]), tensor([87]), tensor([50]), tensor([96]), tensor([66]), tensor([30]), tensor([41]), tensor([11]), tensor([57]), tensor([1]), tensor([74]), tensor([40]), tensor([97]), tensor([33]), tensor([7]), tensor([51]), tensor([96]), tensor([48]), tensor([90]), tensor([9]), tensor([49]), tensor([60]), tensor([75]), tensor([13]), tensor([93]), tensor([32]), tensor([66]), tensor([90]), tensor([85]), tensor([83]), tensor([16]), tensor([77]), tensor([3]), tensor([87]), tensor([63]), tensor([40]), tensor([5]), tensor([14]), tensor([21]), tensor([65]), tensor([84]), tensor([94]), tensor([33]), tensor([11]), tensor([89]), tensor([22]), tensor([83]), tensor([49]), tensor([97]), tensor([77]), tensor([59]), tensor([22]), tensor([65]), tensor([86]), tensor([16]), tensor([16]), tensor([74]), tensor([48]), tensor([42]), tensor([68]), tensor([20]), tensor([56]), tensor([79]), tensor([92]), tensor([79]), tensor([79]), tensor([64]), tensor([19]), tensor([79]), tensor([2]), tensor([74]), tensor([61]), tensor([6]), tensor([28]), tensor([27]), tensor([58]), tensor([90]), tensor([38]), tensor([35]), tensor([91]), tensor([77]), tensor([50]), tensor([40]), tensor([86]), tensor([14]), tensor([12]), tensor([97]), tensor([12]), tensor([15]), tensor([94]), tensor([50]), tensor([30]), tensor([68]), tensor([16]), tensor([67]), tensor([84]), tensor([68]), tensor([56]), tensor([45]), tensor([50]), tensor([99]), tensor([16]), tensor([44]), tensor([88]), tensor([11]), tensor([58]), tensor([81]), tensor([9]), tensor([67]), tensor([17]), tensor([78]), tensor([44]), tensor([74]), tensor([81]), tensor([6]), tensor([3]), tensor([18]), tensor([27]), tensor([18]), tensor([40]), tensor([18]), tensor([16]), tensor([68]), tensor([19]), tensor([27]), tensor([97]), tensor([78]), tensor([8]), tensor([72]), tensor([89]), tensor([4]), tensor([92]), tensor([12]), tensor([30]), tensor([46]), tensor([45]), tensor([43]), tensor([23]), tensor([32]), tensor([61]), tensor([74]), tensor([55]), tensor([8]), tensor([53]), tensor([95]), tensor([53]), tensor([61]), tensor([45]), tensor([88]), tensor([73]), tensor([73]), tensor([90]), tensor([7]), tensor([24]), tensor([48]), tensor([59]), tensor([79]), tensor([31]), tensor([35]), tensor([6]), tensor([77]), tensor([48]), tensor([24]), tensor([97]), tensor([46]), tensor([17]), tensor([54]), tensor([43]), tensor([76]), tensor([74]), tensor([95]), tensor([60]), tensor([0]), tensor([1]), tensor([94]), tensor([33]), tensor([34]), tensor([57]), tensor([30]), tensor([70]), tensor([63]), tensor([63]), tensor([42]), tensor([58]), tensor([18]), tensor([73]), tensor([70]), tensor([46]), tensor([1]), tensor([61]), tensor([2]), tensor([39]), tensor([69]), tensor([24]), tensor([84]), tensor([18]), tensor([38]), tensor([78]), tensor([98]), tensor([31]), tensor([1]), tensor([39]), tensor([24]), tensor([79]), tensor([46]), tensor([13]), tensor([74]), tensor([93]), tensor([60]), tensor([33]), tensor([84]), tensor([15]), tensor([49]), tensor([99]), tensor([99]), tensor([71]), tensor([45]), tensor([77]), tensor([29]), tensor([4]), tensor([20]), tensor([87]), tensor([56]), tensor([17]), tensor([29]), tensor([63]), tensor([3]), tensor([64]), tensor([79]), tensor([20]), tensor([46]), tensor([53]), tensor([12]), tensor([14]), tensor([18]), tensor([80]), tensor([0]), tensor([9]), tensor([35]), tensor([53]), tensor([54]), tensor([31]), tensor([50]), tensor([83]), tensor([91]), tensor([68]), tensor([54]), tensor([71]), tensor([18]), tensor([46]), tensor([20]), tensor([26]), tensor([84]), tensor([16]), tensor([78]), tensor([81]), tensor([10]), tensor([96]), tensor([96]), tensor([70]), tensor([60]), tensor([89]), tensor([37]), tensor([10]), tensor([13]), tensor([48]), tensor([58]), tensor([73]), tensor([14]), tensor([43]), tensor([71]), tensor([89]), tensor([16]), tensor([56]), tensor([21]), tensor([50]), tensor([62]), tensor([52]), tensor([39]), tensor([61]), tensor([67]), tensor([83]), tensor([11]), tensor([75]), tensor([74]), tensor([10]), tensor([25]), tensor([69]), tensor([79]), tensor([97]), tensor([26]), tensor([3]), tensor([44]), tensor([20]), tensor([6]), tensor([78]), tensor([78]), tensor([28]), tensor([52]), tensor([26]), tensor([35]), tensor([37]), tensor([51]), tensor([7]), tensor([15]), tensor([90]), tensor([14]), tensor([69]), tensor([6]), tensor([96]), tensor([96]), tensor([99]), tensor([42]), tensor([64]), tensor([58]), tensor([6]), tensor([76]), tensor([87]), tensor([22]), tensor([7]), tensor([23]), tensor([99]), tensor([75]), tensor([49]), tensor([53]), tensor([53]), tensor([60]), tensor([80]), tensor([41]), tensor([82]), tensor([25]), tensor([19]), tensor([60]), tensor([60]), tensor([69]), tensor([94]), tensor([13]), tensor([5]), tensor([29]), tensor([11]), tensor([92]), tensor([96]), tensor([97]), tensor([66]), tensor([76]), tensor([60]), tensor([10]), tensor([40]), tensor([29]), tensor([0]), tensor([40]), tensor([23]), tensor([0]), tensor([49]), tensor([75]), tensor([90]), tensor([0]), tensor([98]), tensor([5]), tensor([80]), tensor([31]), tensor([75]), tensor([32]), tensor([81]), tensor([27]), tensor([76]), tensor([26]), tensor([83]), tensor([55]), tensor([11]), tensor([88]), tensor([29]), tensor([28]), tensor([35]), tensor([76]), tensor([61]), tensor([30]), tensor([94]), tensor([34]), tensor([46]), tensor([90]), tensor([25]), tensor([84]), tensor([26]), tensor([46]), tensor([58]), tensor([32]), tensor([51]), tensor([97]), tensor([67]), tensor([77]), tensor([26]), tensor([93]), tensor([13]), tensor([82]), tensor([76]), tensor([64]), tensor([11]), tensor([65]), tensor([53]), tensor([72]), tensor([57]), tensor([45]), tensor([58]), tensor([80]), tensor([7]), tensor([94]), tensor([19]), tensor([42]), tensor([24]), tensor([48]), tensor([93]), tensor([97]), tensor([8]), tensor([17]), tensor([42]), tensor([36]), tensor([6]), tensor([82]), tensor([9]), tensor([47]), tensor([87]), tensor([62]), tensor([63]), tensor([28]), tensor([20]), tensor([51]), tensor([5]), tensor([19]), tensor([21]), tensor([52]), tensor([24]), tensor([61]), tensor([90]), tensor([56]), tensor([35]), tensor([45]), tensor([57]), tensor([77]), tensor([65]), tensor([71]), tensor([55]), tensor([26]), tensor([97]), tensor([82]), tensor([7]), tensor([9]), tensor([4]), tensor([49]), tensor([8]), tensor([56]), tensor([38]), tensor([31]), tensor([64]), tensor([18]), tensor([23]), tensor([87]), tensor([30]), tensor([97]), tensor([51]), tensor([35]), tensor([36]), tensor([96]), tensor([28]), tensor([19]), tensor([94]), tensor([4]), tensor([36]), tensor([18]), tensor([86]), tensor([38]), tensor([62]), tensor([63]), tensor([81]), tensor([74]), tensor([86]), tensor([37]), tensor([42]), tensor([92]), tensor([33]), tensor([37]), tensor([39]), tensor([92]), tensor([2]), tensor([17]), tensor([83]), tensor([39]), tensor([82]), tensor([57]), tensor([11]), tensor([32]), tensor([1]), tensor([30]), tensor([81]), tensor([18]), tensor([52]), tensor([85]), tensor([9]), tensor([94]), tensor([83]), tensor([16]), tensor([49]), tensor([55]), tensor([59]), tensor([64]), tensor([82]), tensor([89]), tensor([41]), tensor([86]), tensor([52]), tensor([18]), tensor([34]), tensor([98]), tensor([21]), tensor([42]), tensor([11]), tensor([53]), tensor([82]), tensor([99]), tensor([57]), tensor([99]), tensor([44]), tensor([70]), tensor([4]), tensor([72]), tensor([3]), tensor([9]), tensor([19]), tensor([25]), tensor([5]), tensor([50]), tensor([71]), tensor([73]), tensor([5]), tensor([86]), tensor([33]), tensor([70]), tensor([95]), tensor([49]), tensor([84]), tensor([85]), tensor([17]), tensor([69]), tensor([99]), tensor([13]), tensor([39]), tensor([3]), tensor([29]), tensor([0]), tensor([66]), tensor([34]), tensor([30]), tensor([76]), tensor([14]), tensor([26]), tensor([45]), tensor([65]), tensor([77]), tensor([7]), tensor([64]), tensor([4]), tensor([81]), tensor([46]), tensor([91]), tensor([94]), tensor([38]), tensor([24]), tensor([19]), tensor([38]), tensor([25]), tensor([12]), tensor([40]), tensor([47]), tensor([71]), tensor([45]), tensor([24]), tensor([24]), tensor([69]), tensor([46]), tensor([91]), tensor([92]), tensor([96]), tensor([95]), tensor([98]), tensor([86]), tensor([50]), tensor([20]), tensor([35]), tensor([36]), tensor([55]), tensor([2]), tensor([2]), tensor([19]), tensor([42]), tensor([62]), tensor([44]), tensor([93]), tensor([54]), tensor([59]), tensor([50]), tensor([85]), tensor([6]), tensor([10]), tensor([41]), tensor([48]), tensor([93]), tensor([2]), tensor([87]), tensor([37]), tensor([69]), tensor([93]), tensor([28]), tensor([35]), tensor([80]), tensor([51]), tensor([83]), tensor([48]), tensor([57]), tensor([36]), tensor([66]), tensor([66]), tensor([3]), tensor([18]), tensor([85]), tensor([65]), tensor([47]), tensor([73]), tensor([89]), tensor([84]), tensor([60]), tensor([67]), tensor([32]), tensor([7]), tensor([55]), tensor([85]), tensor([73]), tensor([59]), tensor([2]), tensor([70]), tensor([20]), tensor([79]), tensor([5]), tensor([95]), tensor([39]), tensor([81]), tensor([29]), tensor([73]), tensor([32]), tensor([15]), tensor([46]), tensor([67]), tensor([28]), tensor([91]), tensor([25]), tensor([63]), tensor([15]), tensor([89]), tensor([97]), tensor([10]), tensor([27]), tensor([62]), tensor([90]), tensor([64]), tensor([60]), tensor([85]), tensor([91]), tensor([44]), tensor([4]), tensor([73]), tensor([70]), tensor([35]), tensor([99]), tensor([6]), tensor([58]), tensor([34]), tensor([45]), tensor([98]), tensor([11]), tensor([9]), tensor([78]), tensor([95]), tensor([55]), tensor([12]), tensor([51]), tensor([33]), tensor([42]), tensor([92]), tensor([15]), tensor([45]), tensor([4]), tensor([29]), tensor([47]), tensor([30]), tensor([76]), tensor([12]), tensor([16]), tensor([31]), tensor([19]), tensor([47]), tensor([35]), tensor([25]), tensor([58]), tensor([15]), tensor([56]), tensor([43]), tensor([56]), tensor([0]), tensor([8]), tensor([16]), tensor([29]), tensor([24]), tensor([76]), tensor([5]), tensor([76]), tensor([54]), tensor([28]), tensor([54]), tensor([32]), tensor([0]), tensor([85]), tensor([55]), tensor([5]), tensor([19]), tensor([10]), tensor([42]), tensor([28]), tensor([67]), tensor([31]), tensor([94]), tensor([8]), tensor([76])]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.34 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TEST GROUP:  0.445\n",
            "TEST ALL:  0.3876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fYJBfJyvpng"
      },
      "source": [
        "### Plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MDs9xRsqrc1R",
        "outputId": "87d493da-0022-4af0-cede-fc8c0ed5cb6a"
      },
      "source": [
        "method = \"iCaRL with SVM\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics FINETUNING for seed 31\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-o1n4skl-77"
      },
      "source": [
        "##CLOSED AND OPEN WORLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lUUjCewmNLo"
      },
      "source": [
        "#Dataset divided into 2 halves, 50 for closed 50 for open (choose five different random division)\n",
        "#1) closed world\n",
        "#  1.1)without rejection -> standard incremental scenario (train and test using selected 10 classes) but with 50 classes\n",
        "#      iter = 0 -> 10 or 20 (he does so in BDOC) classes ? ask Dario\n",
        "#      next iters -> add 10 until 50\n",
        "#      result expected -> equal to incremental \n",
        "#\n",
        "#  1.2)with rejection -> same procedure of above but we implement a rejection technique that \n",
        "#      classify as unknown an object that doesn't belong to the classes seen in the training (for the alg follow BDOC)\n",
        "#      result expected -> idealistic the model should not reject any of the object because we've tested the model with classes seen in the training\n",
        "#\n",
        "#2) open world\n",
        "#    at each step -> test the model only on unknown samples (the second half of dataset)\n",
        "#    \n",
        "#    result expected -> idealist the model should reject all of the test objects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFHiLXyLmaWu"
      },
      "source": [
        "###download and dividing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OJxXhGZmdDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69fc4b5f-c1a2-4441-f68c-3a5d3c7311ac"
      },
      "source": [
        "# Import dataset and apply transformations \n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# Check datasets length \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZFLVy0Tmt3G"
      },
      "source": [
        "#closed and open world\n",
        "splits_of_10 = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "#first 5 splits to closed world\n",
        "closed_data = {k:splits_of_10[k] for k in range(5)}\n",
        "\n",
        "#last 5 to open (removing the train val splits)\n",
        "open = []\n",
        "for k in range(5,10):\n",
        "  for j in[\"train\", \"val\"]:\n",
        "    open += splits_of_10[k][j]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQKnRMfjEaA-"
      },
      "source": [
        "### Modified iCaRL for closed/open"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fIvHXxhzwNI"
      },
      "source": [
        "def updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot, old_net, rejection=False, closed=True):\n",
        "  #exemplars_set_tot contiene tutti gli exemplars set ottenuti fino ad ora\n",
        "  #train_iter contiene tutti i dati (immagini + labels) delle classi nuove (s, .., t)\n",
        "  exemplars_subset = []\n",
        "  exemplars_indices = []\n",
        "  total_exemplars = []\n",
        "  labels_tot = []\n",
        "\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "    if (exemplar_set_class_k != []):\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "\n",
        "  if group_id > 1:\n",
        "    train_subset_total = torch.utils.data.ConcatDataset([train_subset, total_exemplars])\n",
        "  else:\n",
        "    train_subset_total = train_subset\n",
        "    \n",
        "  print(\"Len TOTAL train susbset: \", len(train_subset_total))\n",
        "  train_loader = torch.utils.data.DataLoader(train_subset_total, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  # train_loader è la concatenazione delle nuove classi con gli exemplar_sets calcolati fino a questo punto \n",
        "  print(\"training\")\n",
        "  train(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net, rejection = rejection, closed = closed)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAJtrUjIzqz-"
      },
      "source": [
        "import copy\n",
        "def incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_until_now, rejection=False, closed=True):\n",
        "  print(\"Starting the update representation\")\n",
        "  exemplar_indices = None\n",
        "  num_classes = 10\n",
        "  new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "  print(\"NEW CLASSES: \", new_classes_examined)\n",
        "\n",
        "  updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, rejection=rejection, closed=closed)\n",
        "                \n",
        "  iteration = group_id - 1\n",
        "  t = (num_classes * iteration) + num_classes # num_classes ricevute fino a questo momento \n",
        "  m = int(K/t) #casto ad intero ? per difetto o eccesso?\n",
        "  #s = num_classes * iteration\n",
        "\n",
        "  # REDUCING EXEMPLAR SET FOR EXISTING CLASSES\n",
        "  print(\"reducing exemplars for each class\")\n",
        "  print(total_classes_until_now)\n",
        "  for y in total_classes_until_now: #ci serve un set con tutte le classi fino ad ora viste\n",
        "    exemplar_y_new = reduceExemplarSet(m, exemplars_set_tot[y]) # valore associato alla chiave y che rappresenta il label della classe \n",
        "    print(\"REDUCED EXEMPLAR: \", len(exemplar_y_new))\n",
        "    exemplars_set_tot[y] = exemplar_y_new\n",
        "\n",
        "  \n",
        "  # CONSTRUCTION EXEMPLAR SET FOR NEW CLASSES\n",
        "  for y in new_classes_examined: # nuovi classi in arrivo di cui vogliamo costruire il set rappresentativo\n",
        "    images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data']\n",
        "    imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "    class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "    print(\"class train: \", class_train_subset)\n",
        "    print(\"Constructing exemplars of class\", y)\n",
        "    exemplars_set = constructExemplarSet(net, class_train_subset, m) # exemplar set è un set di indici\n",
        "    #devo recuperare dal dataset iniziale l'indice delle immagini dell'exemplar set creato \n",
        "    #for image in exemplars_set:\n",
        "     # exemplars_set = train_dataset.df.index[train_dataset.df['data'] == image].tolist()\n",
        "    exemplars_set_tot[y] = exemplars_set\n",
        "    print(\"exemplar set: \", exemplars_set)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkpzD3lazndM"
      },
      "source": [
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes_till_now, group_id, old_net, num_epochs=NUM_EPOCHS, rejection=False, closed=True, threshold=THRESHOLD):    \n",
        "\n",
        "      \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    print(\"num classes till now: \", num_classes_till_now)\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    print(num_epochs)\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        if rejection == True:\n",
        "          n_sample_known = 0\n",
        "          n_sample_unknown = 0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes_till_now, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            #if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            if rejection == True:\n",
        "              prediction_batch = outputs.data.cpu().numpy()\n",
        "              for i in range(len(prediction_batch)):\n",
        "                current_softmax = softmax(prediction_batch[i])\n",
        "                if max(current_softmax)>THRESHOLD:\n",
        "                  n_sample_known += 1\n",
        "                else:\n",
        "                  n_sample_unknown += 1\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        if rejection == True:\n",
        "          if closed == True:\n",
        "            epoch_acc = n_sample_known / float(len(train_dataloader.dataset))\n",
        "            numb = n_sample_known\n",
        "          else:\n",
        "            epoch_acc = n_sample_unknown / float(len(train_dataloader.dataset))\n",
        "            numb = n_sample_unknown\n",
        "        else:\n",
        "          epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, numb))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0xrVtFyzj75"
      },
      "source": [
        "def validate(net, val_dataloader, criterion, num_classes, rejection = False, closed = False):\n",
        "    #counter for rejection part, known and unknown\n",
        "    if rejection==True:\n",
        "      n_sample_known = 0\n",
        "      n_sample_unknown = 0\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            \n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        if rejection == True:\n",
        "          prediction_batch = outputs.data.cpu().numpy()\n",
        "          print(len(prediction_batch))\n",
        "          for i in range(len(prediction_batch)):\n",
        "            current_softmax = softmax(prediction_batch[i])\n",
        "            if max(current_softmax)>THRESHOLD:\n",
        "              n_sample_known += 1\n",
        "            else:\n",
        "              n_sample_unknown += 1\n",
        "        else:\n",
        "          #_, preds = classify(images, )\n",
        "          running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "          all_preds_cm.extend(preds.tolist())\n",
        "          all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    if rejection == True:\n",
        "      if closed == True:\n",
        "        acc = n_sample_known / float(len(val_dataloader.dataset))\n",
        "      else:\n",
        "        acc = n_sample_unknown / float(len(val_dataloader.dataset))\n",
        "    else:\n",
        "      acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes, rejection = False, closed = True):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes, rejection, closed)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcN9P3Gr1ICR"
      },
      "source": [
        "def sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets, rejection = False, closed = True):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 5\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      print(\"TEST SET LENGHT: \", len(test_set))\n",
        "      print(\"TEST CURRENT GROUP SET LENGHT: \", len(test_subset))\n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      test_classes = list(test_dataset.df.loc[test_set.indices, 'labels'].value_counts().index)\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TEST_SET CLASSES: \", test_classes)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen, rejection=rejection, closed=closed) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen, rejection=rejection, closed=closed)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen, rejection=rejection, closed=closed)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trLViwIa2_bY"
      },
      "source": [
        "### Closed World"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyhNorNImn9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2dbc176-ecda-4ba1-f15b-b3d8b6e15b29"
      },
      "source": [
        "# Reverse indexing for closed and open world\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, splits_of_10)\n",
        "print(outputs_labels_mapping.getGroups())\n",
        "\n",
        "# TEST split\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)\n",
        "#above are 10 splits but I want 5 for closed and 5 for open\n",
        "test_splits_closed = {i:test_splits[i] for i in range(5)}\n",
        "test_splits_open = []\n",
        "for i in range(5,10):\n",
        "  test_splits_open += test_splits[i]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akWuYU2cB1j9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df0047e-b3ca-480b-d84c-187a1fae305f"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "for i in outputs_labels_mapping.getGroups():\n",
        "  print(outputs_labels_mapping.getLabelsOfGroup(i))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    27\n",
            "1    86\n",
            "2    82\n",
            "3    78\n",
            "4    50\n",
            "5    30\n",
            "6    97\n",
            "7    69\n",
            "8    57\n",
            "9    25\n",
            "Name: labels, dtype: object\n",
            "10    95\n",
            "11    59\n",
            "12    58\n",
            "13    34\n",
            "14    81\n",
            "15    49\n",
            "16    13\n",
            "17    88\n",
            "18    68\n",
            "19    60\n",
            "Name: labels, dtype: object\n",
            "20    35\n",
            "21     3\n",
            "22    94\n",
            "23    10\n",
            "24    45\n",
            "25    37\n",
            "26    80\n",
            "27    72\n",
            "28    36\n",
            "29    24\n",
            "Name: labels, dtype: object\n",
            "30    31\n",
            "31    11\n",
            "32    98\n",
            "33    77\n",
            "34    53\n",
            "35    33\n",
            "36    96\n",
            "37    92\n",
            "38    52\n",
            "39    12\n",
            "Name: labels, dtype: object\n",
            "40    91\n",
            "41    67\n",
            "42     7\n",
            "43    62\n",
            "44    46\n",
            "45    65\n",
            "46    21\n",
            "47    32\n",
            "48    16\n",
            "49     0\n",
            "Name: labels, dtype: object\n",
            "50    79\n",
            "51    71\n",
            "52    63\n",
            "53    26\n",
            "54    14\n",
            "55     2\n",
            "56    73\n",
            "57    41\n",
            "58    56\n",
            "59    48\n",
            "Name: labels, dtype: object\n",
            "60    99\n",
            "61    23\n",
            "62    66\n",
            "63    22\n",
            "64     6\n",
            "65    93\n",
            "66    17\n",
            "67     5\n",
            "68     1\n",
            "69    28\n",
            "Name: labels, dtype: object\n",
            "70    87\n",
            "71    75\n",
            "72    55\n",
            "73    47\n",
            "74    39\n",
            "75    70\n",
            "76    42\n",
            "77    38\n",
            "78    29\n",
            "79     4\n",
            "Name: labels, dtype: object\n",
            "80    83\n",
            "81    51\n",
            "82    18\n",
            "83    89\n",
            "84    85\n",
            "85     9\n",
            "86    84\n",
            "87    64\n",
            "88    20\n",
            "89     8\n",
            "Name: labels, dtype: object\n",
            "90    43\n",
            "91    19\n",
            "92    15\n",
            "93    90\n",
            "94    74\n",
            "95    54\n",
            "96    61\n",
            "97    76\n",
            "98    44\n",
            "99    40\n",
            "Name: labels, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W6-U42kmn9F"
      },
      "source": [
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in closed_data.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,5):\n",
        "    v=test_splits_closed[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDPGgRJCtJe4"
      },
      "source": [
        "#for the test of open world\n",
        "open_test = Subset(test_dataset, test_splits_open)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgnJSvjtKYfS"
      },
      "source": [
        "targets_open = set()\n",
        "for i in range(len(open_test.indices)):\n",
        "  targets_open.add(test_dataset.__getitem__(open_test.indices[i])[2])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P76U7RxfLoxo"
      },
      "source": [
        "targets_closed = set()\n",
        "for k in test_splits_closed:\n",
        "  for j in range(len(test_splits_closed[k])):\n",
        "    targets_closed.add(test_dataset.__getitem__(test_splits_closed[k][j])[2])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gp8L6LONeqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e193a9-0fc0-4a83-be37-084772ded7df"
      },
      "source": [
        "#verifing that there aren't objects of the same class\n",
        "list(targets_closed.intersection(targets_open))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMyXZhVemn8q"
      },
      "source": [
        "### Closed world without rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl8IKzFsocw5"
      },
      "source": [
        "#without rejection\n",
        "rejection = False\n",
        "closed = True\n",
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI36G5hjodD8"
      },
      "source": [
        "method = \"Closed world without Rejection\"\n",
        "print(\"metrics ClosedWord for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,5):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBFSAonO0mgQ"
      },
      "source": [
        "### Closed world with rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32Dv40KK1ICR",
        "outputId": "f69af6cc-849f-4ab4-a9e2-1a058929e8ed"
      },
      "source": [
        "# train closed world with rejection\n",
        "rejection = True\n",
        "closed = True\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets, rejection=rejection, closed=closed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  1000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [67, 65, 59, 56, 49, 39, 22, 20, 18, 4]\n",
            "TRAIN_SET CLASSES:  [67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "VALIDATION CLASSES:  [59, 56, 49, 39, 22, 20, 18, 4, 67, 65]\n",
            "GROUP:  1\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "Len TOTAL train susbset:  4950\n",
            "training\n",
            "num classes till now:  10\n",
            "5\n",
            "Starting epoch 1/5, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.6994286775588989\n",
            "Train step - Step 10, Loss 0.30136817693710327\n",
            "Train step - Step 20, Loss 0.2911085784435272\n",
            "Train step - Step 30, Loss 0.26708194613456726\n",
            "Train epoch - Accuracy: 0.14585858585858585 Loss: 0.35155493756135303 Corrects: 1489\n",
            "Starting epoch 2/5, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.27089643478393555\n",
            "Train step - Step 50, Loss 0.2680715024471283\n",
            "Train step - Step 60, Loss 0.2523084878921509\n",
            "Train step - Step 70, Loss 0.2466038018465042\n",
            "Train epoch - Accuracy: 0.31555555555555553 Loss: 0.24956819676389597 Corrects: 2074\n",
            "Starting epoch 3/5, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.2362622767686844\n",
            "Train step - Step 90, Loss 0.24306665360927582\n",
            "Train step - Step 100, Loss 0.23924599587917328\n",
            "Train step - Step 110, Loss 0.1914578527212143\n",
            "Train epoch - Accuracy: 0.4163636363636364 Loss: 0.23055763977946658 Corrects: 2346\n",
            "Starting epoch 4/5, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.21142147481441498\n",
            "Train step - Step 130, Loss 0.2643110156059265\n",
            "Train step - Step 140, Loss 0.21732209622859955\n",
            "Train step - Step 150, Loss 0.20671935379505157\n",
            "Train epoch - Accuracy: 0.4624242424242424 Loss: 0.22319907110748868 Corrects: 2468\n",
            "Starting epoch 5/5, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.1971411406993866\n",
            "Train step - Step 170, Loss 0.2160784751176834\n",
            "Train step - Step 180, Loss 0.20748233795166016\n",
            "Train step - Step 190, Loss 0.1985824704170227\n",
            "Train epoch - Accuracy: 0.5115151515151515 Loss: 0.2116549697548452 Corrects: 2601\n",
            "Training finished in 13.8569917678833 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3d7f90>\n",
            "Constructing exemplars of class 67\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [2493, 27223, 4863, 13909, 3847, 46588, 49104, 28814, 29434, 38330, 49439, 20414, 9853, 36337, 5730, 12995, 37065, 26856, 17396, 45668, 41907, 42180, 9791, 25966, 16820, 1027, 37188, 42128, 22892, 9034, 33991, 37012, 11412, 37583, 5825, 37145, 4362, 9034, 8825, 37065, 40581, 6455, 4076, 18015, 6815, 16047, 33264, 6373, 8176, 1392, 965, 28814, 41706, 20414, 41648, 43635, 9174, 20057, 15332, 13905, 18423, 46553, 5825, 38422, 10194, 23836, 13704, 12575, 40782, 30805, 14515, 40906, 38671, 6928, 20452, 6036, 49244, 13704, 6036, 49104, 39931, 27380, 41648, 1088, 49407, 24092, 965, 30890, 41643, 9653, 37700, 27071, 23836, 20995, 19900, 46574, 35757, 36337, 46644, 2816, 30890, 41643, 34036, 39844, 26266, 11412, 19621, 24720, 43781, 11435, 32687, 11435, 2860, 39931, 9174, 10630, 46608, 1392, 5825, 11761, 36337, 35749, 44900, 16047, 7768, 22218, 47514, 4516, 2785, 30805, 25145, 27223, 1973, 27612, 12575, 10948, 22714, 19621, 14604, 15270, 25356, 18901, 11571, 27183, 40521, 30830, 24336, 12995, 4609, 9791, 6413, 10988, 9653, 40581, 36337, 6413, 14843, 40521, 15268, 37012, 16111, 38671, 27157, 41392, 34679, 36003, 35199, 49458, 37583, 13208, 25011, 35749, 12149, 46608, 1027, 49340, 38366, 49153, 284, 17163, 38671, 20422, 10194, 36938, 39243, 49407, 34679, 47229, 32224, 10630, 47386, 37145, 12908, 26351, 25463, 15465, 13704, 32655, 24336, 42572]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3d66d0>\n",
            "Constructing exemplars of class 59\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [24261, 37987, 15381, 9026, 10920, 42912, 12832, 26977, 19092, 2834, 1270, 12082, 15810, 16001, 14004, 13465, 1571, 11260, 24255, 6973, 34779, 48359, 1160, 11254, 38279, 32614, 35676, 35501, 16738, 24236, 32372, 10073, 17507, 10111, 12254, 47967, 3711, 42912, 47226, 19949, 20713, 25553, 3165, 12054, 3500, 33020, 16001, 15810, 26849, 3165, 30109, 16650, 10087, 13249, 32077, 6973, 37987, 7065, 10919, 27587, 10064, 2987, 16650, 17427, 28218, 27167, 16001, 16777, 36223, 34455, 11004, 10103, 47372, 46093, 15381, 9981, 19949, 33062, 38279, 32264, 30471, 34455, 41109, 1562, 5504, 25930, 30328, 35811, 10111, 4708, 30883, 37676, 37719, 27167, 10920, 35811, 27489, 1048, 22988, 47809, 7093, 35676, 30055, 32264, 5277, 42060, 19474, 41678, 35136, 30109, 38195, 47618, 29305, 18985, 28218, 29872, 40007, 17567, 15810, 6571, 20207, 28860, 25566, 21649, 32192, 973, 20713, 30828, 22130, 36945, 14671, 14376, 13873, 30109, 16738, 26977, 37533, 23210, 38064, 16001, 13178, 13420, 35920, 11260, 16709, 14004, 36637, 6283, 16189, 31388, 12082, 35811, 16250, 20491, 23044, 25142, 48441, 567, 11637, 2688, 34455, 29873, 23044, 26200, 35811, 3488, 30787, 10919, 4560, 40007, 18467, 43463, 16650, 24308, 34799, 28258, 13249, 12082, 7980, 24627, 34779, 8081, 7037, 34799, 25012, 26213, 3995, 18938, 10073, 30817, 2290, 48359, 1160, 30828, 11587, 34779, 33418, 20216, 10064, 34163]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c676a10>\n",
            "Constructing exemplars of class 39\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [49423, 32037, 38905, 25386, 18143, 23943, 218, 24067, 20430, 6574, 41688, 42644, 7493, 13811, 12184, 22599, 42224, 45131, 7060, 17307, 25467, 24442, 6795, 13489, 14383, 44582, 18624, 25177, 1446, 28001, 35244, 47632, 46877, 5752, 44790, 31843, 7060, 3495, 18637, 32758, 32620, 45674, 19268, 29457, 44596, 20453, 25252, 49897, 5752, 13219, 8415, 15132, 34011, 38041, 37857, 15156, 18147, 15148, 30788, 11597, 5021, 32802, 36418, 32758, 41736, 41565, 29450, 17018, 6776, 11597, 5344, 37881, 47366, 200, 49899, 18206, 6696, 32305, 24205, 22776, 20919, 6776, 18147, 15156, 13825, 11169, 21397, 46764, 23492, 11179, 16816, 1062, 48671, 15148, 37727, 11427, 34507, 16816, 2214, 45886, 35280, 5930, 13489, 6124, 26365, 46877, 9660, 46089, 19637, 10125, 5405, 22175, 25252, 36766, 45884, 15098, 13489, 19080, 1446, 14670, 27895, 25704, 27868, 33518, 17472, 32835, 35777, 11179, 20763, 49920, 33238, 30686, 12025, 10953, 11822, 15580, 48448, 49064, 37727, 5930, 43077, 49155, 40098, 11427, 14172, 31970, 46368, 47014, 45385, 7493, 6542, 19725, 23668, 36418, 5688, 35268, 5414, 25537, 42628, 37062, 45674, 19268, 36912, 23011, 28986, 44596, 17437, 5752, 28163, 41383, 37881, 16475, 41688, 3965, 48544, 24369, 47593, 2213, 39279, 7639, 38449, 21409, 101, 29050, 22175, 5021, 37919, 19725, 45146, 19637, 3758, 1446, 38905, 31502, 32439, 12263, 25126, 10340, 13745, 6542]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c23e990>\n",
            "Constructing exemplars of class 22\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [48379, 25457, 22774, 30469, 17596, 30764, 632, 33877, 14862, 15329, 15804, 8411, 38235, 44433, 44520, 27911, 17623, 4915, 14507, 31767, 38016, 45986, 36375, 27012, 37478, 34873, 6544, 26696, 25300, 13977, 8656, 36122, 9305, 28257, 42014, 34912, 4522, 6526, 6376, 11635, 17251, 7341, 35337, 33853, 2053, 38451, 11429, 8088, 44433, 21510, 45561, 20180, 39111, 36675, 31053, 46837, 37861, 40034, 5766, 7341, 16612, 22156, 46552, 13630, 30411, 7761, 8389, 9681, 42949, 12726, 8126, 26982, 21790, 19004, 36397, 2503, 8706, 19813, 41161, 23454, 24970, 44529, 22156, 7678, 28109, 1442, 42949, 5402, 40972, 5293, 4688, 5725, 12872, 8638, 24262, 26696, 11693, 40431, 36844, 8656, 6300, 17599, 19765, 40431, 39621, 44557, 24165, 15667, 40349, 18163, 48818, 19462, 6369, 16472, 46230, 11002, 4647, 35638, 5293, 632, 22036, 28257, 20024, 8389, 24415, 6300, 7761, 25186, 35194, 6090, 26778, 48995, 35732, 31767, 2584, 7761, 30411, 37554, 21333, 40292, 747, 36977, 34912, 29999, 19765, 30472, 5102, 36314, 10826, 13150, 7888, 1442, 5885, 38901, 33527, 6762, 12675, 30505, 17326, 43175, 19004, 19020, 33886, 8638, 37382, 35337, 424, 36204, 3004, 31767, 7784, 14862, 18614, 42949, 27820, 32505, 12670, 9828, 7784, 945, 48818, 11443, 10936, 27911, 31223, 22156, 47193, 9681, 3293, 11203, 9104, 46552, 9038, 46426, 20024, 17646, 36093, 44704, 10656, 48995]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c676fd0>\n",
            "Constructing exemplars of class 18\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [25395, 49145, 17430, 38092, 27799, 8405, 31736, 13477, 43832, 10419, 20720, 32258, 9782, 29034, 31962, 29210, 12968, 15174, 13417, 7557, 29811, 13606, 33465, 11999, 46367, 35442, 15969, 36385, 5060, 43832, 16017, 10829, 12008, 35061, 13030, 1273, 21078, 44655, 30043, 2778, 18176, 22390, 2434, 24328, 27613, 34561, 4107, 440, 2811, 49759, 10144, 4461, 22438, 9144, 5477, 20174, 16929, 23573, 10693, 39132, 7149, 3343, 19596, 29926, 36385, 38118, 2003, 43749, 7908, 22976, 6431, 28528, 46925, 20261, 32963, 43156, 18683, 22115, 47407, 9605, 2435, 40615, 38379, 30216, 4461, 14811, 38976, 41532, 30092, 40085, 35968, 31961, 10693, 13269, 24271, 42724, 30043, 39149, 27698, 46367, 35442, 2554, 30092, 13785, 33465, 14776, 35917, 14102, 45162, 2692, 10693, 27981, 38719, 16819, 13269, 17232, 27981, 46622, 43870, 38974, 32122, 13417, 20584, 21650, 14283, 36802, 41949, 49433, 22390, 47407, 42420, 7649, 31530, 31962, 21960, 35004, 1597, 13172, 18541, 45817, 38776, 2434, 4461, 19077, 30092, 18932, 38379, 49129, 26439, 21078, 41005, 35442, 22397, 9774, 27981, 32431, 25531, 27221, 5189, 27169, 17134, 4646, 24795, 14102, 18503, 19596, 37924, 25982, 24601, 27981, 24252, 33606, 22322, 3570, 36761, 2778, 3114, 39521, 1005, 14279, 37016, 1273, 41094, 21442, 5060, 17561, 21078, 4786, 35637, 21348, 38170, 20386, 27343, 33465, 11808, 10419, 25395, 19921, 3114, 37638]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c966790>\n",
            "Constructing exemplars of class 65\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [47299, 23560, 32485, 39587, 3602, 36020, 14986, 37153, 16237, 47616, 20321, 33367, 23464, 432, 22801, 6254, 26205, 21279, 26586, 16041, 35284, 15499, 28135, 28375, 17685, 8531, 8338, 6708, 45893, 22195, 39396, 7322, 46170, 17480, 18570, 10962, 10704, 19734, 35631, 35284, 2972, 28962, 983, 18720, 7973, 8225, 34105, 22729, 22801, 3206, 18709, 44985, 24955, 4158, 18691, 463, 31718, 33835, 17044, 3576, 22123, 9040, 29000, 16779, 45287, 27400, 12674, 14941, 22498, 7010, 42091, 13805, 3807, 21354, 49493, 21353, 432, 29924, 29675, 44923, 8009, 24955, 4158, 25387, 15499, 18252, 35425, 27376, 42091, 18498, 34695, 11441, 645, 44002, 13399, 22720, 21975, 42353, 38680, 14712, 48711, 49572, 22720, 17268, 29952, 6255, 33781, 43135, 14200, 21639, 365, 39308, 18709, 44985, 13533, 22720, 19895, 48164, 39714, 36776, 3206, 3602, 2424, 31883, 3799, 23825, 16679, 41727, 7762, 6157, 33366, 10177, 35407, 1814, 16039, 30950, 32583, 17044, 36547, 44836, 7354, 41417, 33043, 28962, 24847, 36076, 19220, 10704, 44459, 34695, 7777, 16986, 21279, 37662, 28962, 32987, 45463, 29364, 47616, 33597, 19199, 7323, 34559, 39188, 39587, 18315, 34695, 49493, 23819, 12674, 42643, 36076, 4158, 15789, 11836, 47845, 46475, 39714, 13152, 21347, 15296, 3576, 32585, 26757, 28855, 4603, 49003, 21639, 8531, 8338, 49752, 594, 4158, 22729, 18720, 21366, 8526, 44985, 16505, 43574]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c226510>\n",
            "Constructing exemplars of class 49\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [38251, 30220, 25633, 30522, 23821, 30826, 1542, 16668, 34270, 39979, 38402, 21727, 17272, 46044, 14890, 37237, 8515, 10421, 15401, 8856, 6758, 23625, 6117, 16363, 49043, 46569, 19075, 35671, 19911, 29350, 28201, 38236, 34270, 4264, 30826, 14536, 43512, 31401, 39644, 23169, 30549, 46931, 6144, 30821, 36373, 11308, 15491, 39780, 38404, 18456, 45722, 30522, 6622, 17524, 34224, 23625, 29777, 29526, 43512, 515, 13859, 35949, 34303, 1147, 19676, 25127, 46767, 40890, 28201, 14536, 8301, 27143, 14929, 34951, 6117, 42740, 28054, 2265, 46044, 617, 43738, 1147, 23913, 17099, 31146, 31350, 28691, 31975, 2265, 39780, 30549, 36009, 36373, 1434, 18112, 5061, 45718, 18001, 45095, 24785, 42325, 19812, 40890, 13859, 2920, 43337, 42904, 4047, 41037, 34224, 49659, 11736, 16421, 27170, 9824, 31721, 43512, 17896, 17790, 45945, 19812, 47186, 6809, 11919, 20211, 46044, 49412, 48754, 33433, 40747, 1140, 30522, 12722, 32735, 31003, 46075, 31578, 45633, 48987, 45650, 2236, 2219, 37546, 423, 35911, 18136, 41320, 21923, 5662, 43049, 22725, 43017, 15100, 34801, 20532, 33744, 15544, 11517, 29706, 15958, 21727, 1147, 34270, 49080, 23169, 31975, 1298, 9020, 16251, 45762, 43738, 34951, 17272, 17576, 44670, 27143, 8301, 6351, 11652, 19911, 467, 2262, 48987, 17524, 5908, 38209, 14929, 37546, 14804, 34270, 30821, 34111, 25633, 4351, 37546, 17229, 40191, 2444, 36373, 45762]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c2267d0>\n",
            "Constructing exemplars of class 56\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [34146, 1822, 21594, 26410, 6509, 26150, 22623, 29297, 30691, 9632, 39945, 8914, 12499, 40994, 22971, 47438, 10856, 41541, 48466, 4387, 21613, 7289, 13025, 14392, 11843, 108, 10239, 27809, 25826, 43671, 6766, 43311, 47662, 18867, 39221, 25059, 29639, 30715, 17919, 42473, 40733, 37352, 25780, 47805, 27676, 8628, 49001, 24037, 31891, 46451, 21522, 2762, 629, 26393, 39054, 20305, 48478, 27001, 9102, 31644, 49875, 9542, 39591, 38889, 49633, 14841, 1468, 43671, 5954, 25519, 48484, 48820, 33660, 14392, 33586, 46633, 19681, 16238, 27200, 45142, 43308, 41946, 39314, 31671, 15432, 37299, 34304, 39436, 45836, 6486, 17754, 269, 7751, 6595, 3044, 36898, 19240, 9491, 4493, 6290, 43633, 26544, 16420, 8063, 12564, 11376, 2764, 34304, 34183, 12571, 13189, 3044, 31427, 27876, 29454, 1529, 6486, 42019, 25059, 31574, 48478, 39500, 7125, 14906, 40710, 31644, 26153, 45142, 689, 12499, 32266, 40003, 48788, 21613, 5316, 15000, 42786, 7619, 9452, 7289, 23839, 36137, 6829, 44068, 42473, 7492, 13045, 1701, 35579, 30758, 48440, 44808, 4387, 20305, 43308, 47956, 1468, 26765, 24599, 40710, 49638, 31465, 41268, 34378, 26289, 25059, 26150, 31891, 26765, 1529, 40710, 14752, 19144, 14392, 18192, 5233, 37809, 17822, 47423, 8266, 4657, 28823, 19718, 47662, 29524, 35565, 41663, 43212, 45845, 1918, 39500, 24037, 24599, 26765, 44416, 43011, 7751, 43308, 37281, 48440]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c226150>\n",
            "Constructing exemplars of class 20\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [15166, 34159, 46547, 11785, 10219, 16438, 32921, 42719, 25513, 6728, 14139, 11001, 2747, 26111, 2020, 48170, 25513, 26973, 27424, 4586, 23321, 3158, 48567, 9289, 46677, 6072, 24084, 41466, 11958, 18131, 11488, 5401, 3033, 41209, 4586, 45453, 23976, 57, 23000, 134, 44379, 1827, 34122, 30460, 13794, 49986, 46897, 3033, 28851, 30824, 18453, 118, 5284, 18320, 49354, 29519, 39758, 19539, 15654, 41924, 44861, 36039, 3117, 26407, 5231, 9289, 29259, 44861, 14029, 47829, 25136, 3197, 46108, 30827, 5284, 9507, 14207, 16301, 16649, 30383, 27548, 26613, 41209, 4768, 49657, 38013, 32955, 34493, 16302, 18453, 47829, 44177, 22230, 48340, 26405, 10985, 29591, 5284, 23045, 19132, 20210, 7285, 37657, 37732, 13760, 39849, 2020, 28391, 25513, 9289, 5940, 29591, 27548, 42123, 7038, 12253, 49855, 43736, 40494, 45704, 24084, 7205, 908, 13794, 31785, 46001, 44111, 25504, 32672, 45453, 47798, 9374, 17318, 6279, 14207, 34122, 32955, 12314, 17446, 1819, 32262, 19763, 37898, 41924, 39151, 42719, 31197, 32262, 44914, 47829, 134, 46898, 36514, 10335, 16774, 25015, 32705, 46015, 47108, 49630, 46108, 30827, 32955, 22320, 3158, 30383, 26830, 40989, 4623, 24829, 33829, 5846, 34663, 20631, 49657, 27649, 15143, 26572, 1768, 48324, 1819, 47921, 5643, 37332, 6307, 48767, 46677, 49855, 39401, 1827, 46008, 19855, 33153, 1383, 44839, 636, 39022, 21572, 26768, 23654]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c980210>\n",
            "Constructing exemplars of class 4\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [8616, 19191, 36030, 11074, 13211, 40009, 4813, 36339, 21645, 28584, 40386, 7015, 33312, 44200, 9380, 33880, 4021, 37055, 42042, 24046, 26580, 32837, 28628, 25099, 46992, 49250, 13442, 17304, 48414, 33850, 27056, 46937, 43490, 35690, 6591, 12963, 13422, 43490, 38272, 6834, 30288, 2769, 12526, 46335, 37645, 33791, 21103, 20735, 23505, 42042, 24046, 33174, 10243, 38272, 31321, 37030, 18449, 20446, 39400, 28584, 42727, 20583, 3230, 45367, 21645, 20078, 40753, 2711, 32866, 6993, 21069, 12963, 30683, 5542, 24154, 14436, 33329, 46589, 44162, 12802, 41403, 36124, 24046, 33793, 27902, 25134, 6993, 4851, 4865, 3470, 16148, 12526, 20804, 47355, 32185, 43786, 9833, 825, 36241, 37057, 1308, 6051, 24619, 7083, 20970, 34433, 26476, 7015, 27514, 12122, 46589, 7819, 18449, 5677, 30185, 44872, 12526, 30683, 27884, 18449, 33791, 145, 10417, 3423, 3230, 41403, 11798, 32771, 4449, 18449, 12897, 5139, 45465, 8946, 3423, 12526, 6591, 35390, 21109, 10983, 20446, 24619, 4899, 43786, 8292, 42050, 40230, 44629, 1636, 47109, 3865, 49483, 389, 35649, 29495, 12122, 31043, 44104, 4449, 19181, 12963, 39748, 15063, 3310, 4449, 113, 25099, 42842, 8179, 14826, 23270, 13454, 20699, 12802, 20970, 3801, 1302, 8008, 42842, 7682, 19118, 42349, 3470, 2139, 2866, 3865, 14865, 3779, 15812, 43490, 25090, 8946, 49483, 44237, 2866, 30853, 18400, 32771, 33880, 24046]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.56 0.2094002068042755\n",
            "TEST GROUP:  0.612\n",
            "TEST ALL:  0.508\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  2000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [82, 81, 7, 16, 18, 20, 21, 22, 34, 39, 47, 49, 56, 59, 65, 67, 68, 79, 80, 4]\n",
            "TRAIN_SET CLASSES:  [79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "VALIDATION CLASSES:  [47, 34, 21, 16, 82, 81, 80, 79, 7, 68]\n",
            "GROUP:  2\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  20\n",
            "5\n",
            "Starting epoch 1/5, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.4786575734615326\n",
            "Train step - Step 10, Loss 0.2408965677022934\n",
            "Train step - Step 20, Loss 0.2520991265773773\n",
            "Train step - Step 30, Loss 0.22044594585895538\n",
            "Train step - Step 40, Loss 0.21477492153644562\n",
            "Train step - Step 50, Loss 0.21496336162090302\n",
            "Train epoch - Accuracy: 0.25625899280575537 Loss: 0.24712913065076733 Corrects: 1403\n",
            "Starting epoch 2/5, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.21163992583751678\n",
            "Train step - Step 70, Loss 0.2167361080646515\n",
            "Train step - Step 80, Loss 0.21134009957313538\n",
            "Train step - Step 90, Loss 0.20694971084594727\n",
            "Train step - Step 100, Loss 0.21117554605007172\n",
            "Train epoch - Accuracy: 0.28647482014388487 Loss: 0.20996199115145978 Corrects: 1962\n",
            "Starting epoch 3/5, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.20255409181118011\n",
            "Train step - Step 120, Loss 0.21074318885803223\n",
            "Train step - Step 130, Loss 0.2090960592031479\n",
            "Train step - Step 140, Loss 0.2045198529958725\n",
            "Train step - Step 150, Loss 0.22030936181545258\n",
            "Train step - Step 160, Loss 0.19820432364940643\n",
            "Train epoch - Accuracy: 0.33323741007194246 Loss: 0.2043877715729981 Corrects: 2244\n",
            "Starting epoch 4/5, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.19694355130195618\n",
            "Train step - Step 180, Loss 0.20370490849018097\n",
            "Train step - Step 190, Loss 0.20076122879981995\n",
            "Train step - Step 200, Loss 0.19872407615184784\n",
            "Train step - Step 210, Loss 0.19086265563964844\n",
            "Train epoch - Accuracy: 0.3601438848920863 Loss: 0.19970232619227266 Corrects: 2456\n",
            "Starting epoch 5/5, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.2055506706237793\n",
            "Train step - Step 230, Loss 0.18172195553779602\n",
            "Train step - Step 240, Loss 0.19336792826652527\n",
            "Train step - Step 250, Loss 0.1985117644071579\n",
            "Train step - Step 260, Loss 0.20734453201293945\n",
            "Train step - Step 270, Loss 0.2019658088684082\n",
            "Train epoch - Accuracy: 0.378705035971223 Loss: 0.19654389035358705 Corrects: 2578\n",
            "Training finished in 32.25327444076538 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c366650>\n",
            "Constructing exemplars of class 79\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [32829, 27602, 1977, 23535, 46033, 16341, 29605, 8586, 24098, 7140, 47938, 5404, 49095, 36438, 37142, 38156, 35165, 9793, 4744, 23535, 16517, 47158, 29031, 24260, 25718, 38926, 49072, 36145, 14441, 26446, 40781, 41564, 11310, 8223, 25004, 6211, 5485, 6513, 13555, 33252, 3139, 46745, 22362, 21378, 34943, 28188, 20711, 31813, 24493, 7775, 11310, 9378, 29504, 39947, 41078, 28063, 6211, 35258, 30418, 16514, 13846, 28435, 23381, 363, 42823, 34273, 5804, 29059, 7016, 10556, 30124, 29504, 6494, 12495, 29243, 45237, 11123, 47739, 38175, 24547, 38515, 8336, 13555, 6513, 23272, 48012, 37245, 6842, 7133, 40565, 42723, 26188, 7016, 640, 38725, 38926, 18378, 12646, 40529, 10996]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c369c90>\n",
            "Constructing exemplars of class 47\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [17416, 23245, 33168, 33014, 13857, 39354, 38323, 33273, 18652, 44843, 25483, 22234, 36842, 24080, 38500, 3281, 5134, 19937, 43570, 14964, 1786, 26618, 4240, 49031, 34456, 9555, 2667, 38536, 17620, 38536, 14038, 27133, 23407, 19927, 8878, 13301, 24422, 5345, 3327, 59, 39661, 10799, 1670, 16654, 9644, 59, 49764, 7059, 36230, 16873, 41073, 25576, 6429, 22234, 19824, 7121, 25313, 19157, 5020, 15845, 6704, 43269, 24646, 45364, 44251, 31892, 47802, 44779, 45783, 38536, 49526, 26642, 56, 45979, 31553, 8653, 26632, 42412, 1786, 26618, 45800, 22234, 16937, 14666, 34621, 33168, 31025, 29029, 30680, 8193, 44779, 10245, 21027, 43570, 1786, 8185, 9704, 28601, 34532, 1864]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c7d2810>\n",
            "Constructing exemplars of class 7\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [18920, 9862, 34285, 24978, 24869, 44096, 13986, 23354, 5279, 19980, 15748, 18207, 21893, 20596, 32730, 16035, 31764, 47924, 8110, 38438, 36670, 26002, 11767, 15496, 37815, 42898, 27018, 3810, 43273, 39511, 32909, 22759, 15748, 6367, 9118, 23354, 40476, 41605, 25806, 29293, 13962, 38690, 28243, 29732, 482, 46051, 42288, 377, 40128, 40805, 36892, 26640, 49282, 47435, 28363, 8444, 40805, 26593, 14762, 20596, 8716, 12202, 44860, 38876, 4978, 12293, 12134, 15042, 24869, 23752, 45508, 6367, 6679, 15243, 7910, 4275, 25806, 40734, 35841, 8110, 20569, 44096, 4884, 7110, 2161, 36222, 35960, 11540, 13286, 14701, 37565, 7002, 377, 29697, 45809, 36934, 13986, 30814, 31942, 39511]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f340b95fad0>\n",
            "Constructing exemplars of class 82\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [36301, 36327, 7586, 21064, 40409, 4037, 32449, 26501, 37521, 37189, 37510, 32741, 28265, 12210, 12252, 20686, 35369, 18202, 18702, 26852, 41977, 44523, 30528, 40655, 274, 23719, 5904, 31729, 28164, 40082, 18937, 871, 36690, 36727, 34612, 5454, 15304, 42693, 47534, 23130, 16467, 6277, 6227, 40367, 9111, 41900, 37521, 6141, 32106, 324, 44213, 10381, 30911, 28164, 29650, 4037, 38355, 34412, 27709, 27053, 32644, 6761, 16228, 12518, 15113, 13309, 49631, 19060, 29229, 16897, 18644, 34673, 44928, 25265, 11901, 9637, 45373, 42250, 19839, 7471, 32602, 8061, 43699, 46768, 26562, 5578, 33931, 18937, 6057, 10519, 3842, 3340, 16791, 16838, 33522, 31815, 14807, 11714, 23130, 45482]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c35e150>\n",
            "Constructing exemplars of class 34\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [41684, 49918, 28754, 36335, 48048, 28310, 31864, 42407, 8555, 15671, 5308, 21337, 3773, 15614, 30618, 47152, 5552, 39095, 10216, 35278, 43775, 2382, 13272, 8396, 40019, 31864, 29853, 5379, 35762, 120, 42093, 2324, 16611, 1472, 9108, 4091, 48286, 32312, 4556, 44854, 47241, 24571, 5308, 27664, 27630, 29129, 12329, 29468, 6096, 14310, 14859, 36177, 48940, 45766, 35325, 36330, 45900, 7853, 49605, 7368, 40597, 11646, 48783, 17608, 10940, 27028, 41972, 2648, 21197, 31833, 49111, 24459, 45766, 39095, 37038, 8406, 23411, 46887, 28868, 25510, 40438, 29468, 1125, 6849, 4681, 13335, 37038, 8824, 33386, 47241, 48286, 46124, 13038, 12459, 20892, 16193, 15179, 1076, 25110, 16944]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c35b990>\n",
            "Constructing exemplars of class 81\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [12751, 186, 45181, 23159, 20226, 2097, 30409, 503, 39819, 1943, 43532, 40790, 19383, 4925, 8321, 25857, 47627, 49830, 14183, 34026, 9696, 186, 13014, 38079, 9789, 41277, 8668, 5539, 35648, 40177, 22076, 14183, 20504, 43892, 10584, 16503, 6997, 11223, 47237, 42338, 37290, 49232, 22308, 2841, 20278, 31655, 23159, 10180, 18708, 41277, 45736, 21021, 42418, 35097, 36296, 32821, 37894, 9281, 27129, 18389, 41549, 49742, 24532, 20504, 45736, 7554, 23171, 49102, 19095, 49830, 45157, 31655, 22260, 5539, 8276, 6172, 40631, 32141, 35119, 40965, 39790, 43447, 11223, 48066, 34460, 33201, 12746, 40369, 18325, 14648, 28000, 38528, 49309, 25500, 18037, 42926, 36296, 15149, 15119, 23286]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c23aa10>\n",
            "Constructing exemplars of class 21\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [6133, 40466, 12355, 32641, 10676, 35819, 2495, 12998, 7991, 7314, 21562, 16105, 46760, 49609, 30804, 42398, 12998, 3277, 14427, 48058, 16826, 19339, 27524, 41054, 44316, 45390, 13912, 40270, 15874, 42236, 8700, 30132, 41092, 1902, 38040, 21758, 13788, 17657, 46251, 11367, 13806, 1672, 15256, 35619, 3737, 17263, 23559, 49609, 37353, 30904, 12784, 42398, 5713, 2406, 23522, 3900, 3043, 4610, 22795, 35176, 42679, 172, 11712, 4455, 3900, 36549, 49390, 14101, 25784, 36592, 35619, 18764, 11210, 9433, 6357, 21043, 39688, 43491, 2193, 17386, 8725, 24099, 38201, 1794, 37128, 9177, 39688, 37046, 10284, 2695, 49014, 23328, 37120, 13806, 26004, 48668, 40995, 41431, 14419, 21043]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3ecdd0>\n",
            "Constructing exemplars of class 80\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [8851, 16681, 15026, 684, 361, 30281, 41023, 41504, 42914, 7599, 3095, 31568, 18078, 2586, 7191, 37506, 28931, 30641, 40423, 33526, 6320, 43266, 40896, 17949, 18, 40095, 15106, 21740, 192, 13998, 33526, 19750, 43009, 25878, 7771, 18375, 43204, 28928, 34262, 1404, 37298, 24057, 3095, 28406, 36788, 20875, 17441, 7523, 41504, 45254, 28406, 42529, 37374, 25878, 30112, 39656, 43266, 48245, 46302, 45959, 44870, 4924, 19563, 1212, 25749, 39537, 42210, 28299, 3514, 33392, 42206, 42529, 37298, 18021, 17990, 39496, 9302, 40844, 49357, 19346, 28931, 2045, 35914, 33348, 4380, 1124, 38931, 47146, 340, 37506, 1090, 29240, 22392, 32587, 11142, 15974, 34060, 16519, 697, 35914]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3d7e50>\n",
            "Constructing exemplars of class 68\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [27599, 31919, 31044, 46739, 36879, 36099, 49932, 26793, 44804, 10921, 14817, 37785, 45021, 11225, 41493, 10028, 38288, 22453, 49464, 28443, 35328, 9294, 932, 1245, 17077, 30341, 9441, 18331, 46517, 37234, 47074, 24727, 45714, 35058, 29473, 24338, 39701, 18690, 30834, 36856, 4080, 22345, 5462, 320, 46401, 19562, 44458, 35625, 20816, 4342, 40435, 33990, 2815, 46591, 29117, 22905, 37246, 37785, 3836, 17859, 32979, 44652, 47696, 2116, 13212, 8399, 43263, 22453, 34184, 38822, 15046, 15022, 4342, 9591, 49013, 3918, 47868, 7915, 6378, 9723, 25899, 31434, 20014, 22591, 36099, 26793, 9196, 15573, 24524, 18740, 1831, 20325, 28450, 40017, 41928, 32181, 46739, 21729, 2754, 36856]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c22ab90>\n",
            "Constructing exemplars of class 16\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [13543, 22380, 18738, 6305, 34239, 1414, 40901, 34895, 24199, 20465, 15396, 41484, 32487, 22297, 26710, 7513, 7930, 14309, 36935, 15439, 23953, 11817, 12859, 5810, 19097, 2868, 22451, 4542, 23692, 37111, 10619, 35546, 27967, 46976, 11166, 44380, 38554, 12643, 29271, 37324, 13725, 35745, 16965, 1898, 13689, 47601, 38519, 25632, 29373, 10616, 28550, 49991, 26400, 43035, 7930, 20019, 30494, 22825, 48907, 45065, 18348, 24721, 35320, 39606, 20240, 4567, 24132, 35005, 596, 41771, 699, 13792, 7826, 36421, 40527, 8702, 41823, 26484, 29271, 16534, 30251, 4551, 40313, 13689, 30331, 38743, 49055, 3350, 5867, 25137, 36935, 14182, 45983, 39201, 3869, 22549, 41865, 9864, 29901, 5511]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.2 0.1707935631275177\n",
            "TEST GROUP:  0.391\n",
            "TEST ALL:  0.406\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  3000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [81, 79, 4, 10, 16, 18, 20, 22, 24, 32, 34, 56, 64, 68, 76, 80, 82, 90, 7, 21, 23, 39, 47, 49, 59, 61, 65, 67, 75, 0]\n",
            "TRAIN_SET CLASSES:  [75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "VALIDATION CLASSES:  [61, 32, 90, 24, 23, 76, 75, 10, 0, 64]\n",
            "GROUP:  3\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  30\n",
            "5\n",
            "Starting epoch 1/5, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.4100273847579956\n",
            "Train step - Step 10, Loss 0.22518660128116608\n",
            "Train step - Step 20, Loss 0.2127355933189392\n",
            "Train step - Step 30, Loss 0.204080268740654\n",
            "Train step - Step 40, Loss 0.20063260197639465\n",
            "Train step - Step 50, Loss 0.19907277822494507\n",
            "Train epoch - Accuracy: 0.1776978417266187 Loss: 0.22165248812531396 Corrects: 1164\n",
            "Starting epoch 2/5, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.19283218681812286\n",
            "Train step - Step 70, Loss 0.19041304290294647\n",
            "Train step - Step 80, Loss 0.1957685351371765\n",
            "Train step - Step 90, Loss 0.19302868843078613\n",
            "Train step - Step 100, Loss 0.19334523379802704\n",
            "Train epoch - Accuracy: 0.20014388489208634 Loss: 0.19164166227090274 Corrects: 1844\n",
            "Starting epoch 3/5, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.1907123327255249\n",
            "Train step - Step 120, Loss 0.18456396460533142\n",
            "Train step - Step 130, Loss 0.1883639097213745\n",
            "Train step - Step 140, Loss 0.193313866853714\n",
            "Train step - Step 150, Loss 0.18746034801006317\n",
            "Train step - Step 160, Loss 0.18368922173976898\n",
            "Train epoch - Accuracy: 0.23568345323741008 Loss: 0.18805720366162362 Corrects: 2148\n",
            "Starting epoch 4/5, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.18554319441318512\n",
            "Train step - Step 180, Loss 0.1876971274614334\n",
            "Train step - Step 190, Loss 0.18833711743354797\n",
            "Train step - Step 200, Loss 0.18500302731990814\n",
            "Train step - Step 210, Loss 0.1818116307258606\n",
            "Train epoch - Accuracy: 0.26402877697841726 Loss: 0.1862017927752982 Corrects: 2327\n",
            "Starting epoch 5/5, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.18272311985492706\n",
            "Train step - Step 230, Loss 0.18316835165023804\n",
            "Train step - Step 240, Loss 0.1806299090385437\n",
            "Train step - Step 250, Loss 0.18698008358478546\n",
            "Train step - Step 260, Loss 0.18355143070220947\n",
            "Train step - Step 270, Loss 0.18932068347930908\n",
            "Train epoch - Accuracy: 0.28949640287769784 Loss: 0.1841672743696103 Corrects: 2517\n",
            "Training finished in 32.54950284957886 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3b6250>\n",
            "Constructing exemplars of class 75\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [36676, 19729, 4358, 49346, 43454, 19926, 25325, 15616, 26802, 36510, 48685, 42320, 3022, 30564, 30295, 49346, 48685, 46180, 18908, 2672, 37294, 44017, 11393, 16112, 32982, 33100, 8446, 15984, 2571, 31392, 49868, 42096, 46180, 38077, 30895, 9825, 14355, 9778, 12573, 35128, 14259, 40843, 27180, 25608, 14236, 1625, 39461, 9869, 96, 38527, 1221, 11235, 34141, 33079, 2968, 23138, 10321, 35682, 8446, 15984, 17148, 39246, 46843, 36115, 16990, 37702]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3993d0>\n",
            "Constructing exemplars of class 23\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [901, 26570, 32865, 29128, 4436, 23635, 47380, 42614, 16680, 12494, 44638, 10659, 13380, 27392, 16151, 17152, 31426, 29111, 31830, 31679, 21890, 28100, 11755, 41940, 47077, 33888, 25497, 14482, 22597, 12213, 22703, 42455, 5463, 33316, 36855, 29128, 36377, 18542, 35170, 12213, 31878, 40130, 47216, 37707, 38385, 11418, 43328, 42483, 31329, 23575, 42959, 37869, 17475, 31129, 10461, 26110, 24790, 44940, 31167, 37792, 19910, 19732, 25073, 40523, 7018, 24898]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c399dd0>\n",
            "Constructing exemplars of class 90\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [24007, 22740, 40599, 1476, 23291, 9687, 25809, 46249, 49099, 39540, 15349, 20518, 14321, 27016, 32719, 42217, 10879, 7369, 5222, 34976, 47, 3442, 28622, 45708, 47140, 14866, 20896, 28312, 5304, 21114, 13515, 22385, 37367, 24580, 44748, 22638, 18880, 10482, 22122, 29355, 40811, 9241, 39115, 49712, 26282, 10960, 1956, 34171, 25837, 49313, 28929, 46835, 10960, 49313, 45818, 27734, 21610, 26196, 10106, 746, 41601, 3736, 10174, 791, 4581, 14866]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c35c310>\n",
            "Constructing exemplars of class 10\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [36971, 49077, 30717, 35787, 32162, 30225, 27957, 41961, 22586, 2237, 39855, 17980, 24639, 42813, 7941, 18361, 5193, 18346, 47288, 8048, 13343, 17344, 7406, 42371, 45942, 30498, 13766, 8456, 2127, 42493, 40883, 30225, 11858, 26744, 30042, 23295, 12547, 49516, 14231, 7476, 39040, 26742, 41451, 19503, 7299, 36528, 48795, 28748, 47936, 35241, 26550, 11486, 49415, 28091, 35434, 20233, 1587, 47240, 26106, 44786, 22387, 29045, 15693, 43599, 11486, 1579]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c99f350>\n",
            "Constructing exemplars of class 61\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [20826, 9804, 36734, 25017, 25187, 16330, 16273, 24139, 12562, 22836, 11591, 24156, 22651, 26660, 30244, 19790, 10268, 1552, 6864, 6869, 29425, 39329, 5120, 10392, 29475, 13844, 48512, 44155, 36796, 34818, 24139, 19121, 8709, 104, 49553, 544, 8980, 31935, 44144, 10277, 43495, 7377, 32226, 39247, 6614, 49581, 13110, 41364, 2513, 26601, 32360, 33262, 41422, 26660, 30334, 10277, 37139, 17650, 25239, 12029, 2789, 8865, 48512, 34237, 34475, 49725]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339e772310>\n",
            "Constructing exemplars of class 76\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [1475, 358, 34067, 40173, 18298, 44414, 38736, 18218, 39405, 48023, 36220, 37394, 29227, 44168, 20688, 4781, 5842, 19983, 48571, 34764, 18689, 20640, 14403, 34758, 34811, 41442, 27484, 48023, 44610, 40745, 31940, 3075, 40000, 5515, 4129, 23601, 9263, 18689, 35418, 15619, 49028, 44692, 12586, 14446, 40580, 28912, 20728, 10426, 3245, 34898, 40305, 22767, 30071, 41197, 49443, 10148, 32418, 4129, 5939, 38020, 4111, 32042, 4793, 45873, 10779, 10209]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c98d610>\n",
            "Constructing exemplars of class 64\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [14868, 41783, 44603, 15622, 14009, 17871, 3260, 6185, 46704, 26806, 18505, 173, 25533, 3220, 31797, 20255, 30513, 38436, 37690, 11190, 34409, 32467, 12769, 19318, 29860, 6366, 1373, 49819, 18354, 26611, 13657, 47076, 43917, 19458, 42025, 30209, 14271, 19439, 13835, 5917, 30209, 45275, 42840, 21703, 16397, 33172, 1376, 9474, 39317, 46759, 173, 46299, 10418, 12325, 1810, 10418, 27049, 22931, 12451, 23156, 19811, 10576, 46562, 21417, 17249, 11217]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c366a10>\n",
            "Constructing exemplars of class 32\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [38820, 40870, 16745, 39872, 24938, 26328, 29881, 9818, 26386, 1145, 3322, 42160, 25369, 42928, 9456, 26534, 12804, 29001, 12612, 23846, 48637, 25816, 48706, 22172, 20556, 12612, 13682, 7872, 25991, 27689, 549, 25067, 17680, 33534, 38454, 39950, 4306, 33263, 39019, 40185, 38754, 14494, 32960, 4048, 7455, 13648, 36306, 31411, 9886, 31879, 28059, 23088, 46813, 29195, 48183, 33187, 45661, 31175, 39382, 24837, 48706, 33705, 24010, 25369, 14405, 9133]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339d864a50>\n",
            "Constructing exemplars of class 24\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [17159, 30278, 46602, 5501, 4668, 20144, 23997, 25306, 35318, 18040, 7315, 1110, 6783, 21507, 21175, 24647, 25256, 39052, 44835, 16008, 23202, 17638, 32878, 45410, 35808, 24949, 3518, 21255, 32878, 37247, 31689, 32956, 21844, 48096, 43439, 47898, 37327, 34263, 6482, 37040, 35139, 24177, 49776, 41463, 17584, 19633, 42638, 20718, 1506, 37576, 5574, 8516, 21507, 8440, 1651, 27859, 31960, 41351, 957, 37130, 20144, 31342, 4308, 14595, 44831, 15147]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c226a50>\n",
            "Constructing exemplars of class 0\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [32233, 36817, 32128, 27576, 45171, 3780, 11048, 14513, 15085, 26702, 30731, 30172, 16996, 2640, 40715, 11314, 40607, 15105, 37056, 28648, 39949, 42631, 29405, 26247, 39303, 32826, 23780, 33773, 26819, 2126, 27350, 6851, 21781, 44364, 15085, 40598, 43575, 21870, 33950, 29582, 15701, 40486, 6241, 9236, 7155, 46274, 16353, 5239, 17727, 7234, 6851, 10037, 44429, 2936, 41259, 176, 38525, 31252, 44972, 35389, 29399, 1979, 39318, 8065, 45682, 35137]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.4 0.1220754086971283\n",
            "TEST GROUP:  0.369\n",
            "TEST ALL:  0.3416666666666667\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  4000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 95, 68, 64, 56, 42, 36, 34, 32, 30, 24, 22, 20, 18, 16, 10, 6, 4, 2, 72, 76, 80, 61, 83, 81, 79, 75, 67, 65, 63, 59, 82, 49, 47, 39, 23, 21, 7, 90, 0]\n",
            "TRAIN_SET CLASSES:  [95, 83, 63, 42, 30, 6, 2, 97, 72, 36]\n",
            "VALIDATION CLASSES:  [63, 42, 36, 97, 95, 30, 83, 72, 6, 2]\n",
            "GROUP:  4\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [95, 83, 63, 42, 30, 6, 2, 97, 72, 36]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "num classes till now:  40\n",
            "5\n",
            "Starting epoch 1/5, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.3499222695827484\n",
            "Train step - Step 10, Loss 0.2216702699661255\n",
            "Train step - Step 20, Loss 0.19988541305065155\n",
            "Train step - Step 30, Loss 0.20837430655956268\n",
            "Train step - Step 40, Loss 0.19468402862548828\n",
            "Train step - Step 50, Loss 0.19027912616729736\n",
            "Train epoch - Accuracy: 0.1645021645021645 Loss: 0.21463244940553391 Corrects: 862\n",
            "Starting epoch 2/5, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.19487030804157257\n",
            "Train step - Step 70, Loss 0.19478575885295868\n",
            "Train step - Step 80, Loss 0.19605349004268646\n",
            "Train step - Step 90, Loss 0.1974085420370102\n",
            "Train step - Step 100, Loss 0.1901780217885971\n",
            "Train epoch - Accuracy: 0.172005772005772 Loss: 0.1922725366608577 Corrects: 1148\n",
            "Starting epoch 3/5, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.18605127930641174\n",
            "Train step - Step 120, Loss 0.19892483949661255\n",
            "Train step - Step 130, Loss 0.1892576664686203\n",
            "Train step - Step 140, Loss 0.18869681656360626\n",
            "Train step - Step 150, Loss 0.1864238977432251\n",
            "Train step - Step 160, Loss 0.19479778409004211\n",
            "Train epoch - Accuracy: 0.173015873015873 Loss: 0.19051059945923254 Corrects: 1310\n",
            "Starting epoch 4/5, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.18930546939373016\n",
            "Train step - Step 180, Loss 0.18660981953144073\n",
            "Train step - Step 190, Loss 0.18613505363464355\n",
            "Train step - Step 200, Loss 0.1814662665128708\n",
            "Train step - Step 210, Loss 0.1858462244272232\n",
            "Train epoch - Accuracy: 0.1795093795093795 Loss: 0.19006205088906475 Corrects: 1416\n",
            "Starting epoch 5/5, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.1842668056488037\n",
            "Train step - Step 230, Loss 0.18505416810512543\n",
            "Train step - Step 240, Loss 0.19522976875305176\n",
            "Train step - Step 250, Loss 0.1819898933172226\n",
            "Train step - Step 260, Loss 0.18817420303821564\n",
            "Train step - Step 270, Loss 0.19249795377254486\n",
            "Train epoch - Accuracy: 0.17763347763347764 Loss: 0.1888795723044683 Corrects: 1594\n",
            "Training finished in 32.481855630874634 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0, 95, 83, 63, 42, 30, 6, 2, 97, 72, 36]\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c375f10>\n",
            "Constructing exemplars of class 95\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [45444, 13801, 35178, 46247, 43747, 44254, 16793, 46078, 24033, 47375, 33380, 31795, 29012, 11454, 43508, 5756, 94, 29681, 38855, 44573, 18052, 39789, 5657, 32897, 49681, 23225, 42567, 40371, 34057, 27842, 49256, 35440, 38447, 29194, 23873, 32805, 29648, 46276, 23030, 22169, 17231, 49508, 38952, 37946, 19111, 33847, 48605, 46247, 10805, 46111]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3664d0>\n",
            "Constructing exemplars of class 83\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [37432, 46671, 25416, 29197, 43615, 49471, 9298, 3688, 32150, 48064, 44727, 24713, 971, 21707, 41531, 21496, 1413, 19207, 23548, 42564, 24160, 36933, 38110, 25370, 49484, 43867, 10886, 41423, 22583, 48996, 11283, 38610, 29306, 34101, 48996, 21054, 13342, 27531, 41231, 49226, 21485, 27481, 49905, 14723, 29306, 27481, 46833, 23627, 15223, 21182]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339e7e1f90>\n",
            "Constructing exemplars of class 63\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [23674, 1181, 41040, 16862, 21062, 17225, 15298, 9447, 31587, 2887, 16004, 24060, 37307, 3091, 41334, 28707, 30491, 3027, 435, 25606, 8625, 16980, 19904, 48411, 40347, 10791, 2735, 12717, 18299, 5167, 30401, 15860, 38634, 36402, 46509, 5076, 6099, 49781, 26508, 11426, 30735, 47321, 35602, 21524, 4112, 6562, 4073, 28025, 38067, 39850]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339e741450>\n",
            "Constructing exemplars of class 42\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [22053, 36859, 46380, 28464, 40394, 14935, 35924, 4503, 10621, 18259, 11685, 12191, 25388, 23981, 30417, 39810, 4530, 32697, 35375, 49660, 3855, 4258, 28650, 20907, 22346, 29014, 12681, 24445, 33425, 33165, 12191, 39713, 49864, 12595, 42517, 3531, 250, 15152, 8426, 36214, 33280, 24445, 8426, 34760, 33917, 26098, 41036, 5035, 44048, 18459]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3511d0>\n",
            "Constructing exemplars of class 30\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [32574, 3959, 135, 41012, 18584, 4101, 30650, 8459, 33540, 45711, 36293, 21345, 11894, 31757, 39992, 40787, 23345, 41622, 33600, 45293, 35567, 35939, 29049, 40572, 28309, 9300, 3792, 47630, 48982, 20394, 33061, 6071, 2511, 46979, 6103, 43818, 17433, 35592, 48233, 15711, 42261, 33295, 42265, 42267, 6694, 32574, 18087, 42208, 40217, 13131]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c958f90>\n",
            "Constructing exemplars of class 6\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [27434, 31184, 17784, 38111, 34323, 27369, 47600, 25473, 17226, 19897, 34169, 40694, 15264, 22809, 12257, 714, 21415, 11982, 23470, 38520, 20433, 7856, 19195, 30004, 14373, 46365, 7096, 46016, 42130, 24253, 30398, 33064, 48240, 19313, 37621, 5535, 29127, 25257, 28017, 49107, 35216, 49856, 15269, 26366, 4739, 46065, 39127, 40740, 6226, 16601]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f33a2151090>\n",
            "Constructing exemplars of class 2\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [13261, 12762, 14499, 7802, 20415, 30742, 37797, 41119, 24149, 1804, 38345, 22121, 41309, 49862, 29307, 14048, 580, 19076, 22171, 16587, 31873, 37557, 9192, 30289, 17767, 35886, 8681, 42518, 20891, 10320, 14499, 5390, 16187, 11833, 40800, 22417, 18277, 40643, 33194, 27394, 14642, 25496, 41473, 15134, 27771, 20257, 11139, 2858, 18124, 18527]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c399f10>\n",
            "Constructing exemplars of class 97\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [13507, 28256, 15793, 22433, 14228, 15643, 44415, 20723, 16954, 8317, 11464, 3614, 16054, 19057, 22743, 40829, 20341, 43400, 11948, 40894, 27738, 16693, 1720, 15542, 46154, 13942, 16559, 18898, 8985, 5204, 19263, 48530, 48673, 6930, 31714, 29788, 15666, 34705, 16498, 10128, 45029, 41545, 45262, 12075, 39785, 46630, 44149, 22001, 32104, 36439]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3e6710>\n",
            "Constructing exemplars of class 72\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [43893, 2463, 32996, 38358, 8662, 34074, 39313, 10555, 26513, 43501, 33192, 41207, 15650, 43989, 12090, 27203, 42269, 49124, 41289, 29124, 41205, 34748, 14080, 33944, 26087, 15371, 48930, 33192, 11779, 26513, 32064, 30264, 7909, 3644, 21839, 45617, 37995, 28591, 9011, 23013, 3632, 23323, 49124, 19155, 14114, 45538, 13102, 26283, 6532, 19463]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c65f890>\n",
            "Constructing exemplars of class 36\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [5518, 45214, 30117, 31740, 32402, 24960, 34791, 13607, 34243, 24767, 31599, 7818, 10506, 835, 35932, 37215, 3985, 15161, 34600, 16292, 42753, 20989, 31784, 39190, 5223, 33183, 49552, 12063, 356, 31867, 25775, 15368, 48846, 20508, 8859, 15266, 21898, 16090, 21085, 214, 12339, 21375, 25772, 25601, 9307, 36365, 12969, 36033, 4288, 44451]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.16 0.12240724265575409\n",
            "TEST GROUP:  0.174\n",
            "TEST ALL:  0.29875\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  5000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 80, 97, 93, 85, 81, 65, 61, 49, 21, 9, 96, 76, 83, 72, 68, 64, 56, 36, 32, 24, 20, 16, 4, 2, 6, 10, 18, 79, 75, 67, 63, 59, 55, 47, 39, 31, 23, 19, 7, 98, 94, 90, 82, 54, 42, 34, 30, 22, 0]\n",
            "TRAIN_SET CLASSES:  [55, 31, 19, 98, 94, 54, 93, 85, 9, 96]\n",
            "VALIDATION CLASSES:  [55, 54, 98, 96, 31, 94, 93, 85, 19, 9]\n",
            "GROUP:  5\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [55, 31, 19, 98, 94, 54, 93, 85, 9, 96]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  50\n",
            "5\n",
            "Starting epoch 1/5, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.2794016897678375\n",
            "Train step - Step 10, Loss 0.21458066999912262\n",
            "Train step - Step 20, Loss 0.2031850516796112\n",
            "Train step - Step 30, Loss 0.19893676042556763\n",
            "Train step - Step 40, Loss 0.19937297701835632\n",
            "Train step - Step 50, Loss 0.19884933531284332\n",
            "Train epoch - Accuracy: 0.09237410071942447 Loss: 0.2110676847601966 Corrects: 731\n",
            "Starting epoch 2/5, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.19256888329982758\n",
            "Train step - Step 70, Loss 0.19802996516227722\n",
            "Train step - Step 80, Loss 0.1936100870370865\n",
            "Train step - Step 90, Loss 0.20041754841804504\n",
            "Train step - Step 100, Loss 0.19804838299751282\n",
            "Train epoch - Accuracy: 0.10201438848920863 Loss: 0.19540657784012583 Corrects: 1136\n",
            "Starting epoch 3/5, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.1958869844675064\n",
            "Train step - Step 120, Loss 0.1944384127855301\n",
            "Train step - Step 130, Loss 0.19657917320728302\n",
            "Train step - Step 140, Loss 0.19930724799633026\n",
            "Train step - Step 150, Loss 0.19343560934066772\n",
            "Train step - Step 160, Loss 0.1909271627664566\n",
            "Train epoch - Accuracy: 0.11381294964028776 Loss: 0.19398411812970964 Corrects: 1287\n",
            "Starting epoch 4/5, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.19960711896419525\n",
            "Train step - Step 180, Loss 0.19213896989822388\n",
            "Train step - Step 190, Loss 0.19631388783454895\n",
            "Train step - Step 200, Loss 0.19276291131973267\n",
            "Train step - Step 210, Loss 0.19064150750637054\n",
            "Train epoch - Accuracy: 0.1227338129496403 Loss: 0.1928003161459518 Corrects: 1477\n",
            "Starting epoch 5/5, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.18779638409614563\n",
            "Train step - Step 230, Loss 0.18812400102615356\n",
            "Train step - Step 240, Loss 0.1887567788362503\n",
            "Train step - Step 250, Loss 0.1945568025112152\n",
            "Train step - Step 260, Loss 0.19487348198890686\n",
            "Train step - Step 270, Loss 0.19303111732006073\n",
            "Train epoch - Accuracy: 0.13223021582733813 Loss: 0.19220137341416996 Corrects: 1617\n",
            "Training finished in 33.01742148399353 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0, 95, 83, 63, 42, 30, 6, 2, 97, 72, 36, 55, 31, 19, 98, 94, 54, 93, 85, 9, 96]\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c399dd0>\n",
            "Constructing exemplars of class 55\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [24446, 45731, 1905, 38765, 40120, 23475, 48683, 46844, 21760, 30568, 1544, 24096, 42574, 29596, 46872, 45582, 2993, 9693, 11539, 19309, 24018, 37251, 39217, 3052, 23798, 43894, 36424, 31734, 17697, 8060, 41597, 4544, 27388, 7394, 38832, 4383, 29490, 49779, 38423, 24096]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c375650>\n",
            "Constructing exemplars of class 31\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [47198, 11256, 14564, 30859, 20503, 26056, 16115, 9730, 39085, 15461, 29317, 48911, 32374, 14450, 38882, 48403, 8073, 19931, 4946, 11777, 21784, 42090, 32948, 31113, 30862, 593, 8068, 31845, 46812, 14777, 44187, 1826, 19078, 32318, 30013, 21061, 520, 17696, 9753, 38083]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c980950>\n",
            "Constructing exemplars of class 19\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [4283, 23503, 7821, 48061, 15104, 33564, 351, 31384, 3607, 28983, 45169, 24816, 48937, 20664, 19498, 9878, 27752, 32415, 43221, 5783, 683, 6075, 38116, 43244, 19519, 15495, 40751, 25627, 24350, 25173, 49945, 49974, 32427, 39609, 16807, 4403, 10093, 18457, 35912, 11148]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339bb07bd0>\n",
            "Constructing exemplars of class 98\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [20046, 14930, 6657, 32017, 3677, 20881, 3624, 49923, 888, 49135, 39579, 5738, 12567, 42683, 33067, 12170, 31312, 18876, 29028, 5039, 2106, 41402, 6692, 9787, 40227, 27498, 43239, 40633, 33973, 45343, 7108, 29121, 2103, 41829, 25630, 47339, 6692, 45929, 12792, 14675]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c966190>\n",
            "Constructing exemplars of class 94\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [13563, 23885, 39985, 39092, 21745, 21693, 11233, 6536, 20653, 49023, 16046, 15527, 1297, 5106, 2309, 9445, 46995, 47196, 44067, 29644, 29636, 1435, 27796, 5888, 31910, 9917, 45494, 31429, 43548, 41748, 45955, 7828, 9056, 41133, 12073, 23246, 27762, 31780, 39617, 48746]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339d8c0e10>\n",
            "Constructing exemplars of class 54\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [33810, 14713, 34847, 26599, 40826, 14184, 22238, 22496, 37779, 41410, 19730, 1104, 21722, 36591, 22573, 22588, 10893, 2642, 34910, 10385, 34648, 26899, 9239, 27603, 16372, 11851, 8433, 1295, 45211, 18311, 27243, 1577, 47082, 29281, 33653, 12468, 10229, 43546, 15914, 330]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339ca1a650>\n",
            "Constructing exemplars of class 93\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [5045, 16669, 12516, 31114, 47219, 17283, 45996, 10663, 12110, 45899, 29109, 34989, 10649, 31441, 23520, 3687, 28438, 8410, 36779, 2064, 13042, 45075, 23935, 99, 26905, 4822, 33055, 35432, 12776, 770, 32695, 35786, 27117, 14657, 31633, 29068, 2142, 49709, 17355, 10471]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339d8c0f10>\n",
            "Constructing exemplars of class 85\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [41809, 21627, 19128, 5445, 1372, 48883, 26079, 35358, 44349, 35933, 47574, 20228, 31055, 41943, 3862, 22154, 38961, 36440, 33290, 38121, 26698, 5799, 16156, 26533, 15001, 20927, 26242, 6620, 16640, 21233, 45654, 39362, 23388, 38502, 11559, 43708, 46360, 2221, 43283, 48161]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339bb07bd0>\n",
            "Constructing exemplars of class 9\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [41539, 33879, 35537, 9138, 32516, 24419, 41168, 4355, 3752, 5047, 44810, 16619, 14188, 1876, 43924, 21977, 41376, 15112, 31703, 36678, 13668, 20726, 40717, 13145, 20702, 47368, 8590, 7555, 41610, 24185, 9547, 34658, 8111, 35410, 1962, 9131, 2921, 32659, 19600, 4425]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339ca2ff90>\n",
            "Constructing exemplars of class 96\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [26096, 3681, 24249, 2145, 4177, 44424, 12292, 6839, 38465, 704, 29855, 10348, 2700, 19661, 22207, 4195, 8274, 20531, 47690, 39951, 6839, 28599, 15599, 3575, 2638, 38465, 2552, 28717, 20700, 21935, 25183, 18578, 8769, 18967, 12677, 33085, 46017, 15123, 13491, 18529]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.06 0.10472306609153748\n",
            "TEST GROUP:  0.152\n",
            "TEST ALL:  0.2636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KRvNYlfO1ICS",
        "outputId": "0227b47b-3ccc-4add-f379-05c74153e5c2"
      },
      "source": [
        "method = \"Closed world with Rejection\"\n",
        "print(\"metrics ClosedWord for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,5):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        " #writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics ClosedWord for seed 66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7Wc2tTMTlFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1c6484-fa39-4c2b-f675-aa6d6c5710ae"
      },
      "source": [
        "closed_with_rej_acc = new_accuracies\n",
        "print(closed_with_rej_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.508, 0.406, 0.3416666666666667, 0.29875, 0.2636]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pXLJaU3CtVQ"
      },
      "source": [
        "### open world with rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNW9TdvxHEl2"
      },
      "source": [
        "def sequentialLearningiCaRLOpen(train_subsets, val_subsets, open_dataset, rejection = False, closed = True):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 5\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset in zip(train_subsets, val_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      print(\"BATCH SIZE: \", BATCH_SIZE )\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      old_net = copy.deepcopy(net)\n",
        "      old_net.to(DEVICE)\n",
        "      addOutputs(net,10)\n",
        "    \n",
        "    \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen, rejection=rejection, closed=closed) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      \n",
        "      # val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      # acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen, rejection=rejection, closed=closed)\n",
        "      # print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      #if(group_id ==5):\n",
        "      \"\"\" modify batch size for the variation \"\"\"\n",
        "      #num_classes_seen=100\n",
        "      test_loader = DataLoader(open_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, 100, exemplars_set_tot, rejection, closed)\n",
        "      all_accuracies.append(acc_all)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, all_accuracies, all_preds_cm, all_labels_cm"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozKbRSusQ3am"
      },
      "source": [
        "def meanExemplars(exemplar_set, net):\n",
        "  exemplars_subset = []\n",
        "  exemplar_loader = []\n",
        "  toReturn = {}\n",
        "  for k, exemplar_set_class_k in exemplar_set.items():\n",
        "      distances = []\n",
        "      # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "      if (exemplar_set_class_k != []):\n",
        "        exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "        print(exemplars_subset.__len__())\n",
        "        if exemplars_subset.__len__()==200:\n",
        "            exemplar_loader = torch.utils.data.DataLoader(exemplars_subset, shuffle = True, batch_size=128, num_workers=2, drop_last=True)\n",
        "        else:\n",
        "            exemplar_loader = torch.utils.data.DataLoader(exemplars_subset, shuffle = True, batch_size=exemplars_subset.__len__(), num_workers=2)\n",
        "            \n",
        "        for _, exemplars, labels  in exemplar_loader:\n",
        "          exemplars = exemplars.to(DEVICE)  \n",
        "          feature_exemplar = net.forward(exemplars).detach().cpu().numpy()\n",
        "          del exemplars #delete unnecessary variables \n",
        "          gc.collect()  \n",
        "          #print(feature_exemplar.shape) #len_exemplar_subset * 64\n",
        "          mean_feature_exemplar = feature_exemplar.mean(axis=0) # expected shape -> 1*64\n",
        "          #print(mean_feature_exemplar)\n",
        "          del feature_exemplar\n",
        "          gc.collect()\n",
        "          toReturn[k] = mean_feature_exemplar\n",
        "          print(len(toReturn))\n",
        "  return toReturn      "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tY3UJjmjPji"
      },
      "source": [
        "import gc\n",
        "from scipy.special import softmax\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes, exemplar_set, rejection = False, closed = False):\n",
        "    #counter for rejection part, known and unknown\n",
        "    rejection = True\n",
        "    if rejection==True:\n",
        "      n_sample_known = 0\n",
        "      n_sample_unknown = 0\n",
        "    net.eval()\n",
        "    exemplars_subset = []\n",
        "    exemplar_loader = [] \n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "    print(len(val_dataloader))\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    print(exemplar_set)\n",
        "    #mean of feature exemplars for each class of this iteration, is a dict class-> mean of features (1,64)\n",
        "    mean_exemplars = meanExemplars(exemplar_set, net)\n",
        "\n",
        "    for _, image, labels in val_dataloader:\n",
        "        #print('ciclo immagini')\n",
        "        # Bring images and labels to GPU\n",
        "        image = image.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(image)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * image.size(0)\n",
        "            \n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        if rejection == True:\n",
        "          #passare exemplar_set_tot\n",
        "          reject = True\n",
        "          feature = net.forward(image)\n",
        "          # 128 * 64\n",
        "          #print(feature.shape)\n",
        "          mean_image = []\n",
        "          cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "          \n",
        "          distances = []\n",
        "          dist = 0\n",
        "          \n",
        "          for k, mean in mean_exemplars.items(): #k-> class\n",
        "            adjusted_mean = torch.tensor((np.ones(shape = (len(labels),1))*mean)).to(DEVICE)\n",
        "            output = (cos(adjusted_mean, feature)) # ex1 128distanze \n",
        "            output = output.detach().cpu().numpy() #128x1\n",
        "            distances.append(output) # 128xnum_classes\n",
        "          #print('DISTANZE: ', distances) # 128xnum_classes, distances of every image from each class\n",
        "            # 128 * num_classes\n",
        "          normalized_vector = (distances- np.mean(distances, axis = 0)) / np.std(distances, axis = 0)\n",
        "\n",
        "          #softmax the normalized vector then take the maximum for each row, masking to true or false with the threshold\n",
        "          #if the sum of the vector is > 0 -> at least one value above 0.5 so image is known\n",
        "\n",
        "          flag_known = (softmax(normalized_vector, axis=1).max(axis=1)>0.5).sum()\n",
        "\n",
        "          if flag_known>0:\n",
        "            n_sample_known += 1\n",
        "          else:\n",
        "            n_sample_unknown += 1\n",
        "                    \n",
        "          #for values in normalized_vector:\n",
        "          #  for value in values:\n",
        "          #    if (value < 0.5):\n",
        "          #      reject = False\n",
        "          #  if (reject == False):\n",
        "          #    n_sample_known += 1\n",
        "          #  else:\n",
        "          #    n_sample_unknown += 1\n",
        "        # if rejection == True:\n",
        "        #   prediction_batch = outputs.data.cpu().numpy()\n",
        "        #   print(\"OUTPUTS shape: \", outputs.data.shape)\n",
        "        #   print(\"LEN PRED BATCH:\", len(prediction_batch))\n",
        "        #   for i in range(len(prediction_batch)):\n",
        "        #     current_softmax = softmax(prediction_batch[i])\n",
        "        #     #print(max(current_softmax))\n",
        "        #     if max(current_softmax)>THRESHOLD:\n",
        "        #       n_sample_known += 1\n",
        "        #     else:\n",
        "        #       n_sample_unknown += 1\n",
        "        else:\n",
        "          #_, preds = classify(images, )\n",
        "          running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "          all_preds_cm.extend(preds.tolist())\n",
        "          all_labels_cm.extend(labels.data.tolist())\n",
        "          \n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    if rejection == True:\n",
        "      if closed == True:\n",
        "        acc = n_sample_known / float(len(val_dataloader.dataset))\n",
        "      else:\n",
        "        acc = n_sample_unknown / float(len(val_dataloader.dataset))\n",
        "    else:\n",
        "      acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes, exemplar_set, rejection = False, closed = True):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes, exemplar_set, rejection, closed)\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes, exemplar_set, rejection, closed)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs9uww-HEu08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6537e30-fe9f-484e-ccf7-e922e9958443"
      },
      "source": [
        "#open with rejection\n",
        "rejection = True\n",
        "closed = False\n",
        "# train\n",
        "net, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRLOpen(train_subsets, val_subsets, open_test, rejection=rejection, closed=closed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BATCH SIZE:  128\n",
            "TRAIN:  4950\n",
            "TRAIN_SET CLASSES:  [27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "VALIDATION CLASSES:  [57, 50, 97, 30, 27, 25, 86, 82, 78, 69]\n",
            "GROUP:  1\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "Len TOTAL train susbset:  4950\n",
            "training\n",
            "num classes till now:  10\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.7111218571662903\n",
            "Train step - Step 10, Loss 0.30308443307876587\n",
            "Train step - Step 20, Loss 0.266156405210495\n",
            "Train step - Step 30, Loss 0.2765621244907379\n",
            "Train epoch - Accuracy: 0.8157575757575758 Loss: 0.3418791624151095 Corrects: 4038\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.23264704644680023\n",
            "Train step - Step 50, Loss 0.22463813424110413\n",
            "Train step - Step 60, Loss 0.2416602373123169\n",
            "Train step - Step 70, Loss 0.21246352791786194\n",
            "Train epoch - Accuracy: 0.622020202020202 Loss: 0.22779875755912127 Corrects: 3079\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.21790848672389984\n",
            "Train step - Step 90, Loss 0.2081083506345749\n",
            "Train step - Step 100, Loss 0.23186750710010529\n",
            "Train step - Step 110, Loss 0.1828516721725464\n",
            "Train epoch - Accuracy: 0.5054545454545455 Loss: 0.21158632369956584 Corrects: 2502\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.18253342807292938\n",
            "Train step - Step 130, Loss 0.22105717658996582\n",
            "Train step - Step 140, Loss 0.19153380393981934\n",
            "Train step - Step 150, Loss 0.2136433869600296\n",
            "Train epoch - Accuracy: 0.4090909090909091 Loss: 0.20011203567186991 Corrects: 2025\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.18625995516777039\n",
            "Train step - Step 170, Loss 0.19838541746139526\n",
            "Train step - Step 180, Loss 0.19134727120399475\n",
            "Train step - Step 190, Loss 0.20018577575683594\n",
            "Train epoch - Accuracy: 0.34585858585858587 Loss: 0.1877120785821568 Corrects: 1712\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.17993390560150146\n",
            "Train step - Step 210, Loss 0.16228388249874115\n",
            "Train step - Step 220, Loss 0.1856672763824463\n",
            "Train step - Step 230, Loss 0.17254744470119476\n",
            "Train epoch - Accuracy: 0.31717171717171716 Loss: 0.1813610198702475 Corrects: 1570\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.18411584198474884\n",
            "Train step - Step 250, Loss 0.16557039320468903\n",
            "Train step - Step 260, Loss 0.2015596181154251\n",
            "Train step - Step 270, Loss 0.16834445297718048\n",
            "Train epoch - Accuracy: 0.28606060606060607 Loss: 0.17695897859154325 Corrects: 1416\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.15476933121681213\n",
            "Train step - Step 290, Loss 0.15770640969276428\n",
            "Train step - Step 300, Loss 0.18141722679138184\n",
            "Train step - Step 310, Loss 0.17450405657291412\n",
            "Train epoch - Accuracy: 0.26484848484848483 Loss: 0.16873591256864143 Corrects: 1311\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.14376235008239746\n",
            "Train step - Step 330, Loss 0.17711520195007324\n",
            "Train step - Step 340, Loss 0.1769685000181198\n",
            "Train step - Step 350, Loss 0.1629573553800583\n",
            "Train epoch - Accuracy: 0.23959595959595958 Loss: 0.16509068783485528 Corrects: 1186\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.15173165500164032\n",
            "Train step - Step 370, Loss 0.1490267962217331\n",
            "Train step - Step 380, Loss 0.15734420716762543\n",
            "Train epoch - Accuracy: 0.2317171717171717 Loss: 0.16230629682540892 Corrects: 1147\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.16686956584453583\n",
            "Train step - Step 400, Loss 0.1567101925611496\n",
            "Train step - Step 410, Loss 0.14220145344734192\n",
            "Train step - Step 420, Loss 0.15144141018390656\n",
            "Train epoch - Accuracy: 0.20242424242424242 Loss: 0.1559239889455564 Corrects: 1002\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.15069584548473358\n",
            "Train step - Step 440, Loss 0.1439037173986435\n",
            "Train step - Step 450, Loss 0.16016918420791626\n",
            "Train step - Step 460, Loss 0.1548796147108078\n",
            "Train epoch - Accuracy: 0.20606060606060606 Loss: 0.15426497613540804 Corrects: 1020\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.14455971121788025\n",
            "Train step - Step 480, Loss 0.15293383598327637\n",
            "Train step - Step 490, Loss 0.13923592865467072\n",
            "Train step - Step 500, Loss 0.13536374270915985\n",
            "Train epoch - Accuracy: 0.1995959595959596 Loss: 0.15090185079309676 Corrects: 988\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.13014747202396393\n",
            "Train step - Step 520, Loss 0.14289581775665283\n",
            "Train step - Step 530, Loss 0.15440569818019867\n",
            "Train step - Step 540, Loss 0.13629260659217834\n",
            "Train epoch - Accuracy: 0.17373737373737375 Loss: 0.1475987347448715 Corrects: 860\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.1553954929113388\n",
            "Train step - Step 560, Loss 0.12488045543432236\n",
            "Train step - Step 570, Loss 0.133747860789299\n",
            "Train step - Step 580, Loss 0.1336342990398407\n",
            "Train epoch - Accuracy: 0.1703030303030303 Loss: 0.1416055672457724 Corrects: 843\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.13207048177719116\n",
            "Train step - Step 600, Loss 0.13067810237407684\n",
            "Train step - Step 610, Loss 0.1466415971517563\n",
            "Train step - Step 620, Loss 0.151137575507164\n",
            "Train epoch - Accuracy: 0.16262626262626262 Loss: 0.14052111058825195 Corrects: 805\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.11896049231290817\n",
            "Train step - Step 640, Loss 0.12630172073841095\n",
            "Train step - Step 650, Loss 0.11406543105840683\n",
            "Train step - Step 660, Loss 0.16540835797786713\n",
            "Train epoch - Accuracy: 0.14767676767676768 Loss: 0.1336015858072223 Corrects: 731\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.13885080814361572\n",
            "Train step - Step 680, Loss 0.121560238301754\n",
            "Train step - Step 690, Loss 0.13979244232177734\n",
            "Train step - Step 700, Loss 0.13245265185832977\n",
            "Train epoch - Accuracy: 0.14181818181818182 Loss: 0.13023225921573062 Corrects: 702\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.1578052043914795\n",
            "Train step - Step 720, Loss 0.1394347846508026\n",
            "Train step - Step 730, Loss 0.11216318607330322\n",
            "Train step - Step 740, Loss 0.1481819897890091\n",
            "Train epoch - Accuracy: 0.13292929292929292 Loss: 0.12984218071807516 Corrects: 658\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.13477136194705963\n",
            "Train step - Step 760, Loss 0.12091340869665146\n",
            "Train step - Step 770, Loss 0.1390010416507721\n",
            "Train epoch - Accuracy: 0.12606060606060607 Loss: 0.1258274978489587 Corrects: 624\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.12832234799861908\n",
            "Train step - Step 790, Loss 0.10431993007659912\n",
            "Train step - Step 800, Loss 0.12065203487873077\n",
            "Train step - Step 810, Loss 0.10989052057266235\n",
            "Train epoch - Accuracy: 0.1193939393939394 Loss: 0.12161322617169583 Corrects: 591\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.11051644384860992\n",
            "Train step - Step 830, Loss 0.12528645992279053\n",
            "Train step - Step 840, Loss 0.14655017852783203\n",
            "Train step - Step 850, Loss 0.11720982939004898\n",
            "Train epoch - Accuracy: 0.12585858585858586 Loss: 0.11993179687044837 Corrects: 623\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.12167368084192276\n",
            "Train step - Step 870, Loss 0.0898321270942688\n",
            "Train step - Step 880, Loss 0.11803865432739258\n",
            "Train step - Step 890, Loss 0.10404720157384872\n",
            "Train epoch - Accuracy: 0.11434343434343434 Loss: 0.11430383147314341 Corrects: 566\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.09493260830640793\n",
            "Train step - Step 910, Loss 0.11083871126174927\n",
            "Train step - Step 920, Loss 0.10692522674798965\n",
            "Train step - Step 930, Loss 0.1253904551267624\n",
            "Train epoch - Accuracy: 0.09696969696969697 Loss: 0.1102990542939215 Corrects: 480\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.12031783908605576\n",
            "Train step - Step 950, Loss 0.10362571477890015\n",
            "Train step - Step 960, Loss 0.11221547424793243\n",
            "Train step - Step 970, Loss 0.11637385189533234\n",
            "Train epoch - Accuracy: 0.09474747474747475 Loss: 0.11209818455908034 Corrects: 469\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.09549291431903839\n",
            "Train step - Step 990, Loss 0.10470404475927353\n",
            "Train step - Step 1000, Loss 0.09896420687437057\n",
            "Train step - Step 1010, Loss 0.1007784754037857\n",
            "Train epoch - Accuracy: 0.09575757575757576 Loss: 0.10683483914293423 Corrects: 474\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.13822093605995178\n",
            "Train step - Step 1030, Loss 0.0983082577586174\n",
            "Train step - Step 1040, Loss 0.10623736679553986\n",
            "Train step - Step 1050, Loss 0.12365235388278961\n",
            "Train epoch - Accuracy: 0.09171717171717171 Loss: 0.1070900965459419 Corrects: 454\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.1309293508529663\n",
            "Train step - Step 1070, Loss 0.08694618195295334\n",
            "Train step - Step 1080, Loss 0.08853249996900558\n",
            "Train step - Step 1090, Loss 0.08474042266607285\n",
            "Train epoch - Accuracy: 0.08181818181818182 Loss: 0.10452928579816914 Corrects: 405\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.11269445717334747\n",
            "Train step - Step 1110, Loss 0.09698691964149475\n",
            "Train step - Step 1120, Loss 0.12159281969070435\n",
            "Train step - Step 1130, Loss 0.1396041363477707\n",
            "Train epoch - Accuracy: 0.08747474747474747 Loss: 0.10353136544877832 Corrects: 433\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.08463207632303238\n",
            "Train step - Step 1150, Loss 0.09601782262325287\n",
            "Train step - Step 1160, Loss 0.07890798151493073\n",
            "Train epoch - Accuracy: 0.08222222222222222 Loss: 0.09676017726009542 Corrects: 407\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.08685848861932755\n",
            "Train step - Step 1180, Loss 0.07171645015478134\n",
            "Train step - Step 1190, Loss 0.09825693815946579\n",
            "Train step - Step 1200, Loss 0.09194036573171616\n",
            "Train epoch - Accuracy: 0.07313131313131313 Loss: 0.09580471374771812 Corrects: 362\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.08210726082324982\n",
            "Train step - Step 1220, Loss 0.07623559236526489\n",
            "Train step - Step 1230, Loss 0.09898579120635986\n",
            "Train step - Step 1240, Loss 0.09057524055242538\n",
            "Train epoch - Accuracy: 0.0698989898989899 Loss: 0.09165003150099456 Corrects: 346\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.10209272056818008\n",
            "Train step - Step 1260, Loss 0.12113485485315323\n",
            "Train step - Step 1270, Loss 0.0795825719833374\n",
            "Train step - Step 1280, Loss 0.08878497034311295\n",
            "Train epoch - Accuracy: 0.06686868686868687 Loss: 0.09164325156898209 Corrects: 331\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.0837605744600296\n",
            "Train step - Step 1300, Loss 0.11426950991153717\n",
            "Train step - Step 1310, Loss 0.09178286045789719\n",
            "Train step - Step 1320, Loss 0.10974805802106857\n",
            "Train epoch - Accuracy: 0.06868686868686869 Loss: 0.08992872019909849 Corrects: 340\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.0846419483423233\n",
            "Train step - Step 1340, Loss 0.08027567714452744\n",
            "Train step - Step 1350, Loss 0.11359138786792755\n",
            "Train step - Step 1360, Loss 0.09257323294878006\n",
            "Train epoch - Accuracy: 0.06767676767676768 Loss: 0.08979688613402723 Corrects: 335\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.09014938026666641\n",
            "Train step - Step 1380, Loss 0.08513617515563965\n",
            "Train step - Step 1390, Loss 0.07795693725347519\n",
            "Train step - Step 1400, Loss 0.10511567443609238\n",
            "Train epoch - Accuracy: 0.06202020202020202 Loss: 0.08762655363540457 Corrects: 307\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.08349964767694473\n",
            "Train step - Step 1420, Loss 0.09506922215223312\n",
            "Train step - Step 1430, Loss 0.10718033462762833\n",
            "Train step - Step 1440, Loss 0.09598320722579956\n",
            "Train epoch - Accuracy: 0.05717171717171717 Loss: 0.08622469441457228 Corrects: 283\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.08147665113210678\n",
            "Train step - Step 1460, Loss 0.08023658394813538\n",
            "Train step - Step 1470, Loss 0.06925995647907257\n",
            "Train step - Step 1480, Loss 0.08873754739761353\n",
            "Train epoch - Accuracy: 0.06121212121212121 Loss: 0.08309022401017373 Corrects: 303\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.08125441521406174\n",
            "Train step - Step 1500, Loss 0.08311255276203156\n",
            "Train step - Step 1510, Loss 0.07458598166704178\n",
            "Train step - Step 1520, Loss 0.07649874687194824\n",
            "Train epoch - Accuracy: 0.05070707070707071 Loss: 0.08005785171431724 Corrects: 251\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.0866837128996849\n",
            "Train step - Step 1540, Loss 0.069914311170578\n",
            "Train step - Step 1550, Loss 0.06761829555034637\n",
            "Train epoch - Accuracy: 0.05171717171717172 Loss: 0.0776291528825808 Corrects: 256\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.08570021390914917\n",
            "Train step - Step 1570, Loss 0.09048326313495636\n",
            "Train step - Step 1580, Loss 0.08772635459899902\n",
            "Train step - Step 1590, Loss 0.10318255424499512\n",
            "Train epoch - Accuracy: 0.04909090909090909 Loss: 0.07670963418905181 Corrects: 243\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.08563423156738281\n",
            "Train step - Step 1610, Loss 0.07219727337360382\n",
            "Train step - Step 1620, Loss 0.08571908622980118\n",
            "Train step - Step 1630, Loss 0.0921681746840477\n",
            "Train epoch - Accuracy: 0.05070707070707071 Loss: 0.07488223052687115 Corrects: 251\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.08828479051589966\n",
            "Train step - Step 1650, Loss 0.07327351719141006\n",
            "Train step - Step 1660, Loss 0.07296371459960938\n",
            "Train step - Step 1670, Loss 0.08573281019926071\n",
            "Train epoch - Accuracy: 0.043838383838383836 Loss: 0.07548377415447524 Corrects: 217\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.08700644224882126\n",
            "Train step - Step 1690, Loss 0.06861942261457443\n",
            "Train step - Step 1700, Loss 0.07272310554981232\n",
            "Train step - Step 1710, Loss 0.07220564037561417\n",
            "Train epoch - Accuracy: 0.04262626262626262 Loss: 0.07269724691907564 Corrects: 211\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.061955928802490234\n",
            "Train step - Step 1730, Loss 0.07874015718698502\n",
            "Train step - Step 1740, Loss 0.0662943571805954\n",
            "Train step - Step 1750, Loss 0.05656782537698746\n",
            "Train epoch - Accuracy: 0.04 Loss: 0.06889764013314488 Corrects: 198\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.06286758929491043\n",
            "Train step - Step 1770, Loss 0.07949938625097275\n",
            "Train step - Step 1780, Loss 0.06679044663906097\n",
            "Train step - Step 1790, Loss 0.06910683214664459\n",
            "Train epoch - Accuracy: 0.042222222222222223 Loss: 0.06751833990065738 Corrects: 209\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.05577157065272331\n",
            "Train step - Step 1810, Loss 0.053501784801483154\n",
            "Train step - Step 1820, Loss 0.06808509677648544\n",
            "Train step - Step 1830, Loss 0.08172442764043808\n",
            "Train epoch - Accuracy: 0.04181818181818182 Loss: 0.06582540107495857 Corrects: 207\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.05276642367243767\n",
            "Train step - Step 1850, Loss 0.052858173847198486\n",
            "Train step - Step 1860, Loss 0.03976337984204292\n",
            "Train step - Step 1870, Loss 0.05863776430487633\n",
            "Train epoch - Accuracy: 0.034545454545454546 Loss: 0.0627306468649344 Corrects: 171\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.05505865439772606\n",
            "Train step - Step 1890, Loss 0.08164333552122116\n",
            "Train step - Step 1900, Loss 0.06365897506475449\n",
            "Train step - Step 1910, Loss 0.05738804489374161\n",
            "Train epoch - Accuracy: 0.034545454545454546 Loss: 0.06341616423142077 Corrects: 171\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.04831346496939659\n",
            "Train step - Step 1930, Loss 0.04805151745676994\n",
            "Train step - Step 1940, Loss 0.0474710538983345\n",
            "Train epoch - Accuracy: 0.029494949494949494 Loss: 0.053140101715019254 Corrects: 146\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.03561020269989967\n",
            "Train step - Step 1960, Loss 0.05712373927235603\n",
            "Train step - Step 1970, Loss 0.03668077662587166\n",
            "Train step - Step 1980, Loss 0.04247212037444115\n",
            "Train epoch - Accuracy: 0.025656565656565655 Loss: 0.044747790104210976 Corrects: 127\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.0403703898191452\n",
            "Train step - Step 2000, Loss 0.052721764892339706\n",
            "Train step - Step 2010, Loss 0.042149461805820465\n",
            "Train step - Step 2020, Loss 0.03336726501584053\n",
            "Train epoch - Accuracy: 0.022222222222222223 Loss: 0.04363369456896878 Corrects: 110\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.02880246751010418\n",
            "Train step - Step 2040, Loss 0.055258799344301224\n",
            "Train step - Step 2050, Loss 0.05898047611117363\n",
            "Train step - Step 2060, Loss 0.03857724741101265\n",
            "Train epoch - Accuracy: 0.02707070707070707 Loss: 0.04282668117772449 Corrects: 134\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.046335749328136444\n",
            "Train step - Step 2080, Loss 0.045629438012838364\n",
            "Train step - Step 2090, Loss 0.040260739624500275\n",
            "Train step - Step 2100, Loss 0.030834008008241653\n",
            "Train epoch - Accuracy: 0.024242424242424242 Loss: 0.03995149246219433 Corrects: 120\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.03952336311340332\n",
            "Train step - Step 2120, Loss 0.04652489721775055\n",
            "Train step - Step 2130, Loss 0.03851480036973953\n",
            "Train step - Step 2140, Loss 0.041316207498311996\n",
            "Train epoch - Accuracy: 0.022424242424242423 Loss: 0.039088586275324674 Corrects: 111\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.03619200736284256\n",
            "Train step - Step 2160, Loss 0.046747803688049316\n",
            "Train step - Step 2170, Loss 0.03893556818366051\n",
            "Train step - Step 2180, Loss 0.05622583627700806\n",
            "Train epoch - Accuracy: 0.02404040404040404 Loss: 0.03961987611621317 Corrects: 119\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.0378413125872612\n",
            "Train step - Step 2200, Loss 0.04244748130440712\n",
            "Train step - Step 2210, Loss 0.0482332706451416\n",
            "Train step - Step 2220, Loss 0.033823270350694656\n",
            "Train epoch - Accuracy: 0.018585858585858588 Loss: 0.03878542716304461 Corrects: 92\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.03769512474536896\n",
            "Train step - Step 2240, Loss 0.04020256549119949\n",
            "Train step - Step 2250, Loss 0.050671495497226715\n",
            "Train step - Step 2260, Loss 0.03280722722411156\n",
            "Train epoch - Accuracy: 0.018585858585858588 Loss: 0.03785272124590296 Corrects: 92\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.04522083327174187\n",
            "Train step - Step 2280, Loss 0.04787179455161095\n",
            "Train step - Step 2290, Loss 0.03286616876721382\n",
            "Train step - Step 2300, Loss 0.041148073971271515\n",
            "Train epoch - Accuracy: 0.023232323232323233 Loss: 0.037872337316623844 Corrects: 115\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.04697859287261963\n",
            "Train step - Step 2320, Loss 0.04236651584506035\n",
            "Train step - Step 2330, Loss 0.045150045305490494\n",
            "Train epoch - Accuracy: 0.02101010101010101 Loss: 0.03988005496486269 Corrects: 104\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.028304848819971085\n",
            "Train step - Step 2350, Loss 0.036558639258146286\n",
            "Train step - Step 2360, Loss 0.03360358625650406\n",
            "Train step - Step 2370, Loss 0.024113459512591362\n",
            "Train epoch - Accuracy: 0.018585858585858588 Loss: 0.03660254046621949 Corrects: 92\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.03783820942044258\n",
            "Train step - Step 2390, Loss 0.03256341814994812\n",
            "Train step - Step 2400, Loss 0.023572128266096115\n",
            "Train step - Step 2410, Loss 0.05059540271759033\n",
            "Train epoch - Accuracy: 0.018383838383838384 Loss: 0.03516063909488495 Corrects: 91\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.03469524532556534\n",
            "Train step - Step 2430, Loss 0.028165245428681374\n",
            "Train step - Step 2440, Loss 0.047896064817905426\n",
            "Train step - Step 2450, Loss 0.056571055203676224\n",
            "Train epoch - Accuracy: 0.022222222222222223 Loss: 0.03601668080296179 Corrects: 110\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.034324128180742264\n",
            "Train step - Step 2470, Loss 0.029596520587801933\n",
            "Train step - Step 2480, Loss 0.03344181925058365\n",
            "Train step - Step 2490, Loss 0.04159674048423767\n",
            "Train epoch - Accuracy: 0.020404040404040404 Loss: 0.03361943596842313 Corrects: 101\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.02984570898115635\n",
            "Train step - Step 2510, Loss 0.028389258310198784\n",
            "Train step - Step 2520, Loss 0.04855760931968689\n",
            "Train step - Step 2530, Loss 0.029561007395386696\n",
            "Train epoch - Accuracy: 0.01898989898989899 Loss: 0.033312870580138584 Corrects: 94\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.03268468752503395\n",
            "Train step - Step 2550, Loss 0.05768197774887085\n",
            "Train step - Step 2560, Loss 0.03525526821613312\n",
            "Train step - Step 2570, Loss 0.04110041260719299\n",
            "Train epoch - Accuracy: 0.01575757575757576 Loss: 0.03329336162092108 Corrects: 78\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.0222603939473629\n",
            "Train step - Step 2590, Loss 0.030515477061271667\n",
            "Train step - Step 2600, Loss 0.024312058463692665\n",
            "Train step - Step 2610, Loss 0.03643178567290306\n",
            "Train epoch - Accuracy: 0.016767676767676768 Loss: 0.0323100893846666 Corrects: 83\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.02757253684103489\n",
            "Train step - Step 2630, Loss 0.041571736335754395\n",
            "Train step - Step 2640, Loss 0.03412983939051628\n",
            "Train step - Step 2650, Loss 0.028775811195373535\n",
            "Train epoch - Accuracy: 0.015151515151515152 Loss: 0.03162179574821935 Corrects: 75\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.029759762808680534\n",
            "Train step - Step 2670, Loss 0.031036121770739555\n",
            "Train step - Step 2680, Loss 0.03218599781394005\n",
            "Train step - Step 2690, Loss 0.029619837179780006\n",
            "Train epoch - Accuracy: 0.01292929292929293 Loss: 0.03129489203820927 Corrects: 64\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.024483928456902504\n",
            "Train step - Step 2710, Loss 0.04694381356239319\n",
            "Train step - Step 2720, Loss 0.03264578431844711\n",
            "Train epoch - Accuracy: 0.018383838383838384 Loss: 0.03106349389059375 Corrects: 91\n",
            "Training finished in 209.74472093582153 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25]\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff85b4ee510>\n",
            "Constructing exemplars of class 27\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [20952, 153, 37760, 21305, 34276, 5396, 34577, 778, 47901, 35233, 14891, 42200, 29181, 32806, 1285, 31719, 1538, 28338, 47985, 42910, 21589, 391, 678, 48388, 40216, 19682, 44683, 21583, 25620, 22112, 35253, 29158, 46544, 13787, 17559, 17728, 5253, 19478, 5770, 23799, 8163, 48445, 23799, 20987, 43902, 5434, 11025, 8143, 12953, 9823, 24841, 4296, 37761, 25281, 10410, 49948, 34459, 7593, 9467, 33141, 32474, 1631, 45634, 12553, 5434, 13966, 34887, 30800, 39919, 17530, 42764, 48733, 23132, 778, 34577, 47901, 30848, 29890, 36963, 47928, 45895, 32668, 39659, 12072, 9823, 2526, 20952, 33013, 15502, 15181, 18362, 11177, 10865, 44683, 27286, 9055, 30848, 17566, 39439, 662, 10814, 42220, 35233, 26015, 29890, 28338, 41738, 33141, 19858, 44571, 16566, 3240, 37761, 10814, 25590, 17655, 35192, 11184, 7215, 17695, 41711, 13843, 35362, 44954, 49398, 29158, 40513, 12953, 43133, 12620, 33447, 40767, 7759, 36691, 3235, 30751, 49853, 27513, 32806, 13874, 9085, 26815, 21408, 47144, 15066, 10814, 3094, 48091, 38348, 9085, 12620, 28338, 28024, 46338, 18187, 18806, 29890, 39265, 41505, 45620, 5770, 26015, 23356, 4790, 23508, 42893, 49851, 36691, 48331, 17566, 1041, 36702, 5972, 25590, 20768, 40767, 2340, 33478, 26442, 41262, 45620, 6296, 13165, 19771, 41505, 18362, 34887, 20289, 26590, 2810, 1771, 6296, 23125, 35192, 23408, 42434, 16632, 26315, 11423, 16747]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9eace50>\n",
            "Constructing exemplars of class 86\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [8802, 45861, 7091, 9550, 32608, 6649, 9567, 3895, 48900, 8754, 4512, 40334, 32608, 18107, 37014, 1186, 22718, 42707, 32789, 46169, 19098, 32929, 26097, 11723, 32665, 6546, 496, 46275, 33101, 5967, 28822, 47493, 46967, 14170, 222, 24906, 45536, 35231, 47494, 18310, 40526, 44778, 32129, 49171, 42290, 4246, 2235, 7608, 25310, 21679, 17240, 46275, 8086, 36135, 19219, 19443, 20778, 22949, 29985, 16344, 29635, 222, 10915, 26719, 34586, 15866, 28302, 5767, 5595, 45686, 22033, 24935, 38753, 22365, 30806, 9776, 44794, 9113, 20778, 27070, 40886, 36065, 33812, 35231, 10299, 1244, 5595, 48900, 34195, 20778, 22721, 14170, 43529, 21691, 13053, 38800, 3795, 18567, 6546, 10837, 8149, 8303, 11537, 21600, 1485, 32219, 36135, 43874, 23394, 46826, 40492, 41507, 22327, 31538, 12066, 23586, 49574, 1312, 16995, 38734, 26748, 41756, 49008, 30674, 22982, 45299, 26989, 48253, 17533, 44356, 37044, 6638, 22721, 40573, 36135, 41941, 6546, 24906, 15672, 42218, 35599, 28822, 5967, 2002, 23496, 5767, 24367, 48810, 45306, 47883, 15721, 4374, 45263, 16995, 9980, 23108, 32129, 49880, 15479, 22721, 34153, 22240, 41129, 11747, 29985, 16344, 15501, 25436, 24875, 46659, 45263, 19443, 45871, 43529, 23034, 24004, 5608, 16656, 43711, 39129, 22982, 12755, 1083, 10915, 21404, 9985, 22689, 22317, 47045, 34591, 22982, 5270, 33812, 33095, 29213, 32789, 23108, 26401, 20852, 7646]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7ea3fd7d0>\n",
            "Constructing exemplars of class 82\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [15939, 5454, 1402, 30049, 26976, 36327, 43606, 24921, 10699, 6195, 6570, 32115, 40656, 3555, 21695, 22190, 29103, 20263, 9512, 46394, 43866, 41547, 9637, 44523, 8099, 26471, 32106, 30450, 37510, 21973, 15317, 871, 3507, 20923, 26562, 41221, 45656, 33931, 47306, 5656, 7959, 12408, 25771, 18702, 8253, 10283, 5586, 47534, 32115, 39114, 16897, 5275, 7671, 8518, 26965, 7334, 29478, 25724, 33477, 22970, 11832, 8291, 24715, 28893, 32602, 41968, 40655, 39822, 7415, 14832, 43765, 11600, 3842, 47306, 18702, 14041, 32277, 274, 27035, 11501, 47319, 11084, 44928, 42485, 26501, 6277, 45064, 4381, 46832, 43293, 24024, 35967, 6227, 24852, 13546, 8518, 43293, 10699, 33477, 35336, 41547, 24714, 36437, 22190, 39637, 27707, 3842, 34088, 8727, 13458, 33931, 318, 22110, 14992, 11832, 37521, 41900, 14131, 36194, 33709, 42541, 43453, 9111, 28404, 3842, 32449, 30102, 31775, 8257, 32725, 28101, 23402, 22192, 38486, 24387, 34088, 11084, 625, 44718, 11901, 24024, 18646, 40462, 26852, 26562, 36492, 19830, 26712, 26921, 39228, 42250, 12171, 45373, 43453, 30468, 32847, 25997, 32115, 18644, 12866, 33729, 45757, 1143, 22457, 5586, 49708, 34248, 32602, 35285, 28101, 46832, 4269, 46550, 12380, 3842, 32449, 31443, 29348, 41672, 14557, 46825, 48289, 43830, 16393, 22457, 47306, 23681, 41672, 14557, 40082, 36192, 34911, 26553, 44513, 40656, 27487, 16784, 49368, 16897, 38738]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9ecad90>\n",
            "Constructing exemplars of class 78\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [25903, 7374, 18091, 17292, 15438, 45048, 20522, 15648, 19489, 26668, 2842, 23442, 8598, 8764, 32972, 6645, 30672, 9510, 5912, 7634, 19381, 34349, 39018, 16955, 47100, 19381, 9568, 39058, 29518, 22052, 24843, 19643, 45415, 19985, 45048, 42636, 34803, 25522, 769, 10771, 1135, 21928, 46816, 3626, 47918, 16179, 6907, 20551, 19034, 37054, 15718, 24549, 42663, 29043, 48865, 35594, 20474, 33965, 18321, 43630, 38339, 45606, 14886, 43064, 1619, 25489, 28445, 39058, 44962, 14625, 3032, 10582, 5500, 47100, 1037, 49946, 4105, 26668, 12747, 24329, 12879, 33082, 37110, 5912, 24843, 13832, 18075, 1619, 36435, 49541, 29736, 33965, 8527, 2842, 23442, 29155, 7624, 24233, 34530, 30871, 21129, 7327, 11793, 49, 34157, 48990, 34324, 4260, 25646, 23152, 17964, 27443, 47760, 20404, 5538, 32455, 24529, 5476, 46642, 905, 23175, 8570, 14828, 32227, 9510, 12615, 23945, 45816, 32992, 8570, 45816, 32643, 20304, 36516, 5348, 48973, 40, 2630, 33490, 40749, 48341, 27875, 40382, 25489, 25522, 31473, 7570, 12750, 5912, 5214, 7570, 27253, 10446, 36074, 39920, 7482, 905, 48542, 19865, 37208, 12879, 3815, 27875, 13916, 12285, 49946, 43064, 14886, 36870, 18741, 8527, 20404, 5538, 47715, 6932, 35944, 33005, 17998, 33525, 32227, 13101, 42980, 30845, 19489, 4239, 19865, 43936, 41437, 14820, 12268, 31093, 1264, 25646, 21492, 27624, 2058, 1811, 16230, 6059, 17292]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9eb1b50>\n",
            "Constructing exemplars of class 50\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [17690, 12381, 33345, 49644, 47500, 19649, 8644, 37000, 8609, 24808, 34444, 17857, 14070, 5220, 23929, 24682, 2063, 42627, 12332, 11964, 5923, 21468, 43472, 38076, 21259, 43261, 4682, 18182, 42824, 11475, 39986, 15697, 41217, 10736, 45855, 6677, 9989, 9001, 18864, 32273, 17925, 28621, 35986, 31545, 10432, 9354, 4892, 9081, 10241, 40546, 26392, 1330, 49242, 8030, 30721, 20080, 34148, 8703, 1330, 32917, 32586, 16545, 4236, 18525, 42051, 38256, 48951, 39031, 22479, 39816, 5927, 28002, 21147, 16545, 36110, 2504, 25798, 45042, 44227, 44023, 13702, 28739, 39430, 23755, 1698, 4368, 17335, 18402, 38173, 32917, 12607, 14319, 25418, 32126, 23294, 8159, 14133, 48140, 1330, 44476, 13174, 43764, 49843, 43558, 24879, 8558, 46586, 44063, 38138, 3827, 37949, 48966, 38226, 9525, 18282, 1545, 14864, 39248, 31448, 10316, 40299, 35986, 35713, 37359, 34080, 19649, 37642, 32946, 25815, 43112, 9680, 42422, 43281, 40174, 48077, 44011, 21573, 39006, 18402, 12419, 16788, 1117, 32947, 19649, 17925, 28934, 16971, 43122, 2272, 41930, 34590, 1655, 14897, 49701, 29494, 28753, 10241, 43603, 29076, 16940, 24888, 854, 43799, 12736, 30354, 8159, 43558, 49666, 21259, 14864, 37949, 3746, 32446, 41217, 39446, 2490, 38903, 34590, 45855, 39986, 42699, 1330, 42404, 35983, 28934, 7095, 2504, 11062, 19475, 13677, 44370, 35986, 11906, 11593, 3376, 31637, 19204, 18833, 32586, 27719]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9ebd310>\n",
            "Constructing exemplars of class 30\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [13267, 40253, 23642, 42023, 43702, 44401, 34573, 49204, 42144, 30440, 39110, 34360, 35567, 7451, 45646, 9585, 47038, 7745, 12797, 1019, 505, 33504, 7413, 33571, 12463, 22882, 11290, 18472, 44328, 18116, 17858, 29720, 39662, 33540, 31757, 36716, 41677, 1214, 12659, 49248, 13670, 36358, 34766, 40831, 30360, 7103, 3024, 11290, 29902, 13670, 12463, 3297, 17433, 22415, 44219, 3268, 24104, 44883, 1522, 10671, 27846, 32574, 48612, 46603, 5915, 18698, 48910, 3744, 42312, 41019, 35567, 6568, 22415, 22882, 46979, 8601, 27853, 4057, 12797, 23881, 19509, 43796, 1711, 20163, 10403, 40606, 19962, 22882, 3959, 16711, 48114, 32520, 44333, 43605, 15018, 30490, 28222, 5460, 26581, 37101, 28039, 35939, 41677, 35678, 49248, 48114, 48233, 11290, 11890, 2165, 32330, 36642, 18446, 40217, 4669, 2879, 45235, 5580, 3268, 4057, 23581, 7745, 1491, 43802, 35905, 36293, 3864, 33295, 49334, 32830, 28420, 16088, 41019, 20271, 16711, 22415, 30839, 6936, 33684, 13868, 45273, 12463, 23733, 11890, 24992, 41472, 6457, 9682, 42731, 24379, 41247, 46184, 45293, 24298, 7954, 6694, 9585, 38547, 20163, 24811, 38545, 44583, 41522, 49185, 23732, 39448, 8601, 6215, 3568, 22415, 1001, 48910, 35736, 46979, 39413, 41101, 30454, 669, 3568, 40968, 24298, 48439, 33504, 10526, 34771, 8243, 42993, 25227, 8216, 40837, 42261, 18566, 44467, 5127, 2413, 2238, 23314, 48114, 43838, 3394]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7ea4882d0>\n",
            "Constructing exemplars of class 97\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [14228, 3015, 40327, 25031, 34028, 28463, 22424, 44541, 43193, 5204, 30380, 48494, 44601, 9377, 30855, 3058, 11228, 8673, 9712, 5714, 45500, 5987, 44129, 4058, 48426, 28940, 45262, 42263, 5211, 42903, 41368, 45354, 38414, 38282, 2197, 48617, 6826, 33058, 15542, 8162, 21326, 20999, 45029, 46009, 5142, 32104, 26290, 40556, 23025, 37047, 428, 15960, 45023, 46330, 5211, 12941, 33838, 10275, 13507, 733, 7782, 21924, 13942, 2709, 8417, 38880, 13942, 16693, 45227, 20902, 30709, 41517, 48633, 23332, 15960, 39785, 1733, 18971, 39037, 12890, 5204, 7655, 23104, 3498, 43526, 9280, 33838, 48068, 45867, 13715, 49685, 25821, 17460, 28161, 26771, 15542, 9377, 1168, 25031, 6336, 45326, 29788, 27978, 26637, 42894, 15643, 20564, 8184, 29174, 25332, 14961, 15793, 37032, 45858, 16693, 42286, 39014, 8376, 20373, 5786, 46801, 39885, 44677, 6538, 37424, 2709, 7232, 31118, 32519, 29584, 5184, 8162, 42894, 45023, 46330, 22369, 23332, 14861, 17068, 26423, 15960, 22851, 32991, 4720, 46822, 20073, 25332, 25821, 43706, 29767, 16693, 45858, 47033, 30709, 44415, 19113, 28974, 48530, 21010, 48426, 27385, 48860, 15021, 10453, 28610, 43036, 15960, 45023, 46330, 16054, 37541, 1168, 13270, 3015, 25821, 428, 44473, 28694, 14861, 23332, 46225, 32782, 44415, 28467, 20662, 9712, 42263, 26239, 36498, 18412, 49451, 48426, 5300, 38880, 29788, 9377, 18412, 13500, 12941, 9206]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9eace50>\n",
            "Constructing exemplars of class 69\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [9182, 45614, 40639, 32437, 8588, 14892, 21550, 13946, 12450, 49700, 13506, 39487, 47429, 14071, 5818, 9479, 2931, 48619, 30857, 11180, 37313, 14140, 46493, 14654, 199, 44855, 4612, 1677, 42696, 38134, 6461, 15661, 37886, 21927, 29570, 44819, 12450, 45811, 2152, 6840, 36040, 38162, 17010, 12829, 41963, 16084, 3648, 20291, 46042, 34343, 33841, 23396, 2931, 27877, 7510, 11143, 17311, 12365, 33514, 36040, 20301, 13960, 45550, 41584, 49598, 6572, 23396, 20754, 28563, 19162, 24676, 11143, 2931, 14354, 39962, 15661, 10189, 29665, 40223, 39450, 14220, 9592, 10607, 4273, 6521, 13875, 35343, 23730, 9148, 28271, 31446, 41584, 25881, 49700, 12043, 12365, 36625, 12527, 42127, 25664, 13535, 47452, 12864, 19203, 6521, 20249, 6007, 35115, 49128, 21181, 3819, 26754, 16411, 2036, 34161, 40538, 17096, 12421, 49128, 17911, 37886, 26557, 43890, 29420, 15424, 1405, 33019, 19706, 10888, 20055, 639, 17311, 25544, 18478, 27257, 36129, 15424, 8934, 4273, 41606, 29823, 43225, 35756, 39244, 35869, 12527, 8588, 25202, 30173, 6461, 45113, 7502, 10852, 42356, 10828, 28296, 9148, 43225, 33667, 30506, 12450, 34746, 29866, 27877, 7510, 12043, 7966, 27838, 27611, 31733, 13288, 25108, 22670, 21927, 32177, 17447, 30506, 18342, 31556, 28275, 30506, 34907, 49594, 639, 4612, 6634, 24210, 25881, 1677, 42228, 3479, 19706, 44224, 28327, 27092, 10772, 27611, 44819, 19182, 17911]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7ea4d3150>\n",
            "Constructing exemplars of class 57\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [2768, 22429, 49263, 38372, 18747, 19979, 47470, 2683, 10610, 2661, 46528, 4539, 5693, 47383, 49034, 49750, 43752, 24254, 528, 6723, 23525, 47615, 18595, 20064, 34429, 1501, 31005, 24623, 23240, 22360, 20618, 15920, 38939, 17529, 41313, 39466, 2112, 33657, 25710, 9237, 2023, 9596, 7269, 3560, 42194, 46797, 1618, 6066, 18595, 31623, 19134, 10193, 30607, 5006, 34969, 37407, 28370, 7855, 40142, 34294, 283, 37073, 26372, 8773, 9651, 48001, 20064, 7838, 19555, 4879, 17861, 23264, 43877, 42738, 26755, 48777, 7316, 22429, 7958, 17806, 16512, 3683, 35761, 2087, 27428, 25893, 23915, 3683, 48099, 26481, 25710, 3399, 26794, 19822, 528, 33137, 41799, 40950, 46558, 43067, 36770, 32133, 24962, 33874, 25723, 13333, 13432, 6308, 16422, 14322, 283, 46246, 41799, 14888, 9720, 24370, 9003, 45213, 14698, 32301, 22974, 44647, 27985, 21487, 8401, 36770, 31005, 2112, 47604, 23240, 49263, 41608, 14698, 21683, 13012, 48650, 11474, 4852, 38534, 34473, 24969, 46246, 40996, 6496, 3713, 48654, 38390, 35347, 23264, 31414, 7269, 42621, 24180, 39722, 45831, 30607, 29805, 12757, 33657, 2679, 16729, 7094, 17666, 47929, 48568, 14528, 18595, 49561, 46246, 11215, 29054, 44632, 24287, 21462, 13068, 44770, 36321, 33753, 6668, 13188, 8771, 11701, 32388, 30979, 21228, 49836, 11133, 14490, 14135, 30600, 5006, 2320, 41396, 38762, 40950, 7578, 28484, 25429, 21683, 31414]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9e900d0>\n",
            "Constructing exemplars of class 25\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [44456, 35027, 44474, 46061, 6364, 22202, 16485, 34618, 19860, 40620, 36823, 9733, 20613, 7450, 34618, 35188, 14997, 7077, 44585, 37566, 38074, 38788, 36454, 5317, 8468, 1926, 6942, 34978, 8168, 24309, 11859, 42649, 24158, 8468, 8350, 450, 6233, 41249, 5373, 36174, 6589, 33231, 33303, 9695, 44757, 35163, 32700, 40614, 8468, 5420, 49824, 23243, 25546, 4486, 17325, 44689, 33866, 3651, 32700, 19140, 40472, 49718, 46673, 27616, 1471, 20170, 29580, 33303, 7967, 23320, 19717, 49824, 21005, 4565, 9458, 44739, 40620, 29942, 5664, 21192, 8619, 16485, 46713, 30530, 23031, 12817, 37072, 18489, 14997, 7405, 12817, 27047, 688, 3562, 18166, 19623, 19995, 37590, 30925, 36316, 23031, 23686, 31983, 16155, 14997, 37467, 39267, 49762, 16690, 47164, 14185, 40489, 37590, 45846, 5192, 35770, 10044, 15289, 16741, 35188, 14997, 28042, 40417, 25211, 30836, 34358, 42747, 32311, 7490, 6233, 19623, 16485, 35847, 5555, 46442, 47164, 33254, 37633, 41000, 46620, 9121, 15340, 16741, 15289, 46620, 19010, 40273, 39267, 48335, 39568, 37566, 45458, 40368, 30836, 5192, 45949, 8468, 29801, 41087, 20462, 24309, 8468, 46442, 31480, 43841, 29801, 8168, 35704, 34278, 33963, 4789, 29467, 23320, 18679, 27232, 5653, 14997, 7490, 19140, 17937, 21405, 14997, 40540, 46461, 16155, 35700, 1471, 18374, 49070, 241, 47736, 8619, 34698, 20616, 48335, 6942, 44253, 46990, 18929, 17968]\n",
            "40\n",
            "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 13: [], 14: [], 15: [], 16: [], 17: [], 18: [], 19: [], 20: [], 21: [], 22: [], 23: [], 24: [], 25: [44456, 35027, 44474, 46061, 6364, 22202, 16485, 34618, 19860, 40620, 36823, 9733, 20613, 7450, 34618, 35188, 14997, 7077, 44585, 37566, 38074, 38788, 36454, 5317, 8468, 1926, 6942, 34978, 8168, 24309, 11859, 42649, 24158, 8468, 8350, 450, 6233, 41249, 5373, 36174, 6589, 33231, 33303, 9695, 44757, 35163, 32700, 40614, 8468, 5420, 49824, 23243, 25546, 4486, 17325, 44689, 33866, 3651, 32700, 19140, 40472, 49718, 46673, 27616, 1471, 20170, 29580, 33303, 7967, 23320, 19717, 49824, 21005, 4565, 9458, 44739, 40620, 29942, 5664, 21192, 8619, 16485, 46713, 30530, 23031, 12817, 37072, 18489, 14997, 7405, 12817, 27047, 688, 3562, 18166, 19623, 19995, 37590, 30925, 36316, 23031, 23686, 31983, 16155, 14997, 37467, 39267, 49762, 16690, 47164, 14185, 40489, 37590, 45846, 5192, 35770, 10044, 15289, 16741, 35188, 14997, 28042, 40417, 25211, 30836, 34358, 42747, 32311, 7490, 6233, 19623, 16485, 35847, 5555, 46442, 47164, 33254, 37633, 41000, 46620, 9121, 15340, 16741, 15289, 46620, 19010, 40273, 39267, 48335, 39568, 37566, 45458, 40368, 30836, 5192, 45949, 8468, 29801, 41087, 20462, 24309, 8468, 46442, 31480, 43841, 29801, 8168, 35704, 34278, 33963, 4789, 29467, 23320, 18679, 27232, 5653, 14997, 7490, 19140, 17937, 21405, 14997, 40540, 46461, 16155, 35700, 1471, 18374, 49070, 241, 47736, 8619, 34698, 20616, 48335, 6942, 44253, 46990, 18929, 17968], 26: [], 27: [20952, 153, 37760, 21305, 34276, 5396, 34577, 778, 47901, 35233, 14891, 42200, 29181, 32806, 1285, 31719, 1538, 28338, 47985, 42910, 21589, 391, 678, 48388, 40216, 19682, 44683, 21583, 25620, 22112, 35253, 29158, 46544, 13787, 17559, 17728, 5253, 19478, 5770, 23799, 8163, 48445, 23799, 20987, 43902, 5434, 11025, 8143, 12953, 9823, 24841, 4296, 37761, 25281, 10410, 49948, 34459, 7593, 9467, 33141, 32474, 1631, 45634, 12553, 5434, 13966, 34887, 30800, 39919, 17530, 42764, 48733, 23132, 778, 34577, 47901, 30848, 29890, 36963, 47928, 45895, 32668, 39659, 12072, 9823, 2526, 20952, 33013, 15502, 15181, 18362, 11177, 10865, 44683, 27286, 9055, 30848, 17566, 39439, 662, 10814, 42220, 35233, 26015, 29890, 28338, 41738, 33141, 19858, 44571, 16566, 3240, 37761, 10814, 25590, 17655, 35192, 11184, 7215, 17695, 41711, 13843, 35362, 44954, 49398, 29158, 40513, 12953, 43133, 12620, 33447, 40767, 7759, 36691, 3235, 30751, 49853, 27513, 32806, 13874, 9085, 26815, 21408, 47144, 15066, 10814, 3094, 48091, 38348, 9085, 12620, 28338, 28024, 46338, 18187, 18806, 29890, 39265, 41505, 45620, 5770, 26015, 23356, 4790, 23508, 42893, 49851, 36691, 48331, 17566, 1041, 36702, 5972, 25590, 20768, 40767, 2340, 33478, 26442, 41262, 45620, 6296, 13165, 19771, 41505, 18362, 34887, 20289, 26590, 2810, 1771, 6296, 23125, 35192, 23408, 42434, 16632, 26315, 11423, 16747], 28: [], 29: [], 30: [13267, 40253, 23642, 42023, 43702, 44401, 34573, 49204, 42144, 30440, 39110, 34360, 35567, 7451, 45646, 9585, 47038, 7745, 12797, 1019, 505, 33504, 7413, 33571, 12463, 22882, 11290, 18472, 44328, 18116, 17858, 29720, 39662, 33540, 31757, 36716, 41677, 1214, 12659, 49248, 13670, 36358, 34766, 40831, 30360, 7103, 3024, 11290, 29902, 13670, 12463, 3297, 17433, 22415, 44219, 3268, 24104, 44883, 1522, 10671, 27846, 32574, 48612, 46603, 5915, 18698, 48910, 3744, 42312, 41019, 35567, 6568, 22415, 22882, 46979, 8601, 27853, 4057, 12797, 23881, 19509, 43796, 1711, 20163, 10403, 40606, 19962, 22882, 3959, 16711, 48114, 32520, 44333, 43605, 15018, 30490, 28222, 5460, 26581, 37101, 28039, 35939, 41677, 35678, 49248, 48114, 48233, 11290, 11890, 2165, 32330, 36642, 18446, 40217, 4669, 2879, 45235, 5580, 3268, 4057, 23581, 7745, 1491, 43802, 35905, 36293, 3864, 33295, 49334, 32830, 28420, 16088, 41019, 20271, 16711, 22415, 30839, 6936, 33684, 13868, 45273, 12463, 23733, 11890, 24992, 41472, 6457, 9682, 42731, 24379, 41247, 46184, 45293, 24298, 7954, 6694, 9585, 38547, 20163, 24811, 38545, 44583, 41522, 49185, 23732, 39448, 8601, 6215, 3568, 22415, 1001, 48910, 35736, 46979, 39413, 41101, 30454, 669, 3568, 40968, 24298, 48439, 33504, 10526, 34771, 8243, 42993, 25227, 8216, 40837, 42261, 18566, 44467, 5127, 2413, 2238, 23314, 48114, 43838, 3394], 31: [], 32: [], 33: [], 34: [], 35: [], 36: [], 37: [], 38: [], 39: [], 40: [], 41: [], 42: [], 43: [], 44: [], 45: [], 46: [], 47: [], 48: [], 49: [], 50: [17690, 12381, 33345, 49644, 47500, 19649, 8644, 37000, 8609, 24808, 34444, 17857, 14070, 5220, 23929, 24682, 2063, 42627, 12332, 11964, 5923, 21468, 43472, 38076, 21259, 43261, 4682, 18182, 42824, 11475, 39986, 15697, 41217, 10736, 45855, 6677, 9989, 9001, 18864, 32273, 17925, 28621, 35986, 31545, 10432, 9354, 4892, 9081, 10241, 40546, 26392, 1330, 49242, 8030, 30721, 20080, 34148, 8703, 1330, 32917, 32586, 16545, 4236, 18525, 42051, 38256, 48951, 39031, 22479, 39816, 5927, 28002, 21147, 16545, 36110, 2504, 25798, 45042, 44227, 44023, 13702, 28739, 39430, 23755, 1698, 4368, 17335, 18402, 38173, 32917, 12607, 14319, 25418, 32126, 23294, 8159, 14133, 48140, 1330, 44476, 13174, 43764, 49843, 43558, 24879, 8558, 46586, 44063, 38138, 3827, 37949, 48966, 38226, 9525, 18282, 1545, 14864, 39248, 31448, 10316, 40299, 35986, 35713, 37359, 34080, 19649, 37642, 32946, 25815, 43112, 9680, 42422, 43281, 40174, 48077, 44011, 21573, 39006, 18402, 12419, 16788, 1117, 32947, 19649, 17925, 28934, 16971, 43122, 2272, 41930, 34590, 1655, 14897, 49701, 29494, 28753, 10241, 43603, 29076, 16940, 24888, 854, 43799, 12736, 30354, 8159, 43558, 49666, 21259, 14864, 37949, 3746, 32446, 41217, 39446, 2490, 38903, 34590, 45855, 39986, 42699, 1330, 42404, 35983, 28934, 7095, 2504, 11062, 19475, 13677, 44370, 35986, 11906, 11593, 3376, 31637, 19204, 18833, 32586, 27719], 51: [], 52: [], 53: [], 54: [], 55: [], 56: [], 57: [2768, 22429, 49263, 38372, 18747, 19979, 47470, 2683, 10610, 2661, 46528, 4539, 5693, 47383, 49034, 49750, 43752, 24254, 528, 6723, 23525, 47615, 18595, 20064, 34429, 1501, 31005, 24623, 23240, 22360, 20618, 15920, 38939, 17529, 41313, 39466, 2112, 33657, 25710, 9237, 2023, 9596, 7269, 3560, 42194, 46797, 1618, 6066, 18595, 31623, 19134, 10193, 30607, 5006, 34969, 37407, 28370, 7855, 40142, 34294, 283, 37073, 26372, 8773, 9651, 48001, 20064, 7838, 19555, 4879, 17861, 23264, 43877, 42738, 26755, 48777, 7316, 22429, 7958, 17806, 16512, 3683, 35761, 2087, 27428, 25893, 23915, 3683, 48099, 26481, 25710, 3399, 26794, 19822, 528, 33137, 41799, 40950, 46558, 43067, 36770, 32133, 24962, 33874, 25723, 13333, 13432, 6308, 16422, 14322, 283, 46246, 41799, 14888, 9720, 24370, 9003, 45213, 14698, 32301, 22974, 44647, 27985, 21487, 8401, 36770, 31005, 2112, 47604, 23240, 49263, 41608, 14698, 21683, 13012, 48650, 11474, 4852, 38534, 34473, 24969, 46246, 40996, 6496, 3713, 48654, 38390, 35347, 23264, 31414, 7269, 42621, 24180, 39722, 45831, 30607, 29805, 12757, 33657, 2679, 16729, 7094, 17666, 47929, 48568, 14528, 18595, 49561, 46246, 11215, 29054, 44632, 24287, 21462, 13068, 44770, 36321, 33753, 6668, 13188, 8771, 11701, 32388, 30979, 21228, 49836, 11133, 14490, 14135, 30600, 5006, 2320, 41396, 38762, 40950, 7578, 28484, 25429, 21683, 31414], 58: [], 59: [], 60: [], 61: [], 62: [], 63: [], 64: [], 65: [], 66: [], 67: [], 68: [], 69: [9182, 45614, 40639, 32437, 8588, 14892, 21550, 13946, 12450, 49700, 13506, 39487, 47429, 14071, 5818, 9479, 2931, 48619, 30857, 11180, 37313, 14140, 46493, 14654, 199, 44855, 4612, 1677, 42696, 38134, 6461, 15661, 37886, 21927, 29570, 44819, 12450, 45811, 2152, 6840, 36040, 38162, 17010, 12829, 41963, 16084, 3648, 20291, 46042, 34343, 33841, 23396, 2931, 27877, 7510, 11143, 17311, 12365, 33514, 36040, 20301, 13960, 45550, 41584, 49598, 6572, 23396, 20754, 28563, 19162, 24676, 11143, 2931, 14354, 39962, 15661, 10189, 29665, 40223, 39450, 14220, 9592, 10607, 4273, 6521, 13875, 35343, 23730, 9148, 28271, 31446, 41584, 25881, 49700, 12043, 12365, 36625, 12527, 42127, 25664, 13535, 47452, 12864, 19203, 6521, 20249, 6007, 35115, 49128, 21181, 3819, 26754, 16411, 2036, 34161, 40538, 17096, 12421, 49128, 17911, 37886, 26557, 43890, 29420, 15424, 1405, 33019, 19706, 10888, 20055, 639, 17311, 25544, 18478, 27257, 36129, 15424, 8934, 4273, 41606, 29823, 43225, 35756, 39244, 35869, 12527, 8588, 25202, 30173, 6461, 45113, 7502, 10852, 42356, 10828, 28296, 9148, 43225, 33667, 30506, 12450, 34746, 29866, 27877, 7510, 12043, 7966, 27838, 27611, 31733, 13288, 25108, 22670, 21927, 32177, 17447, 30506, 18342, 31556, 28275, 30506, 34907, 49594, 639, 4612, 6634, 24210, 25881, 1677, 42228, 3479, 19706, 44224, 28327, 27092, 10772, 27611, 44819, 19182, 17911], 70: [], 71: [], 72: [], 73: [], 74: [], 75: [], 76: [], 77: [], 78: [25903, 7374, 18091, 17292, 15438, 45048, 20522, 15648, 19489, 26668, 2842, 23442, 8598, 8764, 32972, 6645, 30672, 9510, 5912, 7634, 19381, 34349, 39018, 16955, 47100, 19381, 9568, 39058, 29518, 22052, 24843, 19643, 45415, 19985, 45048, 42636, 34803, 25522, 769, 10771, 1135, 21928, 46816, 3626, 47918, 16179, 6907, 20551, 19034, 37054, 15718, 24549, 42663, 29043, 48865, 35594, 20474, 33965, 18321, 43630, 38339, 45606, 14886, 43064, 1619, 25489, 28445, 39058, 44962, 14625, 3032, 10582, 5500, 47100, 1037, 49946, 4105, 26668, 12747, 24329, 12879, 33082, 37110, 5912, 24843, 13832, 18075, 1619, 36435, 49541, 29736, 33965, 8527, 2842, 23442, 29155, 7624, 24233, 34530, 30871, 21129, 7327, 11793, 49, 34157, 48990, 34324, 4260, 25646, 23152, 17964, 27443, 47760, 20404, 5538, 32455, 24529, 5476, 46642, 905, 23175, 8570, 14828, 32227, 9510, 12615, 23945, 45816, 32992, 8570, 45816, 32643, 20304, 36516, 5348, 48973, 40, 2630, 33490, 40749, 48341, 27875, 40382, 25489, 25522, 31473, 7570, 12750, 5912, 5214, 7570, 27253, 10446, 36074, 39920, 7482, 905, 48542, 19865, 37208, 12879, 3815, 27875, 13916, 12285, 49946, 43064, 14886, 36870, 18741, 8527, 20404, 5538, 47715, 6932, 35944, 33005, 17998, 33525, 32227, 13101, 42980, 30845, 19489, 4239, 19865, 43936, 41437, 14820, 12268, 31093, 1264, 25646, 21492, 27624, 2058, 1811, 16230, 6059, 17292], 79: [], 80: [], 81: [], 82: [15939, 5454, 1402, 30049, 26976, 36327, 43606, 24921, 10699, 6195, 6570, 32115, 40656, 3555, 21695, 22190, 29103, 20263, 9512, 46394, 43866, 41547, 9637, 44523, 8099, 26471, 32106, 30450, 37510, 21973, 15317, 871, 3507, 20923, 26562, 41221, 45656, 33931, 47306, 5656, 7959, 12408, 25771, 18702, 8253, 10283, 5586, 47534, 32115, 39114, 16897, 5275, 7671, 8518, 26965, 7334, 29478, 25724, 33477, 22970, 11832, 8291, 24715, 28893, 32602, 41968, 40655, 39822, 7415, 14832, 43765, 11600, 3842, 47306, 18702, 14041, 32277, 274, 27035, 11501, 47319, 11084, 44928, 42485, 26501, 6277, 45064, 4381, 46832, 43293, 24024, 35967, 6227, 24852, 13546, 8518, 43293, 10699, 33477, 35336, 41547, 24714, 36437, 22190, 39637, 27707, 3842, 34088, 8727, 13458, 33931, 318, 22110, 14992, 11832, 37521, 41900, 14131, 36194, 33709, 42541, 43453, 9111, 28404, 3842, 32449, 30102, 31775, 8257, 32725, 28101, 23402, 22192, 38486, 24387, 34088, 11084, 625, 44718, 11901, 24024, 18646, 40462, 26852, 26562, 36492, 19830, 26712, 26921, 39228, 42250, 12171, 45373, 43453, 30468, 32847, 25997, 32115, 18644, 12866, 33729, 45757, 1143, 22457, 5586, 49708, 34248, 32602, 35285, 28101, 46832, 4269, 46550, 12380, 3842, 32449, 31443, 29348, 41672, 14557, 46825, 48289, 43830, 16393, 22457, 47306, 23681, 41672, 14557, 40082, 36192, 34911, 26553, 44513, 40656, 27487, 16784, 49368, 16897, 38738], 83: [], 84: [], 85: [], 86: [8802, 45861, 7091, 9550, 32608, 6649, 9567, 3895, 48900, 8754, 4512, 40334, 32608, 18107, 37014, 1186, 22718, 42707, 32789, 46169, 19098, 32929, 26097, 11723, 32665, 6546, 496, 46275, 33101, 5967, 28822, 47493, 46967, 14170, 222, 24906, 45536, 35231, 47494, 18310, 40526, 44778, 32129, 49171, 42290, 4246, 2235, 7608, 25310, 21679, 17240, 46275, 8086, 36135, 19219, 19443, 20778, 22949, 29985, 16344, 29635, 222, 10915, 26719, 34586, 15866, 28302, 5767, 5595, 45686, 22033, 24935, 38753, 22365, 30806, 9776, 44794, 9113, 20778, 27070, 40886, 36065, 33812, 35231, 10299, 1244, 5595, 48900, 34195, 20778, 22721, 14170, 43529, 21691, 13053, 38800, 3795, 18567, 6546, 10837, 8149, 8303, 11537, 21600, 1485, 32219, 36135, 43874, 23394, 46826, 40492, 41507, 22327, 31538, 12066, 23586, 49574, 1312, 16995, 38734, 26748, 41756, 49008, 30674, 22982, 45299, 26989, 48253, 17533, 44356, 37044, 6638, 22721, 40573, 36135, 41941, 6546, 24906, 15672, 42218, 35599, 28822, 5967, 2002, 23496, 5767, 24367, 48810, 45306, 47883, 15721, 4374, 45263, 16995, 9980, 23108, 32129, 49880, 15479, 22721, 34153, 22240, 41129, 11747, 29985, 16344, 15501, 25436, 24875, 46659, 45263, 19443, 45871, 43529, 23034, 24004, 5608, 16656, 43711, 39129, 22982, 12755, 1083, 10915, 21404, 9985, 22689, 22317, 47045, 34591, 22982, 5270, 33812, 33095, 29213, 32789, 23108, 26401, 20852, 7646], 87: [], 88: [], 89: [], 90: [], 91: [], 92: [], 93: [], 94: [], 95: [], 96: [], 97: [14228, 3015, 40327, 25031, 34028, 28463, 22424, 44541, 43193, 5204, 30380, 48494, 44601, 9377, 30855, 3058, 11228, 8673, 9712, 5714, 45500, 5987, 44129, 4058, 48426, 28940, 45262, 42263, 5211, 42903, 41368, 45354, 38414, 38282, 2197, 48617, 6826, 33058, 15542, 8162, 21326, 20999, 45029, 46009, 5142, 32104, 26290, 40556, 23025, 37047, 428, 15960, 45023, 46330, 5211, 12941, 33838, 10275, 13507, 733, 7782, 21924, 13942, 2709, 8417, 38880, 13942, 16693, 45227, 20902, 30709, 41517, 48633, 23332, 15960, 39785, 1733, 18971, 39037, 12890, 5204, 7655, 23104, 3498, 43526, 9280, 33838, 48068, 45867, 13715, 49685, 25821, 17460, 28161, 26771, 15542, 9377, 1168, 25031, 6336, 45326, 29788, 27978, 26637, 42894, 15643, 20564, 8184, 29174, 25332, 14961, 15793, 37032, 45858, 16693, 42286, 39014, 8376, 20373, 5786, 46801, 39885, 44677, 6538, 37424, 2709, 7232, 31118, 32519, 29584, 5184, 8162, 42894, 45023, 46330, 22369, 23332, 14861, 17068, 26423, 15960, 22851, 32991, 4720, 46822, 20073, 25332, 25821, 43706, 29767, 16693, 45858, 47033, 30709, 44415, 19113, 28974, 48530, 21010, 48426, 27385, 48860, 15021, 10453, 28610, 43036, 15960, 45023, 46330, 16054, 37541, 1168, 13270, 3015, 25821, 428, 44473, 28694, 14861, 23332, 46225, 32782, 44415, 28467, 20662, 9712, 42263, 26239, 36498, 18412, 49451, 48426, 5300, 38880, 29788, 9377, 18412, 13500, 12941, 9206], 98: [], 99: []}\n",
            "200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "200\n",
            "2\n",
            "200\n",
            "3\n",
            "200\n",
            "4\n",
            "200\n",
            "5\n",
            "200\n",
            "6\n",
            "200\n",
            "7\n",
            "200\n",
            "8\n",
            "200\n",
            "9\n",
            "200\n",
            "10\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "40\n",
            "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 13: [], 14: [], 15: [], 16: [], 17: [], 18: [], 19: [], 20: [], 21: [], 22: [], 23: [], 24: [], 25: [44456, 35027, 44474, 46061, 6364, 22202, 16485, 34618, 19860, 40620, 36823, 9733, 20613, 7450, 34618, 35188, 14997, 7077, 44585, 37566, 38074, 38788, 36454, 5317, 8468, 1926, 6942, 34978, 8168, 24309, 11859, 42649, 24158, 8468, 8350, 450, 6233, 41249, 5373, 36174, 6589, 33231, 33303, 9695, 44757, 35163, 32700, 40614, 8468, 5420, 49824, 23243, 25546, 4486, 17325, 44689, 33866, 3651, 32700, 19140, 40472, 49718, 46673, 27616, 1471, 20170, 29580, 33303, 7967, 23320, 19717, 49824, 21005, 4565, 9458, 44739, 40620, 29942, 5664, 21192, 8619, 16485, 46713, 30530, 23031, 12817, 37072, 18489, 14997, 7405, 12817, 27047, 688, 3562, 18166, 19623, 19995, 37590, 30925, 36316, 23031, 23686, 31983, 16155, 14997, 37467, 39267, 49762, 16690, 47164, 14185, 40489, 37590, 45846, 5192, 35770, 10044, 15289, 16741, 35188, 14997, 28042, 40417, 25211, 30836, 34358, 42747, 32311, 7490, 6233, 19623, 16485, 35847, 5555, 46442, 47164, 33254, 37633, 41000, 46620, 9121, 15340, 16741, 15289, 46620, 19010, 40273, 39267, 48335, 39568, 37566, 45458, 40368, 30836, 5192, 45949, 8468, 29801, 41087, 20462, 24309, 8468, 46442, 31480, 43841, 29801, 8168, 35704, 34278, 33963, 4789, 29467, 23320, 18679, 27232, 5653, 14997, 7490, 19140, 17937, 21405, 14997, 40540, 46461, 16155, 35700, 1471, 18374, 49070, 241, 47736, 8619, 34698, 20616, 48335, 6942, 44253, 46990, 18929, 17968], 26: [], 27: [20952, 153, 37760, 21305, 34276, 5396, 34577, 778, 47901, 35233, 14891, 42200, 29181, 32806, 1285, 31719, 1538, 28338, 47985, 42910, 21589, 391, 678, 48388, 40216, 19682, 44683, 21583, 25620, 22112, 35253, 29158, 46544, 13787, 17559, 17728, 5253, 19478, 5770, 23799, 8163, 48445, 23799, 20987, 43902, 5434, 11025, 8143, 12953, 9823, 24841, 4296, 37761, 25281, 10410, 49948, 34459, 7593, 9467, 33141, 32474, 1631, 45634, 12553, 5434, 13966, 34887, 30800, 39919, 17530, 42764, 48733, 23132, 778, 34577, 47901, 30848, 29890, 36963, 47928, 45895, 32668, 39659, 12072, 9823, 2526, 20952, 33013, 15502, 15181, 18362, 11177, 10865, 44683, 27286, 9055, 30848, 17566, 39439, 662, 10814, 42220, 35233, 26015, 29890, 28338, 41738, 33141, 19858, 44571, 16566, 3240, 37761, 10814, 25590, 17655, 35192, 11184, 7215, 17695, 41711, 13843, 35362, 44954, 49398, 29158, 40513, 12953, 43133, 12620, 33447, 40767, 7759, 36691, 3235, 30751, 49853, 27513, 32806, 13874, 9085, 26815, 21408, 47144, 15066, 10814, 3094, 48091, 38348, 9085, 12620, 28338, 28024, 46338, 18187, 18806, 29890, 39265, 41505, 45620, 5770, 26015, 23356, 4790, 23508, 42893, 49851, 36691, 48331, 17566, 1041, 36702, 5972, 25590, 20768, 40767, 2340, 33478, 26442, 41262, 45620, 6296, 13165, 19771, 41505, 18362, 34887, 20289, 26590, 2810, 1771, 6296, 23125, 35192, 23408, 42434, 16632, 26315, 11423, 16747], 28: [], 29: [], 30: [13267, 40253, 23642, 42023, 43702, 44401, 34573, 49204, 42144, 30440, 39110, 34360, 35567, 7451, 45646, 9585, 47038, 7745, 12797, 1019, 505, 33504, 7413, 33571, 12463, 22882, 11290, 18472, 44328, 18116, 17858, 29720, 39662, 33540, 31757, 36716, 41677, 1214, 12659, 49248, 13670, 36358, 34766, 40831, 30360, 7103, 3024, 11290, 29902, 13670, 12463, 3297, 17433, 22415, 44219, 3268, 24104, 44883, 1522, 10671, 27846, 32574, 48612, 46603, 5915, 18698, 48910, 3744, 42312, 41019, 35567, 6568, 22415, 22882, 46979, 8601, 27853, 4057, 12797, 23881, 19509, 43796, 1711, 20163, 10403, 40606, 19962, 22882, 3959, 16711, 48114, 32520, 44333, 43605, 15018, 30490, 28222, 5460, 26581, 37101, 28039, 35939, 41677, 35678, 49248, 48114, 48233, 11290, 11890, 2165, 32330, 36642, 18446, 40217, 4669, 2879, 45235, 5580, 3268, 4057, 23581, 7745, 1491, 43802, 35905, 36293, 3864, 33295, 49334, 32830, 28420, 16088, 41019, 20271, 16711, 22415, 30839, 6936, 33684, 13868, 45273, 12463, 23733, 11890, 24992, 41472, 6457, 9682, 42731, 24379, 41247, 46184, 45293, 24298, 7954, 6694, 9585, 38547, 20163, 24811, 38545, 44583, 41522, 49185, 23732, 39448, 8601, 6215, 3568, 22415, 1001, 48910, 35736, 46979, 39413, 41101, 30454, 669, 3568, 40968, 24298, 48439, 33504, 10526, 34771, 8243, 42993, 25227, 8216, 40837, 42261, 18566, 44467, 5127, 2413, 2238, 23314, 48114, 43838, 3394], 31: [], 32: [], 33: [], 34: [], 35: [], 36: [], 37: [], 38: [], 39: [], 40: [], 41: [], 42: [], 43: [], 44: [], 45: [], 46: [], 47: [], 48: [], 49: [], 50: [17690, 12381, 33345, 49644, 47500, 19649, 8644, 37000, 8609, 24808, 34444, 17857, 14070, 5220, 23929, 24682, 2063, 42627, 12332, 11964, 5923, 21468, 43472, 38076, 21259, 43261, 4682, 18182, 42824, 11475, 39986, 15697, 41217, 10736, 45855, 6677, 9989, 9001, 18864, 32273, 17925, 28621, 35986, 31545, 10432, 9354, 4892, 9081, 10241, 40546, 26392, 1330, 49242, 8030, 30721, 20080, 34148, 8703, 1330, 32917, 32586, 16545, 4236, 18525, 42051, 38256, 48951, 39031, 22479, 39816, 5927, 28002, 21147, 16545, 36110, 2504, 25798, 45042, 44227, 44023, 13702, 28739, 39430, 23755, 1698, 4368, 17335, 18402, 38173, 32917, 12607, 14319, 25418, 32126, 23294, 8159, 14133, 48140, 1330, 44476, 13174, 43764, 49843, 43558, 24879, 8558, 46586, 44063, 38138, 3827, 37949, 48966, 38226, 9525, 18282, 1545, 14864, 39248, 31448, 10316, 40299, 35986, 35713, 37359, 34080, 19649, 37642, 32946, 25815, 43112, 9680, 42422, 43281, 40174, 48077, 44011, 21573, 39006, 18402, 12419, 16788, 1117, 32947, 19649, 17925, 28934, 16971, 43122, 2272, 41930, 34590, 1655, 14897, 49701, 29494, 28753, 10241, 43603, 29076, 16940, 24888, 854, 43799, 12736, 30354, 8159, 43558, 49666, 21259, 14864, 37949, 3746, 32446, 41217, 39446, 2490, 38903, 34590, 45855, 39986, 42699, 1330, 42404, 35983, 28934, 7095, 2504, 11062, 19475, 13677, 44370, 35986, 11906, 11593, 3376, 31637, 19204, 18833, 32586, 27719], 51: [], 52: [], 53: [], 54: [], 55: [], 56: [], 57: [2768, 22429, 49263, 38372, 18747, 19979, 47470, 2683, 10610, 2661, 46528, 4539, 5693, 47383, 49034, 49750, 43752, 24254, 528, 6723, 23525, 47615, 18595, 20064, 34429, 1501, 31005, 24623, 23240, 22360, 20618, 15920, 38939, 17529, 41313, 39466, 2112, 33657, 25710, 9237, 2023, 9596, 7269, 3560, 42194, 46797, 1618, 6066, 18595, 31623, 19134, 10193, 30607, 5006, 34969, 37407, 28370, 7855, 40142, 34294, 283, 37073, 26372, 8773, 9651, 48001, 20064, 7838, 19555, 4879, 17861, 23264, 43877, 42738, 26755, 48777, 7316, 22429, 7958, 17806, 16512, 3683, 35761, 2087, 27428, 25893, 23915, 3683, 48099, 26481, 25710, 3399, 26794, 19822, 528, 33137, 41799, 40950, 46558, 43067, 36770, 32133, 24962, 33874, 25723, 13333, 13432, 6308, 16422, 14322, 283, 46246, 41799, 14888, 9720, 24370, 9003, 45213, 14698, 32301, 22974, 44647, 27985, 21487, 8401, 36770, 31005, 2112, 47604, 23240, 49263, 41608, 14698, 21683, 13012, 48650, 11474, 4852, 38534, 34473, 24969, 46246, 40996, 6496, 3713, 48654, 38390, 35347, 23264, 31414, 7269, 42621, 24180, 39722, 45831, 30607, 29805, 12757, 33657, 2679, 16729, 7094, 17666, 47929, 48568, 14528, 18595, 49561, 46246, 11215, 29054, 44632, 24287, 21462, 13068, 44770, 36321, 33753, 6668, 13188, 8771, 11701, 32388, 30979, 21228, 49836, 11133, 14490, 14135, 30600, 5006, 2320, 41396, 38762, 40950, 7578, 28484, 25429, 21683, 31414], 58: [], 59: [], 60: [], 61: [], 62: [], 63: [], 64: [], 65: [], 66: [], 67: [], 68: [], 69: [9182, 45614, 40639, 32437, 8588, 14892, 21550, 13946, 12450, 49700, 13506, 39487, 47429, 14071, 5818, 9479, 2931, 48619, 30857, 11180, 37313, 14140, 46493, 14654, 199, 44855, 4612, 1677, 42696, 38134, 6461, 15661, 37886, 21927, 29570, 44819, 12450, 45811, 2152, 6840, 36040, 38162, 17010, 12829, 41963, 16084, 3648, 20291, 46042, 34343, 33841, 23396, 2931, 27877, 7510, 11143, 17311, 12365, 33514, 36040, 20301, 13960, 45550, 41584, 49598, 6572, 23396, 20754, 28563, 19162, 24676, 11143, 2931, 14354, 39962, 15661, 10189, 29665, 40223, 39450, 14220, 9592, 10607, 4273, 6521, 13875, 35343, 23730, 9148, 28271, 31446, 41584, 25881, 49700, 12043, 12365, 36625, 12527, 42127, 25664, 13535, 47452, 12864, 19203, 6521, 20249, 6007, 35115, 49128, 21181, 3819, 26754, 16411, 2036, 34161, 40538, 17096, 12421, 49128, 17911, 37886, 26557, 43890, 29420, 15424, 1405, 33019, 19706, 10888, 20055, 639, 17311, 25544, 18478, 27257, 36129, 15424, 8934, 4273, 41606, 29823, 43225, 35756, 39244, 35869, 12527, 8588, 25202, 30173, 6461, 45113, 7502, 10852, 42356, 10828, 28296, 9148, 43225, 33667, 30506, 12450, 34746, 29866, 27877, 7510, 12043, 7966, 27838, 27611, 31733, 13288, 25108, 22670, 21927, 32177, 17447, 30506, 18342, 31556, 28275, 30506, 34907, 49594, 639, 4612, 6634, 24210, 25881, 1677, 42228, 3479, 19706, 44224, 28327, 27092, 10772, 27611, 44819, 19182, 17911], 70: [], 71: [], 72: [], 73: [], 74: [], 75: [], 76: [], 77: [], 78: [25903, 7374, 18091, 17292, 15438, 45048, 20522, 15648, 19489, 26668, 2842, 23442, 8598, 8764, 32972, 6645, 30672, 9510, 5912, 7634, 19381, 34349, 39018, 16955, 47100, 19381, 9568, 39058, 29518, 22052, 24843, 19643, 45415, 19985, 45048, 42636, 34803, 25522, 769, 10771, 1135, 21928, 46816, 3626, 47918, 16179, 6907, 20551, 19034, 37054, 15718, 24549, 42663, 29043, 48865, 35594, 20474, 33965, 18321, 43630, 38339, 45606, 14886, 43064, 1619, 25489, 28445, 39058, 44962, 14625, 3032, 10582, 5500, 47100, 1037, 49946, 4105, 26668, 12747, 24329, 12879, 33082, 37110, 5912, 24843, 13832, 18075, 1619, 36435, 49541, 29736, 33965, 8527, 2842, 23442, 29155, 7624, 24233, 34530, 30871, 21129, 7327, 11793, 49, 34157, 48990, 34324, 4260, 25646, 23152, 17964, 27443, 47760, 20404, 5538, 32455, 24529, 5476, 46642, 905, 23175, 8570, 14828, 32227, 9510, 12615, 23945, 45816, 32992, 8570, 45816, 32643, 20304, 36516, 5348, 48973, 40, 2630, 33490, 40749, 48341, 27875, 40382, 25489, 25522, 31473, 7570, 12750, 5912, 5214, 7570, 27253, 10446, 36074, 39920, 7482, 905, 48542, 19865, 37208, 12879, 3815, 27875, 13916, 12285, 49946, 43064, 14886, 36870, 18741, 8527, 20404, 5538, 47715, 6932, 35944, 33005, 17998, 33525, 32227, 13101, 42980, 30845, 19489, 4239, 19865, 43936, 41437, 14820, 12268, 31093, 1264, 25646, 21492, 27624, 2058, 1811, 16230, 6059, 17292], 79: [], 80: [], 81: [], 82: [15939, 5454, 1402, 30049, 26976, 36327, 43606, 24921, 10699, 6195, 6570, 32115, 40656, 3555, 21695, 22190, 29103, 20263, 9512, 46394, 43866, 41547, 9637, 44523, 8099, 26471, 32106, 30450, 37510, 21973, 15317, 871, 3507, 20923, 26562, 41221, 45656, 33931, 47306, 5656, 7959, 12408, 25771, 18702, 8253, 10283, 5586, 47534, 32115, 39114, 16897, 5275, 7671, 8518, 26965, 7334, 29478, 25724, 33477, 22970, 11832, 8291, 24715, 28893, 32602, 41968, 40655, 39822, 7415, 14832, 43765, 11600, 3842, 47306, 18702, 14041, 32277, 274, 27035, 11501, 47319, 11084, 44928, 42485, 26501, 6277, 45064, 4381, 46832, 43293, 24024, 35967, 6227, 24852, 13546, 8518, 43293, 10699, 33477, 35336, 41547, 24714, 36437, 22190, 39637, 27707, 3842, 34088, 8727, 13458, 33931, 318, 22110, 14992, 11832, 37521, 41900, 14131, 36194, 33709, 42541, 43453, 9111, 28404, 3842, 32449, 30102, 31775, 8257, 32725, 28101, 23402, 22192, 38486, 24387, 34088, 11084, 625, 44718, 11901, 24024, 18646, 40462, 26852, 26562, 36492, 19830, 26712, 26921, 39228, 42250, 12171, 45373, 43453, 30468, 32847, 25997, 32115, 18644, 12866, 33729, 45757, 1143, 22457, 5586, 49708, 34248, 32602, 35285, 28101, 46832, 4269, 46550, 12380, 3842, 32449, 31443, 29348, 41672, 14557, 46825, 48289, 43830, 16393, 22457, 47306, 23681, 41672, 14557, 40082, 36192, 34911, 26553, 44513, 40656, 27487, 16784, 49368, 16897, 38738], 83: [], 84: [], 85: [], 86: [8802, 45861, 7091, 9550, 32608, 6649, 9567, 3895, 48900, 8754, 4512, 40334, 32608, 18107, 37014, 1186, 22718, 42707, 32789, 46169, 19098, 32929, 26097, 11723, 32665, 6546, 496, 46275, 33101, 5967, 28822, 47493, 46967, 14170, 222, 24906, 45536, 35231, 47494, 18310, 40526, 44778, 32129, 49171, 42290, 4246, 2235, 7608, 25310, 21679, 17240, 46275, 8086, 36135, 19219, 19443, 20778, 22949, 29985, 16344, 29635, 222, 10915, 26719, 34586, 15866, 28302, 5767, 5595, 45686, 22033, 24935, 38753, 22365, 30806, 9776, 44794, 9113, 20778, 27070, 40886, 36065, 33812, 35231, 10299, 1244, 5595, 48900, 34195, 20778, 22721, 14170, 43529, 21691, 13053, 38800, 3795, 18567, 6546, 10837, 8149, 8303, 11537, 21600, 1485, 32219, 36135, 43874, 23394, 46826, 40492, 41507, 22327, 31538, 12066, 23586, 49574, 1312, 16995, 38734, 26748, 41756, 49008, 30674, 22982, 45299, 26989, 48253, 17533, 44356, 37044, 6638, 22721, 40573, 36135, 41941, 6546, 24906, 15672, 42218, 35599, 28822, 5967, 2002, 23496, 5767, 24367, 48810, 45306, 47883, 15721, 4374, 45263, 16995, 9980, 23108, 32129, 49880, 15479, 22721, 34153, 22240, 41129, 11747, 29985, 16344, 15501, 25436, 24875, 46659, 45263, 19443, 45871, 43529, 23034, 24004, 5608, 16656, 43711, 39129, 22982, 12755, 1083, 10915, 21404, 9985, 22689, 22317, 47045, 34591, 22982, 5270, 33812, 33095, 29213, 32789, 23108, 26401, 20852, 7646], 87: [], 88: [], 89: [], 90: [], 91: [], 92: [], 93: [], 94: [], 95: [], 96: [], 97: [14228, 3015, 40327, 25031, 34028, 28463, 22424, 44541, 43193, 5204, 30380, 48494, 44601, 9377, 30855, 3058, 11228, 8673, 9712, 5714, 45500, 5987, 44129, 4058, 48426, 28940, 45262, 42263, 5211, 42903, 41368, 45354, 38414, 38282, 2197, 48617, 6826, 33058, 15542, 8162, 21326, 20999, 45029, 46009, 5142, 32104, 26290, 40556, 23025, 37047, 428, 15960, 45023, 46330, 5211, 12941, 33838, 10275, 13507, 733, 7782, 21924, 13942, 2709, 8417, 38880, 13942, 16693, 45227, 20902, 30709, 41517, 48633, 23332, 15960, 39785, 1733, 18971, 39037, 12890, 5204, 7655, 23104, 3498, 43526, 9280, 33838, 48068, 45867, 13715, 49685, 25821, 17460, 28161, 26771, 15542, 9377, 1168, 25031, 6336, 45326, 29788, 27978, 26637, 42894, 15643, 20564, 8184, 29174, 25332, 14961, 15793, 37032, 45858, 16693, 42286, 39014, 8376, 20373, 5786, 46801, 39885, 44677, 6538, 37424, 2709, 7232, 31118, 32519, 29584, 5184, 8162, 42894, 45023, 46330, 22369, 23332, 14861, 17068, 26423, 15960, 22851, 32991, 4720, 46822, 20073, 25332, 25821, 43706, 29767, 16693, 45858, 47033, 30709, 44415, 19113, 28974, 48530, 21010, 48426, 27385, 48860, 15021, 10453, 28610, 43036, 15960, 45023, 46330, 16054, 37541, 1168, 13270, 3015, 25821, 428, 44473, 28694, 14861, 23332, 46225, 32782, 44415, 28467, 20662, 9712, 42263, 26239, 36498, 18412, 49451, 48426, 5300, 38880, 29788, 9377, 18412, 13500, 12941, 9206], 98: [], 99: []}\n",
            "200\n",
            "1\n",
            "200\n",
            "2\n",
            "200\n",
            "3\n",
            "200\n",
            "4\n",
            "200\n",
            "5\n",
            "200\n",
            "6\n",
            "200\n",
            "7\n",
            "200\n",
            "8\n",
            "200\n",
            "9\n",
            "200\n",
            "10\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "TEST ALL:  0.008\n",
            "BATCH SIZE:  128\n",
            "TRAIN:  4950\n",
            "TRAIN_SET CLASSES:  [95, 59, 58, 34, 81, 49, 13, 88, 68, 60]\n",
            "VALIDATION CLASSES:  [60, 59, 58, 49, 34, 95, 88, 81, 13, 68]\n",
            "GROUP:  2\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [95, 59, 58, 34, 81, 49, 13, 88, 68, 60]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  20\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.5169129371643066\n",
            "Train step - Step 10, Loss 0.1794978231191635\n",
            "Train step - Step 20, Loss 0.16166308522224426\n",
            "Train step - Step 30, Loss 0.14003399014472961\n",
            "Train step - Step 40, Loss 0.132455974817276\n",
            "Train step - Step 50, Loss 0.13445866107940674\n",
            "Train epoch - Accuracy: 0.2837410071942446 Loss: 0.17695465360614035 Corrects: 1972\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.1262195110321045\n",
            "Train step - Step 70, Loss 0.14079701900482178\n",
            "Train step - Step 80, Loss 0.13009501993656158\n",
            "Train step - Step 90, Loss 0.12504111230373383\n",
            "Train step - Step 100, Loss 0.12451650202274323\n",
            "Train epoch - Accuracy: 0.2775539568345324 Loss: 0.13316957963027543 Corrects: 1929\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.12021937221288681\n",
            "Train step - Step 120, Loss 0.12047584354877472\n",
            "Train step - Step 130, Loss 0.12689785659313202\n",
            "Train step - Step 140, Loss 0.11871840804815292\n",
            "Train step - Step 150, Loss 0.11512317508459091\n",
            "Train step - Step 160, Loss 0.13496458530426025\n",
            "Train epoch - Accuracy: 0.26359712230215826 Loss: 0.12545170731252903 Corrects: 1832\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.13225702941417694\n",
            "Train step - Step 180, Loss 0.13049516081809998\n",
            "Train step - Step 190, Loss 0.12073327600955963\n",
            "Train step - Step 200, Loss 0.10889937728643417\n",
            "Train step - Step 210, Loss 0.11702914535999298\n",
            "Train epoch - Accuracy: 0.24158273381294965 Loss: 0.12137579729659952 Corrects: 1679\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.10698866844177246\n",
            "Train step - Step 230, Loss 0.10521842539310455\n",
            "Train step - Step 240, Loss 0.12342648953199387\n",
            "Train step - Step 250, Loss 0.10840962082147598\n",
            "Train step - Step 260, Loss 0.12597358226776123\n",
            "Train step - Step 270, Loss 0.10516782104969025\n",
            "Train epoch - Accuracy: 0.22949640287769785 Loss: 0.117457317718499 Corrects: 1595\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.11198973655700684\n",
            "Train step - Step 290, Loss 0.10322263091802597\n",
            "Train step - Step 300, Loss 0.12633323669433594\n",
            "Train step - Step 310, Loss 0.1138874813914299\n",
            "Train step - Step 320, Loss 0.1287333071231842\n",
            "Train epoch - Accuracy: 0.22100719424460433 Loss: 0.1142330466328765 Corrects: 1536\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.10434374958276749\n",
            "Train step - Step 340, Loss 0.10486646741628647\n",
            "Train step - Step 350, Loss 0.11931981891393661\n",
            "Train step - Step 360, Loss 0.09634315222501755\n",
            "Train step - Step 370, Loss 0.09635066241025925\n",
            "Train step - Step 380, Loss 0.11437930911779404\n",
            "Train epoch - Accuracy: 0.21079136690647482 Loss: 0.1112058031709074 Corrects: 1465\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.10774026066064835\n",
            "Train step - Step 400, Loss 0.10178196430206299\n",
            "Train step - Step 410, Loss 0.10242731869220734\n",
            "Train step - Step 420, Loss 0.10597683489322662\n",
            "Train step - Step 430, Loss 0.1148415356874466\n",
            "Train epoch - Accuracy: 0.19381294964028778 Loss: 0.11010490771677854 Corrects: 1347\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10518502444028854\n",
            "Train step - Step 450, Loss 0.09347856789827347\n",
            "Train step - Step 460, Loss 0.10829170793294907\n",
            "Train step - Step 470, Loss 0.10334551334381104\n",
            "Train step - Step 480, Loss 0.09084976464509964\n",
            "Train step - Step 490, Loss 0.11507163196802139\n",
            "Train epoch - Accuracy: 0.18489208633093526 Loss: 0.10734391739471354 Corrects: 1285\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.11389422416687012\n",
            "Train step - Step 510, Loss 0.11613311618566513\n",
            "Train step - Step 520, Loss 0.10832827538251877\n",
            "Train step - Step 530, Loss 0.11000990867614746\n",
            "Train step - Step 540, Loss 0.10183463245630264\n",
            "Train epoch - Accuracy: 0.1828776978417266 Loss: 0.10670046436700889 Corrects: 1271\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.1008143201470375\n",
            "Train step - Step 560, Loss 0.10375916957855225\n",
            "Train step - Step 570, Loss 0.10971739143133163\n",
            "Train step - Step 580, Loss 0.1148919016122818\n",
            "Train step - Step 590, Loss 0.10323803871870041\n",
            "Train step - Step 600, Loss 0.11059238761663437\n",
            "Train epoch - Accuracy: 0.18359712230215827 Loss: 0.10460780343777842 Corrects: 1276\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.09777440875768661\n",
            "Train step - Step 620, Loss 0.09489089995622635\n",
            "Train step - Step 630, Loss 0.09858405590057373\n",
            "Train step - Step 640, Loss 0.10955583304166794\n",
            "Train step - Step 650, Loss 0.09896030277013779\n",
            "Train epoch - Accuracy: 0.16244604316546762 Loss: 0.10209973865490166 Corrects: 1129\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.09629624336957932\n",
            "Train step - Step 670, Loss 0.10155389457941055\n",
            "Train step - Step 680, Loss 0.09357964247465134\n",
            "Train step - Step 690, Loss 0.09513437002897263\n",
            "Train step - Step 700, Loss 0.10254843533039093\n",
            "Train step - Step 710, Loss 0.11343886703252792\n",
            "Train epoch - Accuracy: 0.16863309352517986 Loss: 0.1008516416897019 Corrects: 1172\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.0989610105752945\n",
            "Train step - Step 730, Loss 0.0919790044426918\n",
            "Train step - Step 740, Loss 0.11217012256383896\n",
            "Train step - Step 750, Loss 0.10047023743391037\n",
            "Train step - Step 760, Loss 0.0936400294303894\n",
            "Train epoch - Accuracy: 0.15856115107913668 Loss: 0.10078979456810643 Corrects: 1102\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10410847514867783\n",
            "Train step - Step 780, Loss 0.11000001430511475\n",
            "Train step - Step 790, Loss 0.09399572759866714\n",
            "Train step - Step 800, Loss 0.09015186131000519\n",
            "Train step - Step 810, Loss 0.098787322640419\n",
            "Train step - Step 820, Loss 0.09814044833183289\n",
            "Train epoch - Accuracy: 0.14848920863309353 Loss: 0.09883154611578948 Corrects: 1032\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.10070514678955078\n",
            "Train step - Step 840, Loss 0.10554162412881851\n",
            "Train step - Step 850, Loss 0.09494831413030624\n",
            "Train step - Step 860, Loss 0.10371863096952438\n",
            "Train step - Step 870, Loss 0.10269935429096222\n",
            "Train epoch - Accuracy: 0.15194244604316548 Loss: 0.09833178096109157 Corrects: 1056\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.10161431133747101\n",
            "Train step - Step 890, Loss 0.11691299825906754\n",
            "Train step - Step 900, Loss 0.0970960259437561\n",
            "Train step - Step 910, Loss 0.09316828846931458\n",
            "Train step - Step 920, Loss 0.09450530260801315\n",
            "Train step - Step 930, Loss 0.0994807705283165\n",
            "Train epoch - Accuracy: 0.14719424460431654 Loss: 0.09777570379723748 Corrects: 1023\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.09775560349225998\n",
            "Train step - Step 950, Loss 0.09982504695653915\n",
            "Train step - Step 960, Loss 0.09317051619291306\n",
            "Train step - Step 970, Loss 0.10379252582788467\n",
            "Train step - Step 980, Loss 0.08702296763658524\n",
            "Train epoch - Accuracy: 0.14316546762589927 Loss: 0.09620258571218243 Corrects: 995\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.09870107471942902\n",
            "Train step - Step 1000, Loss 0.09752699732780457\n",
            "Train step - Step 1010, Loss 0.0884833112359047\n",
            "Train step - Step 1020, Loss 0.09088607132434845\n",
            "Train step - Step 1030, Loss 0.0991656705737114\n",
            "Train step - Step 1040, Loss 0.08863843232393265\n",
            "Train epoch - Accuracy: 0.13640287769784173 Loss: 0.09592893707666465 Corrects: 948\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.0881110206246376\n",
            "Train step - Step 1060, Loss 0.08933417499065399\n",
            "Train step - Step 1070, Loss 0.09645642340183258\n",
            "Train step - Step 1080, Loss 0.08969835191965103\n",
            "Train step - Step 1090, Loss 0.08599284291267395\n",
            "Train epoch - Accuracy: 0.12503597122302157 Loss: 0.09422696393599612 Corrects: 869\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10922358185052872\n",
            "Train step - Step 1110, Loss 0.10175683349370956\n",
            "Train step - Step 1120, Loss 0.0948505774140358\n",
            "Train step - Step 1130, Loss 0.09787240624427795\n",
            "Train step - Step 1140, Loss 0.091356061398983\n",
            "Train step - Step 1150, Loss 0.09869234263896942\n",
            "Train epoch - Accuracy: 0.1264748201438849 Loss: 0.09437090197698675 Corrects: 879\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.08751604706048965\n",
            "Train step - Step 1170, Loss 0.08451661467552185\n",
            "Train step - Step 1180, Loss 0.08826237171888351\n",
            "Train step - Step 1190, Loss 0.0936855599284172\n",
            "Train step - Step 1200, Loss 0.09632109105587006\n",
            "Train epoch - Accuracy: 0.12431654676258992 Loss: 0.09173472809705803 Corrects: 864\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.09393706917762756\n",
            "Train step - Step 1220, Loss 0.09704749286174774\n",
            "Train step - Step 1230, Loss 0.10819929093122482\n",
            "Train step - Step 1240, Loss 0.08329875767230988\n",
            "Train step - Step 1250, Loss 0.10151737928390503\n",
            "Train step - Step 1260, Loss 0.09239458292722702\n",
            "Train epoch - Accuracy: 0.12057553956834532 Loss: 0.09274960513595197 Corrects: 838\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.0983085185289383\n",
            "Train step - Step 1280, Loss 0.09437863528728485\n",
            "Train step - Step 1290, Loss 0.09039434045553207\n",
            "Train step - Step 1300, Loss 0.08300230652093887\n",
            "Train step - Step 1310, Loss 0.0901242345571518\n",
            "Train epoch - Accuracy: 0.11654676258992806 Loss: 0.09132949670227311 Corrects: 810\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.08817211538553238\n",
            "Train step - Step 1330, Loss 0.08540838956832886\n",
            "Train step - Step 1340, Loss 0.08855699747800827\n",
            "Train step - Step 1350, Loss 0.09014149755239487\n",
            "Train step - Step 1360, Loss 0.08721234649419785\n",
            "Train step - Step 1370, Loss 0.09958019852638245\n",
            "Train epoch - Accuracy: 0.11151079136690648 Loss: 0.09102541404019157 Corrects: 775\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.08454780280590057\n",
            "Train step - Step 1390, Loss 0.08916804939508438\n",
            "Train step - Step 1400, Loss 0.09062331169843674\n",
            "Train step - Step 1410, Loss 0.10236795246601105\n",
            "Train step - Step 1420, Loss 0.09290094673633575\n",
            "Train epoch - Accuracy: 0.11237410071942446 Loss: 0.09045583605766297 Corrects: 781\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.08851996809244156\n",
            "Train step - Step 1440, Loss 0.10282810032367706\n",
            "Train step - Step 1450, Loss 0.09084681421518326\n",
            "Train step - Step 1460, Loss 0.09519615024328232\n",
            "Train step - Step 1470, Loss 0.08870359510183334\n",
            "Train step - Step 1480, Loss 0.07995226234197617\n",
            "Train epoch - Accuracy: 0.11194244604316547 Loss: 0.0896648757492038 Corrects: 778\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.08252305537462234\n",
            "Train step - Step 1500, Loss 0.08985878527164459\n",
            "Train step - Step 1510, Loss 0.09538596868515015\n",
            "Train step - Step 1520, Loss 0.08040865510702133\n",
            "Train step - Step 1530, Loss 0.08047179132699966\n",
            "Train epoch - Accuracy: 0.10546762589928058 Loss: 0.08783881713589319 Corrects: 733\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.08295249938964844\n",
            "Train step - Step 1550, Loss 0.10165679454803467\n",
            "Train step - Step 1560, Loss 0.09363297373056412\n",
            "Train step - Step 1570, Loss 0.08829652518033981\n",
            "Train step - Step 1580, Loss 0.088692806661129\n",
            "Train step - Step 1590, Loss 0.08278892189264297\n",
            "Train epoch - Accuracy: 0.10402877697841727 Loss: 0.08752147416202284 Corrects: 723\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.07396811991930008\n",
            "Train step - Step 1610, Loss 0.0920131728053093\n",
            "Train step - Step 1620, Loss 0.08139719814062119\n",
            "Train step - Step 1630, Loss 0.0869646817445755\n",
            "Train step - Step 1640, Loss 0.08734903484582901\n",
            "Train epoch - Accuracy: 0.1037410071942446 Loss: 0.0867833469904584 Corrects: 721\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.09538062661886215\n",
            "Train step - Step 1660, Loss 0.07985400408506393\n",
            "Train step - Step 1670, Loss 0.09103544056415558\n",
            "Train step - Step 1680, Loss 0.08767536282539368\n",
            "Train step - Step 1690, Loss 0.08726824820041656\n",
            "Train step - Step 1700, Loss 0.08239839226007462\n",
            "Train epoch - Accuracy: 0.10158273381294965 Loss: 0.08639097972096299 Corrects: 706\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.09398167580366135\n",
            "Train step - Step 1720, Loss 0.08080913871526718\n",
            "Train step - Step 1730, Loss 0.07937727123498917\n",
            "Train step - Step 1740, Loss 0.0863838717341423\n",
            "Train step - Step 1750, Loss 0.08418970555067062\n",
            "Train epoch - Accuracy: 0.09640287769784173 Loss: 0.08547700777757082 Corrects: 670\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.08026812225580215\n",
            "Train step - Step 1770, Loss 0.08633362501859665\n",
            "Train step - Step 1780, Loss 0.07599715143442154\n",
            "Train step - Step 1790, Loss 0.08064111322164536\n",
            "Train step - Step 1800, Loss 0.07888730615377426\n",
            "Train step - Step 1810, Loss 0.0839565098285675\n",
            "Train epoch - Accuracy: 0.09510791366906475 Loss: 0.08507395505905152 Corrects: 661\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.08588305860757828\n",
            "Train step - Step 1830, Loss 0.07527658343315125\n",
            "Train step - Step 1840, Loss 0.07607199996709824\n",
            "Train step - Step 1850, Loss 0.09147097915410995\n",
            "Train step - Step 1860, Loss 0.08760777115821838\n",
            "Train epoch - Accuracy: 0.09280575539568345 Loss: 0.08493523639955108 Corrects: 645\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.08083228021860123\n",
            "Train step - Step 1880, Loss 0.08906001597642899\n",
            "Train step - Step 1890, Loss 0.08491425216197968\n",
            "Train step - Step 1900, Loss 0.08096807450056076\n",
            "Train step - Step 1910, Loss 0.08666068315505981\n",
            "Train step - Step 1920, Loss 0.0862782821059227\n",
            "Train epoch - Accuracy: 0.08920863309352518 Loss: 0.08365085374108322 Corrects: 620\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.08549874275922775\n",
            "Train step - Step 1940, Loss 0.07824243605136871\n",
            "Train step - Step 1950, Loss 0.08204823732376099\n",
            "Train step - Step 1960, Loss 0.08930621296167374\n",
            "Train step - Step 1970, Loss 0.08722180128097534\n",
            "Train epoch - Accuracy: 0.08863309352517985 Loss: 0.08389588796406341 Corrects: 616\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.08517978340387344\n",
            "Train step - Step 1990, Loss 0.09335388243198395\n",
            "Train step - Step 2000, Loss 0.08695380389690399\n",
            "Train step - Step 2010, Loss 0.0874551385641098\n",
            "Train step - Step 2020, Loss 0.07412367314100266\n",
            "Train step - Step 2030, Loss 0.08444429934024811\n",
            "Train epoch - Accuracy: 0.08388489208633093 Loss: 0.08270936280917779 Corrects: 583\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.08100829273462296\n",
            "Train step - Step 2050, Loss 0.0846361443400383\n",
            "Train step - Step 2060, Loss 0.0884493961930275\n",
            "Train step - Step 2070, Loss 0.08222674578428268\n",
            "Train step - Step 2080, Loss 0.08971428871154785\n",
            "Train epoch - Accuracy: 0.0860431654676259 Loss: 0.08172102706466647 Corrects: 598\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.08107930421829224\n",
            "Train step - Step 2100, Loss 0.08060901612043381\n",
            "Train step - Step 2110, Loss 0.08007434010505676\n",
            "Train step - Step 2120, Loss 0.08763756603002548\n",
            "Train step - Step 2130, Loss 0.08734627813100815\n",
            "Train step - Step 2140, Loss 0.08072298020124435\n",
            "Train epoch - Accuracy: 0.08489208633093526 Loss: 0.0807724004533651 Corrects: 590\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.08518854528665543\n",
            "Train step - Step 2160, Loss 0.0858917236328125\n",
            "Train step - Step 2170, Loss 0.08798142522573471\n",
            "Train step - Step 2180, Loss 0.08547382056713104\n",
            "Train step - Step 2190, Loss 0.08133397251367569\n",
            "Train epoch - Accuracy: 0.08201438848920864 Loss: 0.08133680895721312 Corrects: 570\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.08406609296798706\n",
            "Train step - Step 2210, Loss 0.08009689301252365\n",
            "Train step - Step 2220, Loss 0.08216539025306702\n",
            "Train step - Step 2230, Loss 0.08776502311229706\n",
            "Train step - Step 2240, Loss 0.08211687952280045\n",
            "Train step - Step 2250, Loss 0.0781916007399559\n",
            "Train epoch - Accuracy: 0.07942446043165467 Loss: 0.08063867489425398 Corrects: 552\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.08132542669773102\n",
            "Train step - Step 2270, Loss 0.07479757070541382\n",
            "Train step - Step 2280, Loss 0.08488154411315918\n",
            "Train step - Step 2290, Loss 0.08313656598329544\n",
            "Train step - Step 2300, Loss 0.08964268118143082\n",
            "Train epoch - Accuracy: 0.07755395683453238 Loss: 0.07984745205735132 Corrects: 539\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.08243211358785629\n",
            "Train step - Step 2320, Loss 0.07694833725690842\n",
            "Train step - Step 2330, Loss 0.08129550516605377\n",
            "Train step - Step 2340, Loss 0.07500135153532028\n",
            "Train step - Step 2350, Loss 0.07092083245515823\n",
            "Train step - Step 2360, Loss 0.07530738413333893\n",
            "Train epoch - Accuracy: 0.07237410071942446 Loss: 0.07873118075749858 Corrects: 503\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.0795673131942749\n",
            "Train step - Step 2380, Loss 0.07606883347034454\n",
            "Train step - Step 2390, Loss 0.07952549308538437\n",
            "Train step - Step 2400, Loss 0.0680827721953392\n",
            "Train step - Step 2410, Loss 0.08459560573101044\n",
            "Train epoch - Accuracy: 0.06920863309352518 Loss: 0.07934641047347364 Corrects: 481\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.07273778319358826\n",
            "Train step - Step 2430, Loss 0.07411303371191025\n",
            "Train step - Step 2440, Loss 0.0792054757475853\n",
            "Train step - Step 2450, Loss 0.08311128616333008\n",
            "Train step - Step 2460, Loss 0.07185035198926926\n",
            "Train step - Step 2470, Loss 0.0737348422408104\n",
            "Train epoch - Accuracy: 0.0725179856115108 Loss: 0.0785567745394844 Corrects: 504\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.07861228287220001\n",
            "Train step - Step 2490, Loss 0.07711120694875717\n",
            "Train step - Step 2500, Loss 0.0726538822054863\n",
            "Train step - Step 2510, Loss 0.08646418899297714\n",
            "Train step - Step 2520, Loss 0.07821498066186905\n",
            "Train epoch - Accuracy: 0.07625899280575539 Loss: 0.07885717460363031 Corrects: 530\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.07886496186256409\n",
            "Train step - Step 2540, Loss 0.07086056470870972\n",
            "Train step - Step 2550, Loss 0.07877860963344574\n",
            "Train step - Step 2560, Loss 0.06760316342115402\n",
            "Train step - Step 2570, Loss 0.08225750923156738\n",
            "Train step - Step 2580, Loss 0.07982831448316574\n",
            "Train epoch - Accuracy: 0.0674820143884892 Loss: 0.07841815888881683 Corrects: 469\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.07474390417337418\n",
            "Train step - Step 2600, Loss 0.07206699997186661\n",
            "Train step - Step 2610, Loss 0.07055478543043137\n",
            "Train step - Step 2620, Loss 0.07899026572704315\n",
            "Train step - Step 2630, Loss 0.08115017414093018\n",
            "Train epoch - Accuracy: 0.07467625899280575 Loss: 0.07786008047114173 Corrects: 519\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.07758217304944992\n",
            "Train step - Step 2650, Loss 0.08350258320569992\n",
            "Train step - Step 2660, Loss 0.0829220786690712\n",
            "Train step - Step 2670, Loss 0.07495055347681046\n",
            "Train step - Step 2680, Loss 0.07923857867717743\n",
            "Train step - Step 2690, Loss 0.08937828987836838\n",
            "Train epoch - Accuracy: 0.06633093525179856 Loss: 0.07727293896160538 Corrects: 461\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.07887675613164902\n",
            "Train step - Step 2710, Loss 0.07717050611972809\n",
            "Train step - Step 2720, Loss 0.06882739067077637\n",
            "Train step - Step 2730, Loss 0.07305276393890381\n",
            "Train step - Step 2740, Loss 0.07514818012714386\n",
            "Train epoch - Accuracy: 0.05841726618705036 Loss: 0.07202287237206809 Corrects: 406\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.06605880707502365\n",
            "Train step - Step 2760, Loss 0.06259294599294662\n",
            "Train step - Step 2770, Loss 0.07183237373828888\n",
            "Train step - Step 2780, Loss 0.06675712764263153\n",
            "Train step - Step 2790, Loss 0.06765004247426987\n",
            "Train step - Step 2800, Loss 0.0747472494840622\n",
            "Train epoch - Accuracy: 0.05971223021582734 Loss: 0.06981823421425098 Corrects: 415\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.0797777995467186\n",
            "Train step - Step 2820, Loss 0.06995439529418945\n",
            "Train step - Step 2830, Loss 0.06844279915094376\n",
            "Train step - Step 2840, Loss 0.0727851465344429\n",
            "Train step - Step 2850, Loss 0.06835160404443741\n",
            "Train epoch - Accuracy: 0.05841726618705036 Loss: 0.0690488637244101 Corrects: 406\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.07062752544879913\n",
            "Train step - Step 2870, Loss 0.07280834764242172\n",
            "Train step - Step 2880, Loss 0.07765115052461624\n",
            "Train step - Step 2890, Loss 0.06481962651014328\n",
            "Train step - Step 2900, Loss 0.0630323514342308\n",
            "Train step - Step 2910, Loss 0.06195002421736717\n",
            "Train epoch - Accuracy: 0.05007194244604317 Loss: 0.06876028269958152 Corrects: 348\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.07072674483060837\n",
            "Train step - Step 2930, Loss 0.074212446808815\n",
            "Train step - Step 2940, Loss 0.06530699878931046\n",
            "Train step - Step 2950, Loss 0.0686722844839096\n",
            "Train step - Step 2960, Loss 0.06779594719409943\n",
            "Train epoch - Accuracy: 0.056258992805755394 Loss: 0.06882775747518745 Corrects: 391\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.06589384377002716\n",
            "Train step - Step 2980, Loss 0.07365533709526062\n",
            "Train step - Step 2990, Loss 0.06429743021726608\n",
            "Train step - Step 3000, Loss 0.06548532098531723\n",
            "Train step - Step 3010, Loss 0.06986923515796661\n",
            "Train step - Step 3020, Loss 0.06171783432364464\n",
            "Train epoch - Accuracy: 0.05611510791366906 Loss: 0.06935279019230561 Corrects: 390\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.06446444243192673\n",
            "Train step - Step 3040, Loss 0.06873346120119095\n",
            "Train step - Step 3050, Loss 0.06483802199363708\n",
            "Train step - Step 3060, Loss 0.06599348038434982\n",
            "Train step - Step 3070, Loss 0.06861985474824905\n",
            "Train epoch - Accuracy: 0.05381294964028777 Loss: 0.06822731492116296 Corrects: 374\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.06800650805234909\n",
            "Train step - Step 3090, Loss 0.06980389356613159\n",
            "Train step - Step 3100, Loss 0.06404470652341843\n",
            "Train step - Step 3110, Loss 0.07719389349222183\n",
            "Train step - Step 3120, Loss 0.0654662474989891\n",
            "Train step - Step 3130, Loss 0.06610830873250961\n",
            "Train epoch - Accuracy: 0.0520863309352518 Loss: 0.06816833750378314 Corrects: 362\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.0631096288561821\n",
            "Train step - Step 3150, Loss 0.059805940836668015\n",
            "Train step - Step 3160, Loss 0.07259750366210938\n",
            "Train step - Step 3170, Loss 0.06082116439938545\n",
            "Train step - Step 3180, Loss 0.06894522160291672\n",
            "Train epoch - Accuracy: 0.051798561151079135 Loss: 0.06886877894615956 Corrects: 360\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.06647234410047531\n",
            "Train step - Step 3200, Loss 0.07254188507795334\n",
            "Train step - Step 3210, Loss 0.061022549867630005\n",
            "Train step - Step 3220, Loss 0.06452324241399765\n",
            "Train step - Step 3230, Loss 0.07123272866010666\n",
            "Train step - Step 3240, Loss 0.07190524786710739\n",
            "Train epoch - Accuracy: 0.04949640287769784 Loss: 0.06805043788693792 Corrects: 344\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.0675896480679512\n",
            "Train step - Step 3260, Loss 0.0674496740102768\n",
            "Train step - Step 3270, Loss 0.061857473105192184\n",
            "Train step - Step 3280, Loss 0.07214537262916565\n",
            "Train step - Step 3290, Loss 0.06719888746738434\n",
            "Train epoch - Accuracy: 0.04949640287769784 Loss: 0.06823281018425235 Corrects: 344\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.06918244808912277\n",
            "Train step - Step 3310, Loss 0.06600599735975266\n",
            "Train step - Step 3320, Loss 0.06480152904987335\n",
            "Train step - Step 3330, Loss 0.06125244125723839\n",
            "Train step - Step 3340, Loss 0.059168826788663864\n",
            "Train step - Step 3350, Loss 0.06854484230279922\n",
            "Train epoch - Accuracy: 0.047769784172661874 Loss: 0.06731753536051126 Corrects: 332\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.06623814254999161\n",
            "Train step - Step 3370, Loss 0.0598013661801815\n",
            "Train step - Step 3380, Loss 0.06464039534330368\n",
            "Train step - Step 3390, Loss 0.0662505179643631\n",
            "Train step - Step 3400, Loss 0.07468625158071518\n",
            "Train epoch - Accuracy: 0.050359712230215826 Loss: 0.06747802811346466 Corrects: 350\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.07540533691644669\n",
            "Train step - Step 3420, Loss 0.07395147532224655\n",
            "Train step - Step 3430, Loss 0.06871943920850754\n",
            "Train step - Step 3440, Loss 0.07108788192272186\n",
            "Train step - Step 3450, Loss 0.06165396049618721\n",
            "Train step - Step 3460, Loss 0.0672016590833664\n",
            "Train epoch - Accuracy: 0.05079136690647482 Loss: 0.06702022155197404 Corrects: 353\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.061297036707401276\n",
            "Train step - Step 3480, Loss 0.06717018038034439\n",
            "Train step - Step 3490, Loss 0.06620221585035324\n",
            "Train step - Step 3500, Loss 0.06968938559293747\n",
            "Train step - Step 3510, Loss 0.06374956667423248\n",
            "Train epoch - Accuracy: 0.04705035971223022 Loss: 0.06688344193877076 Corrects: 327\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.05677231773734093\n",
            "Train step - Step 3530, Loss 0.07256203889846802\n",
            "Train step - Step 3540, Loss 0.0681331679224968\n",
            "Train step - Step 3550, Loss 0.061871256679296494\n",
            "Train step - Step 3560, Loss 0.06025359779596329\n",
            "Train step - Step 3570, Loss 0.062132686376571655\n",
            "Train epoch - Accuracy: 0.04287769784172662 Loss: 0.06608642059693233 Corrects: 298\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.06349725276231766\n",
            "Train step - Step 3590, Loss 0.0656849667429924\n",
            "Train step - Step 3600, Loss 0.059410251677036285\n",
            "Train step - Step 3610, Loss 0.06463570892810822\n",
            "Train step - Step 3620, Loss 0.06297802180051804\n",
            "Train epoch - Accuracy: 0.045179856115107914 Loss: 0.06624775357169213 Corrects: 314\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.07246112823486328\n",
            "Train step - Step 3640, Loss 0.06807327270507812\n",
            "Train step - Step 3650, Loss 0.05870264396071434\n",
            "Train step - Step 3660, Loss 0.06492330133914948\n",
            "Train step - Step 3670, Loss 0.0651353970170021\n",
            "Train step - Step 3680, Loss 0.06506582349538803\n",
            "Train epoch - Accuracy: 0.044172661870503595 Loss: 0.06580488484111621 Corrects: 307\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.060247112065553665\n",
            "Train step - Step 3700, Loss 0.07117266207933426\n",
            "Train step - Step 3710, Loss 0.06249907612800598\n",
            "Train step - Step 3720, Loss 0.06437092274427414\n",
            "Train step - Step 3730, Loss 0.05924645811319351\n",
            "Train epoch - Accuracy: 0.04748201438848921 Loss: 0.06622653941670768 Corrects: 330\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.06652815639972687\n",
            "Train step - Step 3750, Loss 0.059764083474874496\n",
            "Train step - Step 3760, Loss 0.06292635202407837\n",
            "Train step - Step 3770, Loss 0.0640794187784195\n",
            "Train step - Step 3780, Loss 0.06525921821594238\n",
            "Train step - Step 3790, Loss 0.06805813312530518\n",
            "Train epoch - Accuracy: 0.044748201438848924 Loss: 0.06567027835537204 Corrects: 311\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.06839664280414581\n",
            "Train step - Step 3810, Loss 0.0688851997256279\n",
            "Train step - Step 3820, Loss 0.060371752828359604\n",
            "Train step - Step 3830, Loss 0.06202505901455879\n",
            "Train step - Step 3840, Loss 0.06287922710180283\n",
            "Train epoch - Accuracy: 0.048345323741007196 Loss: 0.0660804501904858 Corrects: 336\n",
            "Training finished in 464.16894483566284 seconds\n",
            "reducing exemplars for each class\n",
            "[27, 86, 82, 78, 50, 30, 97, 69, 57, 25, 95, 59, 58, 34, 81, 49, 13, 88, 68, 60]\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9db5cd0>\n",
            "Constructing exemplars of class 95\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [14643, 44254, 23702, 44719, 27030, 43493, 16793, 27500, 3191, 37649, 31424, 31583, 11454, 29194, 46072, 42426, 33883, 33380, 30975, 47545, 4219, 22610, 14251, 9868, 41872, 26558, 31116, 8766, 6813, 33380, 17656, 48719, 47381, 7020, 45225, 16040, 2922, 22610, 16946, 18784, 32186, 31984, 14767, 35063, 40988, 14489, 5557, 27561, 2731, 49406, 44254, 34580, 47545, 6250, 24586, 22169, 9422, 48708, 39304, 38669, 14821, 30429, 30877, 15488, 6646, 36160, 48719, 30463, 7793, 44045, 8297, 14489, 38297, 8739, 24608, 8935, 33121, 16337, 11673, 41872, 26438, 46855, 24878, 15016, 18669, 5648, 934, 16946, 17811, 14626, 46834, 7020, 6236, 25851, 29648, 28993, 23833, 37953, 19756, 9422]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9e9d650>\n",
            "Constructing exemplars of class 59\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [5848, 38957, 15989, 28212, 1378, 41434, 22866, 10664, 28878, 1523, 43534, 47614, 31674, 30817, 1571, 48148, 37676, 16650, 6100, 43682, 10020, 31589, 19183, 36387, 19474, 22751, 5848, 14956, 17831, 31097, 19067, 11187, 21, 43884, 28590, 37987, 25012, 26430, 30883, 687, 17404, 20491, 32022, 4010, 27259, 37490, 48441, 30817, 45723, 34455, 39641, 41307, 46059, 20713, 37228, 26636, 19335, 3165, 31589, 9026, 32112, 6858, 29367, 45448, 14758, 567, 17943, 19136, 41971, 24021, 45339, 17404, 43758, 26560, 4649, 11880, 41436, 47268, 32614, 13005, 35190, 27824, 24021, 13328, 28590, 25774, 406, 28724, 982, 1844, 16738, 45267, 26213, 12113, 16608, 13338, 37490, 39641, 18865, 8241]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9d41810>\n",
            "Constructing exemplars of class 58\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [12764, 30617, 21295, 41401, 29246, 21351, 11103, 1868, 23894, 48857, 42277, 32860, 3652, 39877, 12305, 49301, 1839, 34611, 21533, 40991, 22875, 29724, 16973, 6171, 19968, 42183, 5897, 2396, 25532, 27395, 31692, 21533, 27347, 7843, 7222, 6476, 18653, 5889, 11071, 42790, 5971, 7008, 40195, 47046, 46301, 10537, 26996, 13281, 5010, 33028, 30648, 42183, 37320, 8635, 26545, 3051, 26227, 34033, 30082, 11071, 36887, 49947, 5051, 48294, 38718, 7222, 12219, 47046, 3337, 26227, 43110, 47840, 6915, 44175, 30142, 15139, 16529, 2436, 38795, 36494, 17129, 31436, 1839, 39148, 404, 13495, 33727, 35858, 31692, 9675, 6770, 19550, 1364, 5998, 46301, 7222, 40195, 27939, 38718, 30699]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9df0350>\n",
            "Constructing exemplars of class 34\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [15265, 3111, 2382, 12459, 18623, 21349, 17839, 6188, 5064, 14559, 43022, 10770, 36177, 42407, 43683, 22089, 35397, 20606, 41290, 20724, 43196, 36973, 49348, 26432, 39601, 33735, 264, 35462, 16573, 48505, 28672, 43149, 18623, 23405, 10016, 17841, 5287, 25510, 24459, 8555, 26238, 12228, 15972, 45766, 4901, 49619, 911, 44537, 17581, 13758, 31323, 45666, 20338, 6338, 14596, 49166, 2382, 11646, 26172, 31068, 13748, 38168, 1550, 8643, 41528, 1969, 14310, 25062, 27238, 36789, 40789, 2927, 18308, 43805, 33411, 30789, 7399, 22920, 4901, 27028, 25116, 24600, 24376, 3515, 23884, 31864, 28339, 22132, 39101, 29853, 31755, 34964, 49796, 49111, 16183, 1447, 34859, 44357, 21072, 21205]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9eaa490>\n",
            "Constructing exemplars of class 81\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [675, 30920, 17046, 45901, 36296, 28913, 39292, 43513, 8321, 34460, 22582, 36724, 2841, 16336, 20470, 49309, 28906, 29807, 31782, 8276, 22393, 32821, 33414, 6209, 16877, 35460, 7533, 654, 42926, 7370, 10042, 40161, 46697, 1481, 3802, 19383, 16078, 19993, 32141, 22966, 28607, 6258, 20226, 47627, 48255, 49102, 25894, 10584, 40920, 13364, 48196, 19109, 23171, 654, 47568, 31466, 3802, 20185, 42358, 12246, 6391, 6214, 34515, 14690, 11885, 15149, 40904, 20487, 14144, 22512, 12751, 25283, 7373, 31782, 31466, 503, 29231, 4954, 32109, 20470, 16857, 37372, 49232, 4954, 47807, 38982, 46697, 40369, 24481, 36296, 41277, 47237, 48948, 37364, 40881, 34460, 7370, 13258, 34407, 31180]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7ea47f510>\n",
            "Constructing exemplars of class 49\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [17776, 5908, 38731, 7209, 42642, 25633, 15958, 7188, 1542, 7587, 27454, 39798, 40289, 6898, 25127, 30826, 3638, 38938, 14823, 49922, 11442, 36643, 31578, 39991, 42740, 8267, 41037, 32135, 16326, 20238, 8025, 12722, 7785, 33744, 29350, 45760, 14804, 2862, 16075, 23169, 43512, 29567, 48776, 49922, 13215, 34131, 46075, 23342, 8515, 35518, 23528, 12189, 33433, 10055, 35615, 47461, 19316, 4974, 46508, 16676, 25656, 19394, 40289, 42122, 14246, 43738, 38105, 28691, 49274, 34801, 6245, 5529, 19957, 44771, 47029, 19257, 2612, 18996, 1298, 14018, 17576, 31721, 44670, 47971, 43512, 23219, 29051, 16421, 1298, 43738, 19676, 515, 24778, 33106, 36235, 5471, 17241, 49972, 23821, 44525]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7ea459910>\n",
            "Constructing exemplars of class 13\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [42506, 10399, 3528, 44698, 32033, 44299, 36022, 18290, 31344, 38427, 24649, 36686, 34996, 2654, 14025, 39749, 1779, 4206, 28213, 41913, 27555, 35365, 1176, 46705, 3803, 49597, 35023, 5147, 41990, 39787, 49162, 24621, 35201, 43951, 31874, 35534, 17651, 47182, 47612, 30503, 44296, 27991, 34423, 25254, 40392, 14652, 39773, 38523, 12379, 45608, 30345, 2696, 35821, 5942, 12900, 23834, 38427, 7411, 28334, 27299, 17848, 44289, 23501, 4157, 45070, 28971, 35023, 22187, 7899, 27663, 40220, 25382, 26670, 42532, 35032, 427, 24313, 4157, 45834, 20536, 34252, 36884, 39026, 3908, 23158, 18976, 8090, 13306, 28819, 36005, 23978, 44060, 44944, 34241, 22478, 1907, 39012, 43019, 44490, 8600]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9e02bd0>\n",
            "Constructing exemplars of class 88\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [4664, 21093, 26229, 32671, 44050, 44486, 47935, 36411, 29298, 36952, 28078, 39998, 49844, 20369, 49713, 49917, 32238, 49743, 30430, 11390, 41775, 11746, 27300, 41745, 35402, 44050, 45709, 40194, 23352, 44286, 410, 9375, 44099, 29013, 4614, 10636, 23640, 25453, 24776, 24034, 45844, 19558, 40591, 32448, 26564, 18283, 19775, 42247, 36198, 10024, 27540, 13746, 8731, 26046, 6288, 49656, 21128, 47879, 7594, 17553, 26847, 37230, 20222, 25403, 18283, 48664, 6909, 10286, 27716, 23220, 21316, 11935, 3992, 31713, 25840, 36198, 10708, 15986, 47259, 30113, 19908, 10708, 10554, 41108, 11288, 49459, 337, 31666, 27540, 21473, 9392, 18961, 22505, 49713, 49375, 26774, 49774, 40743, 37604, 26408]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9e029d0>\n",
            "Constructing exemplars of class 68\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [42431, 45714, 20014, 38288, 27005, 10673, 37234, 44438, 21958, 20659, 18381, 32843, 19149, 12331, 25540, 40435, 36703, 23007, 5121, 43333, 8951, 9294, 20014, 14706, 24365, 36010, 932, 9196, 2670, 47799, 12521, 11854, 39000, 34589, 4928, 46591, 8864, 1824, 26658, 10867, 932, 42388, 7421, 42387, 17003, 38104, 20325, 48034, 23007, 17060, 19149, 7817, 36879, 24727, 44939, 21392, 13757, 31919, 22188, 49464, 36856, 17753, 27005, 15022, 30695, 15345, 454, 5462, 38213, 43778, 41441, 27702, 48206, 25987, 39734, 10969, 23877, 12172, 37246, 1855, 26591, 39811, 5934, 11409, 30363, 7073, 39234, 895, 24153, 28655, 43955, 44988, 16762, 2330, 9196, 32628, 40105, 35164, 25904, 23471]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7ff7e9e9e7d0>\n",
            "Constructing exemplars of class 60\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [321, 17543, 6478, 41703, 18047, 13031, 39723, 18511, 43982, 14970, 1516, 19982, 10599, 18511, 11404, 44517, 24169, 7814, 38025, 39241, 997, 33454, 3084, 45351, 28612, 14771, 7227, 3645, 17902, 47021, 24310, 48954, 47221, 2094, 18719, 38625, 30571, 49518, 45351, 33297, 34100, 34697, 38947, 28612, 32193, 18047, 47765, 4917, 14350, 49419, 25736, 17069, 24903, 20729, 34786, 28035, 12902, 7417, 13625, 11173, 23484, 36308, 34687, 49427, 49616, 6292, 36751, 29479, 359, 10312, 19466, 48098, 32347, 1909, 34437, 19978, 3388, 23398, 35697, 33454, 27545, 43007, 38277, 49419, 25648, 12030, 39010, 36308, 10235, 26452, 18380, 17327, 15183, 28818, 49583, 28658, 39010, 5667, 25545, 44305]\n",
            "40\n",
            "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 13: [42506, 10399, 3528, 44698, 32033, 44299, 36022, 18290, 31344, 38427, 24649, 36686, 34996, 2654, 14025, 39749, 1779, 4206, 28213, 41913, 27555, 35365, 1176, 46705, 3803, 49597, 35023, 5147, 41990, 39787, 49162, 24621, 35201, 43951, 31874, 35534, 17651, 47182, 47612, 30503, 44296, 27991, 34423, 25254, 40392, 14652, 39773, 38523, 12379, 45608, 30345, 2696, 35821, 5942, 12900, 23834, 38427, 7411, 28334, 27299, 17848, 44289, 23501, 4157, 45070, 28971, 35023, 22187, 7899, 27663, 40220, 25382, 26670, 42532, 35032, 427, 24313, 4157, 45834, 20536, 34252, 36884, 39026, 3908, 23158, 18976, 8090, 13306, 28819, 36005, 23978, 44060, 44944, 34241, 22478, 1907, 39012, 43019, 44490, 8600], 14: [], 15: [], 16: [], 17: [], 18: [], 19: [], 20: [], 21: [], 22: [], 23: [], 24: [], 25: [44456, 35027, 44474, 46061, 6364, 22202, 16485, 34618, 19860, 40620, 36823, 9733, 20613, 7450, 34618, 35188, 14997, 7077, 44585, 37566, 38074, 38788, 36454, 5317, 8468, 1926, 6942, 34978, 8168, 24309, 11859, 42649, 24158, 8468, 8350, 450, 6233, 41249, 5373, 36174, 6589, 33231, 33303, 9695, 44757, 35163, 32700, 40614, 8468, 5420, 49824, 23243, 25546, 4486, 17325, 44689, 33866, 3651, 32700, 19140, 40472, 49718, 46673, 27616, 1471, 20170, 29580, 33303, 7967, 23320, 19717, 49824, 21005, 4565, 9458, 44739, 40620, 29942, 5664, 21192, 8619, 16485, 46713, 30530, 23031, 12817, 37072, 18489, 14997, 7405, 12817, 27047, 688, 3562, 18166, 19623, 19995, 37590, 30925, 36316], 26: [], 27: [20952, 153, 37760, 21305, 34276, 5396, 34577, 778, 47901, 35233, 14891, 42200, 29181, 32806, 1285, 31719, 1538, 28338, 47985, 42910, 21589, 391, 678, 48388, 40216, 19682, 44683, 21583, 25620, 22112, 35253, 29158, 46544, 13787, 17559, 17728, 5253, 19478, 5770, 23799, 8163, 48445, 23799, 20987, 43902, 5434, 11025, 8143, 12953, 9823, 24841, 4296, 37761, 25281, 10410, 49948, 34459, 7593, 9467, 33141, 32474, 1631, 45634, 12553, 5434, 13966, 34887, 30800, 39919, 17530, 42764, 48733, 23132, 778, 34577, 47901, 30848, 29890, 36963, 47928, 45895, 32668, 39659, 12072, 9823, 2526, 20952, 33013, 15502, 15181, 18362, 11177, 10865, 44683, 27286, 9055, 30848, 17566, 39439, 662], 28: [], 29: [], 30: [13267, 40253, 23642, 42023, 43702, 44401, 34573, 49204, 42144, 30440, 39110, 34360, 35567, 7451, 45646, 9585, 47038, 7745, 12797, 1019, 505, 33504, 7413, 33571, 12463, 22882, 11290, 18472, 44328, 18116, 17858, 29720, 39662, 33540, 31757, 36716, 41677, 1214, 12659, 49248, 13670, 36358, 34766, 40831, 30360, 7103, 3024, 11290, 29902, 13670, 12463, 3297, 17433, 22415, 44219, 3268, 24104, 44883, 1522, 10671, 27846, 32574, 48612, 46603, 5915, 18698, 48910, 3744, 42312, 41019, 35567, 6568, 22415, 22882, 46979, 8601, 27853, 4057, 12797, 23881, 19509, 43796, 1711, 20163, 10403, 40606, 19962, 22882, 3959, 16711, 48114, 32520, 44333, 43605, 15018, 30490, 28222, 5460, 26581, 37101], 31: [], 32: [], 33: [], 34: [15265, 3111, 2382, 12459, 18623, 21349, 17839, 6188, 5064, 14559, 43022, 10770, 36177, 42407, 43683, 22089, 35397, 20606, 41290, 20724, 43196, 36973, 49348, 26432, 39601, 33735, 264, 35462, 16573, 48505, 28672, 43149, 18623, 23405, 10016, 17841, 5287, 25510, 24459, 8555, 26238, 12228, 15972, 45766, 4901, 49619, 911, 44537, 17581, 13758, 31323, 45666, 20338, 6338, 14596, 49166, 2382, 11646, 26172, 31068, 13748, 38168, 1550, 8643, 41528, 1969, 14310, 25062, 27238, 36789, 40789, 2927, 18308, 43805, 33411, 30789, 7399, 22920, 4901, 27028, 25116, 24600, 24376, 3515, 23884, 31864, 28339, 22132, 39101, 29853, 31755, 34964, 49796, 49111, 16183, 1447, 34859, 44357, 21072, 21205], 35: [], 36: [], 37: [], 38: [], 39: [], 40: [], 41: [], 42: [], 43: [], 44: [], 45: [], 46: [], 47: [], 48: [], 49: [17776, 5908, 38731, 7209, 42642, 25633, 15958, 7188, 1542, 7587, 27454, 39798, 40289, 6898, 25127, 30826, 3638, 38938, 14823, 49922, 11442, 36643, 31578, 39991, 42740, 8267, 41037, 32135, 16326, 20238, 8025, 12722, 7785, 33744, 29350, 45760, 14804, 2862, 16075, 23169, 43512, 29567, 48776, 49922, 13215, 34131, 46075, 23342, 8515, 35518, 23528, 12189, 33433, 10055, 35615, 47461, 19316, 4974, 46508, 16676, 25656, 19394, 40289, 42122, 14246, 43738, 38105, 28691, 49274, 34801, 6245, 5529, 19957, 44771, 47029, 19257, 2612, 18996, 1298, 14018, 17576, 31721, 44670, 47971, 43512, 23219, 29051, 16421, 1298, 43738, 19676, 515, 24778, 33106, 36235, 5471, 17241, 49972, 23821, 44525], 50: [17690, 12381, 33345, 49644, 47500, 19649, 8644, 37000, 8609, 24808, 34444, 17857, 14070, 5220, 23929, 24682, 2063, 42627, 12332, 11964, 5923, 21468, 43472, 38076, 21259, 43261, 4682, 18182, 42824, 11475, 39986, 15697, 41217, 10736, 45855, 6677, 9989, 9001, 18864, 32273, 17925, 28621, 35986, 31545, 10432, 9354, 4892, 9081, 10241, 40546, 26392, 1330, 49242, 8030, 30721, 20080, 34148, 8703, 1330, 32917, 32586, 16545, 4236, 18525, 42051, 38256, 48951, 39031, 22479, 39816, 5927, 28002, 21147, 16545, 36110, 2504, 25798, 45042, 44227, 44023, 13702, 28739, 39430, 23755, 1698, 4368, 17335, 18402, 38173, 32917, 12607, 14319, 25418, 32126, 23294, 8159, 14133, 48140, 1330, 44476], 51: [], 52: [], 53: [], 54: [], 55: [], 56: [], 57: [2768, 22429, 49263, 38372, 18747, 19979, 47470, 2683, 10610, 2661, 46528, 4539, 5693, 47383, 49034, 49750, 43752, 24254, 528, 6723, 23525, 47615, 18595, 20064, 34429, 1501, 31005, 24623, 23240, 22360, 20618, 15920, 38939, 17529, 41313, 39466, 2112, 33657, 25710, 9237, 2023, 9596, 7269, 3560, 42194, 46797, 1618, 6066, 18595, 31623, 19134, 10193, 30607, 5006, 34969, 37407, 28370, 7855, 40142, 34294, 283, 37073, 26372, 8773, 9651, 48001, 20064, 7838, 19555, 4879, 17861, 23264, 43877, 42738, 26755, 48777, 7316, 22429, 7958, 17806, 16512, 3683, 35761, 2087, 27428, 25893, 23915, 3683, 48099, 26481, 25710, 3399, 26794, 19822, 528, 33137, 41799, 40950, 46558, 43067], 58: [12764, 30617, 21295, 41401, 29246, 21351, 11103, 1868, 23894, 48857, 42277, 32860, 3652, 39877, 12305, 49301, 1839, 34611, 21533, 40991, 22875, 29724, 16973, 6171, 19968, 42183, 5897, 2396, 25532, 27395, 31692, 21533, 27347, 7843, 7222, 6476, 18653, 5889, 11071, 42790, 5971, 7008, 40195, 47046, 46301, 10537, 26996, 13281, 5010, 33028, 30648, 42183, 37320, 8635, 26545, 3051, 26227, 34033, 30082, 11071, 36887, 49947, 5051, 48294, 38718, 7222, 12219, 47046, 3337, 26227, 43110, 47840, 6915, 44175, 30142, 15139, 16529, 2436, 38795, 36494, 17129, 31436, 1839, 39148, 404, 13495, 33727, 35858, 31692, 9675, 6770, 19550, 1364, 5998, 46301, 7222, 40195, 27939, 38718, 30699], 59: [5848, 38957, 15989, 28212, 1378, 41434, 22866, 10664, 28878, 1523, 43534, 47614, 31674, 30817, 1571, 48148, 37676, 16650, 6100, 43682, 10020, 31589, 19183, 36387, 19474, 22751, 5848, 14956, 17831, 31097, 19067, 11187, 21, 43884, 28590, 37987, 25012, 26430, 30883, 687, 17404, 20491, 32022, 4010, 27259, 37490, 48441, 30817, 45723, 34455, 39641, 41307, 46059, 20713, 37228, 26636, 19335, 3165, 31589, 9026, 32112, 6858, 29367, 45448, 14758, 567, 17943, 19136, 41971, 24021, 45339, 17404, 43758, 26560, 4649, 11880, 41436, 47268, 32614, 13005, 35190, 27824, 24021, 13328, 28590, 25774, 406, 28724, 982, 1844, 16738, 45267, 26213, 12113, 16608, 13338, 37490, 39641, 18865, 8241], 60: [321, 17543, 6478, 41703, 18047, 13031, 39723, 18511, 43982, 14970, 1516, 19982, 10599, 18511, 11404, 44517, 24169, 7814, 38025, 39241, 997, 33454, 3084, 45351, 28612, 14771, 7227, 3645, 17902, 47021, 24310, 48954, 47221, 2094, 18719, 38625, 30571, 49518, 45351, 33297, 34100, 34697, 38947, 28612, 32193, 18047, 47765, 4917, 14350, 49419, 25736, 17069, 24903, 20729, 34786, 28035, 12902, 7417, 13625, 11173, 23484, 36308, 34687, 49427, 49616, 6292, 36751, 29479, 359, 10312, 19466, 48098, 32347, 1909, 34437, 19978, 3388, 23398, 35697, 33454, 27545, 43007, 38277, 49419, 25648, 12030, 39010, 36308, 10235, 26452, 18380, 17327, 15183, 28818, 49583, 28658, 39010, 5667, 25545, 44305], 61: [], 62: [], 63: [], 64: [], 65: [], 66: [], 67: [], 68: [42431, 45714, 20014, 38288, 27005, 10673, 37234, 44438, 21958, 20659, 18381, 32843, 19149, 12331, 25540, 40435, 36703, 23007, 5121, 43333, 8951, 9294, 20014, 14706, 24365, 36010, 932, 9196, 2670, 47799, 12521, 11854, 39000, 34589, 4928, 46591, 8864, 1824, 26658, 10867, 932, 42388, 7421, 42387, 17003, 38104, 20325, 48034, 23007, 17060, 19149, 7817, 36879, 24727, 44939, 21392, 13757, 31919, 22188, 49464, 36856, 17753, 27005, 15022, 30695, 15345, 454, 5462, 38213, 43778, 41441, 27702, 48206, 25987, 39734, 10969, 23877, 12172, 37246, 1855, 26591, 39811, 5934, 11409, 30363, 7073, 39234, 895, 24153, 28655, 43955, 44988, 16762, 2330, 9196, 32628, 40105, 35164, 25904, 23471], 69: [9182, 45614, 40639, 32437, 8588, 14892, 21550, 13946, 12450, 49700, 13506, 39487, 47429, 14071, 5818, 9479, 2931, 48619, 30857, 11180, 37313, 14140, 46493, 14654, 199, 44855, 4612, 1677, 42696, 38134, 6461, 15661, 37886, 21927, 29570, 44819, 12450, 45811, 2152, 6840, 36040, 38162, 17010, 12829, 41963, 16084, 3648, 20291, 46042, 34343, 33841, 23396, 2931, 27877, 7510, 11143, 17311, 12365, 33514, 36040, 20301, 13960, 45550, 41584, 49598, 6572, 23396, 20754, 28563, 19162, 24676, 11143, 2931, 14354, 39962, 15661, 10189, 29665, 40223, 39450, 14220, 9592, 10607, 4273, 6521, 13875, 35343, 23730, 9148, 28271, 31446, 41584, 25881, 49700, 12043, 12365, 36625, 12527, 42127, 25664], 70: [], 71: [], 72: [], 73: [], 74: [], 75: [], 76: [], 77: [], 78: [25903, 7374, 18091, 17292, 15438, 45048, 20522, 15648, 19489, 26668, 2842, 23442, 8598, 8764, 32972, 6645, 30672, 9510, 5912, 7634, 19381, 34349, 39018, 16955, 47100, 19381, 9568, 39058, 29518, 22052, 24843, 19643, 45415, 19985, 45048, 42636, 34803, 25522, 769, 10771, 1135, 21928, 46816, 3626, 47918, 16179, 6907, 20551, 19034, 37054, 15718, 24549, 42663, 29043, 48865, 35594, 20474, 33965, 18321, 43630, 38339, 45606, 14886, 43064, 1619, 25489, 28445, 39058, 44962, 14625, 3032, 10582, 5500, 47100, 1037, 49946, 4105, 26668, 12747, 24329, 12879, 33082, 37110, 5912, 24843, 13832, 18075, 1619, 36435, 49541, 29736, 33965, 8527, 2842, 23442, 29155, 7624, 24233, 34530, 30871], 79: [], 80: [], 81: [675, 30920, 17046, 45901, 36296, 28913, 39292, 43513, 8321, 34460, 22582, 36724, 2841, 16336, 20470, 49309, 28906, 29807, 31782, 8276, 22393, 32821, 33414, 6209, 16877, 35460, 7533, 654, 42926, 7370, 10042, 40161, 46697, 1481, 3802, 19383, 16078, 19993, 32141, 22966, 28607, 6258, 20226, 47627, 48255, 49102, 25894, 10584, 40920, 13364, 48196, 19109, 23171, 654, 47568, 31466, 3802, 20185, 42358, 12246, 6391, 6214, 34515, 14690, 11885, 15149, 40904, 20487, 14144, 22512, 12751, 25283, 7373, 31782, 31466, 503, 29231, 4954, 32109, 20470, 16857, 37372, 49232, 4954, 47807, 38982, 46697, 40369, 24481, 36296, 41277, 47237, 48948, 37364, 40881, 34460, 7370, 13258, 34407, 31180], 82: [15939, 5454, 1402, 30049, 26976, 36327, 43606, 24921, 10699, 6195, 6570, 32115, 40656, 3555, 21695, 22190, 29103, 20263, 9512, 46394, 43866, 41547, 9637, 44523, 8099, 26471, 32106, 30450, 37510, 21973, 15317, 871, 3507, 20923, 26562, 41221, 45656, 33931, 47306, 5656, 7959, 12408, 25771, 18702, 8253, 10283, 5586, 47534, 32115, 39114, 16897, 5275, 7671, 8518, 26965, 7334, 29478, 25724, 33477, 22970, 11832, 8291, 24715, 28893, 32602, 41968, 40655, 39822, 7415, 14832, 43765, 11600, 3842, 47306, 18702, 14041, 32277, 274, 27035, 11501, 47319, 11084, 44928, 42485, 26501, 6277, 45064, 4381, 46832, 43293, 24024, 35967, 6227, 24852, 13546, 8518, 43293, 10699, 33477, 35336], 83: [], 84: [], 85: [], 86: [8802, 45861, 7091, 9550, 32608, 6649, 9567, 3895, 48900, 8754, 4512, 40334, 32608, 18107, 37014, 1186, 22718, 42707, 32789, 46169, 19098, 32929, 26097, 11723, 32665, 6546, 496, 46275, 33101, 5967, 28822, 47493, 46967, 14170, 222, 24906, 45536, 35231, 47494, 18310, 40526, 44778, 32129, 49171, 42290, 4246, 2235, 7608, 25310, 21679, 17240, 46275, 8086, 36135, 19219, 19443, 20778, 22949, 29985, 16344, 29635, 222, 10915, 26719, 34586, 15866, 28302, 5767, 5595, 45686, 22033, 24935, 38753, 22365, 30806, 9776, 44794, 9113, 20778, 27070, 40886, 36065, 33812, 35231, 10299, 1244, 5595, 48900, 34195, 20778, 22721, 14170, 43529, 21691, 13053, 38800, 3795, 18567, 6546, 10837], 87: [], 88: [4664, 21093, 26229, 32671, 44050, 44486, 47935, 36411, 29298, 36952, 28078, 39998, 49844, 20369, 49713, 49917, 32238, 49743, 30430, 11390, 41775, 11746, 27300, 41745, 35402, 44050, 45709, 40194, 23352, 44286, 410, 9375, 44099, 29013, 4614, 10636, 23640, 25453, 24776, 24034, 45844, 19558, 40591, 32448, 26564, 18283, 19775, 42247, 36198, 10024, 27540, 13746, 8731, 26046, 6288, 49656, 21128, 47879, 7594, 17553, 26847, 37230, 20222, 25403, 18283, 48664, 6909, 10286, 27716, 23220, 21316, 11935, 3992, 31713, 25840, 36198, 10708, 15986, 47259, 30113, 19908, 10708, 10554, 41108, 11288, 49459, 337, 31666, 27540, 21473, 9392, 18961, 22505, 49713, 49375, 26774, 49774, 40743, 37604, 26408], 89: [], 90: [], 91: [], 92: [], 93: [], 94: [], 95: [14643, 44254, 23702, 44719, 27030, 43493, 16793, 27500, 3191, 37649, 31424, 31583, 11454, 29194, 46072, 42426, 33883, 33380, 30975, 47545, 4219, 22610, 14251, 9868, 41872, 26558, 31116, 8766, 6813, 33380, 17656, 48719, 47381, 7020, 45225, 16040, 2922, 22610, 16946, 18784, 32186, 31984, 14767, 35063, 40988, 14489, 5557, 27561, 2731, 49406, 44254, 34580, 47545, 6250, 24586, 22169, 9422, 48708, 39304, 38669, 14821, 30429, 30877, 15488, 6646, 36160, 48719, 30463, 7793, 44045, 8297, 14489, 38297, 8739, 24608, 8935, 33121, 16337, 11673, 41872, 26438, 46855, 24878, 15016, 18669, 5648, 934, 16946, 17811, 14626, 46834, 7020, 6236, 25851, 29648, 28993, 23833, 37953, 19756, 9422], 96: [], 97: [14228, 3015, 40327, 25031, 34028, 28463, 22424, 44541, 43193, 5204, 30380, 48494, 44601, 9377, 30855, 3058, 11228, 8673, 9712, 5714, 45500, 5987, 44129, 4058, 48426, 28940, 45262, 42263, 5211, 42903, 41368, 45354, 38414, 38282, 2197, 48617, 6826, 33058, 15542, 8162, 21326, 20999, 45029, 46009, 5142, 32104, 26290, 40556, 23025, 37047, 428, 15960, 45023, 46330, 5211, 12941, 33838, 10275, 13507, 733, 7782, 21924, 13942, 2709, 8417, 38880, 13942, 16693, 45227, 20902, 30709, 41517, 48633, 23332, 15960, 39785, 1733, 18971, 39037, 12890, 5204, 7655, 23104, 3498, 43526, 9280, 33838, 48068, 45867, 13715, 49685, 25821, 17460, 28161, 26771, 15542, 9377, 1168, 25031, 6336], 98: [], 99: []}\n",
            "100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "100\n",
            "2\n",
            "100\n",
            "3\n",
            "100\n",
            "4\n",
            "100\n",
            "5\n",
            "100\n",
            "6\n",
            "100\n",
            "7\n",
            "100\n",
            "8\n",
            "100\n",
            "9\n",
            "100\n",
            "10\n",
            "100\n",
            "11\n",
            "100\n",
            "12\n",
            "100\n",
            "13\n",
            "100\n",
            "14\n",
            "100\n",
            "15\n",
            "100\n",
            "16\n",
            "100\n",
            "17\n",
            "100\n",
            "18\n",
            "100\n",
            "19\n",
            "100\n",
            "20\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "40\n",
            "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 13: [42506, 10399, 3528, 44698, 32033, 44299, 36022, 18290, 31344, 38427, 24649, 36686, 34996, 2654, 14025, 39749, 1779, 4206, 28213, 41913, 27555, 35365, 1176, 46705, 3803, 49597, 35023, 5147, 41990, 39787, 49162, 24621, 35201, 43951, 31874, 35534, 17651, 47182, 47612, 30503, 44296, 27991, 34423, 25254, 40392, 14652, 39773, 38523, 12379, 45608, 30345, 2696, 35821, 5942, 12900, 23834, 38427, 7411, 28334, 27299, 17848, 44289, 23501, 4157, 45070, 28971, 35023, 22187, 7899, 27663, 40220, 25382, 26670, 42532, 35032, 427, 24313, 4157, 45834, 20536, 34252, 36884, 39026, 3908, 23158, 18976, 8090, 13306, 28819, 36005, 23978, 44060, 44944, 34241, 22478, 1907, 39012, 43019, 44490, 8600], 14: [], 15: [], 16: [], 17: [], 18: [], 19: [], 20: [], 21: [], 22: [], 23: [], 24: [], 25: [44456, 35027, 44474, 46061, 6364, 22202, 16485, 34618, 19860, 40620, 36823, 9733, 20613, 7450, 34618, 35188, 14997, 7077, 44585, 37566, 38074, 38788, 36454, 5317, 8468, 1926, 6942, 34978, 8168, 24309, 11859, 42649, 24158, 8468, 8350, 450, 6233, 41249, 5373, 36174, 6589, 33231, 33303, 9695, 44757, 35163, 32700, 40614, 8468, 5420, 49824, 23243, 25546, 4486, 17325, 44689, 33866, 3651, 32700, 19140, 40472, 49718, 46673, 27616, 1471, 20170, 29580, 33303, 7967, 23320, 19717, 49824, 21005, 4565, 9458, 44739, 40620, 29942, 5664, 21192, 8619, 16485, 46713, 30530, 23031, 12817, 37072, 18489, 14997, 7405, 12817, 27047, 688, 3562, 18166, 19623, 19995, 37590, 30925, 36316], 26: [], 27: [20952, 153, 37760, 21305, 34276, 5396, 34577, 778, 47901, 35233, 14891, 42200, 29181, 32806, 1285, 31719, 1538, 28338, 47985, 42910, 21589, 391, 678, 48388, 40216, 19682, 44683, 21583, 25620, 22112, 35253, 29158, 46544, 13787, 17559, 17728, 5253, 19478, 5770, 23799, 8163, 48445, 23799, 20987, 43902, 5434, 11025, 8143, 12953, 9823, 24841, 4296, 37761, 25281, 10410, 49948, 34459, 7593, 9467, 33141, 32474, 1631, 45634, 12553, 5434, 13966, 34887, 30800, 39919, 17530, 42764, 48733, 23132, 778, 34577, 47901, 30848, 29890, 36963, 47928, 45895, 32668, 39659, 12072, 9823, 2526, 20952, 33013, 15502, 15181, 18362, 11177, 10865, 44683, 27286, 9055, 30848, 17566, 39439, 662], 28: [], 29: [], 30: [13267, 40253, 23642, 42023, 43702, 44401, 34573, 49204, 42144, 30440, 39110, 34360, 35567, 7451, 45646, 9585, 47038, 7745, 12797, 1019, 505, 33504, 7413, 33571, 12463, 22882, 11290, 18472, 44328, 18116, 17858, 29720, 39662, 33540, 31757, 36716, 41677, 1214, 12659, 49248, 13670, 36358, 34766, 40831, 30360, 7103, 3024, 11290, 29902, 13670, 12463, 3297, 17433, 22415, 44219, 3268, 24104, 44883, 1522, 10671, 27846, 32574, 48612, 46603, 5915, 18698, 48910, 3744, 42312, 41019, 35567, 6568, 22415, 22882, 46979, 8601, 27853, 4057, 12797, 23881, 19509, 43796, 1711, 20163, 10403, 40606, 19962, 22882, 3959, 16711, 48114, 32520, 44333, 43605, 15018, 30490, 28222, 5460, 26581, 37101], 31: [], 32: [], 33: [], 34: [15265, 3111, 2382, 12459, 18623, 21349, 17839, 6188, 5064, 14559, 43022, 10770, 36177, 42407, 43683, 22089, 35397, 20606, 41290, 20724, 43196, 36973, 49348, 26432, 39601, 33735, 264, 35462, 16573, 48505, 28672, 43149, 18623, 23405, 10016, 17841, 5287, 25510, 24459, 8555, 26238, 12228, 15972, 45766, 4901, 49619, 911, 44537, 17581, 13758, 31323, 45666, 20338, 6338, 14596, 49166, 2382, 11646, 26172, 31068, 13748, 38168, 1550, 8643, 41528, 1969, 14310, 25062, 27238, 36789, 40789, 2927, 18308, 43805, 33411, 30789, 7399, 22920, 4901, 27028, 25116, 24600, 24376, 3515, 23884, 31864, 28339, 22132, 39101, 29853, 31755, 34964, 49796, 49111, 16183, 1447, 34859, 44357, 21072, 21205], 35: [], 36: [], 37: [], 38: [], 39: [], 40: [], 41: [], 42: [], 43: [], 44: [], 45: [], 46: [], 47: [], 48: [], 49: [17776, 5908, 38731, 7209, 42642, 25633, 15958, 7188, 1542, 7587, 27454, 39798, 40289, 6898, 25127, 30826, 3638, 38938, 14823, 49922, 11442, 36643, 31578, 39991, 42740, 8267, 41037, 32135, 16326, 20238, 8025, 12722, 7785, 33744, 29350, 45760, 14804, 2862, 16075, 23169, 43512, 29567, 48776, 49922, 13215, 34131, 46075, 23342, 8515, 35518, 23528, 12189, 33433, 10055, 35615, 47461, 19316, 4974, 46508, 16676, 25656, 19394, 40289, 42122, 14246, 43738, 38105, 28691, 49274, 34801, 6245, 5529, 19957, 44771, 47029, 19257, 2612, 18996, 1298, 14018, 17576, 31721, 44670, 47971, 43512, 23219, 29051, 16421, 1298, 43738, 19676, 515, 24778, 33106, 36235, 5471, 17241, 49972, 23821, 44525], 50: [17690, 12381, 33345, 49644, 47500, 19649, 8644, 37000, 8609, 24808, 34444, 17857, 14070, 5220, 23929, 24682, 2063, 42627, 12332, 11964, 5923, 21468, 43472, 38076, 21259, 43261, 4682, 18182, 42824, 11475, 39986, 15697, 41217, 10736, 45855, 6677, 9989, 9001, 18864, 32273, 17925, 28621, 35986, 31545, 10432, 9354, 4892, 9081, 10241, 40546, 26392, 1330, 49242, 8030, 30721, 20080, 34148, 8703, 1330, 32917, 32586, 16545, 4236, 18525, 42051, 38256, 48951, 39031, 22479, 39816, 5927, 28002, 21147, 16545, 36110, 2504, 25798, 45042, 44227, 44023, 13702, 28739, 39430, 23755, 1698, 4368, 17335, 18402, 38173, 32917, 12607, 14319, 25418, 32126, 23294, 8159, 14133, 48140, 1330, 44476], 51: [], 52: [], 53: [], 54: [], 55: [], 56: [], 57: [2768, 22429, 49263, 38372, 18747, 19979, 47470, 2683, 10610, 2661, 46528, 4539, 5693, 47383, 49034, 49750, 43752, 24254, 528, 6723, 23525, 47615, 18595, 20064, 34429, 1501, 31005, 24623, 23240, 22360, 20618, 15920, 38939, 17529, 41313, 39466, 2112, 33657, 25710, 9237, 2023, 9596, 7269, 3560, 42194, 46797, 1618, 6066, 18595, 31623, 19134, 10193, 30607, 5006, 34969, 37407, 28370, 7855, 40142, 34294, 283, 37073, 26372, 8773, 9651, 48001, 20064, 7838, 19555, 4879, 17861, 23264, 43877, 42738, 26755, 48777, 7316, 22429, 7958, 17806, 16512, 3683, 35761, 2087, 27428, 25893, 23915, 3683, 48099, 26481, 25710, 3399, 26794, 19822, 528, 33137, 41799, 40950, 46558, 43067], 58: [12764, 30617, 21295, 41401, 29246, 21351, 11103, 1868, 23894, 48857, 42277, 32860, 3652, 39877, 12305, 49301, 1839, 34611, 21533, 40991, 22875, 29724, 16973, 6171, 19968, 42183, 5897, 2396, 25532, 27395, 31692, 21533, 27347, 7843, 7222, 6476, 18653, 5889, 11071, 42790, 5971, 7008, 40195, 47046, 46301, 10537, 26996, 13281, 5010, 33028, 30648, 42183, 37320, 8635, 26545, 3051, 26227, 34033, 30082, 11071, 36887, 49947, 5051, 48294, 38718, 7222, 12219, 47046, 3337, 26227, 43110, 47840, 6915, 44175, 30142, 15139, 16529, 2436, 38795, 36494, 17129, 31436, 1839, 39148, 404, 13495, 33727, 35858, 31692, 9675, 6770, 19550, 1364, 5998, 46301, 7222, 40195, 27939, 38718, 30699], 59: [5848, 38957, 15989, 28212, 1378, 41434, 22866, 10664, 28878, 1523, 43534, 47614, 31674, 30817, 1571, 48148, 37676, 16650, 6100, 43682, 10020, 31589, 19183, 36387, 19474, 22751, 5848, 14956, 17831, 31097, 19067, 11187, 21, 43884, 28590, 37987, 25012, 26430, 30883, 687, 17404, 20491, 32022, 4010, 27259, 37490, 48441, 30817, 45723, 34455, 39641, 41307, 46059, 20713, 37228, 26636, 19335, 3165, 31589, 9026, 32112, 6858, 29367, 45448, 14758, 567, 17943, 19136, 41971, 24021, 45339, 17404, 43758, 26560, 4649, 11880, 41436, 47268, 32614, 13005, 35190, 27824, 24021, 13328, 28590, 25774, 406, 28724, 982, 1844, 16738, 45267, 26213, 12113, 16608, 13338, 37490, 39641, 18865, 8241], 60: [321, 17543, 6478, 41703, 18047, 13031, 39723, 18511, 43982, 14970, 1516, 19982, 10599, 18511, 11404, 44517, 24169, 7814, 38025, 39241, 997, 33454, 3084, 45351, 28612, 14771, 7227, 3645, 17902, 47021, 24310, 48954, 47221, 2094, 18719, 38625, 30571, 49518, 45351, 33297, 34100, 34697, 38947, 28612, 32193, 18047, 47765, 4917, 14350, 49419, 25736, 17069, 24903, 20729, 34786, 28035, 12902, 7417, 13625, 11173, 23484, 36308, 34687, 49427, 49616, 6292, 36751, 29479, 359, 10312, 19466, 48098, 32347, 1909, 34437, 19978, 3388, 23398, 35697, 33454, 27545, 43007, 38277, 49419, 25648, 12030, 39010, 36308, 10235, 26452, 18380, 17327, 15183, 28818, 49583, 28658, 39010, 5667, 25545, 44305], 61: [], 62: [], 63: [], 64: [], 65: [], 66: [], 67: [], 68: [42431, 45714, 20014, 38288, 27005, 10673, 37234, 44438, 21958, 20659, 18381, 32843, 19149, 12331, 25540, 40435, 36703, 23007, 5121, 43333, 8951, 9294, 20014, 14706, 24365, 36010, 932, 9196, 2670, 47799, 12521, 11854, 39000, 34589, 4928, 46591, 8864, 1824, 26658, 10867, 932, 42388, 7421, 42387, 17003, 38104, 20325, 48034, 23007, 17060, 19149, 7817, 36879, 24727, 44939, 21392, 13757, 31919, 22188, 49464, 36856, 17753, 27005, 15022, 30695, 15345, 454, 5462, 38213, 43778, 41441, 27702, 48206, 25987, 39734, 10969, 23877, 12172, 37246, 1855, 26591, 39811, 5934, 11409, 30363, 7073, 39234, 895, 24153, 28655, 43955, 44988, 16762, 2330, 9196, 32628, 40105, 35164, 25904, 23471], 69: [9182, 45614, 40639, 32437, 8588, 14892, 21550, 13946, 12450, 49700, 13506, 39487, 47429, 14071, 5818, 9479, 2931, 48619, 30857, 11180, 37313, 14140, 46493, 14654, 199, 44855, 4612, 1677, 42696, 38134, 6461, 15661, 37886, 21927, 29570, 44819, 12450, 45811, 2152, 6840, 36040, 38162, 17010, 12829, 41963, 16084, 3648, 20291, 46042, 34343, 33841, 23396, 2931, 27877, 7510, 11143, 17311, 12365, 33514, 36040, 20301, 13960, 45550, 41584, 49598, 6572, 23396, 20754, 28563, 19162, 24676, 11143, 2931, 14354, 39962, 15661, 10189, 29665, 40223, 39450, 14220, 9592, 10607, 4273, 6521, 13875, 35343, 23730, 9148, 28271, 31446, 41584, 25881, 49700, 12043, 12365, 36625, 12527, 42127, 25664], 70: [], 71: [], 72: [], 73: [], 74: [], 75: [], 76: [], 77: [], 78: [25903, 7374, 18091, 17292, 15438, 45048, 20522, 15648, 19489, 26668, 2842, 23442, 8598, 8764, 32972, 6645, 30672, 9510, 5912, 7634, 19381, 34349, 39018, 16955, 47100, 19381, 9568, 39058, 29518, 22052, 24843, 19643, 45415, 19985, 45048, 42636, 34803, 25522, 769, 10771, 1135, 21928, 46816, 3626, 47918, 16179, 6907, 20551, 19034, 37054, 15718, 24549, 42663, 29043, 48865, 35594, 20474, 33965, 18321, 43630, 38339, 45606, 14886, 43064, 1619, 25489, 28445, 39058, 44962, 14625, 3032, 10582, 5500, 47100, 1037, 49946, 4105, 26668, 12747, 24329, 12879, 33082, 37110, 5912, 24843, 13832, 18075, 1619, 36435, 49541, 29736, 33965, 8527, 2842, 23442, 29155, 7624, 24233, 34530, 30871], 79: [], 80: [], 81: [675, 30920, 17046, 45901, 36296, 28913, 39292, 43513, 8321, 34460, 22582, 36724, 2841, 16336, 20470, 49309, 28906, 29807, 31782, 8276, 22393, 32821, 33414, 6209, 16877, 35460, 7533, 654, 42926, 7370, 10042, 40161, 46697, 1481, 3802, 19383, 16078, 19993, 32141, 22966, 28607, 6258, 20226, 47627, 48255, 49102, 25894, 10584, 40920, 13364, 48196, 19109, 23171, 654, 47568, 31466, 3802, 20185, 42358, 12246, 6391, 6214, 34515, 14690, 11885, 15149, 40904, 20487, 14144, 22512, 12751, 25283, 7373, 31782, 31466, 503, 29231, 4954, 32109, 20470, 16857, 37372, 49232, 4954, 47807, 38982, 46697, 40369, 24481, 36296, 41277, 47237, 48948, 37364, 40881, 34460, 7370, 13258, 34407, 31180], 82: [15939, 5454, 1402, 30049, 26976, 36327, 43606, 24921, 10699, 6195, 6570, 32115, 40656, 3555, 21695, 22190, 29103, 20263, 9512, 46394, 43866, 41547, 9637, 44523, 8099, 26471, 32106, 30450, 37510, 21973, 15317, 871, 3507, 20923, 26562, 41221, 45656, 33931, 47306, 5656, 7959, 12408, 25771, 18702, 8253, 10283, 5586, 47534, 32115, 39114, 16897, 5275, 7671, 8518, 26965, 7334, 29478, 25724, 33477, 22970, 11832, 8291, 24715, 28893, 32602, 41968, 40655, 39822, 7415, 14832, 43765, 11600, 3842, 47306, 18702, 14041, 32277, 274, 27035, 11501, 47319, 11084, 44928, 42485, 26501, 6277, 45064, 4381, 46832, 43293, 24024, 35967, 6227, 24852, 13546, 8518, 43293, 10699, 33477, 35336], 83: [], 84: [], 85: [], 86: [8802, 45861, 7091, 9550, 32608, 6649, 9567, 3895, 48900, 8754, 4512, 40334, 32608, 18107, 37014, 1186, 22718, 42707, 32789, 46169, 19098, 32929, 26097, 11723, 32665, 6546, 496, 46275, 33101, 5967, 28822, 47493, 46967, 14170, 222, 24906, 45536, 35231, 47494, 18310, 40526, 44778, 32129, 49171, 42290, 4246, 2235, 7608, 25310, 21679, 17240, 46275, 8086, 36135, 19219, 19443, 20778, 22949, 29985, 16344, 29635, 222, 10915, 26719, 34586, 15866, 28302, 5767, 5595, 45686, 22033, 24935, 38753, 22365, 30806, 9776, 44794, 9113, 20778, 27070, 40886, 36065, 33812, 35231, 10299, 1244, 5595, 48900, 34195, 20778, 22721, 14170, 43529, 21691, 13053, 38800, 3795, 18567, 6546, 10837], 87: [], 88: [4664, 21093, 26229, 32671, 44050, 44486, 47935, 36411, 29298, 36952, 28078, 39998, 49844, 20369, 49713, 49917, 32238, 49743, 30430, 11390, 41775, 11746, 27300, 41745, 35402, 44050, 45709, 40194, 23352, 44286, 410, 9375, 44099, 29013, 4614, 10636, 23640, 25453, 24776, 24034, 45844, 19558, 40591, 32448, 26564, 18283, 19775, 42247, 36198, 10024, 27540, 13746, 8731, 26046, 6288, 49656, 21128, 47879, 7594, 17553, 26847, 37230, 20222, 25403, 18283, 48664, 6909, 10286, 27716, 23220, 21316, 11935, 3992, 31713, 25840, 36198, 10708, 15986, 47259, 30113, 19908, 10708, 10554, 41108, 11288, 49459, 337, 31666, 27540, 21473, 9392, 18961, 22505, 49713, 49375, 26774, 49774, 40743, 37604, 26408], 89: [], 90: [], 91: [], 92: [], 93: [], 94: [], 95: [14643, 44254, 23702, 44719, 27030, 43493, 16793, 27500, 3191, 37649, 31424, 31583, 11454, 29194, 46072, 42426, 33883, 33380, 30975, 47545, 4219, 22610, 14251, 9868, 41872, 26558, 31116, 8766, 6813, 33380, 17656, 48719, 47381, 7020, 45225, 16040, 2922, 22610, 16946, 18784, 32186, 31984, 14767, 35063, 40988, 14489, 5557, 27561, 2731, 49406, 44254, 34580, 47545, 6250, 24586, 22169, 9422, 48708, 39304, 38669, 14821, 30429, 30877, 15488, 6646, 36160, 48719, 30463, 7793, 44045, 8297, 14489, 38297, 8739, 24608, 8935, 33121, 16337, 11673, 41872, 26438, 46855, 24878, 15016, 18669, 5648, 934, 16946, 17811, 14626, 46834, 7020, 6236, 25851, 29648, 28993, 23833, 37953, 19756, 9422], 96: [], 97: [14228, 3015, 40327, 25031, 34028, 28463, 22424, 44541, 43193, 5204, 30380, 48494, 44601, 9377, 30855, 3058, 11228, 8673, 9712, 5714, 45500, 5987, 44129, 4058, 48426, 28940, 45262, 42263, 5211, 42903, 41368, 45354, 38414, 38282, 2197, 48617, 6826, 33058, 15542, 8162, 21326, 20999, 45029, 46009, 5142, 32104, 26290, 40556, 23025, 37047, 428, 15960, 45023, 46330, 5211, 12941, 33838, 10275, 13507, 733, 7782, 21924, 13942, 2709, 8417, 38880, 13942, 16693, 45227, 20902, 30709, 41517, 48633, 23332, 15960, 39785, 1733, 18971, 39037, 12890, 5204, 7655, 23104, 3498, 43526, 9280, 33838, 48068, 45867, 13715, 49685, 25821, 17460, 28161, 26771, 15542, 9377, 1168, 25031, 6336], 98: [], 99: []}\n",
            "100\n",
            "1\n",
            "100\n",
            "2\n",
            "100\n",
            "3\n",
            "100\n",
            "4\n",
            "100\n",
            "5\n",
            "100\n",
            "6\n",
            "100\n",
            "7\n",
            "100\n",
            "8\n",
            "100\n",
            "9\n",
            "100\n",
            "10\n",
            "100\n",
            "11\n",
            "100\n",
            "12\n",
            "100\n",
            "13\n",
            "100\n",
            "14\n",
            "100\n",
            "15\n",
            "100\n",
            "16\n",
            "100\n",
            "17\n",
            "100\n",
            "18\n",
            "100\n",
            "19\n",
            "100\n",
            "20\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "ciclo immagini\n",
            "TEST ALL:  0.008\n",
            "BATCH SIZE:  128\n",
            "TRAIN:  4950\n",
            "TRAIN_SET CLASSES:  [35, 3, 94, 10, 45, 37, 80, 72, 36, 24]\n",
            "VALIDATION CLASSES:  [45, 37, 36, 35, 94, 24, 80, 10, 72, 3]\n",
            "GROUP:  3\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [35, 3, 94, 10, 45, 37, 80, 72, 36, 24]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  30\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.3310166895389557\n",
            "Train step - Step 10, Loss 0.15779387950897217\n",
            "Train step - Step 20, Loss 0.13539543747901917\n",
            "Train step - Step 30, Loss 0.12818321585655212\n",
            "Train step - Step 40, Loss 0.12026010453701019\n",
            "Train step - Step 50, Loss 0.12654969096183777\n",
            "Train epoch - Accuracy: 0.3316546762589928 Loss: 0.1513284170713356 Corrects: 2305\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12753911316394806\n",
            "Train step - Step 70, Loss 0.11850205063819885\n",
            "Train step - Step 80, Loss 0.1280892938375473\n",
            "Train step - Step 90, Loss 0.11990980058908463\n",
            "Train step - Step 100, Loss 0.12229302525520325\n",
            "Train epoch - Accuracy: 0.3060431654676259 Loss: 0.12130296100815423 Corrects: 2127\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.11983751505613327\n",
            "Train step - Step 120, Loss 0.11761333793401718\n",
            "Train step - Step 130, Loss 0.12002198398113251\n",
            "Train step - Step 140, Loss 0.1141500324010849\n",
            "Train step - Step 150, Loss 0.11829540878534317\n",
            "Train step - Step 160, Loss 0.11831734329462051\n",
            "Train epoch - Accuracy: 0.31510791366906477 Loss: 0.11760145666787951 Corrects: 2190\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.12375164031982422\n",
            "Train step - Step 180, Loss 0.10888966172933578\n",
            "Train step - Step 190, Loss 0.10632827132940292\n",
            "Train step - Step 200, Loss 0.12674900889396667\n",
            "Train step - Step 210, Loss 0.11762099713087082\n",
            "Train epoch - Accuracy: 0.30546762589928056 Loss: 0.11485535110929887 Corrects: 2123\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.1136980876326561\n",
            "Train step - Step 230, Loss 0.13305789232254028\n",
            "Train step - Step 240, Loss 0.10663709044456482\n",
            "Train step - Step 250, Loss 0.11933068186044693\n",
            "Train step - Step 260, Loss 0.11417414993047714\n",
            "Train step - Step 270, Loss 0.10666901618242264\n",
            "Train epoch - Accuracy: 0.2997122302158273 Loss: 0.11328536936705061 Corrects: 2083\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.10760900378227234\n",
            "Train step - Step 290, Loss 0.1132621094584465\n",
            "Train step - Step 300, Loss 0.11779347062110901\n",
            "Train step - Step 310, Loss 0.10996197164058685\n",
            "Train step - Step 320, Loss 0.11059888452291489\n",
            "Train epoch - Accuracy: 0.2962589928057554 Loss: 0.11126425007907606 Corrects: 2059\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11058111488819122\n",
            "Train step - Step 340, Loss 0.10163986682891846\n",
            "Train step - Step 350, Loss 0.10817984491586685\n",
            "Train step - Step 360, Loss 0.09937883913516998\n",
            "Train step - Step 370, Loss 0.10896055400371552\n",
            "Train step - Step 380, Loss 0.10773996263742447\n",
            "Train epoch - Accuracy: 0.300431654676259 Loss: 0.11052579322092825 Corrects: 2088\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.10537686944007874\n",
            "Train step - Step 400, Loss 0.11295778304338455\n",
            "Train step - Step 410, Loss 0.10528576374053955\n",
            "Train step - Step 420, Loss 0.11020105332136154\n",
            "Train step - Step 430, Loss 0.11138640344142914\n",
            "Train epoch - Accuracy: 0.29841726618705033 Loss: 0.10956404518523662 Corrects: 2074\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.1087222620844841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeUV-v0GW7yA"
      },
      "source": [
        "## PLOT \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be-n6SM6uV13"
      },
      "source": [
        "import numpy as np\n",
        "run1 = np.array([0.823,\t0.447,\t0.286,\t0.1985,\t0.163,\t0.1428,\t0.126,\t0.098,\t0.0953, 0.086])\n",
        "run2 = np.array([0.75, 0.438, 0.285, 0.218, 0.17, 0.14, 0.12, 0.1, 0.09, 0.08])\n",
        "run3 = np.array([0.817, 0.44, 0.27, 0.206, 0.168, 0.1442, 0.125, 0.101, 0.0912, 0.0831])\n",
        "\n",
        "finetuning = np.array([run1, run2, run3])\n",
        "mean = np.mean(finetuning, axis = 0)\n",
        "std = np.std(finetuning, axis = 0)\n",
        "\n",
        "run1l = np.array([0.816,\t0.683,\t0.6006,\t0.52175,\t0.4666,\t0.4365,\t0.4108,\t0.378,\t0.3486, 0.3226])\n",
        "run2l = np.array([0.769, 0.6445, 0.5563, 0.46675, 0.4362, 0.4105, 0.3772, 0.35075, 0.3262, 0.3104])\n",
        "run3l = np.array([0.825, 0.6625, 0.572, 0.47125, 0.4132, 0.3883, 0.369, 0.3485, 0.3325, 0.3149])\n",
        "LWF = np.array([run1l, run2l, run3l])\n",
        "meanLWF = np.mean(LWF, axis = 0)\n",
        "stdLWF = np.std(LWF, axis = 0)\n",
        "\n",
        "run1c = np.array([0.812,\t0.7385,\t0.6893,\t0.62025,\t0.582,\t0.5628,\t0.538,\t0.5166,\t0.4948, 0.482])\n",
        "run2c = np.array([0.754, 0.7185, 0.662, 0.608, 0.5796, 0.5598, 0.5278, 0.5005, 0.4985, 0.4707])\n",
        "run3c = np.array([0.833, 0.759, 0.695, 0.6367, 0.5966, 0.5823, 0.545, 0.52125, 0.495, 0.4947])\n",
        "iCaRL = np.array([run1c, run2c, run3c])\n",
        "meaniCaRL = np.mean(iCaRL, axis = 0)\n",
        "stdiCaRL = np.std(iCaRL, axis = 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PVNKrSpx9bk",
        "outputId": "2dba2c1f-7fcc-4e39-e3a5-0fd5ba2b1c4c"
      },
      "source": [
        "stdLWF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0245538 , 0.0157286 , 0.01833921, 0.02493436, 0.02187012,\n",
              "       0.01969794, 0.01808449, 0.01340761, 0.00943198, 0.00503742])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "eX9I7PBPvhuv",
        "outputId": "ffc04d86-6ce7-4e6c-c0da-fe5fbdd41dc4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = np.linspace(10,100, num = 10)\n",
        "plt.show()\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.errorbar(x, mean,yerr=std, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='blue', color = 'tab:blue')\n",
        "plt.errorbar(x, meanLWF, yerr=stdLWF, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='tab:purple', color = 'm')\n",
        "plt.errorbar(x, meaniCaRL, yerr=stdiCaRL, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='r', color = 'tab:orange')\n",
        "plt.legend(['Finetuning', 'LWF', 'iCaRL'])\n",
        "#plt.show()\n",
        "plt.savefig(\"plot.pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHUEaGUhweV_"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR-BSgUeMn5x"
      },
      "source": [
        "run1CEL1 = np.array([0.751,0.713,0.607,0.5175,0.5032,0.48233333333333334,0.475228571428571426,0.463625,0.4451,0.435])\n",
        "run2CEL1 = np.array([0.804, 0.701, 0.632, 0.5235, 0.498, 0.4877, 0.47025, 0.46806, 0.4591244, 0.4431])\n",
        "run3CEL1 = np.array([0.829, 0.747, 0.669, 0.56475, 0.5342, 0.495, 0.487571428571429, 0.471225, 0.46175, 0.4501])\n",
        "CEl1 = np.array([run1CEL1, run2CEL1, run3CEL1])\n",
        "meanCEl1 = np.mean(CEl1, axis = 0)\n",
        "stdCEl1 = np.std(CEl1, axis = 0)\n",
        "\n",
        "run1c = np.array([0.812,\t0.7385,\t0.6893,\t0.62025,\t0.582,\t0.5628,\t0.538,\t0.5166,\t0.4948, 0.4862])\n",
        "run2c = np.array([0.754, 0.7185, 0.662, 0.608, 0.5796, 0.5598, 0.5278, 0.5005, 0.4985, 0.4707])\n",
        "run3c = np.array([0.833, 0.759, 0.695, 0.6367, 0.5966, 0.5823, 0.545, 0.52125, 0.495, 0.4847])\n",
        "iCaRL = np.array([run1c, run2c, run3c])\n",
        "meaniCaRL = np.mean(iCaRL, axis = 0)\n",
        "stdiCaRL = np.std(iCaRL, axis = 0)\n",
        "\n",
        "runCEBCE = np.array([0.84, 0.785, 0.7133333333333334, 0.664, 0.5748, 0.541, 0.4888571428571429, 0.45875, 0.43922222222222224, 0.4009])\n",
        "\n",
        "runCEKlDiv = np.array([0.832, 0.7945, 0.685, 0.6205, 0.5372, 0.49583333333333335, 0.4667142857142857, 0.419125, 0.39222222222222225, 0.3614])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "teuEpnpQNNA0",
        "outputId": "05922855-0dd5-4478-dae4-0277a9204513"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = np.linspace(10,100, num = 10)\n",
        "plt.show()\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.errorbar(x, meanCEl1, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:pink')\n",
        "plt.errorbar(x, meaniCaRL, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:orange')\n",
        "plt.errorbar(x, runCEKlDiv, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:cyan')\n",
        "plt.errorbar(x, runCEBCE, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:purple')\n",
        "plt.axis([0, 105, 0, 1])\n",
        "plt.legend(['CE + L1', 'BCE + BCE', 'CE + KLDiv', 'CE + BCE'])\n",
        "#plt.show()\n",
        "plt.savefig(\"losses.pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3hcxbn/P7O9Sau26tWSZVtusuWCMTYuFGObFjAlCTgJJQTS4IbcEEgChJsbLimE/gMCBgIYMJAApoNt3HvFRZYtF1m9l12V1c7vj7NaSbYkq7ppPs9zntU5Z86cGUl7vmfmfed9hZQShUKhUAxedKe7AQqFQqE4vSghUCgUikGOEgKFQqEY5CghUCgUikGOEgKFQqEY5CghUCgUikHOgAmBEOIlIUSxEGJXJ+eFEOIJIUSOEGKHEGL8QLVFoVAoFJ0zkCOCRcCcLs5fBgz1b7cDzw5gWxQKhULRCQMmBFLKb4DyLopcCbwqNdYBIUKImIFqj0KhUCg6xnAa7x0HHG2zn+c/VnB8QSHE7WijBiwWS1ZiYuIpaeCZiM/nQ6cbvKadwdz/wdx3UP3va/+zs7NLpZSujs6dTiHoNlLK54HnAYYNGyb37dt3mlt0+li+fDkzZsw43c04bQzm/g/mvoPqf1/7L4Q43Nm50ymvx4CENvvx/mMKhUKhOIWcTiH4ALjZ7z10HlAlpTxhWkihUCgUA8uATQ0JId4EZgARQog84A+AEUBK+RzwMTAXyAHcwA8Hqi0KhUKh6JwBEwIp5Y0nOS+Buwbq/gqF4syhqamJvLw86uvre12H0+lkz549/diqs4vu9t9isRAfH4/RaOx23WeFsVihUJzd5OXlERQURHJyMkKIXtVRU1NDUFBQP7fs7KE7/ZdSUlZWRl5eHikpKd2ue/D6YikUilNGfX094eHhvRYBRfcQQhAeHt7jkZcSAoVCcUpQInBq6M3vWQmBQqFQDHKUECgUikFBYWEhN9xwA6mpqWRlZTF37lyys7M5dOgQVquVzMzMwPbqq6/26V7Lly9n/vz5Jxx/6qmnSEtLQwhBaWlpn+7RnyhjsUKhOOeRUnL11VezcOFCFi9eDMD27dspKioiISGB1NRUtm3b1u36kpOTOXToUI/bMXXqVObPn3/GrZBWQqBQKM44vGUeSl/ZjbfEjcFlI2JhBph6X9+yZcswGo3ccccdgWNjx44F6NUDvbeMGzfulN2rJyghUCgUp5TKDw/QmF/XZZnGvBpo8gHgLXZT+PgW9DFWPPqOH1mmWDshl6d2Wt+uXbvIysrq9PyBAwfIzMwM7D/55JNMmzatyzaeSyghUCgUZx5+Eeh0v5/pztTQXXfdxerVqwHIz88PCMeCBQu4//77B7R9A40SAoVCcUrp6s29hcK/bcZb4gYJCDC4bNhvGtrrBWUjR45kyZIlvbq2haeffjrwc3Jyco9sCmc6ymtIoVCccUQszMDgsgVEIGJhRp/qmzVrFg0NDTz//POBYzt27GDlypV9beo5gRIChUJxxmEItxJ9Txbx/zuN6HuyMIRb+1SfEIL333+fL7/8ktTUVEaOHMl9991HdHQ00GojaNmeeOKJPvfhq6++Ij4+PrCtXbuWJ554gvj4ePLy8hgzZgy33nprn+/TH6ipIYVCMSiIjY3l7bff7vCcx+PpUV0n8zSaMWNGh3VOmTKFn//85z2616lAjQgUCoVikKOEQKFQKAY5SggUCoVikKOEQKFQKAY5SggUCoVikKOEQKFQKAY5SggUCsWgQK/Xk5mZydixYxk/fjxr1qwJnNuwYQPTp09n2LBhjBs3jltvvRW3282iRYtwuVzt1hjs3r27T+148MEHiYuLIzMzk+HDh/OTn/wEn08LodHU1MRvfvMbhg4dyvjx45kyZQqffPIJAKNGjWL06NGBdvSnG6paR6BQKAYFVqs1EBbis88+47777mPFihUUFRWxYMECFi9ezJQpUwBYsmQJNTU1AFx//fU89dRT3brH8uXLWbRoEYsWLeqy3N13382vfvUrfD4f06dPZ8WKFcycOZPf/e53FBQUsGvXLsxmM0VFRaxYsSJw3bJly4iIiOhF77tGCYFCoTjzKM+FN2+A0v0QMRRuXAzG/nsAVldXExoaCmgxhBYuXBgQAYBrr7223+7VFY2NjdTX1xMaGorb7eaFF14gNzcXs9kMQFRUFNddd92At0MJgUKhOLV88hso3Nl1mfzN0ORfmVuyF56dgjVqLHQShpro0XDZn7us0uPxkJmZSX19PQUFBXz99deAFqJ64cKFnV731ltvsWrVqsD+2rVrsVr7FvLi73//O//61784fPgwl112GZmZmezYsYPExESCg4M7vW7mzJno9XoAFi5cyN13392ndrSghEChUJx5NHm63u8FbaeG1q5dy80338yuXbtOel13poYmT55MQ0MDtbW1lJeXB0JUP/roo1x66aUnlG+ZGmpqauLaa69l8eLFZGScPLCemhpSKBTnBid5cwfg6clQmg3SB0IHEel4rl/S6zDUxzNlyhRKS0spKSlh5MiRbN68mSuvvLLX9a1fvx7ovo2gBaPRyJw5c/jmm2+44oorOHLkCNXV1V2OCgYC5TWkUCjOPG5cDBHpIPTa542L+7X6vXv30tzcTHh4OD/96U955ZVXAg9zgPfee4+ioqJ+vWdHSClZvXo1qamp2Gw2brnlFn7xi1/Q2NgIQElJCe+8886At0ONCBQKxZlHWArctb79Mb8XT29psRGA9gB+5ZVX0Ov1REVFsXjxYn71q19RXFyMTqdj+vTpzJkzBzjRRvDMM89w/vnn96ktLTaCpqYmxowZw5133gnAI488wgMPPEBGRgYWiwW73c7DDz8cuK6tjWDMmDG8+uqrfWpHC0JK2S8VnSqGDRsm9+3bd8rvW1XiYekz26kschMSZWPenWNxuvpmMOoNy5cvZ8aMGaf8vmcKg7n/Z3Pf9+zZw4gRI/pUR01NTb9NDZ2N9KT/Hf2+hRCbpZQTOiqvpoa6ydJntlNR4Eb6oKLQzdJntp/uJikUCkW/oISgm1QWuVt3JFQUuMnZXEzzACfVVigUioFG2Qi6SWWwgaBKLzrAByDgsxd2YbYbSJ8YzfAp0bgSgxBCnOaWKhQKRc9QQtBNXp9qZ8HKGsJrfJQF6XhrqoNoD0zP81K/8hg7l+cRFmtn+JQY0idFYXeaT3eTFQqFolsoIegm4ZE2nr9Mjw9tPi3abOT8CCcrhtRyaLSJjCONjDvUQPm7Oax5L4ew4SFMuCCOIWNc6I1qBk6hUJy5KCHoJq+NGcJNOw5ywN1Aqs3Ma2OGkGTV3vpLGptYU1nL6opaPsqtJHRfDaMPVVG+pxKvWYdpdChZF8QyIT0cnU6JgkKhOLNQT6VukmQ1883kERybmck3k0cERADAZTJyZWQo/zcsgU/mjOavd0wg7d4x5F8Vw5EYE41bytj4+E4e+e1K7n9tG4v2FXDQ3cDZ5rqrUJzNFBYWcsMNN5CamkpWVhZz584lOzubQ4cOYbVa24Wa7qt//vLly5k/f35g/4EHHmDOnDk0NDQwY8YMNm3adEJ5p9PJuHHjGDZsGNOnT+ejjz4KnH/uued44403+tSmrhjQEYEQYg7wD0APvCil/PNx5xOBV4AQf5nfSCk/Hsg2nQpizCYWxIazIDYc5sD+sjpWrcnDs6mU8NXl1Kwp5x8xRo4MtRE/MpzzXUFMDXGQaFV2BYViIJBScvXVV7Nw4UIWL9ZWKW/fvp2ioiISEhJITU0NxCHqDsnJyRw6dKhbZR955BFWr17Nxx9/HIgq2hHTpk0LPPy3bdvGVVddhdVqZfbs2dxxxx2BsNgDwYAJgRBCDzwNXAzkARuFEB9IKdtmdXgAeFtK+awQIgP4GEgeqDb1ifJceON6KMtpDYsbltKtS4eG2xl6+TC4fBhl+bWsX3UM/cYi0ldUUb+2mk8TTTyaYsYUY2VqWBAXhDiYGhpEtNk4wJ1SKM5MDnsaTpiKDetDfcuWLcNoNHLHHXcEjo0dOxag2w/03vDXv/6VTz75hM8++6xHEUszMzP5/e9/z1NPPcXs2bN58MEHMRqNXHPNNdx8881s2LAB0Np++eWXs3PnSaK5noSBHBFMAnKklAcBhBCLgSuBtkIggZboSk4gfwDb0yfkv66D8mwEIEv2wWvXIX6xscf1hMc6mHvdMHzXpnN0Tzl71xRg217CpJwG3GEeNifX8ut4I3VWHWk2M+eHOLggNIgEi4lf7DlMjnSStn5POxuFQnE28bv9eeyq7Tqa6LZqNx6fNnWa7W5gxoa9jLaZ0Rv0HZYf5bDyx6Hxnda3a9cusrKyOj1/4MCBQPgJgCeffJJp06Z12caTsXr1avbt28fmzZtxOBw9vn78+PE89thj7Y4NHz6cxsZGcnNzSUlJ4a233uL666/vUzthYIUgDjjaZj8PmHxcmQeBz4UQPwPswEUdVSSEuB24HcDlcrF8+fL+butJubA8h5YVAgKJrMgm+/V7KYqaSbPB1ut6zUMhPQmqjggqc5uYtqWJaVvBHaNjd7KJd2M9vJpf5i8tAcF+dz3XrNvFX8TADRXPVGpra0/L3/9M4Gzuu9PpDExtNDY20ext7rJ8iwi03ZfITq9rbGzqcuqkvr6exsbGDsvU1taSkpLCypUr2x0/vuw999wTCEyXn5/PmDFjALjqqqu4995725V1u92kpKRQWVnJBx980C6yaXNzM3V1de3qd7vdeL3edsdqa2vx+XzU1NTQ0NCAXq+npqaGK6+8kldffZV77rmHN998k5dffvmEttbX1/fof+V0ew3dCCySUv5VCDEFeE0IMUpK2W65rpTyeeB50GINnY54K01fx2EQeQghkVIgMZC+/3nSD70OY6+HibdB1MnjiZ+M8oI69q0rYO+6QiasqWeq3UvI2HCeEHXM2ukOrGN4Z1oQMy6f0feOnWWczfF2+srZ3Pc9e/YE4uQ8OvLk8XKmr99Djrsh4K6dZjPzxvD4XscaysrK4qOPPurweofDgU6nO2ndL7zwQuDn5ORkduzY0WlZm81GbGwsixcvZvbs2cTFxTFz5kxAy51st9vb3c9ms2EwGNody87OZuTIkQQFBWE2mwNtvPnmm1mwYAE33ngjer2ecePGnXB/i8XS4fHOGEivoWNAQpv9eP+xttwCvA0gpVwLWID+z7rQD1QG/y9eGY+UOrwynqLGZylufpxG52zktjfg2Snw0mWw613wNvb6PmExdqZcncbCP53P/J+NJWFEGKUbSrh+dS0R1T50EsKrfSxYWcP2GvfJK1QozkJeGzOENJsZPZoIvDZmSJ/qmzVrFg0NDTz//POBYzt27DhhFNDfpKen89577/H973+/R8boHTt28Mc//pG77rrrhHOpqano9Xr++Mc/9su0EAzsiGAjMFQIkYImADcA3z2uzBFgNrBICDECTQhKBrBNvSb0hxdR+kos3hI3BpeNkMuH4NlcRPH2NPTG7xGSsh5L1XuIJT8CRxSMXwhZPwBnXK/up9PrSBoZTtLIcOrrmvjnr1Yi/KNlHRBR7ePq9dncPzyOH8VFqNAWinOKFnftttTU9P4FSwjB+++/zy9/+UseffRRLBYLycnJPP7448CJNoIf/ehH/PznP+/1/doyceJEXn75Za644gqWLVsGwLx58zAaNWeQKVOmcNddd7Fy5UrGjRuH2+0mMjKSJ554gtmzZ3dY5/XXX8+9995Lbm5uv7RxQMNQCyHmAo+juYa+JKX8HyHEw8AmKeUHfk+hFwAH2gT4r6WUn3dV5+kKQ90ZTcVuqr86gmdHCcIkCBl5CFvDB4gDn2uZlYbPhYm3QsqF0IeH9RsPraOy0E3bP1ejVceKdDOuyZH8ZXQSTuPpnukbeM7m6ZG+cjb3XYWh7jsDGYZ6QJ8c/jUBHx937Pdtft4NTB3INgw0xkgb4TcOp2lmAtVfHaFii6TS8kuCJ/wCh+FTxPZ/wZ4PIXyoJgiZN4LF2eP7zLtzrBYKu9BNaLSNiXOT2b26ANP2Ctx7jnDPyDLuuHo4EyNPbYo7hUJx9nPuv0KeIozRdsK/N4LG/FqqvzxC1aoyqq1zCbrgZhzODei2vgSf/jd89RCMuU4ThejR3a7f6bLy3T+c538rPA+AoROjKTxYxZf/ycG2tYpvdm1m83kuFl6ZjtVhGqiuKhSKcwwlBP2MKdZBxM0ZNObVUP3lEaq/KKTWNgTH9EU4Li5Gt/0l2L4YNi+ChPNg0m0w4gow9O7BHT3EyffvziLnQAVvLtmLZWUxL6wrYczMeCZfkqQEQaFQnBQVa2iAMMUHEfGDkUTelYkpIYjqTw9R+FoTNaH34fvZt3DJ/0BdMbx7C/w9A776I1QePXnFnZCWGsoDvz4P/a1pZEcb2fX5URb9dg2r382hrqqhH3umUCjONdSIYIAxJQQR8cNRNByupvrLw1R9nEvNN0aCZlyN444fI46sgI3/hFV/07Zhc2HiLZAyA3oYqVQIwR0TEtkyNJx71+SQvq0G35dH2Lk8j5EXxDLukiQcoWo1skKhaI8SglOEOSkY1y2jaThURfUXh6n66CA1K/IInpmB/drXEXV5sOll2PIq7P0IwtNgwi2Q+V2whvToXuOddt69aCR3xx/l6dxyFhxsxrfiGLtWHmPE+bGMvzSR4PDuxz1RKBTnNmpq6BRjTnbium0MEbeNxhBhofKDAxT+ZSO1ew3IGb+De3bDd14Aaxh8dh/8dTh88DPY9yk8PZkLl18NT0/WguB1QYjRwEujkrlnfCIvZZp568pwwse72LM6n9d/t46vX91DZbFakKYYPJzqMNROp5PMzEzGjBnDRRddRHFxceD8q6++yqhRoxg9ejTjxo3jL3/5CwA/+MEPSElJCbTj/PPP71M7uosaEZwmLKkhmIc4aThQSfUXR6j89wFqlucRNCsBe9a1iDHXQcF2bdpo5zvaSAG0eEel2fDmDXDX+i7vIYTg1ngXE4Lt3P7tIe42N/LbSUMZtauOPasK2Lu2gKGTophwWTKh0faB77RCcZo4HWGo24aVvu+++3j66ad56KGH+OSTT3j88cf5/PPPiY2NpaGhoZ3wPPbYY1x77bU972QfUEJwGhFCYEkLxZwaQsP+Sqq/OEzleznULM8jeFYCtnFjEFc8ARc/DP+XTGA1mfRByT7I2wxx40+6UC0z2MYXE9K5Z99R/lhSyuwRwTw6eyKHVuTz7TfHyN5QRNr4SCbMTSY8rudREhWK/qaqxMPSZ7ZTWeQmJMrGvDvHorP0vr7TFYYaNBGqqakhLS0NgP/93//lL3/5C7GxsQCYzWZuu+22AW3DyVBCcAYghMCSHop5aAj1+yqo/vIwFUv2U73sKMGzErFlRiIihmkjgbbx+F6cBa4RMP4mGHMD2MM7vYfTaODFkcm8fKyUB3PyubzOw3MXJXHzpUls+/IoO5fnkbO5mCGZLibMTcaVOHhXcCoGlpVvZ1N6tLbLMsWHqvE2af/rFQVuFj+8nvAEO3p9x2GoIxIcTLsuvdP6TkcY6pUrV5KZmUlZWRl2u50//elP3WrLvffeyyOPPALAyJEjef311/vUju6ghOAMQgiBdXgYlmGh1O8pp/qLw1S8k03NsqM4xjyOuezHGHxH8eoTENe/hKFmJ2z9F3z2W/jiD1o4i3E3Qeos0J34hRFC8KN4FxOc2lTRd7bl8N8pMfz0qiGMuySR7V8fZcfXeRzcVkLS6HAmzE0mOqXnq6AVir7SIgKd7fc33Zkauuuuu1i9ejWghaFuEY4FCxZw//33n1C+7dTQo48+yq9//Wuee+65k7ZFTQ0pAL8gZIRjGR5G/e4yqr88TOXXXrSEb4AAw1JB9D0/hAk/hKLdmiDsWAy7/wPBcZq3Ueb3OsyiNibIxhcThvGrfUf508EC1lbW8uSIJCZfPoTMixLZuewo2746yruPbiZhRCgT5qYQO7RnnksKRWd09ebeQtvYWkJASLSNS38yvNexhkaOHMmSJUt6dW0LTz/9dODn5OTkHtkUrrjiCq655ppAWzZv3sysWbP61J7+RHkNncEIncA6KoLIn4+HtmYACd6SNh4/URkw509wz1647lWIzICVf4UnMmHRfNjxNjS1zwgVZNDzXEYS/5cez5rKWi7auI+1lbWYrQYmzE3h5v85nylXp1KaV8v7f93CO3/eyCu/Xc0zd37NGw+to6qk6wxTCkVfmHfnWEKibQidJgLz7hzbp/pOVxjqFlatWkVqaiqgGY7vvfdeCgsLAWhsbOTFF188Je3oDDUiOAsQOoHBZdMe/i3RRyVUvL+f4IuT0LeEkTCYIONKbas6BtvegK2vwXu3aYHuRi+Acd+HmEwQAiEEN8dFkOW0c/uuQ1yzNYd7U6L5eVIUJouB8ZcmMXpmPLtX5rP63f0B80RFgZsP/rGVmx45Na5tisFHS2yttvQlefvpCEPdYiOQUuJ0OgMP+7lz51JUVMRFF12ElFKbsv3RjwLXtbURAGzYsAGTaWBDxQxoGOqB4EwLQ32q8JZ5KH1lN00ldRjDbZgSg3BvK0aY9ATPTsQxJRZh6GCA5/PB4VWw5TXY8wF46yFqtGZgHr0AbFpK8FpvM/+dnce7RRVMD3XwdEYSLpMxUM0zd36NPG6aNjTGTlpWJGlZkYTFnBr307M5FHNfOZv7rsJQ952zNgy1ov8whFuJvifL/zDQ/pZBMxKo/OggVUtzqVtfiHNeCpbhYe2T1Oh0kDJd2zyPwa4lmih88mv4/HcwYj6M+z6OlBk8NSKRqSEOfrs/j9kb9/FMRhIXhGr/eCFRttZ8CAJsQSasDiMbl+ay8aNcwmJbRUGtSVAozi6UEJzFGCNtuH40Cs/ecqqWHqTsld2Y00MJmZeCMaqDh7E1RAt/PfFWKNypCcKOt7T0ms5ExLjv8d3M7zIuK53bvz3Egm0H+K/kaO5OjgrkQ2jr1+10WamrauDAlhJyNhex4aNcNnyYS3i8QxOF8ZGERNlO/S9GoVD0CCUE5wDW4WFY0kKoXVtA9VeHKfrHFhznxRJ8USI6m7Hji6JHw9z/0xar7VuqicLyP8PyPzNiyAw+HXczv3GM4y+HCllXWcszGUknzNkC2J1mxsyMZ8zMeGorGjiwpZiczcWs/89B1v/nIBEJjsBIwelSojCYaZkPVwwsvZnuV0JwjiAMOoKmxWEb56L6i8PUrs3Hva2Y4IuTsE+KQeg7+QIaLTDqGm2rPOI3ML+O/d0f8qQ1lKnjfsN9Vedx4fpvsXtKKTCGktpQxGuZw0iKSW1XlSPUzNjZCYydnUBNeX1AFNb9+yDr/n0QV2IQaRO0kUJwhAp6N5iwWCyUlZURHh6uxGAAkVJSVlaGxdKzZdjKWHyW0V2DYWNBHVUfHqDhYBWGKBsh84dgGRravZv4fJC7XBsl7P2IfeYYLs56kUZh1LyNZDNDG4r45rK53aquuszDgc3a9FHxYc3zIzI5ODBSCArr/j/t2Www7Stnc9+bmprIy8ujvr6+13XU19f3+AF3LtHd/lssFuLj4zEa288GKGPxIMQUYyfittHUf1tG5ce5lP5zF5YRYTjnDcF4srdxnU5bnZw6C9zlDNv5Ds11+kBMIyn0ZJtj+LSkiosjgtGf5A0vONzKuEsSGXdJItWlHnI2ayOFNe/msObdHKJSghk6IYrU8S4coYP3i34uYzQaSUk5cXFjT1i+fDnjxo3rpxadfQxk/5UQnMMIoS1IswwPo2bVMWq+PkrR3zfjOD+W4NmJ6Czd+PPbwmDyj0n95GNyTFH4dHqE9KGXzfxgVy7JukZuTY7nhrgoHIaO48C0JTjCyvhLkxh/aRJVJe6AKKx6Zz+r3tlPTKqTVL+h2R6ikugoFKcCJQSDAGHQETwjAXtWFFWfHaJ21THcW4oJvjQJ+4RohO7kc7avZQ7jpm37OGCOIrWhkJfN+9h9ZC/PO6fxgM/EoweO8d1IO7ekppJo7d4D3OmykTUnmaw5yVQWtRGFt1tFIS1LGyl4G30sfWY7FYU+8lesC3gtKRSKvqOEYBChDzIRdm06jvNiqPzwIJXv5VC3toCQy4dgHtJ1LKGkmFS+aWccnkeqlFyeu4ItW1/nBW80/5QX8kLxt1xmbeDHw0cyMSSo24bBkCgbE+YmM2FuMhWFdQFRWPlWNivfzsZg1OFt1Fa0VRa6WfrM9g69mBQKRc9RQjAIMcUH4bpjDJ4dpVR9nEvJ8zuxjgrHOXcIhh4YbhEChsxg/JAZPFudz+82v8XLhVW8Fj6LpdsOMpYqfjwkicsTkjB2Y9TRQmi0nYnzUpg4L4Wy/FoObC5m49JDgfNSQmWRyq6mUPQXKujcIEUIgW2si6j/yiL4okTq91VQ+LdNVH12CF9Dc88rDI4ldubd3H/d/WyOq+LP5R9S667izoOVTFy2mie2baC8sanH1YbHOph0+RBCY2ztAu8ZjDoV+E6h6CeUEAxydCY9wRclEfWrCdhGRVCz7CiFf9lE3eYipK8XrsV6I/aRl/ODa37HyknD+VfTN6TX5PCnChNZK7fy65Wfs7+itMfVzrtzLKHRmhhYg4xICW8+tJ6NS3PxNvVCuBQKRQA1NaQAwOA0E3bDcOznx1L54UEq3smmdm0+IZenYk4K7lWdOlc6F12SzkWNbvZs/4gXjpXxVtB4Xt2WxyzvOm5PSeTC1NHdsiO0RKPUfOmnUVvRwOol+9nwYS771hUy/cZ0EjM6z9CmUCg6R40IFO0wJwYT+ZOxhC5Ip7mqkZJnt1O2eC/eyobeV2qyMWLidfztqp+wKc3Ar+s3sdNn44ajPi78/DP+teEzPA09m+ZxhJq59LZRXPHzTBDw4RPb+fT5XdRW9H7BkkIxWFEjAsUJCJ3AnhWFdVQENcuPUrMyD8+uUnRmPT63F4PLRsTCDAzhPXffdCVlcU9SFnfVlvGfrV/xvMfMr+qS+NOKDSzUF/CD0ZOIihzS7foSMsK48XeT2frFYTZ9cpjD35YxaX4KY2bFo9er9xyFojuob4qiU3RmPc5Lk4m+ZwJCr8NX5w1kRyt9ZXef6jY7wrlu2nV8cel83o2sZlJzMY+LdCbsLArmYToAACAASURBVOOnS19lx66vwNe9uX+9UceEuSnc+PvJxKWHsObdHN7+n43k76/sUxsVisGCEgLFSTGEWZCNbR7KErzFbmRz3xOKC72eqSOns+iyBawZGcFCXR6fmIdySUk4Vy19h09WvEpzTXG36nK6rMy7cwyX3TGaxnov7/91C18t2o27urHP7VQozmXU1JCiW5yQKhMofmY7Ydeld5z7oBekRCXySFQi9zbU88aONfzTG8UPfaEkrdzEd7y5fGhMIdccReonH3cY/RQ0t9ghmS4SRoSx6eNDbPvyCLk7SjnvqlQyLohF14P1DArFYEGNCBTdImJhBgaX5r5piLThvCKV5op6ip7cSs3KY71zNe0Ep9nCTybOYt3sGbyQYCLKbOTvQVPJMcfQLPTkmKK4aetuOLYFmr0d1mE065lydSrXPzCJiAQHK97Yx7uPbqL4cHW/tVOhOFdQIwJFt2hJldkW2+gIKt7bT9XSg3h2lxG2IL1nK5NPdk+d4PK0DC5PyyD26834hBbUzqfTk2OJQ74wE2FyQMJkSJ4KSVMhdhwYWmMdhcXYufKX49i/sYjVS3J458+bGDU9jslXDMFi7yRpj0IxyBhQIRBCzAH+AeiBF6WUf+6gzHXAg2iTDtullN8dyDYp+g99kInwmzNwbyqi8qODFP1jCyGXD8GWFdXvyUfSGooC0U+REp/QMX/2Zzxcu4ys3A/gq4e1ggYLxE+EpPO1LX4iwmQnfVI0SaMj2PDBQXYuz+PAlmKmXpNG+uRolShFMegZMCEQQuiBp4GLgTxgoxDiAynl7jZlhgL3AVOllBVCiMiBao9iYBBCYJ8YjTk1hPJ3sqlYsh/Pt2WEfmco+iBTv92nffTTIhakpPBCmYF5lsv4zozvcn+MlbiijXBoNRxeDd88BtIHOoM2Skg6H3PSVKZdeR7Dp8Sw4s19fLloD7tXFzD9xnTCYx391laF4mxjIEcEk4AcKeVBACHEYuBKoK3f4W3A01LKCgApZffcQxRnHIYwC67bRlO7+hhVnx2i6PHNhF49FOuoiH6pvyX66fLly5nhz4z2Q28zTx0p5rmjxXxcUslPEsbz04vmYDfoob4ajm7QROHwGlj7DKz+ByBwRY/mmtFT2Z0ynbXranj7kY2MnZ3AhHnJmLqTo0GhOMcYsFSVQohrgTlSylv9+zcBk6WUP21T5t9ANjAVbfroQSnlpx3UdTtwO4DL5cp6++23B6TNZwO1tbU4HGf226uxFqJ26LBUC6pjfZSOkPj6aTq+o/6XSMGbWFmDiVB83ICHaTTR1kFI19xAcHU2zqpvCan8luDqveh9jXh8Qaz03MH+mvMxmxuIHdOIdUjwGTlddDb87QcS1f++9X/mzJmdpqo83ULwEdAEXAfEA98Ao6WUna4EUjmLz468tbLZR/XXR6lZdgR9kInQa9O7nzO5C7rq/8aqOn6//xhba9yMCbLycFoc54V08sXxNkLBtsCIoSC7lBVl36fMm0KifQ/Txh8iJGOsZmcITYGKQ/DmDVC6HyKGwo2LIaxvqRd7ytnytx8oVP/71v8+5SwWQlwOLJVS9nT10DEgoc1+vP9YW/KA9VLKJiBXCJENDAU29vBeijMModfhvDgJ6/Awyt/aR+k/d2kpMuckozOdPKVlb5jotLM0ayjvF1XwPwcLuGprDvNdTn6XGkvS8VnTDCZImKRtF9xNjK+Z6wp2svOzPazfnMriVWmM37qE8fZfYgiOgMZaaKgFJJTugzeug5+qf1PFuUF3JkSvBx4XQrwLvCSl3NvNujcCQ4UQKWgCcANwvEfQv4EbgZeFEBFAOnCwm/UrzgJMCUFE/nwc1Z8eonZNPvXZFYRdPwxTQtCA3E8nBNdEh3GZK4RnjxTz1JFiPi/dy+0JLn6RFEVQZ3mVdXp0cZmM/VEmad/RIptu3HQj+8S1TI9ZSVLNk61lpYTSbHgsDZwJEJLg/0xsv2/tOuubQnGmcFIhkFJ+XwgRjPbAXiSEkMDLwJtSypourvMKIX4KfIY2//+SlPJbIcTDwCYp5Qf+c5cIIXYDzcC9UsqyvndLcSahM+kJuSIVS0YYFe/sp/jZbQTNSCB4diJigALD2fQ6/islmu/GhvGngwU8daSYxQXl/GZIDDfGhKHvwgZgDzFzya2jGHFBOd+8mc1H22aR6IilyhNMdXMUIfpjzIt9FuewSVB5FIq+hezPwHtc5FNz8HFCcZxgOCK1LG8KxWmm2zYCIUQ4cBPwS2APkAY8IaV8sssL+xllIzi750l99V4qPziAe0sxxjhHj0NU9Lb/W6vd/CHnGBuq6siwW3goLY5pYScflTQ3+dj65RHW/+cg2lIXAfhwhBi58cGprV5GUkJdiSYMVUf8n0fbfzZUta9cbwZnfBuBSGovGkGxoDdAeS68eQOyJBvhSj8t9okzgbP9f7+vnG4bwRXAD9Ee/K8Ck6SUxUIIG5or6CkVAsXZjc5iIOy6YVgzwql4fz9FT27FeWkyjqlxiAGMAzQu2MZ/xqXxYUkVDx84xoLtB7g0Ipg/pMYxxGbu9Dq9UceEy5LZ8MFBpGxpn47aymZevPsbwuMdxKSGEJPqJDrVSVB8FsRndVxZfdVxAtFGMLI/g7rjvKeFHoJjoa4UvB4tU2fJPnj9WvjZ5v74tSgUQPdsBNcAf5dSftP2oJTSLYS4ZWCapTjXsY6KwJQU7A9RkYtnd3m/h6g4HiEEV0SGcEl4MC/klfD44SIu3LCXH8VFcHdyFCHGzr8OIdE2KgvdSP+gwBFqZvh5MRQcqGLP2gJ2Ls8DtOOaKGjiEB5nR9cy/WVxQrQTokd1fJMmD1QdO3FEsWNxm0ISynLg76M1wYnLgrgJEDMWTLb++UUpBh3dEYIHgYKWHSGEFYiSUh6SUn41UA1TnPsEQlRsLqbywwMUPe4PUTGh/0NUtMWi1/GzpCiujw7j0dwCns8r4Z2icv4rOZqbYyMwdjAymXfnWJY+s53KIjchUTbm3TkWp0tLzONr9lGaV0vBgSoKD1SRn1PF/k3a273RrCcqJZiYVCcxqSFEDQnufNGa0QoRadrWloJtmnFa+gABdpcmAnmb4dv3tTJCD1EjIX5CqzhEpINOxZVUnJzuCME7wPlt9pv9xyYOSIsUgwohBPYJUZhTnVS8k03Fu/vx7O7/EBUdEWk28tfhifwwLoI/5ORz//5jLDpWykNpccwKb5+nuSVnckfo9Doik4KJTApm7KwEpJTUlNdTeKCKAv+26eNDSKnZhsPjHcQMcRKdpolD0MlGQTcu7txGUFsMxzZD3iY4tgl2LoFNL2nnzMFaeI24LL9ATICgqL7+2hTnIN0RAoOUMpDZQ0rZKIQY2G+oYtBhCLUQcetoatfkU/VpLkV/30zI1UOxje6fEBVdMSrIxpLMVD4rreahA8f47o6DzAwL4sG0OIbZez5VJYQgONxKcLiV9EnRADR6vBTmVgVGDXvWFbJzhbasxhFqJjrVGRg1tJtOAu2hf9d6VnRkLHREwrDLtA3A59Omjo5tahWHNU+Azx+u25kAceM1UYifADGZakpJ0S0hKBFCXOF390QIcSVQOrDNUgxGhE4QdEEclqEhlL+dTfnre6gfF0nIFanorAMbA0gIwRyXk1nhQbyUV8rfDhcya+Nebo6N4FfJ0YSb+nZ/k9VAYkY4iRnhgDadVHasjoIDlQFxyPFPJxnMeqJTggPiYAsy8flL31JR6CN/xbp201InoNOBK13bMv3Ldpo8ULCjjThsht3/8XdcD5EZfnuDXxwi0kE3MIv+FGcmJ3UfFUKkAq8DsWi+c0eBm6WUOQPfvBNR7qODw4Xu+BAVwZckU7Mij6aSOowuu5YoJ7yTh2E/UNbo5bFDhbyWX4pdr+OHcRF8XFLFQXcDqTYzr40ZcuJq5T5SU15PwYFKCnOqKDhYRVleLSd8PQWERts6nabqNrUlmiAc8wvDsc2aVxOAKQhiM1unk+yR8OHPTmt4DRg8//udMZDuoz1ZR+AAkFLW9rol/YASgsH1ZWg8WkP52/vwlnhaDwotdebxiXIGgr11Hh7KyWdZeevaSQGk2cysnDxiQO/d6PFSlFvNB09ua5ciFGD8pUmkT4oiPK6fgrD5fFB+oHXEcGwTFO4CX9NxBQUExcD33oHwNDAOnJfX8Qy2//3jOa3rCPwVzANGApYWbw4p5cO9bpFC0U1MCUFE/Xwcx363pvWgBG+xm5J/7sTosmGItGGMtGKItKGzG/vV42i43cqbY1OJXbaNlmBbEtjvbuC2XYeYFR7EzLBgos39n+3MZDWQkBFGaFvXVcBg1rH1iyNs+eww4XEO0idFMXRi1MmNzl2h02lv+xFDIfNG7VhTPRTugJcuoXVoIqEmH56bCkKnBeRzDQfXMO0zcjiED1V2h7OM7iwoew6wATOBF4FrgQ0D3C6FIoAw6jFE2vCWuANvxsKix+f2UrepENnYGg9RZzNgcNkwRtow+MXB6LKhDzH3acFams1MjrsBvwMnQXodG6pq+bBEC5SbYbcwMzyYWWFBTHTaMfWj22aL62pFoZvQaM111WjWk7O5mOwNhax9/wBr/32A2LQQ0idFkTo+sn/ScBotWlC+iGGt7qtCp62Anv17KNnr3/bB/s9aDdIICE1qLxCu4ZrtwTx4w0ifyXTHRrBDSjmmzacD+ERKOe3UNLE9ampocA6PvWUeSl/ZfYKNQPokzdUNeIs9NBW78Za4tc9iD7661mkNYdRhiPALg18kjJE2DOFWhOHkD+3DngZu2nGQA21sBIkWE3vq6vm6rJpl5TVsqKqjSUrseh3TQh3MDAtmZlgQif1kS+jsb19Z7Gb/xiKyNxRRWeRGZxAkjQwnfVI0yWPCMRj7aPj1h7jo0kbQ3ATlBzVhKG4jEGX7obmxtZwz0S8OLSOIEZpAWNq76/ak/4OF02ojEEJskFJOEkKsA74DlAHfSinTurxwgFBCoL4M3e1/c11TO2HQPt00Vza0FtKBIczaOr3UZjShM/fMU6jW28yqilq+Lq/m6/Jq8uo1IRpqMwdE4bwQB9ZeBto7Wd+llJQcqSF7fRH7NxXhrm7EZNEzZHwk6ZOiiEsPRTeAYTw6pNmr5XIo2QslezRxKNmriUrbIH3BcW1GD8PANULzfLKGqlhLfk63jeBDIUQI8BiwBW1w/kKvW6NQnCL0diN6uxNzsrPdcV9jM94SD95iN00lmjg0FXuo31cOza0vRvpgU2AEIewG3BuLaK5qwOCyErFw5AleSw6DnjkuJ3NcTqSUHPA0BEYLr+SX8nxeCVadYEqIg1nhmjAMsZr7zaYhhAgsbDv/2jSO7a0ge0MhB7YUs3dNAXanibSJUQybFE1EguPUZGHTG1pXS4+Y33rc1wyVh9uPHkr2wuZF0ORuLeeIhoZqaPIgkFq5166CW78GW5iK3tpPdCkEQggd8JU/Y9i7/oxiFillVVfXKRRnMjqTHlOcA9NxHjey2Ye3vD4gDC1CUbepCNnYHCjnLfZQ+PgWbGNd/tGDJhZt7RBCCNJsFtJsFm5PiMTd7GNdpTZaWFZWwwP7tcVkiRYTs/y2hakhDi3fcn/0USdIyAgjISOMCxubyd1RSvaGInYuy2P7l0cJjbb5jczRna9JGEh0eggbom3D57Ye9/m0GEsl+1pHENteb3Oh1EYYjw0BgxWccdpowhnv/4yD4PjW492YclKcRAiklD4hxNPAOP9+A9DQ1TUKxdmK0OswujTjsnVk63EpJcd+u6q9C2eTj/o95bg3FbVe39YO4Wpjj4iwYjPotAd+eDAM1WwOy8prWFZezduF5Sw6VopRCCY77YHRwnC7pV/e2g0mPUMnRDF0QhT1tU3kbNGMzOs/yGX9B7lEDwkmfVI0aVmRWAc4rMdJ0ek0Q3NoEqRfoh07trl9rKXgWDj/Z1CVB9XHtM8Dy6C20F+mDWZnG7E4TiRaxOMUusCeqXRnaugrIcQ1wHtyoBIcKxRnMEIIDK42Xktt1jG02CHaGqsbj1Tj2V7SWkGLHcJlDYwgYiJt3OwK4QdxETT4fGysquPrMk0YHj6Qz8MHIMZsZGZYEKMdVv6ZV8pB6SRt/Z4+LWazOIyMmh7HqOlxVJd5AkbmbxZns+rt/SRkhJE+KYqUsS6M5jNkdXFXsZba0uyFmoJWcag+pkVzrT6mjTLyt4K7g6AItojORcIZD94GeOt7p31B3UDSHWNxDWAHvEA9mveclFKeljGXMhYrY/Hp6H+L15K3xI3BZTvpyuaAHSJgrPZPN5V5OrVDGPzG6tIQI980eFhWXsuKimqqve3fcqNNBt7KTCPNZu4y01pPKM2rJXtDIfs3FlFb0YDBrGfI2AjSJ0WTMCK0feyj00S//O2b6jVhCIhEnj/0dxvhOD6B0PFYQuGCX2ojk6CY1s8BXjtxWo3FUsqBSS6rUJxFGMKtPVrJ3Fc7xIVWAxe5rBBpJTPMg6/N876w0cuFG/Zi0+sY5bAyJsjKmCAbY4KspFktGHrhGRQR7yAiPo0pV6WSn1NJ9oYiDmwpJntDEWa7AaS20jkkysa8u7qIdXSmY7RAeKq2dUZDTXuR+PAXtJsXrK+AL/9w4nUWpzaKCIqB4Bgtw1zg07/Zws9IA3d3FpRN7+j48YlqFArFyenKDtFc1egXiJb1EB68eytIHmsk16ZD6gTCJ4nzSG4/5mVPiIG9jjper6jjxZbcNxJG6I2MNpkYbbMwNthGeogNk92EMOpOanMQOkFceihx6aFMvz6dw9+W8eWi3TTVayJVUejmzYfWMWxyNJHJmodSWJwd/RkwYug3zEHaCunI4dr+umfaL6iLSIdbv/JPQ+VrW00+VBe0HiveDbVFJ9os9CYIiu5AJNoKRwwY2kz9+d1nLyzJhm8Hxn22OzaCe9v8bAEmAZuBWf3aEoViECOEwBBixhBixpIe2u7c3x5exT3jrByy6Uh2+/jbFg/DxkThc3vx5TfR5PaS6/Oy2+Rjt03H3mAvS4IbebW+DsrLMDdLhtb4GFHbzIh6wUivjjSdHrPNhN5mRNgM6G0GdDYjOqv/07+fMiocb0Nzu/Y0eyUHtpawe7WWr0pv0BEe7yAqKYjI5GBcSUGERttP/ZqFgcJvo2hnIzA7wOwPydEZzV5NDFrEocV+0SIYBTu0FKVt3WVbsIW3isTRdVBfrbnPlmZrbblrfb92sTtTQ5e33RdCJACP92srFApFpyQ7rLy9pr2hOvTq9g+gWGAqIJt8+DxNeOuaOFDtYUeNh52eenbqG/nY6eUd/7PZ5IOhngZG1HgYXtjE8MpmUmt9GDswGToE1LQ5HqQXzJ8ajVtCudtLWXUjZeUN7F1TEMixYDDpcCUG+UcNQUQmBeN0WU/N2oX+xp8PosfoDZrx2RnXeRkptaivHY0qWo7Vt7FZSJ8mSP1Mb4Ks5wEDG3ZRoVAEiFiYcUJ4jc4QRh16oxl9sJmMGAcZwA3+cz4pyfU0sLPGw/YaNztqPHxe62ZJtOYdZBKCYSYTowxGRmFgpFeQVg9Jy46wytNMcJ2ParuOUVY99XvLwe0lrFkSBgwFpE1HjU9HZbOk0iupzK1mZ05VIFifUS8ICzYSHmohItKKK8aOI9yC3t5mJGI1oLMa2oX9aDHUp5boKNyyecBDkJ9ShABriLZFdfJ3fXrycVNTXYxCekl3bARP0mop0QGZaCuMFQrFKaDFUK15jfQ+9LZOCFJtFlJtFq6K0qaffFJy2NPI9ho3O2s97Khx83GNhze92nSQUScQMxw0CkBoNoqlDbBm7jiklP4RiBfp8eJzNxHh8WpTVh5t89Y2UlFWT2l5PeVVjZTXedlTUY08WA2AWUCIXhBiEIToBaF6gVknEEadNj1lNdJU5oEmHwKBt9hN8bPbCZk3BJ3diM5h1ITEbuxWzKizkrbusxF+G0E/050RwaY2P3uBN6WUq/u9JQqF4pSjE4IUm5kUmzkgDlJKjtQ3sr1GE4anjhQHykud4KAVXswrYZ7LSYzZhM6kB2fn6xrCgbaBybyNzZQeraEop4riQ9UUH61hX2lr3CGbzUC4RU+oQU+oToDHyxZ3M7U+cOhgcrPE99aJLuTCokfvMGkCYTeidxhP/Nlh8guHAXG2GLi7SlXaT3RHCJYA9VLKZgAhhF4IYZNSdmDhUCgUZztCCJKsZpKsZq6IDOHz0qp2IbiNQvDA/mM8sP8YWcE25rlCmOdydnuRm8GkJzo1hOjUkMCxxnovpUdrKD5co4nD4RqOHqg+4doaH6xyNzPrpuE4LHpsQiDrvfhqm/DVNdFc5/8s99B4pBqfuwl8J1Sj9dNq0ETB0V4sWo+Z0DuM+BqbqViSjbfE0601JGcj3VpZDFwEtGQmswKfA+cPVKMUCsWZw2tjhpwQgrtJSpYWV7G0pNK/Ejqf0Q4r81xO5rlCGGrvWdgGk8VA7NBQYoe2ekzV1zVRcrjmhAxt9c2SjxftAfx5rsPMBEdYCXZZcUZYCU4PIzjCQliEFbPVoE1T1TXhq20RisY2P2vHvaUeGg/5haOLNbbeYjeFj2/BOjysdVrKoQlGq6CYEBb9WWUY744QWNqmp5RS1gohVPohhWKQkGQ1800HaTl/kWzhF8lRHPY08HGJJgp/zi3kz7mFpNsszI90Mt8Vwohexkyy2I0nZGgTAoJdVmZ+bzhVpR6qSz1Ul9ZTXeohd1sJnpr2qTXNNoMmEhEW/6dfLFJDCQ4zn7D+QfokPrd/VOEfZZS/ufeEOFNNhXU01zYhPV46RC9aRxYt01GOllGHqfVnv5iIvuaM6CPdEYI6IcR4KeUWACFEFuA5yTUKhWKQkGQ185PESH6SGElBQyMfl1TxUUkljx8q4m+HikixmvzTRyFkBvXchbRthrYQf4Y2p8tK3LDQE8o21nsDwlBd6qGqRPssO1ZH7o5SfN7WJ3q70UQbsXC6tH1zpA0hBPkfH2LNkZqAjeL8xCDS/kuL1CC9Pnxuv2i0jDJq/SOOlumq2ka8JW58tU3Ipo7nqYRJ30YotFFFywgDJLWr8kmtHDivqe4IwS+Bd4QQ+WhThNHA9f3aCoVCcU4QYzZxS7yLW+JdlDQ28WlpFUuLq3juaDFPHSkmzmwM2BQmOu3ouiEKTpeV7/7hPL/X1HldljVZDP5wGSemxPT5JHWVDQGRqC6tDwhF7vYTRxMmq4HgCAtVxW5ant81Ptjg9gaM38KgQx+suet2B19js18kGlvtGrWaeDS3TF9VNNCYV6Nl2GujGwKBt8RN6Su7exTupDt0Z0HZRiHEcGCY/9A+KWVTV9coFAqFy2TkptgIboqNoKLJy+el1SwtqQwk6Yk0GbgsQps+mhLi6FWMpJ6g0wmCwiwEhVmIS+94NFFT1ioO1SUeqkrrKT1a265cRbGHb97cR/zwMGLTQ3qUH1pn0qML02MIO7kNRfokPo+XgkfWtU5NSbQouP1Md9YR3AW8LqXc5d8PFULcKKV8pt9bo1AozklCjQaujwnj+pgwar3NfFlWzUcllbxdWMEr+WWEGfXMidAMzdNCHZh0p96102QxEB7nIPy4QIFvPLQuYKMAMJh17FlXqK2iFuBKCCJ+eCjxw0OJSQvBaOqf+X6h0+wMHYVA72+6MzV0m5Ty6ZYdKWWFEOI2QAmBQqHoMQ6DnquiQrkqKhR3s4/l5dV8VFLFB8WVvFFQTrBBxyXhTua5nMwIC+51juf+osVGUVnk1qKv3jkWR6iZokPV5O2tIG9vOdu/OsrWz4+gMwiiU5x+YQgjMjmozwH5erKyvLd0Rwj0QgjRkpRGCKEHTnMaI4VCcS5g0+uY6wphriuEBp+Pb8prWFpSxWelVSwpqsCm13Ge086e2nqK+iExT29osVEcT2xaCLFpIUyan0JTQzMFOZWaMOyrYMNHuWz4MBejWU9segjxw7QRQ3isI5DOtLv018ryLu/RjTKfAm8JIf6ff//HwCcD0hqFQjFoMet0XBzh5OIIJ00+ydrKWj4qqeT1/DK0gBeCbHcDszfu487ESEY6rGQ4rMSbjafdZ99o1pM4MpzEkeEA1Nc2cSy7IiAMh3eWAWANMhKXHhqYSgqOODMC8XVHCP4buB24w7+/A81zSKFQKAYEo04wPSyI6WFBvJ5f1u5cbbOP/8stDOw7DXpG2C2MdFgD4jDMbjmtU0oWh5HU8ZGkjo8EoLai3j+NpE0l5WzWwnYEhVkCohA3LBR7F6E6BpLueA35hBDrgVTgOiACeLc7lQsh5gD/APTAi1LKP3dS7hq0UBYTpZSbOiqjUCgGJ6k2cyDEhQ5Is5n5JCudPXX1fFvr4dtaD7trPbxZWI67WfO31PmvaysOIx1WokyG0/IG7gi1MHxKDMOnxCClpLLIHRgtHNxWwp41Wm6H0Bi7JgzDNGEwW3sTILrndHoXIUQ6cKN/KwXeApBSzuxOxX5bwtPAxWihqzcKIT6QUu4+rlwQ8AugfzMtKBSKc4KWEBc57nrSbBZeGzMEu0HPBKedCU57oFxLJNWAONR52FRdx7+LKwNlwox6MuxtxcFCut1ySr2UhBCERtsJjbYzekY8Pp+k9GhNQBj2rMpn57I8hABXUjCuRAeHd5VRW+Ejf8W6wIK6/qQrudkLrATmSylz/B24uwd1TwJypJQH/dcuBq4Edh9X7o/Ao7TPhKZQKBRAa4iL5cuXM2PyuE7LtY2kOj+yNaBdVZOX3f7Rw26/SLySX0q9T/MHNQgYarO0GzlkOCy4TN1fH9AXdDpBZJKW9nP8pUk0N/kozK0ib28Fx/ZV8O03+YGylYVulj6zvUPjdV/oSgi+g5bTYpkQ4lNgMdrK4u4SBxxts58HTG5bQAgxHkiQUi4VQnQqBEKI29HsFLhcLpYvX96DZpxb1NbWqv4P0v4P5r5D3/uf5t+uAJolFKLjMHoOSz2H65r4us7NkqLWkUEIPpJoUzmbcgAADE5JREFUJhwfWzFShSAGH7+mjijRSUjT/iQYwiZCwUECC8qk1PJG9/f/QadCIKX8N/BvIYQd7U3+l0CkEOJZ4H0p5ed9ubEQQgf8DfjBycpKKZ8HngcYNmyYHKiY3GcDywcwJvnZwGDu/2DuO5ya/pc1etlT52lje/j/7d19bF11Hcfx96ft2nFHuw2cPGwEBozBQmCtC6CiTsUIaEAT0fnIH5iFRAKixkA0JJIY40N8fpyKIIoIA3HBCShQZzQgD9uQgUqZRphzA5x7oDg29vWP36/bXWlZN3p61/v7vJKmPeee3fv79Xd3Pz3n/M73/I+7tuwqrbaGVi5rmcyHZ0xjbleN7s4ah1U8a+lfv7tnt6J7Uw6t7bHUxt4aycniZ4HrgOskTQXOI80k2lMQrAGOqFuekdcN6AROBHrzL/FQYImkc3zC2Mwa4eD2Nk5v7+T0qZ07102/e0Wevpo8tyP47hNPsS1fanxIexvdXTW6OyfR3VXj5M4DmDxh9E7yDlV0b7TtVWsjYgPpL/NFI9j8PmCWpJmkAFgAvK/uuTaSZiABIKkX+IRDwMz2J0PNWrpj3mwe2fIcD27uZ8WmfpZv6ue2p3fdSOeYAzro7qoxt6tGT2eNOQcewMR9nM66N0X39lVlc5MiYruki4DbSdNHr4qIVZKuBO6PiCVVvbaZ2WgZ6sY8E1tb6Jk8iZ66WUsbt21n5ebnWL6pn+Wbn2XZhs0sXrcBSHd1O+HAiXR31tLeQ9ckjq110LofXEwGFQYBQEQsBZYOWnfFMNvOr7ItZmb7Yrgb8ww2eULbzovgIN37ee3WbazYnPYYlm/q56Z1qcgewKTWFk4eCIb8/fAGXSU9NlcrmJkVRhKHT2zn8IntnD0tTWfdEUFf/9bdwmFR3fmGV7a3MXfnXkONuZ01Nm5/IV1HUWGtJQeBmdkYaZE4blK6iO3dhx4EwNYdO1i15bl0riGfc7jjmV3nGyZIOShEX/9WPvjQ6hHtoewNB4GZWQN1tLTQ0zWJnq5d5xs2bX+BlTkYPrd67c71O4DH+7eOehsaW+jbzMxepKutldcd1MnFRx7CrFrHzg/qgRpKo81BYGa2H7v2pKM5ttZBC8GxedbSaHMQmJntxwZmLV2njSw79YRKbsrjIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PCOQjMzApXaRBIOlPSXyX1SbpsiMc/JukRSQ9JulPSkVW2x8zMXqyyIJDUCnwLOAuYA7xX0pxBmy0H5kXEScBi4AtVtcfMzIZW5R7BKUBfRKyOiOeB64Fz6zeIiLsjoj8v3gPMqLA9ZmY2hLYKn3s68ETd8pPAqS+x/QXAr4d6QNJCYCHAtGnT6O3tHaUmjj9btmxx/wvtf8l9B/e/yv5XGQQjJukDwDzgDUM9HhGLgEUAs2fPjvnz549d4/Yzvb29uP/zG92Mhii57+D+V9n/KoNgDXBE3fKMvG43ks4APgW8ISK2VtgeMzMbQpXnCO4DZkmaKakdWAAsqd9AUjfwPeCciFhfYVvMzGwYlQVBRGwHLgJuBx4FboiIVZKulHRO3uyLwIHAjZJWSFoyzNOZmVlFKj1HEBFLgaWD1l1R9/MZVb6+mZntma8sNjMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8I5CMzMCucgMDMrnIPAzKxwDgIzs8JVGgSSzpT0V0l9ki4b4vEOST/Pj98r6agq22NmZi9WWRBIagW+BZwFzAHeK2nOoM0uADZExLHAV4DPV9UeMzMbWpV7BKcAfRGxOiKeB64Hzh20zbnANfnnxcCbJanCNpmZ2SBtFT73dOCJuuUngVOH2yYitkvaCBwMPF2/kaSFwMK8uFXSw5W0eHx4BYN+P4Upuf8l9x3c/5fb/yOHe6DKIBg1EbEIWAQg6f6ImNfgJjWM+19u/0vuO7j/Vfa/ykNDa4Aj6pZn5HVDbiOpDZgMPFNhm8zMbJAqg+A+YJakmZLagQXAkkHbLAHOzz+/C7grIqLCNpmZ2SCVHRrKx/wvAm4HWoGrImKVpCuB+yNiCfBD4FpJfcB/SGGxJ4uqavM44f6Xq+S+g/tfWf/lP8DNzMrmK4vNzArnIDAzK9y4CoI9laxoJpKOkHS3pEckrZJ0SV5/kKTfSHosf5/a6LZWSVKrpOWSbs3LM3M5kr5cnqS90W2siqQpkhZL+oukRyW9uqTxl3Rpfu8/LOlnkiY28/hLukrS+vrrpIYbbyVfz7+HhyT1vJzXHjdBMMKSFc1kO/DxiJgDnAZ8JPf3MuDOiJgF3JmXm9klwKN1y58HvpLLkmwglSlpVl8DbouI44GTSb+HIsZf0nTgYmBeRJxImnCygOYe/6uBMwetG268zwJm5a+FwHdezguPmyBgZCUrmkZErI2IB/PPm0kfAtPZvSzHNcA7GtPC6kmaAbwN+EFeFvAmUjkSaOL+S5oMvJ40s46IeD4i/ktB40+a1XhAvsaoBqylicc/IpaRZk/WG268zwV+HMk9wBRJh+3ra4+nIBiqZMX0BrVlTOWqrN3AvcAhEbE2P/Rv4JAGNWssfBX4JLAjLx8M/DcituflZn4PzASeAn6UD439QNIkChn/iFgDfAn4JykANgIPUM74DxhuvEf183A8BUGRJB0I3AR8NCI21T+WL75ryvm/kt4OrI+IBxrdlgZpA3qA70REN/Asgw4DNfn4TyX91TsTOByYxIsPmxSlyvEeT0EwkpIVTUXSBFII/DQibs6r1w3sAubv6xvVvoq9FjhH0j9IhwHfRDpmPiUfKoDmfg88CTwZEffm5cWkYChl/M8A/h4RT0XENuBm0nuilPEfMNx4j+rn4XgKgpGUrGga+Xj4D4FHI+LLdQ/Vl+U4H/jlWLdtLETE5RExIyKOIo31XRHxfuBuUjkSaO7+/xt4QtLsvOrNwCMUMv6kQ0KnSarl/wsD/S9i/OsMN95LgA/l2UOnARvrDiHtvYgYN1/A2cDfgMeBTzW6PRX39XTSbuBDwIr8dTbpOPmdwGPAb4GDGt3WMfhdzAduzT8fDfwJ6ANuBDoa3b4K+z0XuD+/B24BppY0/sBngL8ADwPXAh3NPP7Az0jnQ7aR9ggvGG68AZFmUT4O/Jk0u2qfX9slJszMCjeeDg2ZmVkFHARmZoVzEJiZFc5BYGZWOAeBmVnhHARmgKRDJV0v6XFJD0haKum4+kqQZs2qsltVmo0X+YKlXwDXRMSCvO5kmrSOj9lg3iMwgzcC2yLiuwMrImIldUW9JB0l6feSHsxfr8nrD5O0TNKKXDf/dfkeClfn5T9LujRve4yk2/Iex+8lHZ/Xn5e3XSlp2dh23cx7BGYAJ5IqW76U9cBbIuJ/kmaRrgKdB7wPuD0iPpvvmVEjXRE8PVIdfSRNyc+xCLgwIh6TdCrwbVINpSuAt0bEmrptzcaMg8BsZCYA35Q0F3gBOC6vvw+4KhcIvCUiVkhaDRwt6RvAr4A7chXZ1wA3piNRQCqZAPAH4GpJN5CKq5mNKR8aMoNVwKv2sM2lwDrSncLmAe2w82YirydVfrxa0ociYkPerhe4kHRjnRZSLf25dV8n5Oe4EPg0qZrkA5IOHuX+mb0kB4EZ3AV0SFo4sELSSexe5ncysDYidgAfJN06EUlHAusi4vukD/weSa8AWiLiJtIHfE+ke0n8XdJ5+d8pn5BG0jERcW9EXEG6GU3965pVzkFgxYtUefGdwBl5+ugq4HOkO0IN+DZwvqSVwPGkG8VAqoy6UtJy4D2keyZMB3olrQB+Alyet30/cEF+jlXsutXqF/NJ5YeBPwIrq+mp2dBcfdTMrHDeIzAzK5yDwMyscA4CM7PCOQjMzArnIDAzK5yDwMyscA4CM7PC/R/Rx2kfnc5yJgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2adzRbEPLbH"
      },
      "source": [
        "runSVM = np.array([0.808, 0.7615, 0.6686666666666666,0.60725,0.5356,0.499,0.47,0.441875,0.4142222222222222,0.3876])\n",
        "\n",
        "runKNN = np.array([0.853, 0.797, 0.68, 0.632, 0.547, 0.5197, 0.503, 0.489, 0.475, 0.455])\n",
        "\n",
        "\n",
        "run1c = np.array([0.812,\t0.7385,\t0.6893,\t0.62025,\t0.582,\t0.5628,\t0.538,\t0.5166,\t0.4948, 0.4862])\n",
        "run2c = np.array([0.754, 0.7185, 0.662, 0.608, 0.5796, 0.5598, 0.5278, 0.5005, 0.4985, 0.4707])\n",
        "run3c = np.array([0.833, 0.759, 0.695, 0.6367, 0.5966, 0.5823, 0.545, 0.52125, 0.495, 0.4847])\n",
        "iCaRL = np.array([run1c, run2c, run3c])\n",
        "meaniCaRL = np.mean(iCaRL, axis = 0)\n",
        "stdiCaRL = np.std(iCaRL, axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "92lLwTKaunVv",
        "outputId": "1b3c50ee-9a74-4118-b17c-923bea2750f2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = np.linspace(10,100, num = 10)\n",
        "plt.show()\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.errorbar(x, runSVM, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:green')\n",
        "plt.errorbar(x, runKNN, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:orange')\n",
        "plt.errorbar(x, meaniCaRL, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'm')\n",
        "plt.axis([0, 105, 0, 1])\n",
        "plt.legend(['SVM', 'KNN', 'NME'])\n",
        "#plt.show()\n",
        "plt.savefig(\"classifiers.pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "oWXkGcIhu4P2",
        "outputId": "8da1ce74-0d8e-4807-f743-fd2a23c3bda6"
      },
      "source": [
        "import numpy as np\n",
        "runBF = np.array([0.784,\t0.705,\t0.6433,\t0.563,\t0.5128,\t0.4801,\t0.4497,\t0.405,\t0.3926, 0.3875])\n",
        "\n",
        "run1c = np.array([0.812,\t0.7385,\t0.6893,\t0.62025,\t0.582,\t0.5628,\t0.538,\t0.5166,\t0.4948, 0.4862])\n",
        "run2c = np.array([0.754, 0.7185, 0.662, 0.608, 0.5796, 0.5598, 0.5278, 0.5005, 0.4985, 0.4707])\n",
        "run3c = np.array([0.833, 0.759, 0.695, 0.6367, 0.5966, 0.5823, 0.545, 0.52125, 0.495, 0.4847])\n",
        "iCaRL = np.array([run1c, run2c, run3c])\n",
        "meaniCaRL = np.mean(iCaRL, axis = 0)\n",
        "stdiCaRL = np.std(iCaRL, axis = 0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "x = np.linspace(10,100, num = 10)\n",
        "plt.show()\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "plt.errorbar(x, runBF, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:green')\n",
        "plt.errorbar(x, meaniCaRL, linestyle=\"-\", marker = 'o', markersize = 4, ecolor='k', color = 'tab:blue')\n",
        "plt.axis([0, 105, 0, 1])\n",
        "plt.legend(['Balanced Finetune', 'iCaRL'])\n",
        "#plt.show()\n",
        "plt.savefig(\"variation.pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0hlF1v58W5h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}