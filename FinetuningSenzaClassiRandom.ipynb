{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinetuningSenzaClassiRandom.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "c9XO0l5B9WDO",
        "bSAhwwBr9iZ9",
        "nLE5LnQkZXSI"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoManca-FM/Project_MLDL/blob/main/FinetuningSenzaClassiRandom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXuDQrvO35Pb"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj4HVjkW2RBe"
      },
      "source": [
        "#import sys\n",
        "#sys.path.insert(0,\"/content/drive/MyDrive/OWR-project/Project_MLDL\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXZiclrtCLr7"
      },
      "source": [
        "Classification Network (using Resnet32) on CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlNz0nWYCR0L"
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9XO0l5B9WDO"
      },
      "source": [
        "### Data Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMcd8DV4C1nw",
        "outputId": "a2d2a7d8-5df8-465a-a87b-0e2c308875a0"
      },
      "source": [
        "# we build a transform to normalize images: Data normalization is an important step which ensures \n",
        "# each input parameter (pixel, in this case) has a similar data distribution. This makes convergence \n",
        "# faster while training the network.\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, \n",
        "                                         download=True, transform=transform)\n",
        "# DataLoader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
        "# batch_size = how many samples per batch to load\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhi2ESn89cml"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZyFOrsg4-L7"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        \n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=100):\n",
        "        self.inplanes = 16\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def addOutputNodes(self, num_new_outputs):\n",
        "        in_features = self.fc.in_features\n",
        "        out_features = self.fc.out_features\n",
        "        weight = self.fc.weight.data\n",
        "        self.fc = nn.Linear(in_features, out_features + num_new_outputs)\n",
        "        self.fc.weight.data[:out_features] = weight\n",
        "        #print(self.fc.out_features)\n",
        "\n",
        "def resnet20(pretrained=False, **kwargs):\n",
        "    n = 3\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet32(pretrained=False, **kwargs):\n",
        "    n = 5\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet56(pretrained=False, **kwargs):\n",
        "    n = 9\n",
        "    model = ResNet(Bottleneck, [n, n, n], **kwargs)\n",
        "    return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzrCPkfN5txO",
        "outputId": "88bf0ada-5620-43a5-8886-53e8b19e895e"
      },
      "source": [
        "net = resnet32()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
        "net.to(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "  (fc): Linear(in_features=64, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSAhwwBr9iZ9"
      },
      "source": [
        "### Define the loss and the optimization technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI6grd9hI2as"
      },
      "source": [
        "lr = 0.01\n",
        "decay = 0.0001\n",
        "epochs = 70\n",
        "momentum = 0.9\n",
        "factor = 5"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJqiRlCoI-b0"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.SGD(net.parameters(), lr = lr, weight_decay=decay,momentum= momentum)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCnzW3XCF0JW"
      },
      "source": [
        "### NON INCREMENTAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9zwIkLXJPx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30afe50-029e-4b30-c96e-e2d22c87ef5a"
      },
      "source": [
        "#train the network\n",
        "for epoch in range(10):\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "    # get the inputs; data is a list of  [input,labels]\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    optimizer.zero_grad() # Sets the gradients of all optimized torch.Tensor to zero.\n",
        "\n",
        "    outputs = net.forward(inputs) # forward: assign weights to each edge in each layer\n",
        "    loss = criterion(outputs,labels) # calculate the loss \n",
        "    loss.backward() # redesign the weights evaluating the performance of the network\n",
        "    optimizer.step() # update parameters \n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 100 == 99:    # print every 100 mini-batches the average value of the loss accumulated in each batch\n",
        "      print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
        "      running_loss = 0.0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 4.522\n",
            "[1,   200] loss: 4.186\n",
            "[1,   300] loss: 3.936\n",
            "[2,   100] loss: 3.657\n",
            "[2,   200] loss: 3.563\n",
            "[2,   300] loss: 3.483\n",
            "[3,   100] loss: 3.300\n",
            "[3,   200] loss: 3.231\n",
            "[3,   300] loss: 3.168\n",
            "[4,   100] loss: 2.971\n",
            "[4,   200] loss: 2.936\n",
            "[4,   300] loss: 2.892\n",
            "[5,   100] loss: 2.720\n",
            "[5,   200] loss: 2.672\n",
            "[5,   300] loss: 2.660\n",
            "[6,   100] loss: 2.509\n",
            "[6,   200] loss: 2.476\n",
            "[6,   300] loss: 2.445\n",
            "[7,   100] loss: 2.299\n",
            "[7,   200] loss: 2.308\n",
            "[7,   300] loss: 2.302\n",
            "[8,   100] loss: 2.155\n",
            "[8,   200] loss: 2.178\n",
            "[8,   300] loss: 2.159\n",
            "[9,   100] loss: 1.999\n",
            "[9,   200] loss: 2.023\n",
            "[9,   300] loss: 2.040\n",
            "[10,   100] loss: 1.879\n",
            "[10,   200] loss: 1.922\n",
            "[10,   300] loss: 1.915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "g6YoczYtQu90",
        "outputId": "21639139-5909-4d9e-c6d3-9f181ec241c6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "confusion_matrix = torch.zeros(100,100)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "          confusion_matrix[t.long(),p.long()] += 1\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=plt.cm.jet)\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 40 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdXyhojz73ba"
      },
      "source": [
        "###INCREMENTAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClFYYN4dYzKf"
      },
      "source": [
        "#### training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi8z0BUyK9tt"
      },
      "source": [
        "# Each time we call training function we must pass a different trainloader, updated with the following 10 classes\n",
        "def training(trainloader, iteration, network, device, epochs, num_classes):\n",
        "  if (iteration != 0):\n",
        "    # add 10 output nodes to the network\n",
        "    network.addOutputNodes(num_classes)\n",
        "    network.to(device)\n",
        "  \n",
        "  optimizer = optim.SGD(net.parameters(), lr = lr, weight_decay=decay,momentum= momentum)\n",
        "\n",
        "  #train the network\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      # get the inputs; data is a list of  [input,labels]\n",
        "\n",
        "      inputs = data[0].to(device)\n",
        "      labels = data[1].to(device)\n",
        "      optimizer.zero_grad() # Sets the gradients of all optimized torch.Tensor to zero.\n",
        "\n",
        "      outputs = network.forward(inputs) # forward: assign weights to each edge in each layer\n",
        "      loss = criterion(outputs,labels) # calculate the loss \n",
        "      loss.backward() # redesign the weights evaluating the performance of the network\n",
        "      optimizer.step() # update parameters \n",
        "\n",
        "      running_loss += loss.item()\n",
        "      if i % 20 == 19:    # print every 20 mini-batches the average value of the loss accumulated in each batch\n",
        "        print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 20))\n",
        "        running_loss = 0.0"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9vHvoVoY4XR"
      },
      "source": [
        "####test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmbA81iYSSSc"
      },
      "source": [
        "def test(testloader, iteration, network, acc):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  print(\"ITERATION: \", iteration)\n",
        "  # if(iteration == 9):\n",
        "  #   confusion_matrix = torch.zeros(100,100)\n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          images, labels = data[0].to(device), data[1].to(device)\n",
        "          \n",
        "          # calculate outputs by running images through the network\n",
        "          outputs = network.forward(images)\n",
        "\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "          \n",
        "  acc.append(100*correct/total)\n",
        "  print(f'Accuracy of the network on the {iteration} iteration: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BBkS-svY697"
      },
      "source": [
        "#### train execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDn00aWqQ2FM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f5e175-6089-4ef4-9e23-848f80306014"
      },
      "source": [
        "# divided our dataset into sample of 10 classes each\n",
        "# train the network on the first 10 classes\n",
        "# evaluate the network on the first 10 classes\n",
        "# train the network on the second 10 classes (adding 10 output layers)\n",
        "# evaluate the network on the first 20 classes\n",
        "iterations = 10\n",
        "num_classes = 10\n",
        "test_set = [] #initialized here because we test over all the classes not only those one in which I train\n",
        "acc = []\n",
        "import random\n",
        "#indices = list(range(0,100))\n",
        "#random.shuffle(indices)\n",
        "for i in range(iterations):\n",
        "  classes_current_iter = range(i*num_classes, i*num_classes+num_classes)\n",
        "  train_iter = [] \n",
        "  for j in range(len(trainset)):\n",
        "    if(trainset[j][-1] in classes_current_iter):\n",
        "      test_set.append(trainset[j]) \n",
        "      train_iter.append(trainset[j])\n",
        "\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_iter, shuffle = True, batch_size=batch_size, num_workers=2)\n",
        "  valid_loader = torch.utils.data.DataLoader(test_set, shuffle = True, batch_size = batch_size, num_workers=2) \n",
        "  training(train_loader, i, net, device, epochs, num_classes) # Train the network with 10 classes at a time\n",
        "\n",
        "  test(valid_loader, i, net, acc) # Test the network with all classes seen until this iteration\n",
        "\n",
        "  # train loader contains at each iteration the new 10 classes used to evaluate the network, while valid loader contains all classes seen so far"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,    20] loss: 0.899\n",
            "[1,    40] loss: 0.587\n",
            "[2,    20] loss: 0.494\n",
            "[2,    40] loss: 0.441\n",
            "[3,    20] loss: 0.322\n",
            "[3,    40] loss: 0.284\n",
            "[4,    20] loss: 0.170\n",
            "[4,    40] loss: 0.161\n",
            "[5,    20] loss: 0.121\n",
            "[5,    40] loss: 0.157\n",
            "[6,    20] loss: 0.199\n",
            "[6,    40] loss: 0.165\n",
            "[7,    20] loss: 0.258\n",
            "[7,    40] loss: 0.187\n",
            "[8,    20] loss: 0.098\n",
            "[8,    40] loss: 0.073\n",
            "[9,    20] loss: 0.055\n",
            "[9,    40] loss: 0.066\n",
            "[10,    20] loss: 0.115\n",
            "[10,    40] loss: 0.088\n",
            "[11,    20] loss: 0.113\n",
            "[11,    40] loss: 0.089\n",
            "[12,    20] loss: 0.039\n",
            "[12,    40] loss: 0.039\n",
            "[13,    20] loss: 0.129\n",
            "[13,    40] loss: 0.067\n",
            "[14,    20] loss: 0.028\n",
            "[14,    40] loss: 0.025\n",
            "[15,    20] loss: 0.012\n",
            "[15,    40] loss: 0.009\n",
            "[16,    20] loss: 0.007\n",
            "[16,    40] loss: 0.008\n",
            "[17,    20] loss: 0.004\n",
            "[17,    40] loss: 0.022\n",
            "[18,    20] loss: 0.149\n",
            "[18,    40] loss: 0.128\n",
            "[19,    20] loss: 0.109\n",
            "[19,    40] loss: 0.110\n",
            "[20,    20] loss: 0.269\n",
            "[20,    40] loss: 0.206\n",
            "[21,    20] loss: 0.226\n",
            "[21,    40] loss: 0.152\n",
            "[22,    20] loss: 0.159\n",
            "[22,    40] loss: 0.137\n",
            "[23,    20] loss: 0.289\n",
            "[23,    40] loss: 0.157\n",
            "[24,    20] loss: 0.097\n",
            "[24,    40] loss: 0.054\n",
            "[25,    20] loss: 0.018\n",
            "[25,    40] loss: 0.013\n",
            "[26,    20] loss: 0.008\n",
            "[26,    40] loss: 0.040\n",
            "[27,    20] loss: 0.230\n",
            "[27,    40] loss: 0.172\n",
            "[28,    20] loss: 0.158\n",
            "[28,    40] loss: 0.121\n",
            "[29,    20] loss: 0.056\n",
            "[29,    40] loss: 0.086\n",
            "[30,    20] loss: 0.269\n",
            "[30,    40] loss: 0.149\n",
            "[31,    20] loss: 0.137\n",
            "[31,    40] loss: 0.078\n",
            "[32,    20] loss: 0.111\n",
            "[32,    40] loss: 0.086\n",
            "[33,    20] loss: 0.129\n",
            "[33,    40] loss: 0.071\n",
            "[34,    20] loss: 0.031\n",
            "[34,    40] loss: 0.016\n",
            "[35,    20] loss: 0.009\n",
            "[35,    40] loss: 0.012\n",
            "[36,    20] loss: 0.017\n",
            "[36,    40] loss: 0.034\n",
            "[37,    20] loss: 0.139\n",
            "[37,    40] loss: 0.087\n",
            "[38,    20] loss: 0.140\n",
            "[38,    40] loss: 0.094\n",
            "[39,    20] loss: 0.187\n",
            "[39,    40] loss: 0.106\n",
            "[40,    20] loss: 0.103\n",
            "[40,    40] loss: 0.102\n",
            "[41,    20] loss: 0.438\n",
            "[41,    40] loss: 0.152\n",
            "[42,    20] loss: 0.070\n",
            "[42,    40] loss: 0.042\n",
            "[43,    20] loss: 0.014\n",
            "[43,    40] loss: 0.010\n",
            "[44,    20] loss: 0.004\n",
            "[44,    40] loss: 0.015\n",
            "[45,    20] loss: 0.104\n",
            "[45,    40] loss: 0.046\n",
            "[46,    20] loss: 0.018\n",
            "[46,    40] loss: 0.029\n",
            "[47,    20] loss: 0.066\n",
            "[47,    40] loss: 0.043\n",
            "[48,    20] loss: 0.060\n",
            "[48,    40] loss: 0.050\n",
            "[49,    20] loss: 0.153\n",
            "[49,    40] loss: 0.065\n",
            "[50,    20] loss: 0.028\n",
            "[50,    40] loss: 0.012\n",
            "[51,    20] loss: 0.004\n",
            "[51,    40] loss: 0.004\n",
            "[52,    20] loss: 0.003\n",
            "[52,    40] loss: 0.002\n",
            "[53,    20] loss: 0.002\n",
            "[53,    40] loss: 0.009\n",
            "[54,    20] loss: 0.029\n",
            "[54,    40] loss: 0.064\n",
            "[55,    20] loss: 0.145\n",
            "[55,    40] loss: 0.137\n",
            "[56,    20] loss: 0.203\n",
            "[56,    40] loss: 0.097\n",
            "[57,    20] loss: 0.041\n",
            "[57,    40] loss: 0.032\n",
            "[58,    20] loss: 0.089\n",
            "[58,    40] loss: 0.037\n",
            "[59,    20] loss: 0.027\n",
            "[59,    40] loss: 0.032\n",
            "[60,    20] loss: 0.096\n",
            "[60,    40] loss: 0.032\n",
            "[61,    20] loss: 0.017\n",
            "[61,    40] loss: 0.015\n",
            "[62,    20] loss: 0.028\n",
            "[62,    40] loss: 0.020\n",
            "[63,    20] loss: 0.025\n",
            "[63,    40] loss: 0.024\n",
            "[64,    20] loss: 0.055\n",
            "[64,    40] loss: 0.033\n",
            "[65,    20] loss: 0.055\n",
            "[65,    40] loss: 0.085\n",
            "[66,    20] loss: 0.140\n",
            "[66,    40] loss: 0.068\n",
            "[67,    20] loss: 0.062\n",
            "[67,    40] loss: 0.023\n",
            "[68,    20] loss: 0.008\n",
            "[68,    40] loss: 0.030\n",
            "[69,    20] loss: 0.079\n",
            "[69,    40] loss: 0.110\n",
            "[70,    20] loss: 0.164\n",
            "[70,    40] loss: 0.127\n",
            "ITERATION:  0\n",
            "Accuracy of the network on the 0 iteration: 89 %\n",
            "[1,    20] loss: 4.391\n",
            "[1,    40] loss: 1.057\n",
            "[2,    20] loss: 0.759\n",
            "[2,    40] loss: 0.707\n",
            "[3,    20] loss: 0.573\n",
            "[3,    40] loss: 0.555\n",
            "[4,    20] loss: 0.448\n",
            "[4,    40] loss: 0.477\n",
            "[5,    20] loss: 0.367\n",
            "[5,    40] loss: 0.322\n",
            "[6,    20] loss: 0.222\n",
            "[6,    40] loss: 0.248\n",
            "[7,    20] loss: 0.162\n",
            "[7,    40] loss: 0.164\n",
            "[8,    20] loss: 0.132\n",
            "[8,    40] loss: 0.124\n",
            "[9,    20] loss: 0.087\n",
            "[9,    40] loss: 0.099\n",
            "[10,    20] loss: 0.145\n",
            "[10,    40] loss: 0.117\n",
            "[11,    20] loss: 0.172\n",
            "[11,    40] loss: 0.158\n",
            "[12,    20] loss: 0.190\n",
            "[12,    40] loss: 0.161\n",
            "[13,    20] loss: 0.139\n",
            "[13,    40] loss: 0.102\n",
            "[14,    20] loss: 0.053\n",
            "[14,    40] loss: 0.052\n",
            "[15,    20] loss: 0.112\n",
            "[15,    40] loss: 0.085\n",
            "[16,    20] loss: 0.311\n",
            "[16,    40] loss: 0.253\n",
            "[17,    20] loss: 0.213\n",
            "[17,    40] loss: 0.162\n",
            "[18,    20] loss: 0.194\n",
            "[18,    40] loss: 0.107\n",
            "[19,    20] loss: 0.101\n",
            "[19,    40] loss: 0.090\n",
            "[20,    20] loss: 0.136\n",
            "[20,    40] loss: 0.137\n",
            "[21,    20] loss: 0.205\n",
            "[21,    40] loss: 0.149\n",
            "[22,    20] loss: 0.112\n",
            "[22,    40] loss: 0.124\n",
            "[23,    20] loss: 0.194\n",
            "[23,    40] loss: 0.090\n",
            "[24,    20] loss: 0.080\n",
            "[24,    40] loss: 0.047\n",
            "[25,    20] loss: 0.022\n",
            "[25,    40] loss: 0.017\n",
            "[26,    20] loss: 0.012\n",
            "[26,    40] loss: 0.015\n",
            "[27,    20] loss: 0.019\n",
            "[27,    40] loss: 0.031\n",
            "[28,    20] loss: 0.077\n",
            "[28,    40] loss: 0.073\n",
            "[29,    20] loss: 0.167\n",
            "[29,    40] loss: 0.113\n",
            "[30,    20] loss: 0.224\n",
            "[30,    40] loss: 0.100\n",
            "[31,    20] loss: 0.049\n",
            "[31,    40] loss: 0.028\n",
            "[32,    20] loss: 0.011\n",
            "[32,    40] loss: 0.007\n",
            "[33,    20] loss: 0.005\n",
            "[33,    40] loss: 0.011\n",
            "[34,    20] loss: 0.027\n",
            "[34,    40] loss: 0.055\n",
            "[35,    20] loss: 0.068\n",
            "[35,    40] loss: 0.042\n",
            "[36,    20] loss: 0.054\n",
            "[36,    40] loss: 0.036\n",
            "[37,    20] loss: 0.050\n",
            "[37,    40] loss: 0.097\n",
            "[38,    20] loss: 0.141\n",
            "[38,    40] loss: 0.089\n",
            "[39,    20] loss: 0.037\n",
            "[39,    40] loss: 0.074\n",
            "[40,    20] loss: 0.141\n",
            "[40,    40] loss: 0.169\n",
            "[41,    20] loss: 0.316\n",
            "[41,    40] loss: 0.157\n",
            "[42,    20] loss: 0.112\n",
            "[42,    40] loss: 0.073\n",
            "[43,    20] loss: 0.027\n",
            "[43,    40] loss: 0.050\n",
            "[44,    20] loss: 0.178\n",
            "[44,    40] loss: 0.129\n",
            "[45,    20] loss: 0.138\n",
            "[45,    40] loss: 0.122\n",
            "[46,    20] loss: 0.168\n",
            "[46,    40] loss: 0.138\n",
            "[47,    20] loss: 0.113\n",
            "[47,    40] loss: 0.079\n",
            "[48,    20] loss: 0.058\n",
            "[48,    40] loss: 0.041\n",
            "[49,    20] loss: 0.036\n",
            "[49,    40] loss: 0.042\n",
            "[50,    20] loss: 0.048\n",
            "[50,    40] loss: 0.029\n",
            "[51,    20] loss: 0.011\n",
            "[51,    40] loss: 0.010\n",
            "[52,    20] loss: 0.009\n",
            "[52,    40] loss: 0.025\n",
            "[53,    20] loss: 0.028\n",
            "[53,    40] loss: 0.018\n",
            "[54,    20] loss: 0.009\n",
            "[54,    40] loss: 0.018\n",
            "[55,    20] loss: 0.020\n",
            "[55,    40] loss: 0.027\n",
            "[56,    20] loss: 0.033\n",
            "[56,    40] loss: 0.017\n",
            "[57,    20] loss: 0.021\n",
            "[57,    40] loss: 0.018\n",
            "[58,    20] loss: 0.017\n",
            "[58,    40] loss: 0.028\n",
            "[59,    20] loss: 0.054\n",
            "[59,    40] loss: 0.028\n",
            "[60,    20] loss: 0.011\n",
            "[60,    40] loss: 0.005\n",
            "[61,    20] loss: 0.004\n",
            "[61,    40] loss: 0.005\n",
            "[62,    20] loss: 0.003\n",
            "[62,    40] loss: 0.005\n",
            "[63,    20] loss: 0.004\n",
            "[63,    40] loss: 0.007\n",
            "[64,    20] loss: 0.008\n",
            "[64,    40] loss: 0.007\n",
            "[65,    20] loss: 0.004\n",
            "[65,    40] loss: 0.003\n",
            "[66,    20] loss: 0.002\n",
            "[66,    40] loss: 0.003\n",
            "[67,    20] loss: 0.002\n",
            "[67,    40] loss: 0.030\n",
            "[68,    20] loss: 0.123\n",
            "[68,    40] loss: 0.080\n",
            "[69,    20] loss: 0.079\n",
            "[69,    40] loss: 0.046\n",
            "[70,    20] loss: 0.076\n",
            "[70,    40] loss: 0.035\n",
            "ITERATION:  1\n",
            "Accuracy of the network on the 1 iteration: 49 %\n",
            "[1,    20] loss: 7.056\n",
            "[1,    40] loss: 1.077\n",
            "[2,    20] loss: 0.785\n",
            "[2,    40] loss: 0.762\n",
            "[3,    20] loss: 0.652\n",
            "[3,    40] loss: 0.558\n",
            "[4,    20] loss: 0.457\n",
            "[4,    40] loss: 0.438\n",
            "[5,    20] loss: 0.325\n",
            "[5,    40] loss: 0.388\n",
            "[6,    20] loss: 0.326\n",
            "[6,    40] loss: 0.283\n",
            "[7,    20] loss: 0.220\n",
            "[7,    40] loss: 0.218\n",
            "[8,    20] loss: 0.185\n",
            "[8,    40] loss: 0.158\n",
            "[9,    20] loss: 0.109\n",
            "[9,    40] loss: 0.113\n",
            "[10,    20] loss: 0.093\n",
            "[10,    40] loss: 0.076\n",
            "[11,    20] loss: 0.058\n",
            "[11,    40] loss: 0.085\n",
            "[12,    20] loss: 0.138\n",
            "[12,    40] loss: 0.090\n",
            "[13,    20] loss: 0.044\n",
            "[13,    40] loss: 0.054\n",
            "[14,    20] loss: 0.072\n",
            "[14,    40] loss: 0.051\n",
            "[15,    20] loss: 0.021\n",
            "[15,    40] loss: 0.044\n",
            "[16,    20] loss: 0.053\n",
            "[16,    40] loss: 0.078\n",
            "[17,    20] loss: 0.108\n",
            "[17,    40] loss: 0.068\n",
            "[18,    20] loss: 0.053\n",
            "[18,    40] loss: 0.041\n",
            "[19,    20] loss: 0.055\n",
            "[19,    40] loss: 0.109\n",
            "[20,    20] loss: 0.252\n",
            "[20,    40] loss: 0.136\n",
            "[21,    20] loss: 0.049\n",
            "[21,    40] loss: 0.040\n",
            "[22,    20] loss: 0.026\n",
            "[22,    40] loss: 0.024\n",
            "[23,    20] loss: 0.014\n",
            "[23,    40] loss: 0.043\n",
            "[24,    20] loss: 0.037\n",
            "[24,    40] loss: 0.023\n",
            "[25,    20] loss: 0.014\n",
            "[25,    40] loss: 0.010\n",
            "[26,    20] loss: 0.009\n",
            "[26,    40] loss: 0.043\n",
            "[27,    20] loss: 0.065\n",
            "[27,    40] loss: 0.051\n",
            "[28,    20] loss: 0.036\n",
            "[28,    40] loss: 0.021\n",
            "[29,    20] loss: 0.008\n",
            "[29,    40] loss: 0.013\n",
            "[30,    20] loss: 0.018\n",
            "[30,    40] loss: 0.018\n",
            "[31,    20] loss: 0.015\n",
            "[31,    40] loss: 0.020\n",
            "[32,    20] loss: 0.092\n",
            "[32,    40] loss: 0.119\n",
            "[33,    20] loss: 0.160\n",
            "[33,    40] loss: 0.112\n",
            "[34,    20] loss: 0.073\n",
            "[34,    40] loss: 0.046\n",
            "[35,    20] loss: 0.015\n",
            "[35,    40] loss: 0.049\n",
            "[36,    20] loss: 0.142\n",
            "[36,    40] loss: 0.136\n",
            "[37,    20] loss: 0.241\n",
            "[37,    40] loss: 0.155\n",
            "[38,    20] loss: 0.087\n",
            "[38,    40] loss: 0.078\n",
            "[39,    20] loss: 0.064\n",
            "[39,    40] loss: 0.057\n",
            "[40,    20] loss: 0.085\n",
            "[40,    40] loss: 0.062\n",
            "[41,    20] loss: 0.038\n",
            "[41,    40] loss: 0.023\n",
            "[42,    20] loss: 0.007\n",
            "[42,    40] loss: 0.008\n",
            "[43,    20] loss: 0.004\n",
            "[43,    40] loss: 0.013\n",
            "[44,    20] loss: 0.032\n",
            "[44,    40] loss: 0.017\n",
            "[45,    20] loss: 0.008\n",
            "[45,    40] loss: 0.033\n",
            "[46,    20] loss: 0.074\n",
            "[46,    40] loss: 0.047\n",
            "[47,    20] loss: 0.105\n",
            "[47,    40] loss: 0.060\n",
            "[48,    20] loss: 0.035\n",
            "[48,    40] loss: 0.024\n",
            "[49,    20] loss: 0.014\n",
            "[49,    40] loss: 0.014\n",
            "[50,    20] loss: 0.011\n",
            "[50,    40] loss: 0.013\n",
            "[51,    20] loss: 0.005\n",
            "[51,    40] loss: 0.008\n",
            "[52,    20] loss: 0.003\n",
            "[52,    40] loss: 0.009\n",
            "[53,    20] loss: 0.017\n",
            "[53,    40] loss: 0.051\n",
            "[54,    20] loss: 0.101\n",
            "[54,    40] loss: 0.061\n",
            "[55,    20] loss: 0.020\n",
            "[55,    40] loss: 0.081\n",
            "[56,    20] loss: 0.141\n",
            "[56,    40] loss: 0.085\n",
            "[57,    20] loss: 0.043\n",
            "[57,    40] loss: 0.073\n",
            "[58,    20] loss: 0.050\n",
            "[58,    40] loss: 0.025\n",
            "[59,    20] loss: 0.006\n",
            "[59,    40] loss: 0.023\n",
            "[60,    20] loss: 0.090\n",
            "[60,    40] loss: 0.050\n",
            "[61,    20] loss: 0.031\n",
            "[61,    40] loss: 0.048\n",
            "[62,    20] loss: 0.041\n",
            "[62,    40] loss: 0.027\n",
            "[63,    20] loss: 0.008\n",
            "[63,    40] loss: 0.015\n",
            "[64,    20] loss: 0.008\n",
            "[64,    40] loss: 0.010\n",
            "[65,    20] loss: 0.006\n",
            "[65,    40] loss: 0.003\n",
            "[66,    20] loss: 0.003\n",
            "[66,    40] loss: 0.021\n",
            "[67,    20] loss: 0.014\n",
            "[67,    40] loss: 0.016\n",
            "[68,    20] loss: 0.037\n",
            "[68,    40] loss: 0.085\n",
            "[69,    20] loss: 0.149\n",
            "[69,    40] loss: 0.062\n",
            "[70,    20] loss: 0.021\n",
            "[70,    40] loss: 0.063\n",
            "ITERATION:  2\n",
            "Accuracy of the network on the 2 iteration: 33 %\n",
            "[1,    20] loss: 8.598\n",
            "[1,    40] loss: 1.339\n",
            "[2,    20] loss: 1.068\n",
            "[2,    40] loss: 0.948\n",
            "[3,    20] loss: 0.839\n",
            "[3,    40] loss: 0.754\n",
            "[4,    20] loss: 0.610\n",
            "[4,    40] loss: 0.675\n",
            "[5,    20] loss: 0.539\n",
            "[5,    40] loss: 0.527\n",
            "[6,    20] loss: 0.405\n",
            "[6,    40] loss: 0.396\n",
            "[7,    20] loss: 0.315\n",
            "[7,    40] loss: 0.353\n",
            "[8,    20] loss: 0.286\n",
            "[8,    40] loss: 0.263\n",
            "[9,    20] loss: 0.178\n",
            "[9,    40] loss: 0.167\n",
            "[10,    20] loss: 0.216\n",
            "[10,    40] loss: 0.217\n",
            "[11,    20] loss: 0.309\n",
            "[11,    40] loss: 0.275\n",
            "[12,    20] loss: 0.165\n",
            "[12,    40] loss: 0.165\n",
            "[13,    20] loss: 0.086\n",
            "[13,    40] loss: 0.129\n",
            "[14,    20] loss: 0.207\n",
            "[14,    40] loss: 0.170\n",
            "[15,    20] loss: 0.180\n",
            "[15,    40] loss: 0.103\n",
            "[16,    20] loss: 0.047\n",
            "[16,    40] loss: 0.073\n",
            "[17,    20] loss: 0.074\n",
            "[17,    40] loss: 0.048\n",
            "[18,    20] loss: 0.026\n",
            "[18,    40] loss: 0.058\n",
            "[19,    20] loss: 0.145\n",
            "[19,    40] loss: 0.137\n",
            "[20,    20] loss: 0.149\n",
            "[20,    40] loss: 0.133\n",
            "[21,    20] loss: 0.164\n",
            "[21,    40] loss: 0.105\n",
            "[22,    20] loss: 0.041\n",
            "[22,    40] loss: 0.050\n",
            "[23,    20] loss: 0.081\n",
            "[23,    40] loss: 0.113\n",
            "[24,    20] loss: 0.087\n",
            "[24,    40] loss: 0.059\n",
            "[25,    20] loss: 0.088\n",
            "[25,    40] loss: 0.056\n",
            "[26,    20] loss: 0.028\n",
            "[26,    40] loss: 0.052\n",
            "[27,    20] loss: 0.102\n",
            "[27,    40] loss: 0.101\n",
            "[28,    20] loss: 0.137\n",
            "[28,    40] loss: 0.100\n",
            "[29,    20] loss: 0.064\n",
            "[29,    40] loss: 0.078\n",
            "[30,    20] loss: 0.073\n",
            "[30,    40] loss: 0.063\n",
            "[31,    20] loss: 0.066\n",
            "[31,    40] loss: 0.106\n",
            "[32,    20] loss: 0.105\n",
            "[32,    40] loss: 0.062\n",
            "[33,    20] loss: 0.023\n",
            "[33,    40] loss: 0.032\n",
            "[34,    20] loss: 0.064\n",
            "[34,    40] loss: 0.103\n",
            "[35,    20] loss: 0.128\n",
            "[35,    40] loss: 0.110\n",
            "[36,    20] loss: 0.139\n",
            "[36,    40] loss: 0.082\n",
            "[37,    20] loss: 0.092\n",
            "[37,    40] loss: 0.054\n",
            "[38,    20] loss: 0.040\n",
            "[38,    40] loss: 0.038\n",
            "[39,    20] loss: 0.020\n",
            "[39,    40] loss: 0.016\n",
            "[40,    20] loss: 0.011\n",
            "[40,    40] loss: 0.032\n",
            "[41,    20] loss: 0.092\n",
            "[41,    40] loss: 0.056\n",
            "[42,    20] loss: 0.030\n",
            "[42,    40] loss: 0.043\n",
            "[43,    20] loss: 0.065\n",
            "[43,    40] loss: 0.043\n",
            "[44,    20] loss: 0.013\n",
            "[44,    40] loss: 0.012\n",
            "[45,    20] loss: 0.005\n",
            "[45,    40] loss: 0.030\n",
            "[46,    20] loss: 0.067\n",
            "[46,    40] loss: 0.187\n",
            "[47,    20] loss: 0.328\n",
            "[47,    40] loss: 0.162\n",
            "[48,    20] loss: 0.109\n",
            "[48,    40] loss: 0.095\n",
            "[49,    20] loss: 0.123\n",
            "[49,    40] loss: 0.092\n",
            "[50,    20] loss: 0.059\n",
            "[50,    40] loss: 0.049\n",
            "[51,    20] loss: 0.030\n",
            "[51,    40] loss: 0.025\n",
            "[52,    20] loss: 0.009\n",
            "[52,    40] loss: 0.019\n",
            "[53,    20] loss: 0.042\n",
            "[53,    40] loss: 0.049\n",
            "[54,    20] loss: 0.062\n",
            "[54,    40] loss: 0.083\n",
            "[55,    20] loss: 0.136\n",
            "[55,    40] loss: 0.084\n",
            "[56,    20] loss: 0.060\n",
            "[56,    40] loss: 0.036\n",
            "[57,    20] loss: 0.018\n",
            "[57,    40] loss: 0.014\n",
            "[58,    20] loss: 0.008\n",
            "[58,    40] loss: 0.005\n",
            "[59,    20] loss: 0.004\n",
            "[59,    40] loss: 0.004\n",
            "[60,    20] loss: 0.002\n",
            "[60,    40] loss: 0.017\n",
            "[61,    20] loss: 0.062\n",
            "[61,    40] loss: 0.031\n",
            "[62,    20] loss: 0.013\n",
            "[62,    40] loss: 0.022\n",
            "[63,    20] loss: 0.025\n",
            "[63,    40] loss: 0.029\n",
            "[64,    20] loss: 0.036\n",
            "[64,    40] loss: 0.053\n",
            "[65,    20] loss: 0.089\n",
            "[65,    40] loss: 0.077\n",
            "[66,    20] loss: 0.109\n",
            "[66,    40] loss: 0.071\n",
            "[67,    20] loss: 0.045\n",
            "[67,    40] loss: 0.084\n",
            "[68,    20] loss: 0.090\n",
            "[68,    40] loss: 0.046\n",
            "[69,    20] loss: 0.023\n",
            "[69,    40] loss: 0.011\n",
            "[70,    20] loss: 0.006\n",
            "[70,    40] loss: 0.027\n",
            "ITERATION:  3\n",
            "Accuracy of the network on the 3 iteration: 24 %\n",
            "[1,    20] loss: 8.792\n",
            "[1,    40] loss: 1.286\n",
            "[2,    20] loss: 0.977\n",
            "[2,    40] loss: 0.875\n",
            "[3,    20] loss: 0.726\n",
            "[3,    40] loss: 0.727\n",
            "[4,    20] loss: 0.597\n",
            "[4,    40] loss: 0.593\n",
            "[5,    20] loss: 0.481\n",
            "[5,    40] loss: 0.496\n",
            "[6,    20] loss: 0.384\n",
            "[6,    40] loss: 0.423\n",
            "[7,    20] loss: 0.370\n",
            "[7,    40] loss: 0.338\n",
            "[8,    20] loss: 0.237\n",
            "[8,    40] loss: 0.229\n",
            "[9,    20] loss: 0.160\n",
            "[9,    40] loss: 0.160\n",
            "[10,    20] loss: 0.136\n",
            "[10,    40] loss: 0.209\n",
            "[11,    20] loss: 0.176\n",
            "[11,    40] loss: 0.131\n",
            "[12,    20] loss: 0.075\n",
            "[12,    40] loss: 0.100\n",
            "[13,    20] loss: 0.126\n",
            "[13,    40] loss: 0.087\n",
            "[14,    20] loss: 0.045\n",
            "[14,    40] loss: 0.073\n",
            "[15,    20] loss: 0.056\n",
            "[15,    40] loss: 0.054\n",
            "[16,    20] loss: 0.063\n",
            "[16,    40] loss: 0.046\n",
            "[17,    20] loss: 0.024\n",
            "[17,    40] loss: 0.051\n",
            "[18,    20] loss: 0.088\n",
            "[18,    40] loss: 0.112\n",
            "[19,    20] loss: 0.155\n",
            "[19,    40] loss: 0.143\n",
            "[20,    20] loss: 0.093\n",
            "[20,    40] loss: 0.116\n",
            "[21,    20] loss: 0.066\n",
            "[21,    40] loss: 0.115\n",
            "[22,    20] loss: 0.105\n",
            "[22,    40] loss: 0.055\n",
            "[23,    20] loss: 0.023\n",
            "[23,    40] loss: 0.041\n",
            "[24,    20] loss: 0.050\n",
            "[24,    40] loss: 0.062\n",
            "[25,    20] loss: 0.057\n",
            "[25,    40] loss: 0.087\n",
            "[26,    20] loss: 0.195\n",
            "[26,    40] loss: 0.116\n",
            "[27,    20] loss: 0.111\n",
            "[27,    40] loss: 0.158\n",
            "[28,    20] loss: 0.201\n",
            "[28,    40] loss: 0.111\n",
            "[29,    20] loss: 0.074\n",
            "[29,    40] loss: 0.095\n",
            "[30,    20] loss: 0.139\n",
            "[30,    40] loss: 0.117\n",
            "[31,    20] loss: 0.076\n",
            "[31,    40] loss: 0.073\n",
            "[32,    20] loss: 0.049\n",
            "[32,    40] loss: 0.082\n",
            "[33,    20] loss: 0.076\n",
            "[33,    40] loss: 0.043\n",
            "[34,    20] loss: 0.048\n",
            "[34,    40] loss: 0.076\n",
            "[35,    20] loss: 0.074\n",
            "[35,    40] loss: 0.059\n",
            "[36,    20] loss: 0.072\n",
            "[36,    40] loss: 0.052\n",
            "[37,    20] loss: 0.016\n",
            "[37,    40] loss: 0.055\n",
            "[38,    20] loss: 0.050\n",
            "[38,    40] loss: 0.100\n",
            "[39,    20] loss: 0.097\n",
            "[39,    40] loss: 0.073\n",
            "[40,    20] loss: 0.081\n",
            "[40,    40] loss: 0.040\n",
            "[41,    20] loss: 0.016\n",
            "[41,    40] loss: 0.093\n",
            "[42,    20] loss: 0.110\n",
            "[42,    40] loss: 0.055\n",
            "[43,    20] loss: 0.058\n",
            "[43,    40] loss: 0.031\n",
            "[44,    20] loss: 0.011\n",
            "[44,    40] loss: 0.013\n",
            "[45,    20] loss: 0.006\n",
            "[45,    40] loss: 0.029\n",
            "[46,    20] loss: 0.043\n",
            "[46,    40] loss: 0.041\n",
            "[47,    20] loss: 0.043\n",
            "[47,    40] loss: 0.085\n",
            "[48,    20] loss: 0.066\n",
            "[48,    40] loss: 0.115\n",
            "[49,    20] loss: 0.123\n",
            "[49,    40] loss: 0.090\n",
            "[50,    20] loss: 0.033\n",
            "[50,    40] loss: 0.050\n",
            "[51,    20] loss: 0.185\n",
            "[51,    40] loss: 0.076\n",
            "[52,    20] loss: 0.089\n",
            "[52,    40] loss: 0.050\n",
            "[53,    20] loss: 0.015\n",
            "[53,    40] loss: 0.022\n",
            "[54,    20] loss: 0.012\n",
            "[54,    40] loss: 0.020\n",
            "[55,    20] loss: 0.026\n",
            "[55,    40] loss: 0.018\n",
            "[56,    20] loss: 0.007\n",
            "[56,    40] loss: 0.038\n",
            "[57,    20] loss: 0.044\n",
            "[57,    40] loss: 0.027\n",
            "[58,    20] loss: 0.030\n",
            "[58,    40] loss: 0.032\n",
            "[59,    20] loss: 0.051\n",
            "[59,    40] loss: 0.060\n",
            "[60,    20] loss: 0.047\n",
            "[60,    40] loss: 0.080\n",
            "[61,    20] loss: 0.091\n",
            "[61,    40] loss: 0.046\n",
            "[62,    20] loss: 0.012\n",
            "[62,    40] loss: 0.044\n",
            "[63,    20] loss: 0.050\n",
            "[63,    40] loss: 0.036\n",
            "[64,    20] loss: 0.026\n",
            "[64,    40] loss: 0.074\n",
            "[65,    20] loss: 0.067\n",
            "[65,    40] loss: 0.049\n",
            "[66,    20] loss: 0.021\n",
            "[66,    40] loss: 0.017\n",
            "[67,    20] loss: 0.012\n",
            "[67,    40] loss: 0.048\n",
            "[68,    20] loss: 0.035\n",
            "[68,    40] loss: 0.038\n",
            "[69,    20] loss: 0.093\n",
            "[69,    40] loss: 0.053\n",
            "[70,    20] loss: 0.015\n",
            "[70,    40] loss: 0.011\n",
            "ITERATION:  4\n",
            "Accuracy of the network on the 4 iteration: 19 %\n",
            "[1,    20] loss: 8.211\n",
            "[1,    40] loss: 1.419\n",
            "[2,    20] loss: 1.066\n",
            "[2,    40] loss: 0.986\n",
            "[3,    20] loss: 0.840\n",
            "[3,    40] loss: 0.789\n",
            "[4,    20] loss: 0.681\n",
            "[4,    40] loss: 0.684\n",
            "[5,    20] loss: 0.543\n",
            "[5,    40] loss: 0.585\n",
            "[6,    20] loss: 0.456\n",
            "[6,    40] loss: 0.464\n",
            "[7,    20] loss: 0.392\n",
            "[7,    40] loss: 0.389\n",
            "[8,    20] loss: 0.292\n",
            "[8,    40] loss: 0.298\n",
            "[9,    20] loss: 0.209\n",
            "[9,    40] loss: 0.252\n",
            "[10,    20] loss: 0.180\n",
            "[10,    40] loss: 0.188\n",
            "[11,    20] loss: 0.211\n",
            "[11,    40] loss: 0.200\n",
            "[12,    20] loss: 0.129\n",
            "[12,    40] loss: 0.122\n",
            "[13,    20] loss: 0.083\n",
            "[13,    40] loss: 0.122\n",
            "[14,    20] loss: 0.116\n",
            "[14,    40] loss: 0.107\n",
            "[15,    20] loss: 0.148\n",
            "[15,    40] loss: 0.153\n",
            "[16,    20] loss: 0.171\n",
            "[16,    40] loss: 0.169\n",
            "[17,    20] loss: 0.175\n",
            "[17,    40] loss: 0.136\n",
            "[18,    20] loss: 0.057\n",
            "[18,    40] loss: 0.070\n",
            "[19,    20] loss: 0.055\n",
            "[19,    40] loss: 0.050\n",
            "[20,    20] loss: 0.057\n",
            "[20,    40] loss: 0.030\n",
            "[21,    20] loss: 0.016\n",
            "[21,    40] loss: 0.014\n",
            "[22,    20] loss: 0.011\n",
            "[22,    40] loss: 0.014\n",
            "[23,    20] loss: 0.016\n",
            "[23,    40] loss: 0.016\n",
            "[24,    20] loss: 0.009\n",
            "[24,    40] loss: 0.081\n",
            "[25,    20] loss: 0.249\n",
            "[25,    40] loss: 0.274\n",
            "[26,    20] loss: 0.210\n",
            "[26,    40] loss: 0.141\n",
            "[27,    20] loss: 0.112\n",
            "[27,    40] loss: 0.076\n",
            "[28,    20] loss: 0.027\n",
            "[28,    40] loss: 0.026\n",
            "[29,    20] loss: 0.016\n",
            "[29,    40] loss: 0.023\n",
            "[30,    20] loss: 0.027\n",
            "[30,    40] loss: 0.040\n",
            "[31,    20] loss: 0.042\n",
            "[31,    40] loss: 0.058\n",
            "[32,    20] loss: 0.096\n",
            "[32,    40] loss: 0.048\n",
            "[33,    20] loss: 0.019\n",
            "[33,    40] loss: 0.062\n",
            "[34,    20] loss: 0.151\n",
            "[34,    40] loss: 0.100\n",
            "[35,    20] loss: 0.034\n",
            "[35,    40] loss: 0.034\n",
            "[36,    20] loss: 0.026\n",
            "[36,    40] loss: 0.045\n",
            "[37,    20] loss: 0.103\n",
            "[37,    40] loss: 0.072\n",
            "[38,    20] loss: 0.050\n",
            "[38,    40] loss: 0.042\n",
            "[39,    20] loss: 0.016\n",
            "[39,    40] loss: 0.013\n",
            "[40,    20] loss: 0.007\n",
            "[40,    40] loss: 0.027\n",
            "[41,    20] loss: 0.032\n",
            "[41,    40] loss: 0.039\n",
            "[42,    20] loss: 0.036\n",
            "[42,    40] loss: 0.066\n",
            "[43,    20] loss: 0.093\n",
            "[43,    40] loss: 0.075\n",
            "[44,    20] loss: 0.085\n",
            "[44,    40] loss: 0.053\n",
            "[45,    20] loss: 0.061\n",
            "[45,    40] loss: 0.197\n",
            "[46,    20] loss: 0.191\n",
            "[46,    40] loss: 0.100\n",
            "[47,    20] loss: 0.041\n",
            "[47,    40] loss: 0.060\n",
            "[48,    20] loss: 0.113\n",
            "[48,    40] loss: 0.079\n",
            "[49,    20] loss: 0.026\n",
            "[49,    40] loss: 0.028\n",
            "[50,    20] loss: 0.024\n",
            "[50,    40] loss: 0.030\n",
            "[51,    20] loss: 0.027\n",
            "[51,    40] loss: 0.055\n",
            "[52,    20] loss: 0.060\n",
            "[52,    40] loss: 0.086\n",
            "[53,    20] loss: 0.086\n",
            "[53,    40] loss: 0.074\n",
            "[54,    20] loss: 0.087\n",
            "[54,    40] loss: 0.033\n",
            "[55,    20] loss: 0.020\n",
            "[55,    40] loss: 0.021\n",
            "[56,    20] loss: 0.032\n",
            "[56,    40] loss: 0.077\n",
            "[57,    20] loss: 0.050\n",
            "[57,    40] loss: 0.030\n",
            "[58,    20] loss: 0.014\n",
            "[58,    40] loss: 0.017\n",
            "[59,    20] loss: 0.013\n",
            "[59,    40] loss: 0.020\n",
            "[60,    20] loss: 0.020\n",
            "[60,    40] loss: 0.020\n",
            "[61,    20] loss: 0.020\n",
            "[61,    40] loss: 0.021\n",
            "[62,    20] loss: 0.015\n",
            "[62,    40] loss: 0.075\n",
            "[63,    20] loss: 0.077\n",
            "[63,    40] loss: 0.052\n",
            "[64,    20] loss: 0.059\n",
            "[64,    40] loss: 0.035\n",
            "[65,    20] loss: 0.046\n",
            "[65,    40] loss: 0.046\n",
            "[66,    20] loss: 0.039\n",
            "[66,    40] loss: 0.032\n",
            "[67,    20] loss: 0.013\n",
            "[67,    40] loss: 0.009\n",
            "[68,    20] loss: 0.006\n",
            "[68,    40] loss: 0.006\n",
            "[69,    20] loss: 0.003\n",
            "[69,    40] loss: 0.010\n",
            "[70,    20] loss: 0.008\n",
            "[70,    40] loss: 0.040\n",
            "ITERATION:  5\n",
            "Accuracy of the network on the 5 iteration: 16 %\n",
            "[1,    20] loss: 8.145\n",
            "[1,    40] loss: 1.314\n",
            "[2,    20] loss: 1.006\n",
            "[2,    40] loss: 0.994\n",
            "[3,    20] loss: 0.809\n",
            "[3,    40] loss: 0.787\n",
            "[4,    20] loss: 0.706\n",
            "[4,    40] loss: 0.688\n",
            "[5,    20] loss: 0.602\n",
            "[5,    40] loss: 0.618\n",
            "[6,    20] loss: 0.537\n",
            "[6,    40] loss: 0.489\n",
            "[7,    20] loss: 0.449\n",
            "[7,    40] loss: 0.426\n",
            "[8,    20] loss: 0.350\n",
            "[8,    40] loss: 0.349\n",
            "[9,    20] loss: 0.278\n",
            "[9,    40] loss: 0.295\n",
            "[10,    20] loss: 0.222\n",
            "[10,    40] loss: 0.230\n",
            "[11,    20] loss: 0.175\n",
            "[11,    40] loss: 0.194\n",
            "[12,    20] loss: 0.219\n",
            "[12,    40] loss: 0.189\n",
            "[13,    20] loss: 0.130\n",
            "[13,    40] loss: 0.110\n",
            "[14,    20] loss: 0.065\n",
            "[14,    40] loss: 0.075\n",
            "[15,    20] loss: 0.066\n",
            "[15,    40] loss: 0.075\n",
            "[16,    20] loss: 0.046\n",
            "[16,    40] loss: 0.059\n",
            "[17,    20] loss: 0.099\n",
            "[17,    40] loss: 0.093\n",
            "[18,    20] loss: 0.137\n",
            "[18,    40] loss: 0.135\n",
            "[19,    20] loss: 0.077\n",
            "[19,    40] loss: 0.078\n",
            "[20,    20] loss: 0.060\n",
            "[20,    40] loss: 0.058\n",
            "[21,    20] loss: 0.055\n",
            "[21,    40] loss: 0.045\n",
            "[22,    20] loss: 0.031\n",
            "[22,    40] loss: 0.047\n",
            "[23,    20] loss: 0.070\n",
            "[23,    40] loss: 0.050\n",
            "[24,    20] loss: 0.022\n",
            "[24,    40] loss: 0.037\n",
            "[25,    20] loss: 0.047\n",
            "[25,    40] loss: 0.085\n",
            "[26,    20] loss: 0.097\n",
            "[26,    40] loss: 0.060\n",
            "[27,    20] loss: 0.030\n",
            "[27,    40] loss: 0.035\n",
            "[28,    20] loss: 0.013\n",
            "[28,    40] loss: 0.020\n",
            "[29,    20] loss: 0.021\n",
            "[29,    40] loss: 0.036\n",
            "[30,    20] loss: 0.045\n",
            "[30,    40] loss: 0.036\n",
            "[31,    20] loss: 0.015\n",
            "[31,    40] loss: 0.038\n",
            "[32,    20] loss: 0.087\n",
            "[32,    40] loss: 0.052\n",
            "[33,    20] loss: 0.041\n",
            "[33,    40] loss: 0.071\n",
            "[34,    20] loss: 0.136\n",
            "[34,    40] loss: 0.098\n",
            "[35,    20] loss: 0.057\n",
            "[35,    40] loss: 0.049\n",
            "[36,    20] loss: 0.059\n",
            "[36,    40] loss: 0.083\n",
            "[37,    20] loss: 0.141\n",
            "[37,    40] loss: 0.097\n",
            "[38,    20] loss: 0.073\n",
            "[38,    40] loss: 0.077\n",
            "[39,    20] loss: 0.042\n",
            "[39,    40] loss: 0.064\n",
            "[40,    20] loss: 0.075\n",
            "[40,    40] loss: 0.056\n",
            "[41,    20] loss: 0.041\n",
            "[41,    40] loss: 0.030\n",
            "[42,    20] loss: 0.017\n",
            "[42,    40] loss: 0.019\n",
            "[43,    20] loss: 0.010\n",
            "[43,    40] loss: 0.127\n",
            "[44,    20] loss: 0.154\n",
            "[44,    40] loss: 0.100\n",
            "[45,    20] loss: 0.067\n",
            "[45,    40] loss: 0.031\n",
            "[46,    20] loss: 0.014\n",
            "[46,    40] loss: 0.018\n",
            "[47,    20] loss: 0.010\n",
            "[47,    40] loss: 0.043\n",
            "[48,    20] loss: 0.059\n",
            "[48,    40] loss: 0.064\n",
            "[49,    20] loss: 0.121\n",
            "[49,    40] loss: 0.060\n",
            "[50,    20] loss: 0.047\n",
            "[50,    40] loss: 0.090\n",
            "[51,    20] loss: 0.046\n",
            "[51,    40] loss: 0.107\n",
            "[52,    20] loss: 0.096\n",
            "[52,    40] loss: 0.080\n",
            "[53,    20] loss: 0.040\n",
            "[53,    40] loss: 0.080\n",
            "[54,    20] loss: 0.122\n",
            "[54,    40] loss: 0.064\n",
            "[55,    20] loss: 0.028\n",
            "[55,    40] loss: 0.040\n",
            "[56,    20] loss: 0.054\n",
            "[56,    40] loss: 0.028\n",
            "[57,    20] loss: 0.020\n",
            "[57,    40] loss: 0.013\n",
            "[58,    20] loss: 0.006\n",
            "[58,    40] loss: 0.018\n",
            "[59,    20] loss: 0.015\n",
            "[59,    40] loss: 0.028\n",
            "[60,    20] loss: 0.016\n",
            "[60,    40] loss: 0.014\n",
            "[61,    20] loss: 0.008\n",
            "[61,    40] loss: 0.014\n",
            "[62,    20] loss: 0.014\n",
            "[62,    40] loss: 0.021\n",
            "[63,    20] loss: 0.016\n",
            "[63,    40] loss: 0.116\n",
            "[64,    20] loss: 0.100\n",
            "[64,    40] loss: 0.043\n",
            "[65,    20] loss: 0.015\n",
            "[65,    40] loss: 0.023\n",
            "[66,    20] loss: 0.024\n",
            "[66,    40] loss: 0.019\n",
            "[67,    20] loss: 0.011\n",
            "[67,    40] loss: 0.006\n",
            "[68,    20] loss: 0.004\n",
            "[68,    40] loss: 0.005\n",
            "[69,    20] loss: 0.003\n",
            "[69,    40] loss: 0.003\n",
            "[70,    20] loss: 0.002\n",
            "[70,    40] loss: 0.078\n",
            "ITERATION:  6\n",
            "Accuracy of the network on the 6 iteration: 14 %\n",
            "[1,    20] loss: 8.478\n",
            "[1,    40] loss: 1.717\n",
            "[2,    20] loss: 1.322\n",
            "[2,    40] loss: 1.160\n",
            "[3,    20] loss: 0.978\n",
            "[3,    40] loss: 0.921\n",
            "[4,    20] loss: 0.793\n",
            "[4,    40] loss: 0.745\n",
            "[5,    20] loss: 0.624\n",
            "[5,    40] loss: 0.612\n",
            "[6,    20] loss: 0.488\n",
            "[6,    40] loss: 0.475\n",
            "[7,    20] loss: 0.365\n",
            "[7,    40] loss: 0.398\n",
            "[8,    20] loss: 0.307\n",
            "[8,    40] loss: 0.314\n",
            "[9,    20] loss: 0.206\n",
            "[9,    40] loss: 0.236\n",
            "[10,    20] loss: 0.203\n",
            "[10,    40] loss: 0.178\n",
            "[11,    20] loss: 0.103\n",
            "[11,    40] loss: 0.122\n",
            "[12,    20] loss: 0.096\n",
            "[12,    40] loss: 0.125\n",
            "[13,    20] loss: 0.138\n",
            "[13,    40] loss: 0.145\n",
            "[14,    20] loss: 0.143\n",
            "[14,    40] loss: 0.112\n",
            "[15,    20] loss: 0.175\n",
            "[15,    40] loss: 0.113\n",
            "[16,    20] loss: 0.067\n",
            "[16,    40] loss: 0.077\n",
            "[17,    20] loss: 0.066\n",
            "[17,    40] loss: 0.066\n",
            "[18,    20] loss: 0.066\n",
            "[18,    40] loss: 0.051\n",
            "[19,    20] loss: 0.031\n",
            "[19,    40] loss: 0.049\n",
            "[20,    20] loss: 0.083\n",
            "[20,    40] loss: 0.056\n",
            "[21,    20] loss: 0.024\n",
            "[21,    40] loss: 0.042\n",
            "[22,    20] loss: 0.084\n",
            "[22,    40] loss: 0.076\n",
            "[23,    20] loss: 0.075\n",
            "[23,    40] loss: 0.043\n",
            "[24,    20] loss: 0.020\n",
            "[24,    40] loss: 0.021\n",
            "[25,    20] loss: 0.015\n",
            "[25,    40] loss: 0.056\n",
            "[26,    20] loss: 0.060\n",
            "[26,    40] loss: 0.042\n",
            "[27,    20] loss: 0.028\n",
            "[27,    40] loss: 0.043\n",
            "[28,    20] loss: 0.044\n",
            "[28,    40] loss: 0.069\n",
            "[29,    20] loss: 0.083\n",
            "[29,    40] loss: 0.079\n",
            "[30,    20] loss: 0.056\n",
            "[30,    40] loss: 0.054\n",
            "[31,    20] loss: 0.022\n",
            "[31,    40] loss: 0.041\n",
            "[32,    20] loss: 0.033\n",
            "[32,    40] loss: 0.027\n",
            "[33,    20] loss: 0.009\n",
            "[33,    40] loss: 0.105\n",
            "[34,    20] loss: 0.063\n",
            "[34,    40] loss: 0.078\n",
            "[35,    20] loss: 0.129\n",
            "[35,    40] loss: 0.178\n",
            "[36,    20] loss: 0.170\n",
            "[36,    40] loss: 0.132\n",
            "[37,    20] loss: 0.110\n",
            "[37,    40] loss: 0.081\n",
            "[38,    20] loss: 0.101\n",
            "[38,    40] loss: 0.053\n",
            "[39,    20] loss: 0.019\n",
            "[39,    40] loss: 0.017\n",
            "[40,    20] loss: 0.008\n",
            "[40,    40] loss: 0.020\n",
            "[41,    20] loss: 0.028\n",
            "[41,    40] loss: 0.026\n",
            "[42,    20] loss: 0.012\n",
            "[42,    40] loss: 0.012\n",
            "[43,    20] loss: 0.014\n",
            "[43,    40] loss: 0.031\n",
            "[44,    20] loss: 0.044\n",
            "[44,    40] loss: 0.044\n",
            "[45,    20] loss: 0.043\n",
            "[45,    40] loss: 0.038\n",
            "[46,    20] loss: 0.010\n",
            "[46,    40] loss: 0.055\n",
            "[47,    20] loss: 0.089\n",
            "[47,    40] loss: 0.085\n",
            "[48,    20] loss: 0.052\n",
            "[48,    40] loss: 0.035\n",
            "[49,    20] loss: 0.014\n",
            "[49,    40] loss: 0.121\n",
            "[50,    20] loss: 0.146\n",
            "[50,    40] loss: 0.067\n",
            "[51,    20] loss: 0.063\n",
            "[51,    40] loss: 0.068\n",
            "[52,    20] loss: 0.050\n",
            "[52,    40] loss: 0.036\n",
            "[53,    20] loss: 0.021\n",
            "[53,    40] loss: 0.042\n",
            "[54,    20] loss: 0.046\n",
            "[54,    40] loss: 0.095\n",
            "[55,    20] loss: 0.118\n",
            "[55,    40] loss: 0.061\n",
            "[56,    20] loss: 0.016\n",
            "[56,    40] loss: 0.016\n",
            "[57,    20] loss: 0.009\n",
            "[57,    40] loss: 0.084\n",
            "[58,    20] loss: 0.108\n",
            "[58,    40] loss: 0.052\n",
            "[59,    20] loss: 0.022\n",
            "[59,    40] loss: 0.034\n",
            "[60,    20] loss: 0.027\n",
            "[60,    40] loss: 0.059\n",
            "[61,    20] loss: 0.078\n",
            "[61,    40] loss: 0.038\n",
            "[62,    20] loss: 0.027\n",
            "[62,    40] loss: 0.058\n",
            "[63,    20] loss: 0.097\n",
            "[63,    40] loss: 0.042\n",
            "[64,    20] loss: 0.015\n",
            "[64,    40] loss: 0.027\n",
            "[65,    20] loss: 0.029\n",
            "[65,    40] loss: 0.027\n",
            "[66,    20] loss: 0.011\n",
            "[66,    40] loss: 0.008\n",
            "[67,    20] loss: 0.004\n",
            "[67,    40] loss: 0.014\n",
            "[68,    20] loss: 0.029\n",
            "[68,    40] loss: 0.021\n",
            "[69,    20] loss: 0.006\n",
            "[69,    40] loss: 0.019\n",
            "[70,    20] loss: 0.031\n",
            "[70,    40] loss: 0.058\n",
            "ITERATION:  7\n",
            "Accuracy of the network on the 7 iteration: 12 %\n",
            "[1,    20] loss: 9.207\n",
            "[1,    40] loss: 2.131\n",
            "[2,    20] loss: 1.469\n",
            "[2,    40] loss: 1.230\n",
            "[3,    20] loss: 1.047\n",
            "[3,    40] loss: 0.914\n",
            "[4,    20] loss: 0.798\n",
            "[4,    40] loss: 0.703\n",
            "[5,    20] loss: 0.577\n",
            "[5,    40] loss: 0.578\n",
            "[6,    20] loss: 0.444\n",
            "[6,    40] loss: 0.453\n",
            "[7,    20] loss: 0.326\n",
            "[7,    40] loss: 0.324\n",
            "[8,    20] loss: 0.238\n",
            "[8,    40] loss: 0.227\n",
            "[9,    20] loss: 0.147\n",
            "[9,    40] loss: 0.182\n",
            "[10,    20] loss: 0.150\n",
            "[10,    40] loss: 0.126\n",
            "[11,    20] loss: 0.088\n",
            "[11,    40] loss: 0.096\n",
            "[12,    20] loss: 0.074\n",
            "[12,    40] loss: 0.070\n",
            "[13,    20] loss: 0.042\n",
            "[13,    40] loss: 0.061\n",
            "[14,    20] loss: 0.135\n",
            "[14,    40] loss: 0.139\n",
            "[15,    20] loss: 0.219\n",
            "[15,    40] loss: 0.155\n",
            "[16,    20] loss: 0.092\n",
            "[16,    40] loss: 0.094\n",
            "[17,    20] loss: 0.089\n",
            "[17,    40] loss: 0.115\n",
            "[18,    20] loss: 0.150\n",
            "[18,    40] loss: 0.154\n",
            "[19,    20] loss: 0.194\n",
            "[19,    40] loss: 0.133\n",
            "[20,    20] loss: 0.062\n",
            "[20,    40] loss: 0.059\n",
            "[21,    20] loss: 0.058\n",
            "[21,    40] loss: 0.048\n",
            "[22,    20] loss: 0.036\n",
            "[22,    40] loss: 0.064\n",
            "[23,    20] loss: 0.059\n",
            "[23,    40] loss: 0.054\n",
            "[24,    20] loss: 0.030\n",
            "[24,    40] loss: 0.025\n",
            "[25,    20] loss: 0.026\n",
            "[25,    40] loss: 0.042\n",
            "[26,    20] loss: 0.060\n",
            "[26,    40] loss: 0.066\n",
            "[27,    20] loss: 0.114\n",
            "[27,    40] loss: 0.120\n",
            "[28,    20] loss: 0.067\n",
            "[28,    40] loss: 0.038\n",
            "[29,    20] loss: 0.015\n",
            "[29,    40] loss: 0.036\n",
            "[30,    20] loss: 0.057\n",
            "[30,    40] loss: 0.066\n",
            "[31,    20] loss: 0.060\n",
            "[31,    40] loss: 0.088\n",
            "[32,    20] loss: 0.090\n",
            "[32,    40] loss: 0.049\n",
            "[33,    20] loss: 0.032\n",
            "[33,    40] loss: 0.020\n",
            "[34,    20] loss: 0.009\n",
            "[34,    40] loss: 0.062\n",
            "[35,    20] loss: 0.061\n",
            "[35,    40] loss: 0.088\n",
            "[36,    20] loss: 0.160\n",
            "[36,    40] loss: 0.103\n",
            "[37,    20] loss: 0.071\n",
            "[37,    40] loss: 0.069\n",
            "[38,    20] loss: 0.043\n",
            "[38,    40] loss: 0.047\n",
            "[39,    20] loss: 0.048\n",
            "[39,    40] loss: 0.036\n",
            "[40,    20] loss: 0.013\n",
            "[40,    40] loss: 0.016\n",
            "[41,    20] loss: 0.011\n",
            "[41,    40] loss: 0.013\n",
            "[42,    20] loss: 0.007\n",
            "[42,    40] loss: 0.008\n",
            "[43,    20] loss: 0.004\n",
            "[43,    40] loss: 0.014\n",
            "[44,    20] loss: 0.031\n",
            "[44,    40] loss: 0.017\n",
            "[45,    20] loss: 0.007\n",
            "[45,    40] loss: 0.018\n",
            "[46,    20] loss: 0.063\n",
            "[46,    40] loss: 0.064\n",
            "[47,    20] loss: 0.059\n",
            "[47,    40] loss: 0.078\n",
            "[48,    20] loss: 0.039\n",
            "[48,    40] loss: 0.039\n",
            "[49,    20] loss: 0.018\n",
            "[49,    40] loss: 0.020\n",
            "[50,    20] loss: 0.014\n",
            "[50,    40] loss: 0.062\n",
            "[51,    20] loss: 0.083\n",
            "[51,    40] loss: 0.113\n",
            "[52,    20] loss: 0.161\n",
            "[52,    40] loss: 0.114\n",
            "[53,    20] loss: 0.049\n",
            "[53,    40] loss: 0.035\n",
            "[54,    20] loss: 0.014\n",
            "[54,    40] loss: 0.018\n",
            "[55,    20] loss: 0.007\n",
            "[55,    40] loss: 0.020\n",
            "[56,    20] loss: 0.073\n",
            "[56,    40] loss: 0.073\n",
            "[57,    20] loss: 0.064\n",
            "[57,    40] loss: 0.060\n",
            "[58,    20] loss: 0.022\n",
            "[58,    40] loss: 0.028\n",
            "[59,    20] loss: 0.019\n",
            "[59,    40] loss: 0.034\n",
            "[60,    20] loss: 0.058\n",
            "[60,    40] loss: 0.052\n",
            "[61,    20] loss: 0.042\n",
            "[61,    40] loss: 0.033\n",
            "[62,    20] loss: 0.013\n",
            "[62,    40] loss: 0.043\n",
            "[63,    20] loss: 0.123\n",
            "[63,    40] loss: 0.090\n",
            "[64,    20] loss: 0.067\n",
            "[64,    40] loss: 0.062\n",
            "[65,    20] loss: 0.029\n",
            "[65,    40] loss: 0.024\n",
            "[66,    20] loss: 0.022\n",
            "[66,    40] loss: 0.013\n",
            "[67,    20] loss: 0.012\n",
            "[67,    40] loss: 0.023\n",
            "[68,    20] loss: 0.027\n",
            "[68,    40] loss: 0.025\n",
            "[69,    20] loss: 0.019\n",
            "[69,    40] loss: 0.021\n",
            "[70,    20] loss: 0.012\n",
            "[70,    40] loss: 0.022\n",
            "ITERATION:  8\n",
            "Accuracy of the network on the 8 iteration: 10 %\n",
            "[1,    20] loss: 9.382\n",
            "[1,    40] loss: 1.756\n",
            "[2,    20] loss: 1.090\n",
            "[2,    40] loss: 0.926\n",
            "[3,    20] loss: 0.767\n",
            "[3,    40] loss: 0.762\n",
            "[4,    20] loss: 0.585\n",
            "[4,    40] loss: 0.582\n",
            "[5,    20] loss: 0.478\n",
            "[5,    40] loss: 0.498\n",
            "[6,    20] loss: 0.338\n",
            "[6,    40] loss: 0.348\n",
            "[7,    20] loss: 0.256\n",
            "[7,    40] loss: 0.279\n",
            "[8,    20] loss: 0.190\n",
            "[8,    40] loss: 0.200\n",
            "[9,    20] loss: 0.149\n",
            "[9,    40] loss: 0.147\n",
            "[10,    20] loss: 0.091\n",
            "[10,    40] loss: 0.096\n",
            "[11,    20] loss: 0.076\n",
            "[11,    40] loss: 0.083\n",
            "[12,    20] loss: 0.090\n",
            "[12,    40] loss: 0.091\n",
            "[13,    20] loss: 0.100\n",
            "[13,    40] loss: 0.083\n",
            "[14,    20] loss: 0.054\n",
            "[14,    40] loss: 0.062\n",
            "[15,    20] loss: 0.058\n",
            "[15,    40] loss: 0.043\n",
            "[16,    20] loss: 0.032\n",
            "[16,    40] loss: 0.039\n",
            "[17,    20] loss: 0.046\n",
            "[17,    40] loss: 0.046\n",
            "[18,    20] loss: 0.026\n",
            "[18,    40] loss: 0.055\n",
            "[19,    20] loss: 0.048\n",
            "[19,    40] loss: 0.060\n",
            "[20,    20] loss: 0.132\n",
            "[20,    40] loss: 0.070\n",
            "[21,    20] loss: 0.026\n",
            "[21,    40] loss: 0.040\n",
            "[22,    20] loss: 0.036\n",
            "[22,    40] loss: 0.030\n",
            "[23,    20] loss: 0.018\n",
            "[23,    40] loss: 0.027\n",
            "[24,    20] loss: 0.029\n",
            "[24,    40] loss: 0.035\n",
            "[25,    20] loss: 0.062\n",
            "[25,    40] loss: 0.074\n",
            "[26,    20] loss: 0.077\n",
            "[26,    40] loss: 0.066\n",
            "[27,    20] loss: 0.062\n",
            "[27,    40] loss: 0.067\n",
            "[28,    20] loss: 0.049\n",
            "[28,    40] loss: 0.038\n",
            "[29,    20] loss: 0.022\n",
            "[29,    40] loss: 0.077\n",
            "[30,    20] loss: 0.064\n",
            "[30,    40] loss: 0.098\n",
            "[31,    20] loss: 0.075\n",
            "[31,    40] loss: 0.113\n",
            "[32,    20] loss: 0.076\n",
            "[32,    40] loss: 0.096\n",
            "[33,    20] loss: 0.114\n",
            "[33,    40] loss: 0.097\n",
            "[34,    20] loss: 0.088\n",
            "[34,    40] loss: 0.105\n",
            "[35,    20] loss: 0.114\n",
            "[35,    40] loss: 0.100\n",
            "[36,    20] loss: 0.133\n",
            "[36,    40] loss: 0.075\n",
            "[37,    20] loss: 0.028\n",
            "[37,    40] loss: 0.030\n",
            "[38,    20] loss: 0.013\n",
            "[38,    40] loss: 0.011\n",
            "[39,    20] loss: 0.005\n",
            "[39,    40] loss: 0.011\n",
            "[40,    20] loss: 0.009\n",
            "[40,    40] loss: 0.035\n",
            "[41,    20] loss: 0.064\n",
            "[41,    40] loss: 0.050\n",
            "[42,    20] loss: 0.013\n",
            "[42,    40] loss: 0.030\n",
            "[43,    20] loss: 0.034\n",
            "[43,    40] loss: 0.025\n",
            "[44,    20] loss: 0.020\n",
            "[44,    40] loss: 0.014\n",
            "[45,    20] loss: 0.006\n",
            "[45,    40] loss: 0.037\n",
            "[46,    20] loss: 0.035\n",
            "[46,    40] loss: 0.033\n",
            "[47,    20] loss: 0.027\n",
            "[47,    40] loss: 0.042\n",
            "[48,    20] loss: 0.017\n",
            "[48,    40] loss: 0.017\n",
            "[49,    20] loss: 0.005\n",
            "[49,    40] loss: 0.013\n",
            "[50,    20] loss: 0.027\n",
            "[50,    40] loss: 0.047\n",
            "[51,    20] loss: 0.083\n",
            "[51,    40] loss: 0.054\n",
            "[52,    20] loss: 0.021\n",
            "[52,    40] loss: 0.025\n",
            "[53,    20] loss: 0.028\n",
            "[53,    40] loss: 0.032\n",
            "[54,    20] loss: 0.034\n",
            "[54,    40] loss: 0.018\n",
            "[55,    20] loss: 0.010\n",
            "[55,    40] loss: 0.020\n",
            "[56,    20] loss: 0.030\n",
            "[56,    40] loss: 0.024\n",
            "[57,    20] loss: 0.019\n",
            "[57,    40] loss: 0.026\n",
            "[58,    20] loss: 0.005\n",
            "[58,    40] loss: 0.009\n",
            "[59,    20] loss: 0.005\n",
            "[59,    40] loss: 0.007\n",
            "[60,    20] loss: 0.004\n",
            "[60,    40] loss: 0.007\n",
            "[61,    20] loss: 0.004\n",
            "[61,    40] loss: 0.011\n",
            "[62,    20] loss: 0.009\n",
            "[62,    40] loss: 0.033\n",
            "[63,    20] loss: 0.051\n",
            "[63,    40] loss: 0.036\n",
            "[64,    20] loss: 0.034\n",
            "[64,    40] loss: 0.043\n",
            "[65,    20] loss: 0.043\n",
            "[65,    40] loss: 0.043\n",
            "[66,    20] loss: 0.023\n",
            "[66,    40] loss: 0.031\n",
            "[67,    20] loss: 0.038\n",
            "[67,    40] loss: 0.143\n",
            "[68,    20] loss: 0.116\n",
            "[68,    40] loss: 0.057\n",
            "[69,    20] loss: 0.236\n",
            "[69,    40] loss: 0.102\n",
            "[70,    20] loss: 0.072\n",
            "[70,    40] loss: 0.060\n",
            "ITERATION:  9\n",
            "Accuracy of the network on the 9 iteration: 9 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLE5LnQkZXSI"
      },
      "source": [
        "#### confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "yw2RugH6WcEv",
        "outputId": "11f89c31-7466-48d8-d9d0-ab5af9cb07e3"
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "confusion_matrix = torch.zeros(100,100)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in valid_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net.forward(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "          confusion_matrix[t.long(),p.long()] += 1\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=plt.cm.jet)\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 9 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYrUlEQVR4nO3de7BdZXnH8e9TEjwBhECokBDHQ5tUYFARzwBOaDmVMCWKYCmDqDikEjO0lYviUFRaQgsdcFAu0sLE0NKKNVBgANHgJJTDlFiCB4iABEwKB0kg4RouykEiT/94n3fv91x2snM76yTr95k586z17r0usycrz1rvei/m7ojI9u/3qj4BERkZuthFakIXu0hN6GIXqQld7CI1oYtdpCY262I3s6PN7AkzW2Fm526pkxKRLc829T27me0A/BI4ClgJ/Az4jLs/tuVOT0S2lDGbse0hwAp3fxLAzOYDxwEtL3aznRzGb8YhRbYnQ2+sD/zIGgAefWBSlOwW8cWI+ZJdV2y1Z3y0I/yuD3/nRRvuaJtzse8DPFOsrwQOHfwlM5sNzE5ruzUXRWpv3JCS23ovB+APG0/Fn4w4L+IeEV8utpqVwp6T4cWulkfbnIu9Le4+F5gLYDZJbXNFGmakMPODjZKD3zolLRy0d4pLL4pPpkR8PeJexX6WpLD6EeDVlkfbnAq6VcB7i/XJUSYio9DmZPafAVPNbF/SRX4S8NktclYitfBwCtc1S169bp9YuiFiXn8zYr59/2Rjm+/4lQCcblcC7255tE2+2N19nZl9CfgJsAPwr+7+i03dn4hsXZv1zO7uPwZ+vIXORUS2oq1eQScirUyIuKwoi8q2w7+Y4r3Xp9g5M8W+uPU/rVmpd/r+uaZ+Ac0KvKHUXFakJpTZRSrzgYhrirL7U5gZq/c+nWLfS1EQFXWPF5uszgtvA63fbiuzi9TEJreN36SD2SRXCzqR0DEnxROKsuu/GwuRwfv+KsXOW6K8L2LZ+u7g2N/h8FYX/k7vsM1lldlFakLP7CJV6V+Q4vVF2WlRC59bx3ZeOWij7igvuqH0xTN//73AGy0Pp8wuUhPK7CKVifS9y9RmUc7ys3JBfm9+TsSeFPreLvYT797Zh/Vd0srsIjWhzC5SlTGR0d8o37PHc/xVM2P9PQPL8zv5zrHNTfqiZv6OA+CsjpaHU2YXqQld7CI1odt4kaqsyxVri4rCP4/PrgZgf0+v2JbZyvg8+rP3fbe5yafidd0x6z+cMrtITSizi1QmV8x9pSh7LYUTUjPZZZYz+K6Dti3Wb41OMvdNaHagGYYyu0hNKLOLVCaPJ9dTlHWncFNuJptfseWxXPNYdM3RZV/yyQBMsOuBV1oeTZldpCaU2UUqMsNT1l5gRxSll0TsjBijyE7fKcVFt0d5cyirCZbHln8TDV4hIsrsIlX5V74AwEQ+XJTmZ/FcUx/P7I2MnsvLwStyp5jXgd+1PJ4yu0hN6GIXqQndxotUZKL9RywV0y8vnZlid6yvzZVv+0XMt/nlbXxU4k2ZAM9c2/J4yuwiNaHMLlKVg45NcWlPUbY4xTwDzNo8tnzO5NGo5ktHNbfJ/Wgevxk1qhERjRsvUp3jI64qynImT+l6qqfXcsstTwHTF7HoCDM+xpYfDzzbhb+lceNFak2ZXaQi3/E0IMXpVjx/N5rBThv07WhMM+vkFOf1FJ89GPE1YC7uzyqzi9SZauNFKnK6paGnBowUu3ZQ5IaI8Yx+ay4v37Pn5/yjgDtaHk+ZXaQmlNlFqtIVGb33paIw18zvE3H/geUvLk/xhGKut5tyR5jNfM9uZu81s7vN7DEz+4WZnRnle5jZQjNbHnH3De1LRKrTzm38OuBsdz8AOAz4GzM7ADgXuMvdpwJ3xbqIjFIb/erNzG4Droq/bnd/zswmAj3u/v71b6tXbyJNUbHW8RfNov48s+PzAOyx7tMAvDzmt1H+nxHf3dxmyhkprgTe6sLfGb5RzUY9s5tZJ/BhYAmwl7s/Fx+tphwBb+A2s2lc4bttzOFEZAtq+2I3s11INQBnuftrZs3/PNzdzWzYWwR3nwvMTfuYNHIteES2Ff1XFyv5kkxjyb88JndxPS1irrib2djinOUXAPBNOwR4teVh2nr1ZmZjSRf69939liheE7fvRHy+nX2JSDU2mNktpfBrgWXu/u3io9uBU4CLI962Vc5QZLuVB60om8a+Oeg750Rs5NhBEb457vy0sBT4bOtH5XZu46cBnwceMbOlUfZ10kV+o5mdCjwNnNjGvkSkIhu82N39XmDY2j3gyC17OiJ1kmeEea0oi8EryI1megD4ji8E4HT7xoByAPojyx90KPDrlkdTc1mRmlBzWZHKxOAVY4q31uuii2t3rPek7qunf39eFOQZY/ZvbsOMFDqmwls7tzyaMrtITSizi1SlOzJ6T1kYz+o98e794r9N8eTc2WWPYXa0IoX+TjTXm4joYhepC93Gi1SlJ/qmj5naLNvvgymeFPHcXCGXK/Fyo5spxY4OKJZbvSVXZhepDWV2kcpEpdusouiaOSkeFrHRpDbmc+ORFDqKbN5/XSx0kqZtHp4yu0hNKLOLVOaeFK55uShLXVuZfl2s5+ax+dXbkhT6VzY3+dTMFL8KnFoMajGIMrtITSizi1Tm4IhlQ5nU4YV5M1OclWvjo3EN01M47CPNTW79TcT70TO7iCizi1Qnz9FWvjOP2var8iCU6b2635ben9txMVDFffcU2+Tn+bHAb1oeTZldpCZ0sYvUhG7jRSpzdMQfFmUxYePSPHpNNwB2XPeAdVb2F7vpSPHRm4EdWx5NmV2kJpTZRSqTJ3E8oyiLV28ronHMlGhEk0ejYUEKk9/T3OTA/BpuLOoIIyLK7CLVybO7LCjKYvrmKTfGeh43/hoAvuSp0cxVv39Oc5NH83QO76bZcWYoZXaRmlBmF6nM/RGPL8qioc34GItubZ4HLjWpvcrSqLK79a9ubPFqx5dj6R7gXS2PpswuUhPK7CKVyV1bFw39aG2epz03hc1zu6WRZF/tGNf4qvdMBMC656NndhFRZhepTm4ld0RRNjZizuQ9EfNdQLx//9ShjS1sSh6E8hfomV1EdLGL1IVu40UqssBTY5oZNr0ozf3U8+g1+bVcfgUXI9bc+lhzk1tzA5yxwKstj6fMLlITyuwiFZlhMZLssK/LZka8MmLO8KmjzBH+u8Y377E8Au2DwPW0oswuUhNtZ3Yz2wHoBVa5+zFmti8wH5gAPAB83t1/u3VOU2R7dGzEh4uy3FgmN6aJrq3TYz64RemV3D1WzA+XX9NNORSe2bnl0TYms58JLCvWLwEuc/cpwCvAqRuxLxEZYW1ldjObDHwCuAj4ipkZ8DHgs/GVfwfm0KwyFJENygNTHFKU5UtycQpTulPsyZ/n2vo8Mi0wOQa/6KLRQ3Y47Wb2y0kda9+J9QnAWnfPNQsraXbOHcDMZptZr5n1rm+YWxHZujaY2c3sGOB5d3/AzLo39gDuPheYm/Y1yTf6DEW2V5dGk9evFu/MG7OzRlxxe6zn+dnzLDJFCl8Zl9X852k+6w/Vzm38NOBYM/s40AHsClwBjDezMZHdJ9McUEtERqEN3sa7+9fcfbK7dwInAf/t7p8D7gZOiK+dAty21c5SRDbb5jSq+VtgvpldCDwEXLtlTkmkJr5689Cy/jtTnBINblbEdFD0pbB3qow7+7kLG5t8y+bF0mTWVy+2URe7u/cQ9YLu/iQDqxFFZBRTc1mRyuRqrnFFWYwHvyKv5w4x0Wd99cLYosjgh38xxXuvB5rNaAdTc1mRmlBmF6lMZwoXH9ssOveGFE+K9fl5FJpPRkzdYi+0i4r9xLjzXSfDY5e3PJoyu0hNKLOLVCYa1ZxbFM36dIp9uSA/zy8etJ7HpIPGGHa9L6HRZUVEmV2kOjFja6MpLDAvjy67PGKerTVn7FyD38zsb/TvB8AuHdcAa1seTZldpCaU2UUqkwegKGdxnRUx18LnZ/TUHdZ/dB4A9onmu/ldOv4hllYBrcePUWYXqQld7CI1odt4kapcE6/eTtu/KNw1hTGTU1yXG8+kSjj7xBWxfkaxzQMRPwLc0fJwyuwiNaHMLlKVe/NCX1H4wxTmxVjwM2Pkmcv/IsWz8ve+W2yTX8fdD7zQ8nDK7CI1Ye4jNyxcGoNu9ogdT2R0y91Xy+fvGNBiv8jkj+cZYfLYc6+ncN2M5ia5ue3q14AjcH/IhjuaMrtITeiZXaQyUQs/syh6MTL6HSujIDI590f8XApXFduszgNZLAbeaHk0ZXaRmlBmF6nadXOGKTxx0HqqlfdpewNgi79VfPZasfzrlodRZhepCV3sIjWh23iRyuTJGZs92B71uQAcaLlsesTUM84W5/Hiy1v3GGP+6F3hp2ouK1J7yuwilZvVWJrInFiaEnHw2HN5JJtyfpaY5PHOHjRSjYgos4tUJ48F33z+nmBz0sLJ0bjm+qtTnBnP5dfFeudRzd3k9jfrVrG+S1qZXaQmlNlFKnNAxLK7ajyb35nXo4vrfSl0rE3jyvePf6DYpi/iuub3h6HMLlIT6uIqUpncxfXQRsnvrT4QgHf27o+SGMyiMdosQ9c743l+OnBrF/5Cr7q4itSZMrtIRf7XbwXgo/ZQUZrfq6+JGANP8nDEnNHLQSpzS7w9gItw71NmF6kzXewiNdHWqzczGw/MAw4EHPgC8ARwA2lG+T7gRHd/Zaucpch26KPjlsbSt4vSuE0fE6PLNsaNTx1ibvTUEeZE6y62yU1pt8z0T1cAd7r7fsCHgGWkYe7ucvepwF0MnGVaREaZDVbQmdluwFLgD7z4spk9AXS7+3NmNhHocff3r39fqqATabhvToqHLSwKcwXdPhFjOueOYwF435uPA/C0zR9mh53ABbg/tckVdPuSRp7/NzN7yMzmmdnOwF7u/lx8ZzUDJpluMrPZZtZrZr3wm+G+IiIjoJ3M3kVqrDfN3ZeY2RWklvunu/v44nuvuPvu69+XMrtIU+egCI1XamMid66bE+UxB1zj1dsRQ/b2Pb+cv++6iyd7X9nkzL4SWOnuS2L9JtKI9Wvi9p2Iz7exLxGpyAZr4919tZk9Y2bvd/cngCOBx+LvFODiiLdt1TMV2e7khjPNLq4+6U8BsGfzbK1jB30nZ/jFNKW7gM/bn9AcX36odnu9nQ5838x2BJ4E/pJ0V3CjmZ0KPM3QsW9FZBRp62J396VA1zAfHbllT0ekTnJHmM5GiT37L2lhxWkpTvlmfJKHqUpdXLm82M1ZPbEwDti55dHUgk6kJnSxi9SERqoRqdyuxXK8ejstvz1bF/HliGkK56lnNl+9Lb+mOy08vqD4/lDK7CI1ocwuUpmchQ8tyu5NYVEeS26/QdscD8DyvyvazazICzOA3VoeTZldpCaU2UUq0xmxpyjLjWWmRswNU98dcREAc/5xUWOLORdfEksLGTgH3EDK7CI1ocwuUpllEacXZSmDf8ifAuDnFk1qF/1VfDWNMT/HphXbxCwxHE85I+xgyuwiNaHMLlKRX/plAPyRTSlK0/v0n9vTA798Xl7IGf2HxYe52+tC4PWWx1NmF6kJZXaRivyR9cbSyqJ0VcT8HB8dXy7MxffEQtnq7oMpHD0Dflr2kBlImV2kJnSxi9SEbuNFKrMgYnOSRv/MXwNgP8iNY2KkmumDG8uUr9hiINc77wXeaHk0ZXaRmlBmF6lKx8wU+3saRfaD82OpM2KqqPuW/x0AZ9vFUV7OIvN4xLGoi6uIKLOLVKY/urPydlEYr9Sui1duM1Mnl7N/FGPTcX3EOcU2L0VcALzT8nDK7CI1ocwuUpkY473jK82i/gdTzBO8Rk39P33iywB8nb2j/MpiP7lmfldg2MlgAGV2kdpQZhepTMzU2l+WxTP7ngO/+XXLPWHinfy5k5sfXnx7LHSTJmganjK7SE0os4tUJmrclxZFB0U876lYyB1e8vvzaHV38ReLjbojvolq40VEF7tIXeg2XqQyD6dw0MNFWb4k47b96Hgtd000vOn8XHx+dbFNniByGuAtj6bMLlITyuwilcldW09uFuWJ0Y+JOGdOip0fiIJcUXdwsZ8Yw27yBFizY8ujKbOL1IQyu0hlYmbWXYqi3p4U3+ge9N1Bo8buUswP90YMXnEp8PXWR1NmF6mJtjK7mX0ZmEWq6nsE+EtgIjAfmAA8AHze3X+7lc5TZPv1xsJiZfDgE50pjJkZH8cMMXsXX1kRz/6X7gSrWx9mg5ndzPYBzgC63P1AYAfgJOAS4DJ3nwK8Apy6oX2JSHXafWYfA4wzs7eBnYDngI8Bn43P/53Um/7qYbcWkWHk8eI/WJRF5m4k+HiuX5ffxafOM2Pvaw5A+fae0cW1dzMHnHT3VaRH/1+RLvJXSbfta909n9LKxlkMYmazzazXzHobo2CKyIhr5zZ+d+A4YF9gErAzcHS7B3D3ue7e5e5d6aZARKrQzm38dOApd38BwMxuIbXLG29mYyK7T6Y5b42ItME/PhsA+/G3itJoFrvikVgvn5Qh30C/veebxTb5tdw44Nctj9fOq7dfAYeZ2U5mZsCRwGPA3cAJ8Z1TgNva2JeIVGSDmd3dl5jZTcCDpGqDh4C5wI+A+WZ2YZRduzVPVGR7Yz/O+XFFo+x//EYA/tjmR0mMU5dnhjkw+rE/elGxp/dFfJP0smx4bdXGu/v5wPmDip8EDmlnexGpnprLilQmvz5rjht/+H/F6LIsjjhu4Hce7RmyTfM7nUBHy6OpuaxITSizi1QmxqCbN7ZRYifuH0t9KXx1ZoqX3hzlSyJ+sthPNLzpOBzeKnvVDKTMLlITyuwiFTnPU533hfZPRWk8s+8yJ8VLI+YBKpgR8QM0XZZC/2Lg2ZbHU2YXqQlzbz1A3RY/mE1ymD1ixxMZ3XJ2fn2Yz45IYfq+KfZE8bo5sdDd/OpBsXwycHkX/kzvsBO+KbOL1IQudpGaUAWdSGXyCLEPFmW5g8t/AnD/wtRs9hD7TpTnBjTFWPNLow/80k+v92jK7CI1ocwuUplFw5TtFfEbABxieUyYnP2PB+BU/0lji2ttWiz9Bk3sKCLK7CJV+e3a9Bp6x/H/UpTmZ/YbIubM/tqAeK0dVGyTO82sAV5seTxldpGaUGYXqcg3x+elNUXpmkHfyp1kcpfWH25gr4PHnW9SZhepCWV2kYqcF4M/nc8FjbILhgwI9TZbijK7SE3oYhepCd3Gi1SsvHW/MG7pzxtyO7/5lNlFakKZXWQUyRl9a2R4ZXaRmlBmFxmFBr+WG/pKbuMps4vUhDK7yCh2wRbM8MrsIjWhzC6yDdgSGV6ZXaQmlNlFtiEXDHoPD+2/i1dmF6kJXewiNaHbeJFt0HnDdp65EBh25idAmV2kNjSxo8ioNHjsuTzK7KqWW3yPC/h74El3TewoUmcjnNntBeDXrG9w69FlT7adc4Vt63y3pXOFbed83+fuvz/cByN6sQOYWa+7d43oQTfRtnSusG2d77Z0rrDtne9wdBsvUhO62EVqooqLfW4Fx9xU29K5wrZ1vtvSucK2d75DjPgzu4hUQ7fxIjWhi12kJkbsYjezo83sCTNbYWbnjtRx22Vm7zWzu83sMTP7hZmdGeV7mNlCM1secfeqzzUzsx3M7CEzuyPW9zWzJfEb32BmO1Z9jpmZjTezm8zscTNbZmYfHa2/rZl9Of4NPGpmPzCzjtH827ZrRC52M9sB+GdgBnAA8BkzO2Akjr0R1gFnu/sBwGHA38Q5ngvc5e5TgbtifbQ4E1hWrF8CXObuU4BXgFMrOavhXQHc6e77AR8infeo+23NbB/gDKDL3Q8EdgBOYnT/tu1x963+B3wU+Emx/jXgayNx7M0459uAo4AngIlRNhF4oupzi3OZTLpAPgbcQeru9CIwZrjfvOJz3Q14iqgQLspH3W9LaoT+DLAHqVfoHcCfjdbfdmP+Ruo2Pv+A2UqaLftHHTPrBD4MLAH2cvfn4qPVwF4VndZglwPnAO/E+gRgrbuvi/XR9BvvC7wA/Fs8dswzs50Zhb+tu68CLgV+BTwHvAo8wOj9bdumCrpBzGwX4GbgLHd/rfzM03/rlb+rNLNjgOfd/YGqz6VNY4CDgavd/cOk/hEDbtlH0W+7O3Ac6T+oScDOwNGVntQWMlIX+yrgvcX6ZNbXV68iZjaWdKF/391vieI1ZjYxPp8IPF/V+RWmAceaWR8wn3QrfwUw3szygCSj6TdeCax09yWxfhPp4h+Nv+104Cl3f8Hd3wZuIf3eo/W3bdtIXew/A6ZGjeaOpAqP20fo2G0xMwOuBZa5+7eLj24HTonlU0jP8pVy96+5+2R37yT9lv/t7p8D7gZOiK+NinMFcPfVwDNm9v4oOhJ4jFH425Ju3w8zs53i30Q+11H5226UEaz4+DjwS+D/gG9UXVkxzPkdTrqNfBhYGn8fJz0L3wUsBxYBe1R9roPOuxu4I5b/ALgfWAH8F/Cuqs+vOM+DgN74fW8Fdh+tvy1wAfA48CjwPeBdo/m3bfdPzWVFakIVdCI1oYtdpCZ0sYvUhC52kZrQxS5SE7rYRWpCF7tITfw/1nbm7FuMjfgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}