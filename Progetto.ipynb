{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LWF funzionante.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoManca-FM/Project_MLDL/blob/main/Progetto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAfp19qRVa7l"
      },
      "source": [
        "### GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag2iq0PaVWC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bd8976-a263-49ae-96d2-6ad5c9958bd0"
      },
      "source": [
        "# Check GPU assigned\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jun 29 11:18:03 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OWtqytyzmMF"
      },
      "source": [
        "## Network, dataset, functions and parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itMjhanGVTUp"
      },
      "source": [
        "### LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6tCA_s2rru"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nHVORYbiZRx"
      },
      "source": [
        "### Resnet32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIdSy6PnibZA"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\"\"\"\n",
        "Credits to @hshustc\n",
        "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from torch.nn import init\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_relu=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.stride = stride\n",
        "        self.use_relu = use_relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "        if self.use_relu:\n",
        "            out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        assert stride == 2\n",
        "        self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat((x, x.mul(0)), 1)\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block=BasicBlock, depth=32, num_classes=0, channels=3):\n",
        "\n",
        "        super(CifarResNet, self).__init__()\n",
        "\n",
        "        # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
        "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
        "        layer_blocks = (depth - 2) // 6\n",
        "        bn = nn.BatchNorm2d\n",
        "        self.inplanes = 16\n",
        "\n",
        "        self.conv_1_3x3 = nn.Conv2d(channels, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn_1 = bn(self.inplanes)\n",
        "\n",
        "        self.stage_1 = self._make_layer(block, 16, layer_blocks, 1)\n",
        "        self.stage_2 = self._make_layer(block, 32, layer_blocks, 2)\n",
        "        self.stage_3 = self._make_layer(block, 64, layer_blocks, 2, last=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.relu = nn.ReLU()\n",
        "        #self.linear = nn.Linear(64, num_classes)\n",
        "        self.fcs = nn.ModuleList([nn.Linear(64, num_classes)])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, math.sqrt(1. / 64.))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, last=False):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = Downsample(self.\n",
        "inplanes, planes * block.expansion, stride)\n",
        "\n",
        "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        if last:\n",
        "            for i in range(1, blocks - 1):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            layers.append(block(self.inplanes, planes, use_relu=False))\n",
        "\n",
        "        else:\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv_1_3x3(x)\n",
        "\n",
        "        x = self.bn_1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.stage_1(x)\n",
        "        x = self.stage_2(x)\n",
        "        x = self.stage_3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        out = []\n",
        "        for fc in self.fcs:\n",
        "            out.append(fc(x))\n",
        "        out = torch.cat(out, dim=1)\n",
        "        return out\n",
        "\n",
        "    def addOutputNodes(self, num_classes):\n",
        "        self.fcs.append(nn.Linear(64, num_classes))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ga-SaCsjkYV"
      },
      "source": [
        "### Cifar100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHAyljuejmkd"
      },
      "source": [
        "# ref:\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py\n",
        "# https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR10\n",
        "# homework2 (caltech)\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# This is an handler class for the Cifar dataset\n",
        "class CIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    `CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "    This is a subclass of the `CIFAR100` Dataset.\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar-100-python'\n",
        "    train_file = 'train'\n",
        "    test_file = 'test'\n",
        "    meta_file = 'meta'\n",
        "\n",
        "    def __init__(self, root, split = 'train', transform = None):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              root (string): Root directory of the dataset where directory\n",
        "                  cifar-100-python exists.\n",
        "              split (string, optional): If 'train', creates dataset from training\n",
        "                  set, otherwise creates from test set.\n",
        "              transform (callable, optional): A function/transform that takes in a\n",
        "                  PIL image and returns a transformed version.\n",
        "        \"\"\"\n",
        "        super(CIFAR100, self).__init__(root, transform=transform)\n",
        "\n",
        "        self.split = split\n",
        "        if split == 'train':\n",
        "            filename = self.train_file\n",
        "        else:\n",
        "            filename = self.test_file\n",
        "\n",
        "        data_path = os.path.join(self.root, self.base_folder, filename)\n",
        "        data = None\n",
        "        labels = None\n",
        "\n",
        "        with open(data_path, 'rb') as f:\n",
        "            entry = pickle.load(f, encoding='latin1')\n",
        "            data = entry['data']\n",
        "            labels = entry['fine_labels']\n",
        "        \n",
        "        data = np.vstack(data).reshape(-1, 3, 32, 32)\n",
        "        data = data.transpose((0, 2, 3, 1))  # Convert to HWC\n",
        "        \n",
        "        labels = np.array(labels)\n",
        "\n",
        "        self.df = pd.DataFrame()\n",
        "        self.df['data'] = pd.Series(list(data))\n",
        "        self.df['labels'] = labels\n",
        "\n",
        "        self.data = self.df['data']\n",
        "        self.labels = self.df['labels']\n",
        "\n",
        "        self._load_meta()\n",
        "\n",
        "    def _load_meta(self):\n",
        "        meta_path = os.path.join(self.root, self.base_folder, self.meta_file)\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f, encoding='latin1')\n",
        "            self.label_names = meta['fine_label_names']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.df.loc[index, 'data'], self.df.loc[index, 'labels']\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img) # Return a PIL image\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return index, img, target\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def getTargets(self):\n",
        "        return set(self.labels)\n",
        "     \n",
        "    # test\n",
        "    def get_indices(self, labels):\n",
        "        return list(self.df[self.df['labels'].isin(labels)].index)\n",
        "\n",
        "    def split_classes(self, n_splits=10, seed=None, dictionary_of='dataframes'):\n",
        "        if dictionary_of not in ['dataframes','indices']:\n",
        "            raise ValueError(\"'dictionary_of' must be equal to 'dataframes' or 'indices'\")\n",
        "\n",
        "        all_classes = list(self.df['labels'].value_counts().index)\n",
        "        dictionary = {}\n",
        "        random.seed(seed)\n",
        "        random.shuffle(all_classes)\n",
        "        split_size = int(len(all_classes)/n_splits)\n",
        "        for j in range(n_splits):\n",
        "            if ((j+1)*split_size < len(all_classes)):\n",
        "                split_end = (j+1)*split_size\n",
        "            else:\n",
        "                split_end = None\n",
        "            subgroup = all_classes[j*split_size:split_end]\n",
        "            if dictionary_of == 'dataframes':\n",
        "                dictionary[j] = self.df[self.df['labels'].isin(subgroup)]\n",
        "            elif dictionary_of == 'indices':\n",
        "                dictionary[j] = list(self.df[self.df['labels'].isin(subgroup)].index)\n",
        "        return dictionary\n",
        "\n",
        "    def split_groups_in_train_validation(self, groups, ratio=0.5, seed=None):\n",
        "        groups_train_val = dict()\n",
        "        for k, subdf in groups.items():\n",
        "            train_indexes = []\n",
        "            val_indexes = []\n",
        "            split_labels = list(subdf['labels'].value_counts().index)\n",
        "            for l in split_labels:\n",
        "                indexes_to_sample = list(subdf[subdf['labels'] == l].index)\n",
        "                random.seed(seed)\n",
        "                train_samples = random.sample(indexes_to_sample, int(len(indexes_to_sample)*ratio))\n",
        "                train_indexes = train_indexes + train_samples\n",
        "                val_indexes = val_indexes + list(set(indexes_to_sample).difference(set(train_samples)))\n",
        "            groups_train_val[k] = {\n",
        "                'train': train_indexes,\n",
        "                'val': val_indexes\n",
        "            }\n",
        "        return groups_train_val\n",
        "    \n",
        "    def split_in_train_val_groups(self, n_splits=10, ratio=0.5, seed=None):\n",
        "        groups = self.split_classes(n_splits=n_splits, seed=seed)\n",
        "        return self.split_groups_in_train_validation(groups, ratio=ratio, seed=seed)\n",
        "\n",
        "    # given a tensors returns an image (used in exemplars)\n",
        "    #def tensorToImg(self, tensor):\n",
        "    #   return Variable(transform(Image.fromarray(img)), volatile=True)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_M1anVYkRnF"
      },
      "source": [
        "### Reverse Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j66mUOXekTlM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "__all__ = ['ReverseIndex']\n",
        "\n",
        "class ReverseIndex():\n",
        "\n",
        "    def __init__(self, dataset, splits, device='cuda'):\n",
        "\n",
        "        self.df = pd.DataFrame(columns=['group', 'labels'])\n",
        "\n",
        "        for k in splits.keys():\n",
        "            labels = list(dataset.df.loc[splits[k]['train'],'labels'].value_counts().index)\n",
        "            group = [k for i in range(len(labels))]\n",
        "            data = pd.DataFrame(np.array([group, labels]).T, columns=['group', 'labels'])\n",
        "            self.df = self.df.append(data, ignore_index=True)\n",
        "\n",
        "        self.df['nodes'] = self.df.index\n",
        "        self.device = device\n",
        "    \n",
        "    def _changeIndex(self, reverse_index, column):\n",
        "        reverse_index = reverse_index.set_index(column)\n",
        "        reverse_index[column] = reverse_index.index\n",
        "        return reverse_index\n",
        "\n",
        "    def getLabels(self, outputs):\n",
        "        outs = outputs.cpu().numpy()\n",
        "        reverse_index = self._changeIndex(self.df, 'nodes')\n",
        "        labels = reverse_index.loc[outs, 'labels']\n",
        "\n",
        "        labels = torch.tensor(list(labels))\n",
        "        return labels.to(self.device)\n",
        "\n",
        "    def getNodes(self, labels):\n",
        "        labels = labels.cpu().numpy()\n",
        "       \n",
        "        reverse_index = self._changeIndex(self.df, 'labels')\n",
        "\n",
        "        nodes = reverse_index.loc[labels, 'nodes']\n",
        "\n",
        "        nodes = torch.tensor(list(nodes))\n",
        "        return nodes.to(self.device)\n",
        "\n",
        "    def getGroups(self, distinct=True):\n",
        "        return self.df['group'].value_counts().index.sort_values()\n",
        "    \n",
        "    def getLabelsOfGroup(self, group):\n",
        "        return self.df.loc[self.df['group'] == group, 'labels']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVfOeOCuknB8"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QpAHzFkoW0"
      },
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Module\n",
        "\n",
        "# These are the default iCaRL hyper-parameters\n",
        "def getHyperparams():\n",
        "\tdictHyperparams = {\n",
        "\t\t\"LR\": 2,\n",
        "\t\t\"MOMENTUM\": 0.9,\n",
        "\t\t\"WEIGHT_DECAY\": 1e-5,\n",
        "\t\t\"NUM_EPOCHS\": 70,\n",
        "\t\t\"MILESTONES\": [49, 63],\n",
        "\t\t\"BATCH_SIZE\": 128,\n",
        "\t\t\"DEVICE\": 'cuda',\n",
        "\t\t\"GAMMA\": 0.2,\n",
        "\t\t\"SEED\": 66, #use 30, 42, 16\n",
        "\t\t\"LOG_FREQUENCY\": 10,\n",
        "\t\t\"NUM_CLASSES\": 100\n",
        "\t}\n",
        "\treturn dictHyperparams\n",
        "\n",
        "def getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize):\n",
        "\toptimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\tscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA, last_epoch=-1) \n",
        "\treturn optimizer, scheduler\n",
        "\n",
        "# the mean and the std have been found on the web as mean and std of cifar100\n",
        "# alternative (realistic): compute mean and std for the dataset\n",
        "def getTransformations():\n",
        "\t# Define transforms for training phase\n",
        "\ttrain_transform = transforms.Compose([transforms.RandomHorizontalFlip(), # Randomly flip the image with probability of 0.5\n",
        "\t                                      transforms.Pad(4), # Add padding\n",
        "\t                                      transforms.RandomCrop(32),# Crops a random squares of the image\n",
        "\t                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\t# Define transforms for the evaluation phase\n",
        "\teval_transform = transforms.Compose([\n",
        "\t                                      transforms.ToTensor(),\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\treturn train_transform, eval_transform\n",
        "\n",
        "# BCEWithLogits = Sigmoid + BCE, is the loss used in iCaRL\n",
        "def getLossCriterion():\n",
        "\tcriterion = nn.BCEWithLogitsLoss(reduction = 'mean') \n",
        "\treturn criterion\n",
        "\n",
        "# CrossEntropyLoss \n",
        "def computeLoss(criterion, outputs, labels):\n",
        "\treturn criterion(outputs, labels)\n",
        " \n",
        "# Loss L2\n",
        "def l2Loss (outputs, labels):\n",
        "  criterion = nn.MSELoss()\n",
        "  return criterion(outputs, labels)\n",
        "\n",
        "# Loss L1\n",
        "def l1Loss(outputs, labels):\n",
        "  criterion = nn.L1Loss()\n",
        "  return criterion(outputs, labels)\n",
        "\n",
        "# support BCE\n",
        "def _one_hot_encode(labels, n_classes, reverse_index, dtype=None, device='cuda'):\n",
        "\tbatch_size = len(labels)\n",
        "\tenconded = torch.zeros(batch_size, n_classes, dtype=dtype, device=device)\n",
        "\tlabels=map_to_outputs(labels, reverse_index)\n",
        "\tfor i, l in enumerate(labels):\n",
        "\t  enconded[i, l] = 1\n",
        "\treturn enconded\n",
        "\n",
        "def map_to_outputs(labels, reverse_index):\n",
        "\tif reverse_index is None:\n",
        "\t  return labels\n",
        "\tif type(labels) == int:\n",
        "\t  return int(reverse_index.getNodes(torch.tensor([labels])))\n",
        "\telif type(labels) == torch.Tensor:\n",
        "\t\treturn reverse_index.getNodes(labels)\n",
        "\n",
        "\n",
        "def plotAccuracyTrend(method, data_plot_line, seed):\n",
        "\tplt.figure(figsize=(20,7))\n",
        "\taccuracyDF=pd.DataFrame(data_plot_line, columns = ['Classes','Accuracy'])\n",
        "\tax = sns.lineplot(x=\"Classes\", y=\"Accuracy\",data=accuracyDF, marker = 'o')\n",
        "\tax.minorticks_on()\n",
        "\tax.set_xticks(np.arange(10,110,10))\n",
        "\tax.set_xlim(xmin=9, xmax=101)\n",
        "\tax.set_ylim(ymin=0, ymax=1)\n",
        "\tplt.legend(['Accuracy {}'.format(method)])\n",
        "\tax.grid(axis='y')\n",
        "\tplt.title(\"Accuracies against seen classes {} - seed: {}\".format(method, seed))\n",
        "\t\n",
        "\tfilename = \"acc_{}_{}.jpg\".format(method, seed) # ex. acc_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "def plotConfusionMatrix(method, confusionMatrixData, seed):\n",
        "\tfig,ax=plt.subplots(figsize=(10,10))\n",
        "\tsns.heatmap(confusionMatrixData,cmap='terrain',ax=ax)\n",
        "\tplt.ylabel('True label')\n",
        "\tplt.xlabel('Predicted label')\n",
        "\tplt.title(\"Confusion Matrix {} - seed: {}\".format(method, seed))\n",
        "\n",
        "\tfilename = \"cm_{}_{}.jpg\".format(method, seed) # ex. cm_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "# Write down the metrics (accuracy trand and confusion matrix)\n",
        "# this method is a shortcut when perfoming multiple tests with different splits (random_seed)\n",
        "# and allow us to plot on the same graph the data from multiple models (accuracy)\n",
        "def writeMetrics(method, seed, accuracies, confusionMatrixData):\n",
        "  data = {}\n",
        "  data['accuracies'] = []\n",
        "  data['cm'] = [] #cm line\n",
        "  i = 0\n",
        "  for classes_seen in range(10, 110, 10): #x axis on the plot\n",
        "    data['accuracies'].append({classes_seen : accuracies[i]}) \n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  for class_num in range(0,len(confusionMatrixData)): #rows of the cm\n",
        "    data['cm'].append({class_num : confusionMatrixData[i].tolist()}) \n",
        "    i += 1\n",
        "  \n",
        "  # dump to file\n",
        "  aus = method + '_' + str(seed)\n",
        "  filename = 'data_{}.json'.format(aus)\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "\n",
        "# Functions\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getResNet32():\n",
        "    net = CifarResNet()\n",
        "    # net.fc = nn.Linear(net.fc.in_features, output_size) # embedded in the class\n",
        "\n",
        "    criterion = getLossCriterion()\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return net, criterion, optimizer, scheduler\n",
        "\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getNet():\n",
        "    return getResNet32()\n",
        "\n",
        "def getSchedulerOptimizer(net):\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return optimizer, scheduler"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfJj1z2LSxp0"
      },
      "source": [
        "### DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysghtAWOPYZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79124891-f2f4-4f7a-8bdd-bea1a9ee09a0"
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "if not os.path.isdir('./{}'.format(\"$DATA_DIR/cifar-100-python\")):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-29 11:18:05--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  31.7MB/s    in 5.7s    \n",
            "\n",
            "2021-06-29 11:18:12 (28.1 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n",
            "mv: cannot move 'cifar-100-python' to 'DATA/cifar-100-python/cifar-100-python': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY10HfpTS1lq"
      },
      "source": [
        "### HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-pqSNg4_Ris"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100\n",
        "BATCH_SIZE = 128 \n",
        "LR = 0.01    \n",
        "MOMENTUM = 0.9     \n",
        "WEIGHT_DECAY = 1e-05\n",
        "NUM_EPOCHS = 30\n",
        "GAMMA = 0.2\n",
        "LOG_FREQUENCY = 10\n",
        "MILESTONES = [49,63]\n",
        "RANDOM_SEED = 66"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW7WxV_QS4re"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_jHycn_1kk"
      },
      "source": [
        "train_transform, eval_transform = getTransformations()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPrwhTcIqUbA"
      },
      "source": [
        "### train & test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJKwvGljJj2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08ab2e7-b016-483c-b868-87573cc14dcd"
      },
      "source": [
        "# Import dataset and apply transformations \n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# Check datasets length \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnooWwcyS_YN"
      },
      "source": [
        "### SPLIT DATA IN CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckn3H69iJj2X"
      },
      "source": [
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpcJvhxhJOLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f203fcbd-ceab-4ac2-f608-569422b6d9da"
      },
      "source": [
        "# TRAIN / VAL split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "print(outputs_labels_mapping)\n",
        "\n",
        "# TEST split\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.ReverseIndex object at 0x7f71d6e12450>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MSItI0QVpn"
      },
      "source": [
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10):\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpWv5ZkhxTPJ"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4NQa-wNxWL1"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pisadUTPxcRP"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            # if (group_id > 1):\n",
        "            #   old_features = old_net.forward(images)\n",
        "            #   old_outputs = old_net.predict(old_features)\n",
        "            #   labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX_61nSuxh3v"
      },
      "source": [
        "### sequential learning fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM0GysHZxk1B"
      },
      "source": [
        "### Fine tuning\n",
        "def sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    old_net = None\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    #confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2X2IVZ-xrVT"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DGpd8lslxtUW",
        "outputId": "2a7a5171-8688-4b1f-fe52-f88f4b532a6a"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.7173749208450317\n",
            "Train step - Step 10, Loss 0.31387636065483093\n",
            "Train step - Step 20, Loss 0.2874338626861572\n",
            "Train step - Step 30, Loss 0.2654590606689453\n",
            "Train epoch - Accuracy: 0.318989898989899 Loss: 0.34560690457772725 Corrects: 1579\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.26789265871047974\n",
            "Train step - Step 50, Loss 0.258944034576416\n",
            "Train step - Step 60, Loss 0.2350231260061264\n",
            "Train step - Step 70, Loss 0.23865528404712677\n",
            "Train epoch - Accuracy: 0.44363636363636366 Loss: 0.24301170718790305 Corrects: 2196\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.23784291744232178\n",
            "Train step - Step 90, Loss 0.22167158126831055\n",
            "Train step - Step 100, Loss 0.22836807370185852\n",
            "Train step - Step 110, Loss 0.21694694459438324\n",
            "Train epoch - Accuracy: 0.48707070707070704 Loss: 0.22610946089330344 Corrects: 2411\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.21196965873241425\n",
            "Train step - Step 130, Loss 0.20575585961341858\n",
            "Train step - Step 140, Loss 0.22211872041225433\n",
            "Train step - Step 150, Loss 0.20619133114814758\n",
            "Train epoch - Accuracy: 0.5016161616161616 Loss: 0.21912972303954037 Corrects: 2483\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.2055768072605133\n",
            "Train step - Step 170, Loss 0.20553207397460938\n",
            "Train step - Step 180, Loss 0.22383907437324524\n",
            "Train step - Step 190, Loss 0.19269728660583496\n",
            "Train epoch - Accuracy: 0.5418181818181819 Loss: 0.20608459960932685 Corrects: 2682\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.19962547719478607\n",
            "Train step - Step 210, Loss 0.1956397444009781\n",
            "Train step - Step 220, Loss 0.23317204415798187\n",
            "Train step - Step 230, Loss 0.1870502084493637\n",
            "Train epoch - Accuracy: 0.5523232323232323 Loss: 0.19966931800047558 Corrects: 2734\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.19728612899780273\n",
            "Train step - Step 250, Loss 0.17967957258224487\n",
            "Train step - Step 260, Loss 0.1929226964712143\n",
            "Train step - Step 270, Loss 0.19188766181468964\n",
            "Train epoch - Accuracy: 0.5834343434343434 Loss: 0.19277908541939476 Corrects: 2888\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1998911201953888\n",
            "Train step - Step 290, Loss 0.18329913914203644\n",
            "Train step - Step 300, Loss 0.16586630046367645\n",
            "Train step - Step 310, Loss 0.17675089836120605\n",
            "Train epoch - Accuracy: 0.598989898989899 Loss: 0.1864176897809963 Corrects: 2965\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.17738604545593262\n",
            "Train step - Step 330, Loss 0.18055176734924316\n",
            "Train step - Step 340, Loss 0.17641305923461914\n",
            "Train step - Step 350, Loss 0.14969894289970398\n",
            "Train epoch - Accuracy: 0.6155555555555555 Loss: 0.17903075204955207 Corrects: 3047\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.1671830266714096\n",
            "Train step - Step 370, Loss 0.1687958985567093\n",
            "Train step - Step 380, Loss 0.159011110663414\n",
            "Train epoch - Accuracy: 0.6309090909090909 Loss: 0.1738420210462628 Corrects: 3123\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.15419088304042816\n",
            "Train step - Step 400, Loss 0.14775967597961426\n",
            "Train step - Step 410, Loss 0.16979841887950897\n",
            "Train step - Step 420, Loss 0.1525963395833969\n",
            "Train epoch - Accuracy: 0.6539393939393939 Loss: 0.1639658623993999 Corrects: 3237\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.19196446239948273\n",
            "Train step - Step 440, Loss 0.1466265171766281\n",
            "Train step - Step 450, Loss 0.17056730389595032\n",
            "Train step - Step 460, Loss 0.18163080513477325\n",
            "Train epoch - Accuracy: 0.6608080808080808 Loss: 0.16202940798167026 Corrects: 3271\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.176211878657341\n",
            "Train step - Step 480, Loss 0.1588747799396515\n",
            "Train step - Step 490, Loss 0.1342204362154007\n",
            "Train step - Step 500, Loss 0.14058950543403625\n",
            "Train epoch - Accuracy: 0.6767676767676768 Loss: 0.15383696049752862 Corrects: 3350\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.1555892676115036\n",
            "Train step - Step 520, Loss 0.13571906089782715\n",
            "Train step - Step 530, Loss 0.1515876054763794\n",
            "Train step - Step 540, Loss 0.1359780877828598\n",
            "Train epoch - Accuracy: 0.702020202020202 Loss: 0.14594818502363532 Corrects: 3475\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.15486030280590057\n",
            "Train step - Step 560, Loss 0.13412602245807648\n",
            "Train step - Step 570, Loss 0.15869688987731934\n",
            "Train step - Step 580, Loss 0.13754160702228546\n",
            "Train epoch - Accuracy: 0.7034343434343434 Loss: 0.14330182038172326 Corrects: 3482\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.13797789812088013\n",
            "Train step - Step 600, Loss 0.14471013844013214\n",
            "Train step - Step 610, Loss 0.1659015417098999\n",
            "Train step - Step 620, Loss 0.13035106658935547\n",
            "Train epoch - Accuracy: 0.712929292929293 Loss: 0.1402043603164981 Corrects: 3529\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.10688119381666183\n",
            "Train step - Step 640, Loss 0.10551120340824127\n",
            "Train step - Step 650, Loss 0.11231940239667892\n",
            "Train step - Step 660, Loss 0.12159333378076553\n",
            "Train epoch - Accuracy: 0.7434343434343434 Loss: 0.1296956703397963 Corrects: 3680\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.13172531127929688\n",
            "Train step - Step 680, Loss 0.142404243350029\n",
            "Train step - Step 690, Loss 0.12234494835138321\n",
            "Train step - Step 700, Loss 0.14414963126182556\n",
            "Train epoch - Accuracy: 0.756969696969697 Loss: 0.12469814313180519 Corrects: 3747\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.1333446353673935\n",
            "Train step - Step 720, Loss 0.15927483141422272\n",
            "Train step - Step 730, Loss 0.11001887172460556\n",
            "Train step - Step 740, Loss 0.12100914865732193\n",
            "Train epoch - Accuracy: 0.7450505050505051 Loss: 0.12714890332836093 Corrects: 3688\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.11506552994251251\n",
            "Train step - Step 760, Loss 0.10524898767471313\n",
            "Train step - Step 770, Loss 0.0984029471874237\n",
            "Train epoch - Accuracy: 0.7668686868686869 Loss: 0.11733168877435453 Corrects: 3796\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.12441835552453995\n",
            "Train step - Step 790, Loss 0.11353017389774323\n",
            "Train step - Step 800, Loss 0.13449843227863312\n",
            "Train step - Step 810, Loss 0.12670955061912537\n",
            "Train epoch - Accuracy: 0.7739393939393939 Loss: 0.11266988289175611 Corrects: 3831\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.10420570522546768\n",
            "Train step - Step 830, Loss 0.10050588846206665\n",
            "Train step - Step 840, Loss 0.09052333980798721\n",
            "Train step - Step 850, Loss 0.07840898633003235\n",
            "Train epoch - Accuracy: 0.7808080808080808 Loss: 0.11037116950509523 Corrects: 3865\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.13141851127147675\n",
            "Train step - Step 870, Loss 0.11617157608270645\n",
            "Train step - Step 880, Loss 0.11001942306756973\n",
            "Train step - Step 890, Loss 0.08741968870162964\n",
            "Train epoch - Accuracy: 0.7844444444444445 Loss: 0.10794938291564132 Corrects: 3883\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.11431687325239182\n",
            "Train step - Step 910, Loss 0.0919078066945076\n",
            "Train step - Step 920, Loss 0.1131000742316246\n",
            "Train step - Step 930, Loss 0.12487182766199112\n",
            "Train epoch - Accuracy: 0.7935353535353535 Loss: 0.10410453104310566 Corrects: 3928\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.0851389467716217\n",
            "Train step - Step 950, Loss 0.10327544063329697\n",
            "Train step - Step 960, Loss 0.09313042461872101\n",
            "Train step - Step 970, Loss 0.12224413454532623\n",
            "Train epoch - Accuracy: 0.8014141414141415 Loss: 0.10208582940125706 Corrects: 3967\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.10216508060693741\n",
            "Train step - Step 990, Loss 0.10625173896551132\n",
            "Train step - Step 1000, Loss 0.12439626455307007\n",
            "Train step - Step 1010, Loss 0.09492490440607071\n",
            "Train epoch - Accuracy: 0.805050505050505 Loss: 0.10121105768764863 Corrects: 3985\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.1075296625494957\n",
            "Train step - Step 1030, Loss 0.08558668196201324\n",
            "Train step - Step 1040, Loss 0.10448060184717178\n",
            "Train step - Step 1050, Loss 0.0987623855471611\n",
            "Train epoch - Accuracy: 0.8115151515151515 Loss: 0.09598487866647316 Corrects: 4017\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.11706822365522385\n",
            "Train step - Step 1070, Loss 0.08957835286855698\n",
            "Train step - Step 1080, Loss 0.1004997119307518\n",
            "Train step - Step 1090, Loss 0.10269360989332199\n",
            "Train epoch - Accuracy: 0.8151515151515152 Loss: 0.093790872918837 Corrects: 4035\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10780717432498932\n",
            "Train step - Step 1110, Loss 0.0829559937119484\n",
            "Train step - Step 1120, Loss 0.06946247071027756\n",
            "Train step - Step 1130, Loss 0.08679612725973129\n",
            "Train epoch - Accuracy: 0.8232323232323232 Loss: 0.09224864899811118 Corrects: 4075\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.0877915769815445\n",
            "Train step - Step 1150, Loss 0.07081689685583115\n",
            "Train step - Step 1160, Loss 0.09684779495000839\n",
            "Train epoch - Accuracy: 0.8305050505050505 Loss: 0.08737428996298048 Corrects: 4111\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.08674424141645432\n",
            "Train step - Step 1180, Loss 0.08422601222991943\n",
            "Train step - Step 1190, Loss 0.09191498905420303\n",
            "Train step - Step 1200, Loss 0.09191594272851944\n",
            "Train epoch - Accuracy: 0.834949494949495 Loss: 0.08547621402174535 Corrects: 4133\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.07304417341947556\n",
            "Train step - Step 1220, Loss 0.09637052565813065\n",
            "Train step - Step 1230, Loss 0.08423800766468048\n",
            "Train step - Step 1240, Loss 0.09754743427038193\n",
            "Train epoch - Accuracy: 0.8418181818181818 Loss: 0.0811596822106477 Corrects: 4167\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.08752617239952087\n",
            "Train step - Step 1260, Loss 0.08612791448831558\n",
            "Train step - Step 1270, Loss 0.09345199167728424\n",
            "Train step - Step 1280, Loss 0.06988094002008438\n",
            "Train epoch - Accuracy: 0.8432323232323232 Loss: 0.08195995536717501 Corrects: 4174\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.063926100730896\n",
            "Train step - Step 1300, Loss 0.09519979357719421\n",
            "Train step - Step 1310, Loss 0.0794561430811882\n",
            "Train step - Step 1320, Loss 0.09330239146947861\n",
            "Train epoch - Accuracy: 0.8434343434343434 Loss: 0.08210123597973525 Corrects: 4175\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.06706669181585312\n",
            "Train step - Step 1340, Loss 0.07936250418424606\n",
            "Train step - Step 1350, Loss 0.08014490455389023\n",
            "Train step - Step 1360, Loss 0.08957503736019135\n",
            "Train epoch - Accuracy: 0.8488888888888889 Loss: 0.07934777613541093 Corrects: 4202\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.08020991086959839\n",
            "Train step - Step 1380, Loss 0.09724964946508408\n",
            "Train step - Step 1390, Loss 0.06381719559431076\n",
            "Train step - Step 1400, Loss 0.06528539955615997\n",
            "Train epoch - Accuracy: 0.8549494949494949 Loss: 0.07585879642855037 Corrects: 4232\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.07746966183185577\n",
            "Train step - Step 1420, Loss 0.08617367595434189\n",
            "Train step - Step 1430, Loss 0.06970410794019699\n",
            "Train step - Step 1440, Loss 0.08160582929849625\n",
            "Train epoch - Accuracy: 0.8527272727272728 Loss: 0.07876499318715298 Corrects: 4221\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.06916437298059464\n",
            "Train step - Step 1460, Loss 0.06720501184463501\n",
            "Train step - Step 1470, Loss 0.06595558673143387\n",
            "Train step - Step 1480, Loss 0.0707656592130661\n",
            "Train epoch - Accuracy: 0.8654545454545455 Loss: 0.07132082007449082 Corrects: 4284\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.05659304931759834\n",
            "Train step - Step 1500, Loss 0.06974741071462631\n",
            "Train step - Step 1510, Loss 0.0784800574183464\n",
            "Train step - Step 1520, Loss 0.06901217252016068\n",
            "Train epoch - Accuracy: 0.8672727272727273 Loss: 0.07315080337753199 Corrects: 4293\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.07148046046495438\n",
            "Train step - Step 1540, Loss 0.08015324175357819\n",
            "Train step - Step 1550, Loss 0.07519340515136719\n",
            "Train epoch - Accuracy: 0.8682828282828283 Loss: 0.06976768631826748 Corrects: 4298\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.061872996389865875\n",
            "Train step - Step 1570, Loss 0.08952672779560089\n",
            "Train step - Step 1580, Loss 0.0770217701792717\n",
            "Train step - Step 1590, Loss 0.06781128793954849\n",
            "Train epoch - Accuracy: 0.8719191919191919 Loss: 0.06902587638659911 Corrects: 4316\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.0734851062297821\n",
            "Train step - Step 1610, Loss 0.055662430822849274\n",
            "Train step - Step 1620, Loss 0.08112241327762604\n",
            "Train step - Step 1630, Loss 0.07834818959236145\n",
            "Train epoch - Accuracy: 0.8739393939393939 Loss: 0.06590250587222551 Corrects: 4326\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.05307411774992943\n",
            "Train step - Step 1650, Loss 0.04475534334778786\n",
            "Train step - Step 1660, Loss 0.06450628489255905\n",
            "Train step - Step 1670, Loss 0.06469791382551193\n",
            "Train epoch - Accuracy: 0.8838383838383839 Loss: 0.06405000432874217 Corrects: 4375\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.055307772010564804\n",
            "Train step - Step 1690, Loss 0.06018352508544922\n",
            "Train step - Step 1700, Loss 0.05738571286201477\n",
            "Train step - Step 1710, Loss 0.07564640045166016\n",
            "Train epoch - Accuracy: 0.883030303030303 Loss: 0.06291912011124871 Corrects: 4371\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.0579797700047493\n",
            "Train step - Step 1730, Loss 0.05599530413746834\n",
            "Train step - Step 1740, Loss 0.07791357487440109\n",
            "Train step - Step 1750, Loss 0.07570113986730576\n",
            "Train epoch - Accuracy: 0.8860606060606061 Loss: 0.06122048602411241 Corrects: 4386\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.046970587223768234\n",
            "Train step - Step 1770, Loss 0.05448814854025841\n",
            "Train step - Step 1780, Loss 0.0570836178958416\n",
            "Train step - Step 1790, Loss 0.052846431732177734\n",
            "Train epoch - Accuracy: 0.8921212121212121 Loss: 0.05948092536342264 Corrects: 4416\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.0464363619685173\n",
            "Train step - Step 1810, Loss 0.056273628026247025\n",
            "Train step - Step 1820, Loss 0.0658160001039505\n",
            "Train step - Step 1830, Loss 0.0690997764468193\n",
            "Train epoch - Accuracy: 0.8955555555555555 Loss: 0.05688923456151076 Corrects: 4433\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.059149112552404404\n",
            "Train step - Step 1850, Loss 0.05253974348306656\n",
            "Train step - Step 1860, Loss 0.07070215791463852\n",
            "Train step - Step 1870, Loss 0.05269395932555199\n",
            "Train epoch - Accuracy: 0.8927272727272727 Loss: 0.05812157820571553 Corrects: 4419\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.04611649736762047\n",
            "Train step - Step 1890, Loss 0.06521129608154297\n",
            "Train step - Step 1900, Loss 0.0597667470574379\n",
            "Train step - Step 1910, Loss 0.07057886570692062\n",
            "Train epoch - Accuracy: 0.9002020202020202 Loss: 0.05608734115506663 Corrects: 4456\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.042010191828012466\n",
            "Train step - Step 1930, Loss 0.043184537440538406\n",
            "Train step - Step 1940, Loss 0.0397011898458004\n",
            "Train epoch - Accuracy: 0.9242424242424242 Loss: 0.04325935698518849 Corrects: 4575\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.039822064340114594\n",
            "Train step - Step 1960, Loss 0.036546554416418076\n",
            "Train step - Step 1970, Loss 0.04635452851653099\n",
            "Train step - Step 1980, Loss 0.033635109663009644\n",
            "Train epoch - Accuracy: 0.9432323232323232 Loss: 0.037418713830035144 Corrects: 4669\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.03440694510936737\n",
            "Train step - Step 2000, Loss 0.04546314477920532\n",
            "Train step - Step 2010, Loss 0.03715860843658447\n",
            "Train step - Step 2020, Loss 0.050867773592472076\n",
            "Train epoch - Accuracy: 0.9412121212121212 Loss: 0.03600090804876703 Corrects: 4659\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.02928098477423191\n",
            "Train step - Step 2040, Loss 0.03901729732751846\n",
            "Train step - Step 2050, Loss 0.037106212228536606\n",
            "Train step - Step 2060, Loss 0.03669671341776848\n",
            "Train epoch - Accuracy: 0.9452525252525252 Loss: 0.03400145272564406 Corrects: 4679\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.0314776748418808\n",
            "Train step - Step 2080, Loss 0.031674575060606\n",
            "Train step - Step 2090, Loss 0.018039535731077194\n",
            "Train step - Step 2100, Loss 0.0348820686340332\n",
            "Train epoch - Accuracy: 0.9468686868686869 Loss: 0.033380321872354755 Corrects: 4687\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.02144073322415352\n",
            "Train step - Step 2120, Loss 0.029142146930098534\n",
            "Train step - Step 2130, Loss 0.03788841888308525\n",
            "Train step - Step 2140, Loss 0.032097745686769485\n",
            "Train epoch - Accuracy: 0.9496969696969697 Loss: 0.03260654991022264 Corrects: 4701\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.027982238680124283\n",
            "Train step - Step 2160, Loss 0.020777106285095215\n",
            "Train step - Step 2170, Loss 0.030742371454834938\n",
            "Train step - Step 2180, Loss 0.0357779823243618\n",
            "Train epoch - Accuracy: 0.9543434343434344 Loss: 0.03025592699345916 Corrects: 4724\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.031895961612463\n",
            "Train step - Step 2200, Loss 0.025660132989287376\n",
            "Train step - Step 2210, Loss 0.038197994232177734\n",
            "Train step - Step 2220, Loss 0.025359375402331352\n",
            "Train epoch - Accuracy: 0.9525252525252526 Loss: 0.03145596713730783 Corrects: 4715\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.03523494675755501\n",
            "Train step - Step 2240, Loss 0.02871292270720005\n",
            "Train step - Step 2250, Loss 0.028518855571746826\n",
            "Train step - Step 2260, Loss 0.028394445776939392\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.031200148419599342 Corrects: 4703\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.026044586673378944\n",
            "Train step - Step 2280, Loss 0.03078872337937355\n",
            "Train step - Step 2290, Loss 0.03193800523877144\n",
            "Train step - Step 2300, Loss 0.05611391365528107\n",
            "Train epoch - Accuracy: 0.9555555555555556 Loss: 0.030103796654277377 Corrects: 4730\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.02573627233505249\n",
            "Train step - Step 2320, Loss 0.026890838518738747\n",
            "Train step - Step 2330, Loss 0.03133704885840416\n",
            "Train epoch - Accuracy: 0.953939393939394 Loss: 0.0305143504032884 Corrects: 4722\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.029058242216706276\n",
            "Train step - Step 2350, Loss 0.02426040545105934\n",
            "Train step - Step 2360, Loss 0.03160322457551956\n",
            "Train step - Step 2370, Loss 0.04511835798621178\n",
            "Train epoch - Accuracy: 0.9563636363636364 Loss: 0.029337084354324774 Corrects: 4734\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.031107379123568535\n",
            "Train step - Step 2390, Loss 0.029924381524324417\n",
            "Train step - Step 2400, Loss 0.024590641260147095\n",
            "Train step - Step 2410, Loss 0.020830800756812096\n",
            "Train epoch - Accuracy: 0.961010101010101 Loss: 0.02768125159346094 Corrects: 4757\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.02306819148361683\n",
            "Train step - Step 2430, Loss 0.05020679160952568\n",
            "Train step - Step 2440, Loss 0.032019324600696564\n",
            "Train step - Step 2450, Loss 0.035698115825653076\n",
            "Train epoch - Accuracy: 0.9587878787878787 Loss: 0.028257839812172784 Corrects: 4746\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.03360297158360481\n",
            "Train step - Step 2470, Loss 0.02808537892997265\n",
            "Train step - Step 2480, Loss 0.01882505789399147\n",
            "Train step - Step 2490, Loss 0.01958288811147213\n",
            "Train epoch - Accuracy: 0.96 Loss: 0.026811572033347504 Corrects: 4752\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.024240998551249504\n",
            "Train step - Step 2510, Loss 0.015550089068710804\n",
            "Train step - Step 2520, Loss 0.01995779201388359\n",
            "Train step - Step 2530, Loss 0.02513287402689457\n",
            "Train epoch - Accuracy: 0.962020202020202 Loss: 0.025361865840174934 Corrects: 4762\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.021166345104575157\n",
            "Train step - Step 2550, Loss 0.018641367554664612\n",
            "Train step - Step 2560, Loss 0.021410910412669182\n",
            "Train step - Step 2570, Loss 0.03057834319770336\n",
            "Train epoch - Accuracy: 0.9646464646464646 Loss: 0.023534335130167127 Corrects: 4775\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.029833734035491943\n",
            "Train step - Step 2590, Loss 0.018363453447818756\n",
            "Train step - Step 2600, Loss 0.026480138301849365\n",
            "Train step - Step 2610, Loss 0.028178563341498375\n",
            "Train epoch - Accuracy: 0.9660606060606061 Loss: 0.024363452725187695 Corrects: 4782\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.0342327281832695\n",
            "Train step - Step 2630, Loss 0.02055048942565918\n",
            "Train step - Step 2640, Loss 0.020426509901881218\n",
            "Train step - Step 2650, Loss 0.021581510081887245\n",
            "Train epoch - Accuracy: 0.9648484848484848 Loss: 0.025252315486320343 Corrects: 4776\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.03008689358830452\n",
            "Train step - Step 2670, Loss 0.0230241846293211\n",
            "Train step - Step 2680, Loss 0.019735518842935562\n",
            "Train step - Step 2690, Loss 0.01372586376965046\n",
            "Train epoch - Accuracy: 0.9644444444444444 Loss: 0.024579011685769966 Corrects: 4774\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.02005911059677601\n",
            "Train step - Step 2710, Loss 0.023675382137298584\n",
            "Train step - Step 2720, Loss 0.02118927799165249\n",
            "Train epoch - Accuracy: 0.9634343434343434 Loss: 0.024567253339772274 Corrects: 4769\n",
            "Training finished in 203.4930591583252 seconds\n",
            "EVALUATION:  0.78 0.12797264754772186\n",
            "TEST GROUP:  0.836\n",
            "TEST ALL:  0.836\n",
            "GROUP:  2\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.5198947787284851\n",
            "Train step - Step 10, Loss 0.18119554221630096\n",
            "Train step - Step 20, Loss 0.1347174048423767\n",
            "Train step - Step 30, Loss 0.10982789844274521\n",
            "Train epoch - Accuracy: 0.36808080808080806 Loss: 0.17154767145111102 Corrects: 1822\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.09604242444038391\n",
            "Train step - Step 50, Loss 0.09236812591552734\n",
            "Train step - Step 60, Loss 0.08287163078784943\n",
            "Train step - Step 70, Loss 0.08627394586801529\n",
            "Train epoch - Accuracy: 0.6482828282828282 Loss: 0.08684955637563359 Corrects: 3209\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.08100271224975586\n",
            "Train step - Step 90, Loss 0.07603686302900314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-754cea9f07ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequentialLearningFineTuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_subsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-31e6b3e7dcf7>\u001b[0m in \u001b[0;36msequentialLearningFineTuning\u001b[0;34m(train_subsets, val_subsets, test_subsets)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSchedulerOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset learning rate and step_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes_seen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# Validate on current group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-20cff53f0192>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Bring images and labels to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aakVopSdyeGr"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ0VbtGQygRH"
      },
      "source": [
        "method = \"finetuning\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ-J8aC_xx-c"
      },
      "source": [
        "## LWF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZBNivOix1FB"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdfIllqBx59v"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkzFiDVDyAm_"
      },
      "source": [
        "### sequential learning LWF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf7REFkIyEiQ"
      },
      "source": [
        "### LWF\n",
        "def sequentialLearningLWF(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZTwTS_4TcES"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkrMQy2TuUAb"
      },
      "source": [
        "# train\n",
        "# net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningLWF(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2qUxVyMTfB4"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_4xLfwcpDz"
      },
      "source": [
        "method = \"Learning Without Forgetting\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGHBWaLNXGeI"
      },
      "source": [
        "\"\"\"num_classes_seen = 100\n",
        "dif_accuracies=printAccuracyDifference(net,old_accuracies, num_classes_seen)\n",
        "dif_accuracies\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjGRIqDtNQP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nX10znUi6qd"
      },
      "source": [
        "## iCaRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKBdrJ8Csy2a"
      },
      "source": [
        "### classify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAPTOU9bszTY"
      },
      "source": [
        "def classify(batch_img, net, exemplar_sets):\n",
        "  \"\"\" classify images by nearest mean-of-exemplars \"\"\" \n",
        "  \"\"\" \n",
        "  class1: list_of_indices for images that represent class1\n",
        "  class2: list_of_indices for images that represent class2\n",
        "  ...\n",
        "  class100: list_of_indices for images that represent class 100\n",
        "  with Subset I retrieve the images from the train_dataset\n",
        "  iterate over it to calculate mean for each class\n",
        "  \"\"\"\n",
        "  net.eval()\n",
        "  classes_mean = []\n",
        "  for k, exemplars_indices in exemplar_sets.items():\n",
        "    features = []\n",
        "    class_images_set = Subset(train_dataset, exemplars_indices)\n",
        "    class_images = DataLoader(class_images_set, batch_size=BATCH_SIZE, num_workers=2)\n",
        "    for _, images, labels in class_images:\n",
        "      # for each class (paper from y=1...t) calculate features and then mean\n",
        "      feature = net.forward(images)\n",
        "      features.append(feature)\n",
        "    features_s = torch.cat(features)\n",
        "    class_mean = features_s.mean(0)\n",
        "    classes_mean.append(class_mean)\n",
        "    means_exemplars = torch.cat(classes_mean, dim=0)\n",
        "    means_exemplars = torch.stack([means_exemplars] * BATCH_SIZE)\n",
        "    means_exemplars = means_exemplars.transpose(1,2)\n",
        "  feature_images_to_classify = net.forward(batch_img)\n",
        "  # sono da normalizzare?\n",
        "  feature_images_to_classify = feature_images_to_classify.unsqueeze(2)\n",
        "  feature_images_to_classify = feature_images_to_classify.expand_as(means_exemplars) # expand_as to get the same dimension\n",
        "  preds = torch.argmin((feature_images_to_classify - means_exemplars).pow(2).sum(1), dim=1)\n",
        "  return preds"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5HRagcujHgM"
      },
      "source": [
        "### construct exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blwBqG6BjNsM"
      },
      "source": [
        "import sys\n",
        "\n",
        "def constructExemplarSet(net, Xclass, m):\n",
        "#Xclass contiene immagini e label della classe X\n",
        "  exemplars_set = []\n",
        "  feature_exemplars = []\n",
        "  indexes = []\n",
        "  features = [] \n",
        "  class_images = []\n",
        "  net.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    XtrainLoader = torch.utils.data.DataLoader(Xclass, shuffle = True, batch_size=1, num_workers=2)\n",
        "    for _, image, labels in XtrainLoader:\n",
        "      image = image.to(DEVICE)\n",
        "      class_images.append(image)\n",
        "      # per ogni immagine della classe x, prendiamo le rispettive feature e le uniamo nel vettore features, che contiene tutte quelle delle immagini della classe x\n",
        "      feature = net.forward(image)\n",
        "      #feature = net.predict(featuretmp)\n",
        "      feature = feature/np.linalg.norm(feature.cpu())\n",
        "      features.append(feature)\n",
        "\n",
        "    features = torch.cat(features, dim=0) #cat solve the problem of inequal size of tensors \n",
        "    current_class_mean = features.mean(0) # mu = media delle features delle immagini della classe\n",
        "\n",
        "    for k in range(1, m+1):\n",
        "      min = 100000\n",
        "      sum = 0\n",
        "      for j in range(k-1):\n",
        "        sum += feature_exemplars[j]\n",
        "      for x in range(len(Xclass)): \n",
        "        if (x not in indexes):\n",
        "          phiX = features[x]\n",
        "          val = current_class_mean - ((phiX + sum)/k)\n",
        "          val = np.linalg.norm(val.cpu().numpy()) ## NORMA \n",
        "          if (val < min):\n",
        "            min = val\n",
        "            feature_min = phiX\n",
        "            index_min = x\n",
        "      feature_exemplars.append(feature_min)\n",
        "      exemplars_set.append(Xclass[index_min][0])\n",
        "\n",
        "    print(\"lunghezza exemplar set: \", len(exemplars_set))\n",
        "    return exemplars_set"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy_HVL-tjZcb"
      },
      "source": [
        "### reduce exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7kuQ5DnjcCP"
      },
      "source": [
        "# DA CHIAMARE PER OGNI CLASSE k: riduce il numero di exemplars all'interno dell'exemplar_set della classe k \n",
        "# EXEMPLAR SET è una lista di indici che rappresentano la posizione dell'immagine selezionata per l'exemplar della classe corrente nel dataset di partenza\n",
        "def reduceExemplarSet(m, exemplars_set):\n",
        "  exemplars_new = []\n",
        "  for i in range(m):\n",
        "    if (exemplars_set != []):\n",
        "      exemplars_new.append(exemplars_set[i])\n",
        "\n",
        "  return exemplars_new"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMMlEpLsjfaj"
      },
      "source": [
        "### update representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRdA9RtDjhak"
      },
      "source": [
        "def updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot, old_net):\n",
        "  #exemplars_set_tot contiene tutti gli exemplars set ottenuti fino ad ora\n",
        "  #train_iter contiene tutti i dati (immagini + labels) delle classi nuove (s, .., t)\n",
        "  exemplars_subset = []\n",
        "  exemplars_indices = []\n",
        "  total_exemplars = []\n",
        "  labels_tot = []\n",
        "\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "    if (exemplar_set_class_k != []):\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "\n",
        "  if group_id > 1:\n",
        "    train_subset_total = torch.utils.data.ConcatDataset([train_subset, total_exemplars])\n",
        "  else:\n",
        "    train_subset_total = train_subset\n",
        "    \n",
        "  print(\"Len TOTAL train susbset: \", len(train_subset_total))\n",
        "  train_loader = torch.utils.data.DataLoader(train_subset_total, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  # train_loader è la concatenazione delle nuove classi con gli exemplar_sets calcolati fino a questo punto \n",
        "  print(\"training\")\n",
        "  #train(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)\n",
        "  trainWithOtherLosses(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)\n",
        "  #trainCEandL1(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac65-6QGjm9O"
      },
      "source": [
        "### incremental train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPU_32ArjpPH"
      },
      "source": [
        "import copy\n",
        "def incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_until_now):\n",
        "  print(\"Starting the update representation\")\n",
        "  exemplar_indices = None\n",
        "  num_classes = 10\n",
        "  new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "  print(\"NEW CLASSES: \", new_classes_examined)\n",
        "\n",
        "  updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net)\n",
        "                \n",
        "  iteration = group_id - 1\n",
        "  t = (num_classes * iteration) + num_classes # num_classes ricevute fino a questo momento \n",
        "  m = int(K/t) #casto ad intero ? per difetto o eccesso?\n",
        "  #s = num_classes * iteration\n",
        "\n",
        "  # REDUCING EXEMPLAR SET FOR EXISTING CLASSES\n",
        "  print(\"reducing exemplars for each class\")\n",
        "  print(total_classes_until_now)\n",
        "  for y in total_classes_until_now: #ci serve un set con tutte le classi fino ad ora viste\n",
        "    exemplar_y_new = reduceExemplarSet(m, exemplars_set_tot[y]) # valore associato alla chiave y che rappresenta il label della classe \n",
        "    print(\"REDUCED EXEMPLAR: \", len(exemplar_y_new))\n",
        "    exemplars_set_tot[y] = exemplar_y_new\n",
        "\n",
        "  \n",
        "  # CONSTRUCTION EXEMPLAR SET FOR NEW CLASSES\n",
        "  for y in new_classes_examined: # nuovi classi in arrivo di cui vogliamo costruire il set rappresentativo\n",
        "    images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data']\n",
        "    imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "    class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "    print(\"class train: \", class_train_subset)\n",
        "    print(\"Constructing exemplars of class\", y)\n",
        "    exemplars_set = constructExemplarSet(net, class_train_subset, m) # exemplar set è un set di indici\n",
        "    #devo recuperare dal dataset iniziale l'indice delle immagini dell'exemplar set creato \n",
        "    #for image in exemplars_set:\n",
        "     # exemplars_set = train_dataset.df.index[train_dataset.df['data'] == image].tolist()\n",
        "    exemplars_set_tot[y] = exemplars_set\n",
        "    print(\"exemplar set: \", exemplars_set)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDxmsQlWjp5D"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dQznGoEju9J"
      },
      "source": [
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes_till_now, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    print(\"num classes till now: \", num_classes_till_now)\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes_till_now, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            #if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcZUqVCjeMG4"
      },
      "source": [
        "### Train con CE + L1Loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl4zFMYGtAEJ"
      },
      "source": [
        "import copy\n",
        "def trainCEandL1(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    distillation_loss = 0\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "            # Classification LOSS\n",
        "            lr = 0.01\n",
        "            classification_loss = nn.CrossEntropyLoss()(outputs, labels) # BCE\n",
        "\n",
        "            # Distillation LOSS \n",
        "            if (group_id > 1):\n",
        "               old_features = old_net.forward(images)\n",
        "               old_outputs = old_net.predict(old_features)\n",
        "               #labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "               new_labels = torch.sigmoid(old_outputs)\n",
        "               new_outputs = outputs[:, 0:num_classes_till_previous_step]\n",
        "               lr = 1e-3\n",
        "               distillation_loss = l1Loss(new_outputs, new_labels) # L2\n",
        "               print(distillation_loss)\n",
        "\n",
        "            loss = classification_loss + distillation_loss\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrbPsdT2eRho"
      },
      "source": [
        "### Train con .. Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzTrBE-Dea1S"
      },
      "source": [
        "import copy\n",
        "def trainWithOtherLosses(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    distillation_loss = 0\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "            # Classification LOSS\n",
        "            classification_loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "            # Distillation LOSS \n",
        "            if (group_id > 1):\n",
        "               old_features = old_net.forward(images)\n",
        "               old_outputs = old_net.predict(old_features)\n",
        "               T = 2\n",
        "               beta = 0.25\n",
        "               distillation_loss = nn.KLDivLoss()(F.log_softmax(outputs[:, 0:num_classes_till_previous_step]/T, dim = 1), F.softmax(old_outputs.detach()/T, dim = 1)) * T * T * beta * num_classes_till_previous_step\n",
        "\n",
        "            loss = classification_loss + distillation_loss\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHpi410ljzWD"
      },
      "source": [
        "### test and validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQYB0-cLj0sP"
      },
      "source": [
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        #_, preds = classify(images, )\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTv6zBetj3Cb"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_cnELYdkap4"
      },
      "source": [
        "def sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 10\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      print(\"TEST SET LENGHT: \", len(test_set))\n",
        "      print(\"TEST CURRENT GROUP SET LENGHT: \", len(test_subset))\n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      test_classes = list(test_dataset.df.loc[test_set.indices, 'labels'].value_counts().index)\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TEST_SET CLASSES: \", test_classes)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTV1Gxq6yF5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa7c9cb-11be-43cc-acb5-4cfc9565dbe3"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  1000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [67, 65, 59, 56, 49, 39, 22, 20, 18, 4]\n",
            "TRAIN_SET CLASSES:  [67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "VALIDATION CLASSES:  [59, 56, 49, 39, 22, 20, 18, 4, 67, 65]\n",
            "GROUP:  1\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "Len TOTAL train susbset:  4950\n",
            "training\n",
            "Starting epoch 1/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 2.421779155731201\n",
            "Train step - Step 10, Loss 2.0073137283325195\n",
            "Train step - Step 20, Loss 1.8010530471801758\n",
            "Train step - Step 30, Loss 1.5596801042556763\n",
            "Train epoch - Accuracy: 0.3375757575757576 Loss: 1.8731828733887335 Corrects: 1671\n",
            "Starting epoch 2/30, LR = [0.01]\n",
            "Train step - Step 40, Loss 1.7201118469238281\n",
            "Train step - Step 50, Loss 1.4024146795272827\n",
            "Train step - Step 60, Loss 1.478386640548706\n",
            "Train step - Step 70, Loss 1.4212284088134766\n",
            "Train epoch - Accuracy: 0.44303030303030305 Loss: 1.5603929465708106 Corrects: 2193\n",
            "Starting epoch 3/30, LR = [0.01]\n",
            "Train step - Step 80, Loss 1.387869119644165\n",
            "Train step - Step 90, Loss 1.292657494544983\n",
            "Train step - Step 100, Loss 1.5050193071365356\n",
            "Train step - Step 110, Loss 1.267802357673645\n",
            "Train epoch - Accuracy: 0.48383838383838385 Loss: 1.457144638793637 Corrects: 2395\n",
            "Starting epoch 4/30, LR = [0.01]\n",
            "Train step - Step 120, Loss 1.343897819519043\n",
            "Train step - Step 130, Loss 1.4587172269821167\n",
            "Train step - Step 140, Loss 1.2369213104248047\n",
            "Train step - Step 150, Loss 1.379278540611267\n",
            "Train epoch - Accuracy: 0.5254545454545455 Loss: 1.3677388511041197 Corrects: 2601\n",
            "Starting epoch 5/30, LR = [0.01]\n",
            "Train step - Step 160, Loss 1.3784325122833252\n",
            "Train step - Step 170, Loss 1.2489529848098755\n",
            "Train step - Step 180, Loss 1.1525694131851196\n",
            "Train step - Step 190, Loss 1.4352823495864868\n",
            "Train epoch - Accuracy: 0.546060606060606 Loss: 1.3057746634820495 Corrects: 2703\n",
            "Starting epoch 6/30, LR = [0.01]\n",
            "Train step - Step 200, Loss 1.2334473133087158\n",
            "Train step - Step 210, Loss 1.2418887615203857\n",
            "Train step - Step 220, Loss 1.1612776517868042\n",
            "Train step - Step 230, Loss 1.1371853351593018\n",
            "Train epoch - Accuracy: 0.5678787878787879 Loss: 1.2422727087772254 Corrects: 2811\n",
            "Starting epoch 7/30, LR = [0.01]\n",
            "Train step - Step 240, Loss 1.24611496925354\n",
            "Train step - Step 250, Loss 1.1992099285125732\n",
            "Train step - Step 260, Loss 1.275200605392456\n",
            "Train step - Step 270, Loss 1.1506787538528442\n",
            "Train epoch - Accuracy: 0.5923232323232324 Loss: 1.1842676249417392 Corrects: 2932\n",
            "Starting epoch 8/30, LR = [0.01]\n",
            "Train step - Step 280, Loss 1.2025281190872192\n",
            "Train step - Step 290, Loss 1.2206504344940186\n",
            "Train step - Step 300, Loss 1.0549300909042358\n",
            "Train step - Step 310, Loss 1.091296911239624\n",
            "Train epoch - Accuracy: 0.6018181818181818 Loss: 1.1541152454145027 Corrects: 2979\n",
            "Starting epoch 9/30, LR = [0.01]\n",
            "Train step - Step 320, Loss 0.9298255443572998\n",
            "Train step - Step 330, Loss 1.117367148399353\n",
            "Train step - Step 340, Loss 0.9218665361404419\n",
            "Train step - Step 350, Loss 1.075899600982666\n",
            "Train epoch - Accuracy: 0.62 Loss: 1.1006890389413544 Corrects: 3069\n",
            "Starting epoch 10/30, LR = [0.01]\n",
            "Train step - Step 360, Loss 0.9585439562797546\n",
            "Train step - Step 370, Loss 1.0422096252441406\n",
            "Train step - Step 380, Loss 0.9735966324806213\n",
            "Train epoch - Accuracy: 0.6519191919191919 Loss: 1.0209424392141477 Corrects: 3227\n",
            "Starting epoch 11/30, LR = [0.01]\n",
            "Train step - Step 390, Loss 0.8980872631072998\n",
            "Train step - Step 400, Loss 1.08375883102417\n",
            "Train step - Step 410, Loss 0.8261536359786987\n",
            "Train step - Step 420, Loss 0.9316489100456238\n",
            "Train epoch - Accuracy: 0.6525252525252525 Loss: 1.0183970392111577 Corrects: 3230\n",
            "Starting epoch 12/30, LR = [0.01]\n",
            "Train step - Step 430, Loss 0.9123464226722717\n",
            "Train step - Step 440, Loss 0.8976420760154724\n",
            "Train step - Step 450, Loss 0.8936296701431274\n",
            "Train step - Step 460, Loss 1.0281847715377808\n",
            "Train epoch - Accuracy: 0.6616161616161617 Loss: 0.9831558119648635 Corrects: 3275\n",
            "Starting epoch 13/30, LR = [0.01]\n",
            "Train step - Step 470, Loss 1.0953302383422852\n",
            "Train step - Step 480, Loss 1.062380075454712\n",
            "Train step - Step 490, Loss 0.9258745908737183\n",
            "Train step - Step 500, Loss 1.0036699771881104\n",
            "Train epoch - Accuracy: 0.6866666666666666 Loss: 0.9226034959638961 Corrects: 3399\n",
            "Starting epoch 14/30, LR = [0.01]\n",
            "Train step - Step 510, Loss 0.8224800229072571\n",
            "Train step - Step 520, Loss 1.0301817655563354\n",
            "Train step - Step 530, Loss 0.8515259623527527\n",
            "Train step - Step 540, Loss 1.058668851852417\n",
            "Train epoch - Accuracy: 0.6941414141414142 Loss: 0.8917049502844763 Corrects: 3436\n",
            "Starting epoch 15/30, LR = [0.01]\n",
            "Train step - Step 550, Loss 0.8469603061676025\n",
            "Train step - Step 560, Loss 0.8258761763572693\n",
            "Train step - Step 570, Loss 0.9403610229492188\n",
            "Train step - Step 580, Loss 0.8591117858886719\n",
            "Train epoch - Accuracy: 0.7101010101010101 Loss: 0.8566559393718989 Corrects: 3515\n",
            "Starting epoch 16/30, LR = [0.01]\n",
            "Train step - Step 590, Loss 0.9189956188201904\n",
            "Train step - Step 600, Loss 0.9138644337654114\n",
            "Train step - Step 610, Loss 0.876905620098114\n",
            "Train step - Step 620, Loss 0.7282149195671082\n",
            "Train epoch - Accuracy: 0.7145454545454546 Loss: 0.8301788701433124 Corrects: 3537\n",
            "Starting epoch 17/30, LR = [0.01]\n",
            "Train step - Step 630, Loss 0.8300343155860901\n",
            "Train step - Step 640, Loss 0.82343989610672\n",
            "Train step - Step 650, Loss 0.8031043410301208\n",
            "Train step - Step 660, Loss 0.8032488226890564\n",
            "Train epoch - Accuracy: 0.7242424242424242 Loss: 0.8148406688131468 Corrects: 3585\n",
            "Starting epoch 18/30, LR = [0.01]\n",
            "Train step - Step 670, Loss 0.6783286333084106\n",
            "Train step - Step 680, Loss 0.6828328967094421\n",
            "Train step - Step 690, Loss 0.991070568561554\n",
            "Train step - Step 700, Loss 0.7914112210273743\n",
            "Train epoch - Accuracy: 0.7323232323232324 Loss: 0.794438924765346 Corrects: 3625\n",
            "Starting epoch 19/30, LR = [0.01]\n",
            "Train step - Step 710, Loss 0.760129451751709\n",
            "Train step - Step 720, Loss 0.9081723093986511\n",
            "Train step - Step 730, Loss 0.610443651676178\n",
            "Train step - Step 740, Loss 0.6365012526512146\n",
            "Train epoch - Accuracy: 0.7353535353535353 Loss: 0.7705318463691557 Corrects: 3640\n",
            "Starting epoch 20/30, LR = [0.01]\n",
            "Train step - Step 750, Loss 0.8309736251831055\n",
            "Train step - Step 760, Loss 0.9549220204353333\n",
            "Train step - Step 770, Loss 0.6427121758460999\n",
            "Train epoch - Accuracy: 0.7470707070707071 Loss: 0.74523551815688 Corrects: 3698\n",
            "Starting epoch 21/30, LR = [0.01]\n",
            "Train step - Step 780, Loss 0.8249792456626892\n",
            "Train step - Step 790, Loss 0.8861881494522095\n",
            "Train step - Step 800, Loss 0.7755769491195679\n",
            "Train step - Step 810, Loss 0.6535322070121765\n",
            "Train epoch - Accuracy: 0.7628282828282829 Loss: 0.7058388560950154 Corrects: 3776\n",
            "Starting epoch 22/30, LR = [0.01]\n",
            "Train step - Step 820, Loss 0.5536507368087769\n",
            "Train step - Step 830, Loss 0.65401691198349\n",
            "Train step - Step 840, Loss 0.797188937664032\n",
            "Train step - Step 850, Loss 0.7322360277175903\n",
            "Train epoch - Accuracy: 0.758989898989899 Loss: 0.6956659701135424 Corrects: 3757\n",
            "Starting epoch 23/30, LR = [0.01]\n",
            "Train step - Step 860, Loss 0.721487283706665\n",
            "Train step - Step 870, Loss 0.7344793677330017\n",
            "Train step - Step 880, Loss 0.752159595489502\n",
            "Train step - Step 890, Loss 0.643766462802887\n",
            "Train epoch - Accuracy: 0.7682828282828282 Loss: 0.6672832707443622 Corrects: 3803\n",
            "Starting epoch 24/30, LR = [0.01]\n",
            "Train step - Step 900, Loss 0.6344066262245178\n",
            "Train step - Step 910, Loss 0.5998998880386353\n",
            "Train step - Step 920, Loss 0.7466157674789429\n",
            "Train step - Step 930, Loss 0.7458378076553345\n",
            "Train epoch - Accuracy: 0.7852525252525252 Loss: 0.6384175394159375 Corrects: 3887\n",
            "Starting epoch 25/30, LR = [0.01]\n",
            "Train step - Step 940, Loss 0.5798671841621399\n",
            "Train step - Step 950, Loss 0.7376460433006287\n",
            "Train step - Step 960, Loss 0.6683644652366638\n",
            "Train step - Step 970, Loss 0.7269111275672913\n",
            "Train epoch - Accuracy: 0.7733333333333333 Loss: 0.6479507580429616 Corrects: 3828\n",
            "Starting epoch 26/30, LR = [0.01]\n",
            "Train step - Step 980, Loss 0.5377217531204224\n",
            "Train step - Step 990, Loss 0.7740260362625122\n",
            "Train step - Step 1000, Loss 0.6575940847396851\n",
            "Train step - Step 1010, Loss 0.7771332263946533\n",
            "Train epoch - Accuracy: 0.7915151515151515 Loss: 0.6068514717949761 Corrects: 3918\n",
            "Starting epoch 27/30, LR = [0.01]\n",
            "Train step - Step 1020, Loss 0.557150661945343\n",
            "Train step - Step 1030, Loss 0.6893876194953918\n",
            "Train step - Step 1040, Loss 0.5066934823989868\n",
            "Train step - Step 1050, Loss 0.7219095230102539\n",
            "Train epoch - Accuracy: 0.8008080808080809 Loss: 0.5739770380415098 Corrects: 3964\n",
            "Starting epoch 28/30, LR = [0.01]\n",
            "Train step - Step 1060, Loss 0.4940601587295532\n",
            "Train step - Step 1070, Loss 0.46873581409454346\n",
            "Train step - Step 1080, Loss 0.582275390625\n",
            "Train step - Step 1090, Loss 0.5407489538192749\n",
            "Train epoch - Accuracy: 0.8038383838383838 Loss: 0.5712900824016995 Corrects: 3979\n",
            "Starting epoch 29/30, LR = [0.01]\n",
            "Train step - Step 1100, Loss 0.4992428719997406\n",
            "Train step - Step 1110, Loss 0.606153666973114\n",
            "Train step - Step 1120, Loss 0.4546428322792053\n",
            "Train step - Step 1130, Loss 0.5308824181556702\n",
            "Train epoch - Accuracy: 0.8143434343434344 Loss: 0.5448748263686595 Corrects: 4031\n",
            "Starting epoch 30/30, LR = [0.01]\n",
            "Train step - Step 1140, Loss 0.578815221786499\n",
            "Train step - Step 1150, Loss 0.34678542613983154\n",
            "Train step - Step 1160, Loss 0.6353767514228821\n",
            "Train epoch - Accuracy: 0.812929292929293 Loss: 0.5449574199830642 Corrects: 4024\n",
            "Training finished in 161.1762945652008 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6aaef50>\n",
            "Constructing exemplars of class 67\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [2974, 5730, 33228, 8475, 18830, 22434, 21363, 34036, 35158, 22925, 21286, 8484, 29655, 20414, 30929, 40180, 38671, 36511, 27380, 40605, 2062, 34679, 23259, 23728, 13995, 8206, 39089, 14999, 2816, 20057, 49407, 26266, 15332, 27612, 28657, 42945, 38373, 6152, 17163, 22218, 5825, 40875, 18830, 25011, 33991, 4677, 24855, 508, 32224, 284, 37255, 46619, 8484, 30929, 4411, 46268, 32400, 25278, 11571, 8206, 44094, 38373, 39311, 47386, 49458, 22434, 11050, 30115, 28494, 16863, 21031, 38330, 191, 39931, 3818, 36003, 26453, 24855, 5415, 30184, 14168, 18901, 9853, 23457, 3847, 36810, 4516, 2771, 28625, 6373, 32943, 22335, 27642, 11730, 39311, 49439, 14999, 19194, 9092, 23457, 14249, 47104, 15633, 47059, 14914, 44469, 25278, 18314, 7279, 18015, 9034, 48076, 34351, 29048, 41927, 26351, 8475, 34332, 18577, 30805, 30517, 34679, 24720, 5543, 36810, 30551, 23728, 32020, 13575, 25966, 21663, 13266, 18488, 18324, 14168, 23070, 35298, 33036, 9653, 30551, 30184, 5415, 7164, 19244, 41927, 10194, 2155, 49795, 42128, 43250, 22892, 7430, 22335, 30115, 27360, 46608, 41279, 47059, 49244, 45516, 4677, 18577, 6373, 35093, 41583, 23608, 8812, 26856, 20899, 9853, 4516, 22218, 9702, 19673, 45668, 1973, 44686, 17476, 36003, 26453, 191, 30517, 22925, 2906, 31731, 6919, 42128, 43164, 6928, 39768, 22434, 28403, 7752, 9653, 37255, 18314, 191, 30929, 4411, 20614]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d63f6790>\n",
            "Constructing exemplars of class 59\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [4022, 27824, 30109, 21269, 30302, 8271, 42330, 45470, 7061, 4113, 46536, 45339, 12082, 8843, 4649, 15810, 28806, 17507, 36584, 13338, 16471, 22751, 35136, 8113, 20864, 8241, 40985, 21624, 27341, 731, 19261, 45838, 11254, 14413, 38279, 34129, 10020, 49156, 10664, 7831, 35676, 22470, 23210, 6100, 1048, 21649, 26200, 14358, 14332, 45470, 26884, 39993, 27601, 23545, 35106, 24021, 19092, 13004, 4708, 30470, 13541, 42192, 8624, 43511, 13178, 36387, 41109, 32077, 8843, 567, 10773, 47614, 42912, 7980, 16471, 13830, 1616, 14478, 3488, 45468, 30328, 30474, 41109, 34852, 43511, 5277, 21671, 4022, 43534, 31559, 7980, 34862, 23210, 517, 41861, 29352, 33173, 45953, 12054, 9026, 31974, 31559, 16244, 23289, 22788, 2530, 110, 20864, 17090, 15950, 16738, 31388, 24261, 13338, 14376, 43511, 36637, 42336, 13328, 19949, 17894, 17404, 2688, 13541, 33173, 19136, 21624, 18828, 7831, 11880, 8271, 4649, 39993, 17831, 19261, 45838, 29305, 110, 45267, 8918, 14376, 38279, 15079, 19677, 42060, 14332, 15810, 2530, 24627, 28325, 34504, 46599, 10103, 37533, 27956, 39993, 11187, 20812, 21382, 43182, 7980, 19384, 16608, 40744, 9727, 17567, 28325, 7065, 29320, 15798, 6045, 20207, 24, 13249, 35190, 13830, 48148, 27167, 45470, 4018, 8578, 36584, 28590, 1048, 19474, 22988, 29305, 31150, 30474, 849, 982, 27536, 45659, 26688, 6045, 8918, 28526, 45723, 4708, 31974]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6ec8210>\n",
            "Constructing exemplars of class 39\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [45744, 2300, 47583, 27640, 41852, 101, 5320, 31964, 5525, 38395, 6006, 4457, 2984, 21774, 5021, 49064, 5466, 40640, 10384, 36793, 42815, 31970, 18749, 13387, 20120, 30326, 5887, 30725, 27986, 46634, 32839, 14096, 12263, 22654, 30760, 20430, 9764, 47632, 14383, 33694, 6124, 37857, 16316, 16610, 33859, 9764, 31843, 29457, 4999, 14346, 18522, 43859, 10384, 20763, 49729, 30725, 38041, 17522, 35634, 32685, 42181, 3307, 17931, 20120, 36185, 30307, 7493, 2757, 11117, 46231, 10689, 34011, 32835, 5752, 13473, 2311, 28010, 41951, 48448, 8683, 20919, 3970, 49423, 49920, 16682, 25177, 35634, 14346, 12786, 40856, 30268, 46764, 16610, 5414, 14438, 38395, 32305, 8082, 15044, 11597, 4800, 13724, 42644, 48448, 41951, 27986, 30647, 41341, 42909, 5525, 33506, 5340, 38893, 32039, 25126, 6124, 23370, 36066, 5930, 19268, 18522, 45547, 10790, 14659, 28472, 5344, 42552, 46104, 30307, 41852, 32758, 9660, 24442, 41901, 45674, 30229, 13387, 18253, 44285, 30229, 34501, 33241, 4800, 34896, 32839, 20997, 17437, 49074, 28010, 39069, 35634, 21397, 2808, 22663, 47340, 11046, 4155, 20919, 31593, 21343, 34501, 41196, 30930, 38868, 13825, 32439, 32307, 36912, 12263, 41565, 11117, 36581, 25126, 15148, 32848, 1062, 5688, 6006, 33368, 29050, 223, 2823, 7584, 40353, 44790, 36066, 30930, 28524, 18875, 16211, 28733, 34070, 19268, 34896, 4800, 41951, 44596, 18206, 21397, 2808]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d8c63a50>\n",
            "Constructing exemplars of class 22\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [29641, 10992, 34825, 31223, 40966, 46817, 36310, 27093, 37807, 32199, 13084, 29969, 40092, 25178, 37554, 48818, 47317, 36489, 26835, 30254, 3530, 23414, 43824, 11443, 2752, 44298, 19131, 45986, 44520, 34873, 36122, 16552, 7678, 41570, 38901, 6544, 10656, 27820, 17542, 37382, 22156, 6544, 31976, 12648, 16472, 21749, 13781, 11972, 33335, 47785, 15017, 18905, 21200, 15804, 23776, 24750, 36977, 32180, 47145, 15182, 12675, 41014, 8603, 23776, 4688, 9828, 46997, 36397, 5794, 29021, 48485, 36069, 31223, 18894, 6669, 20137, 36122, 10652, 36314, 9204, 22774, 26778, 45265, 46649, 4807, 48592, 22222, 8638, 31161, 26532, 33853, 35337, 29366, 46817, 23664, 21712, 8402, 49498, 12994, 25300, 47193, 41765, 32169, 12670, 44433, 3941, 46230, 23443, 33877, 24448, 47110, 15708, 17611, 13977, 44433, 43562, 19401, 14460, 39147, 27847, 30371, 30505, 16135, 872, 32169, 12675, 13084, 22774, 49821, 25379, 16458, 24176, 6771, 37807, 32199, 26474, 43562, 31742, 42396, 21712, 35638, 49498, 12726, 28968, 40034, 34825, 17464, 35685, 36224, 21219, 42062, 3849, 4915, 12994, 26429, 30505, 10291, 18163, 45265, 36979, 6376, 47785, 4228, 29969, 25304, 36981, 48724, 10476, 26532, 24352, 5250, 45561, 21557, 47193, 30254, 21510, 12675, 42396, 16458, 36093, 38016, 6526, 36397, 35236, 37250, 17599, 45699, 12648, 9104, 46837, 48724, 22367, 28665, 46821, 30764, 48485, 19434, 15804, 10251, 8389]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d8bbfa90>\n",
            "Constructing exemplars of class 18\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [13148, 32792, 27483, 46712, 25714, 26947, 41576, 35267, 42406, 34561, 20557, 36707, 43156, 44938, 10127, 19577, 29345, 11385, 35637, 47098, 10829, 31800, 16819, 28378, 20143, 31961, 22976, 41949, 35525, 36280, 4107, 40615, 20557, 18031, 30738, 75, 27981, 2819, 20261, 407, 44991, 6028, 49145, 24839, 3343, 39460, 6764, 27221, 12541, 20720, 28389, 22981, 8405, 22322, 2560, 1005, 10536, 11385, 35917, 26711, 30092, 22467, 37843, 14633, 46035, 46578, 29034, 17561, 32431, 16819, 44530, 45833, 14694, 21942, 10144, 5189, 27943, 15174, 17833, 35651, 37016, 33980, 33587, 21960, 18847, 34889, 20939, 18889, 2819, 3252, 13732, 43287, 22981, 21942, 46854, 34561, 18503, 38921, 1597, 5470, 8639, 22322, 25395, 5520, 33848, 29811, 10696, 440, 9782, 42310, 19197, 20112, 24252, 18889, 14102, 26947, 44938, 45598, 20871, 32203, 18031, 30738, 9414, 35942, 24602, 41576, 38921, 41344, 35637, 22322, 41532, 5205, 14085, 10536, 32101, 44274, 46753, 43156, 2807, 5477, 3556, 27897, 47373, 31800, 33606, 11574, 39440, 3556, 32720, 8218, 10168, 28456, 10829, 16536, 28528, 43749, 39132, 39460, 18692, 11385, 12886, 18031, 1043, 39583, 27221, 35525, 17228, 36689, 5520, 12541, 35968, 4386, 24825, 39184, 14776, 21371, 32122, 26756, 47090, 41405, 24650, 17907, 48761, 22017, 40907, 42153, 43749, 8965, 48232, 34411, 47373, 46753, 21552, 33606, 13269, 42724, 7104, 31530, 10462, 15174]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d249ecd0>\n",
            "Constructing exemplars of class 65\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [22720, 36547, 33366, 43296, 22820, 19648, 1175, 33472, 9010, 5150, 7354, 37205, 31254, 14214, 41236, 43365, 9777, 46190, 49210, 26627, 42332, 27467, 10704, 48625, 46771, 31657, 43396, 15842, 15605, 43356, 3807, 33366, 35631, 30223, 28543, 29364, 16986, 28614, 17388, 9979, 48164, 33056, 34695, 11925, 8255, 20484, 10962, 37635, 25261, 45893, 26193, 44459, 40426, 10263, 46771, 6157, 27378, 43574, 36776, 20321, 42091, 12674, 6914, 28375, 12674, 17959, 28750, 396, 983, 9010, 21771, 45255, 15789, 21675, 38799, 3834, 34105, 16039, 13533, 35969, 6411, 45229, 16779, 25655, 30223, 35631, 37635, 20016, 19895, 25261, 47720, 802, 18570, 3734, 15657, 36877, 3799, 48829, 48692, 7563, 37293, 210, 18570, 18580, 10579, 9073, 23942, 30303, 33472, 45229, 16779, 15296, 17992, 514, 29675, 33781, 36877, 14941, 32585, 44459, 22801, 21875, 36776, 25077, 13805, 29000, 30989, 7323, 1691, 16099, 49140, 19161, 47804, 12321, 34559, 594, 292, 18977, 33364, 6157, 16647, 43176, 2972, 22220, 11415, 33596, 49210, 21354, 32767, 26182, 6708, 48464, 20579, 27467, 48692, 38262, 10695, 39954, 6411, 22746, 35007, 23825, 32045, 27376, 20484, 23819, 26376, 48126, 30950, 33597, 396, 28750, 18570, 7365, 27378, 45287, 34371, 18805, 29364, 46148, 21339, 12309, 37293, 19294, 4977, 47714, 21639, 48625, 46475, 48164, 22195, 19956, 32767, 40973, 5103, 49210, 33367, 3834, 40426, 21515]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d8bf2310>\n",
            "Constructing exemplars of class 49\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [27098, 47556, 3667, 28827, 2233, 11517, 23188, 37363, 41320, 19394, 23966, 38851, 45369, 35731, 9911, 26204, 20485, 19486, 12011, 22292, 2294, 45624, 10421, 8267, 49659, 3750, 5908, 18797, 30549, 12856, 9463, 35615, 13215, 43858, 12722, 38251, 28844, 19127, 10421, 17576, 44090, 19075, 7188, 6335, 14929, 49922, 7209, 8515, 49274, 27276, 12722, 8267, 47186, 6245, 33136, 48754, 8025, 22074, 23438, 23966, 30826, 17790, 6117, 29462, 4193, 45718, 30549, 31747, 40191, 25068, 41836, 4643, 26888, 31975, 32592, 35767, 6335, 3750, 8267, 49659, 41320, 24785, 30522, 11919, 33271, 2233, 42, 6287, 40747, 22431, 21962, 6554, 7903, 17229, 18138, 29491, 2444, 46569, 9824, 49080, 6747, 46075, 22101, 33409, 16888, 3750, 8267, 47186, 39004, 23161, 10421, 1298, 19316, 4047, 5444, 45848, 19048, 15563, 13850, 23528, 49966, 37363, 31299, 27289, 41836, 27170, 47688, 2372, 27289, 10609, 23913, 26063, 1147, 37725, 48987, 28355, 10421, 4351, 34228, 23161, 13797, 13850, 49690, 3127, 35867, 43858, 40747, 37546, 35448, 33676, 6809, 22292, 7209, 47688, 49310, 27289, 41836, 16421, 9142, 29462, 20894, 24785, 9045, 42671, 36073, 20132, 49659, 5908, 18895, 48776, 35915, 38402, 43858, 49525, 42740, 1610, 11122, 33332, 3029, 14543, 9911, 3750, 32352, 19676, 18136, 3667, 7773, 6403, 22101, 8267, 35867, 34446, 22298, 42230, 32316, 42622, 46931, 11694, 479, 17524]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d3cac210>\n",
            "Constructing exemplars of class 56\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [49875, 4387, 27069, 26393, 34378, 21594, 425, 19946, 6243, 40733, 43633, 644, 25926, 7964, 36843, 46633, 11023, 15023, 21127, 2001, 30262, 39892, 36623, 37023, 41914, 35305, 36908, 40994, 31891, 42287, 29110, 42758, 29454, 33738, 17027, 38176, 38459, 24138, 37090, 42287, 12921, 2764, 47360, 269, 43671, 37259, 33536, 27588, 29524, 45501, 9632, 36491, 4319, 20482, 46688, 8914, 2196, 12564, 108, 39892, 14334, 24629, 44147, 43212, 10394, 28828, 31427, 11727, 14423, 4787, 16174, 46755, 1410, 10982, 23839, 37730, 689, 34304, 6829, 6838, 43650, 30966, 16963, 48478, 37622, 12013, 48285, 41553, 8973, 3044, 22627, 43308, 9542, 39591, 36578, 37426, 29524, 787, 21522, 18183, 14906, 29524, 25059, 14752, 25519, 37910, 43374, 44, 17822, 32111, 37622, 45218, 689, 26349, 30453, 4319, 47438, 6829, 46309, 10274, 37259, 16233, 44674, 37352, 425, 4395, 14215, 6838, 27997, 32509, 37090, 35792, 30145, 31465, 11760, 9503, 26410, 644, 10352, 4787, 11727, 26289, 27814, 32147, 30076, 39221, 34378, 21020, 47336, 23382, 33660, 5361, 26393, 34018, 16233, 26289, 26303, 43374, 11857, 38988, 45621, 49837, 17919, 31671, 24037, 39314, 9333, 39054, 44648, 36137, 49001, 27200, 38176, 35305, 49633, 644, 19681, 17692, 9050, 37622, 48788, 36491, 12499, 13749, 16238, 39721, 34892, 1918, 28131, 29524, 39500, 48820, 18801, 425, 36843, 17799, 25519, 24629, 49837, 27588]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f7247f1f1d0>\n",
            "Constructing exemplars of class 20\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [39720, 2747, 39206, 28240, 4623, 34819, 44861, 16649, 23033, 5401, 33677, 6937, 31326, 30704, 13195, 45704, 49657, 26973, 35508, 9507, 2747, 35384, 38237, 41235, 11479, 32522, 31968, 30425, 48989, 28555, 41235, 33915, 40770, 47798, 25839, 29137, 17082, 41732, 9807, 32836, 10269, 37808, 42963, 3393, 44379, 24747, 31197, 33915, 44986, 6781, 29303, 32522, 12922, 34159, 34247, 17940, 9508, 46101, 12922, 19305, 18131, 22520, 28851, 39758, 34663, 13900, 47447, 18453, 9724, 7890, 2604, 33677, 1505, 32705, 31326, 31251, 45704, 6038, 32285, 45453, 42123, 29137, 46108, 22243, 26853, 4586, 45876, 8767, 41499, 4648, 37694, 37332, 6667, 45022, 26768, 12922, 23045, 4831, 36946, 26701, 18115, 8128, 2510, 47108, 38013, 24084, 48548, 41209, 42221, 34850, 47846, 40053, 17940, 12831, 10716, 26538, 19539, 24864, 12086, 14710, 5080, 32297, 21426, 1819, 14269, 1383, 4833, 30025, 47921, 5002, 26058, 1505, 15378, 16462, 1210, 10985, 23897, 37086, 1505, 16775, 6038, 5722, 12314, 2587, 26615, 39401, 37971, 26973, 49604, 19539, 48234, 8023, 32285, 41924, 11479, 44512, 41767, 13760, 26973, 45876, 20199, 26120, 6307, 17940, 24326, 16438, 23530, 10716, 24084, 48548, 40497, 28364, 16006, 15904, 20199, 6072, 4989, 27669, 44986, 34336, 28340, 5401, 33677, 19356, 26120, 21572, 12831, 34493, 14029, 23838, 22865, 26291, 25504, 30704, 1377, 45573, 1505, 24540, 48989, 44455]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d640c250>\n",
            "Constructing exemplars of class 4\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [113, 28628, 40758, 37075, 47681, 3779, 40009, 20699, 41747, 36715, 10099, 46335, 7290, 16210, 39730, 6993, 24809, 23361, 29496, 30853, 17304, 35775, 6514, 27921, 19801, 12053, 30665, 4846, 36710, 4899, 26476, 1308, 36395, 20078, 36340, 4051, 11950, 5542, 27417, 35641, 3654, 8008, 49250, 2601, 29663, 40806, 32837, 31801, 18449, 26714, 4051, 10066, 30665, 10416, 19094, 18393, 49929, 1729, 36715, 24358, 33272, 27678, 24046, 37771, 28205, 34974, 27884, 37057, 23505, 15442, 7270, 43660, 18393, 15812, 40917, 36241, 1892, 38364, 27242, 20620, 25973, 16148, 45151, 13695, 44258, 13454, 35882, 39730, 24619, 28116, 37609, 41747, 14826, 22732, 12963, 951, 34225, 4125, 40009, 3470, 488, 35445, 14727, 44237, 4051, 27056, 49186, 36124, 5034, 45436, 25134, 2601, 12897, 7290, 8616, 40917, 20699, 7524, 43490, 46335, 40230, 39400, 44258, 44568, 36715, 4051, 638, 40386, 5481, 30853, 7819, 3801, 33850, 10058, 45545, 18393, 33312, 45163, 15766, 21967, 5841, 38364, 48926, 13211, 5677, 37547, 30378, 15073, 33634, 13454, 16148, 42042, 45465, 32866, 28576, 13253, 36063, 35882, 43969, 32117, 11038, 42349, 4449, 35690, 8616, 5519, 30335, 45545, 43943, 13029, 30665, 25090, 3875, 29170, 48172, 35390, 1180, 3213, 5519, 35690, 27761, 24046, 42842, 995, 16201, 1929, 20039, 43897, 40386, 21069, 32771, 12526, 4304, 33272, 24358, 33996, 40296, 35576, 24923, 4846]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.68 0.9616193771362305\n",
            "TEST GROUP:  0.743\n",
            "TEST ALL:  0.743\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  2000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [82, 81, 7, 16, 18, 20, 21, 22, 34, 39, 47, 49, 56, 59, 65, 67, 68, 79, 80, 4]\n",
            "TRAIN_SET CLASSES:  [79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "VALIDATION CLASSES:  [47, 34, 21, 16, 82, 81, 80, 79, 7, 68]\n",
            "GROUP:  2\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "Starting epoch 1/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 3.985090494155884\n",
            "Train step - Step 10, Loss 2.6320652961730957\n",
            "Train step - Step 20, Loss 2.183755874633789\n",
            "Train step - Step 30, Loss 1.9825936555862427\n",
            "Train step - Step 40, Loss 1.8563868999481201\n",
            "Train step - Step 50, Loss 1.6701228618621826\n",
            "Train epoch - Accuracy: 0.3915107913669065 Loss: 2.2188444772555673 Corrects: 2721\n",
            "Starting epoch 2/30, LR = [0.01]\n",
            "Train step - Step 60, Loss 1.4353177547454834\n",
            "Train step - Step 70, Loss 1.612402319908142\n",
            "Train step - Step 80, Loss 1.3626642227172852\n",
            "Train step - Step 90, Loss 1.4182358980178833\n",
            "Train step - Step 100, Loss 1.3698689937591553\n",
            "Train epoch - Accuracy: 0.563884892086331 Loss: 1.4519287821200255 Corrects: 3919\n",
            "Starting epoch 3/30, LR = [0.01]\n",
            "Train step - Step 110, Loss 1.5272387266159058\n",
            "Train step - Step 120, Loss 1.517708420753479\n",
            "Train step - Step 130, Loss 1.5195937156677246\n",
            "Train step - Step 140, Loss 1.3123054504394531\n",
            "Train step - Step 150, Loss 1.262943148612976\n",
            "Train step - Step 160, Loss 1.393984079360962\n",
            "Train epoch - Accuracy: 0.6050359712230216 Loss: 1.3309132604804828 Corrects: 4205\n",
            "Starting epoch 4/30, LR = [0.01]\n",
            "Train step - Step 170, Loss 1.30386483669281\n",
            "Train step - Step 180, Loss 1.1506879329681396\n",
            "Train step - Step 190, Loss 1.2099415063858032\n",
            "Train step - Step 200, Loss 1.1624284982681274\n",
            "Train step - Step 210, Loss 1.2230753898620605\n",
            "Train epoch - Accuracy: 0.6376978417266187 Loss: 1.2304155265684609 Corrects: 4432\n",
            "Starting epoch 5/30, LR = [0.01]\n",
            "Train step - Step 220, Loss 0.9819907546043396\n",
            "Train step - Step 230, Loss 1.192338466644287\n",
            "Train step - Step 240, Loss 1.1450334787368774\n",
            "Train step - Step 250, Loss 1.317293405532837\n",
            "Train step - Step 260, Loss 1.0978918075561523\n",
            "Train step - Step 270, Loss 1.163296103477478\n",
            "Train epoch - Accuracy: 0.6494964028776978 Loss: 1.1805703747015206 Corrects: 4514\n",
            "Starting epoch 6/30, LR = [0.01]\n",
            "Train step - Step 280, Loss 1.0459063053131104\n",
            "Train step - Step 290, Loss 1.0899999141693115\n",
            "Train step - Step 300, Loss 0.9970000982284546\n",
            "Train step - Step 310, Loss 1.0269616842269897\n",
            "Train step - Step 320, Loss 1.109933614730835\n",
            "Train epoch - Accuracy: 0.6646043165467626 Loss: 1.1258183242948794 Corrects: 4619\n",
            "Starting epoch 7/30, LR = [0.01]\n",
            "Train step - Step 330, Loss 1.046716332435608\n",
            "Train step - Step 340, Loss 1.180464506149292\n",
            "Train step - Step 350, Loss 1.15171480178833\n",
            "Train step - Step 360, Loss 1.0563448667526245\n",
            "Train step - Step 370, Loss 1.121000051498413\n",
            "Train step - Step 380, Loss 0.980904757976532\n",
            "Train epoch - Accuracy: 0.6874820143884892 Loss: 1.0842956782759523 Corrects: 4778\n",
            "Starting epoch 8/30, LR = [0.01]\n",
            "Train step - Step 390, Loss 1.0036689043045044\n",
            "Train step - Step 400, Loss 1.115918755531311\n",
            "Train step - Step 410, Loss 1.0266679525375366\n",
            "Train step - Step 420, Loss 1.1020351648330688\n",
            "Train step - Step 430, Loss 1.110468864440918\n",
            "Train epoch - Accuracy: 0.6988489208633093 Loss: 1.0435630596970482 Corrects: 4857\n",
            "Starting epoch 9/30, LR = [0.01]\n",
            "Train step - Step 440, Loss 0.949171781539917\n",
            "Train step - Step 450, Loss 0.9592173099517822\n",
            "Train step - Step 460, Loss 1.0202078819274902\n",
            "Train step - Step 470, Loss 1.0242244005203247\n",
            "Train step - Step 480, Loss 1.204475998878479\n",
            "Train step - Step 490, Loss 1.0616281032562256\n",
            "Train epoch - Accuracy: 0.7090647482014388 Loss: 1.0188707574665976 Corrects: 4928\n",
            "Starting epoch 10/30, LR = [0.01]\n",
            "Train step - Step 500, Loss 1.0151540040969849\n",
            "Train step - Step 510, Loss 0.8291465640068054\n",
            "Train step - Step 520, Loss 1.0914663076400757\n",
            "Train step - Step 530, Loss 0.9587881565093994\n",
            "Train step - Step 540, Loss 0.9810488820075989\n",
            "Train epoch - Accuracy: 0.7181294964028777 Loss: 0.9786231531170633 Corrects: 4991\n",
            "Starting epoch 11/30, LR = [0.01]\n",
            "Train step - Step 550, Loss 0.8520102500915527\n",
            "Train step - Step 560, Loss 1.072513222694397\n",
            "Train step - Step 570, Loss 1.076027750968933\n",
            "Train step - Step 580, Loss 0.7976786494255066\n",
            "Train step - Step 590, Loss 1.0814275741577148\n",
            "Train step - Step 600, Loss 1.023582100868225\n",
            "Train epoch - Accuracy: 0.7223021582733813 Loss: 0.948270281081577 Corrects: 5020\n",
            "Starting epoch 12/30, LR = [0.01]\n",
            "Train step - Step 610, Loss 1.0222928524017334\n",
            "Train step - Step 620, Loss 0.8554882407188416\n",
            "Train step - Step 630, Loss 1.0686538219451904\n",
            "Train step - Step 640, Loss 1.0757781267166138\n",
            "Train step - Step 650, Loss 0.7538381218910217\n",
            "Train epoch - Accuracy: 0.7312230215827338 Loss: 0.9432365526912881 Corrects: 5082\n",
            "Starting epoch 13/30, LR = [0.01]\n",
            "Train step - Step 660, Loss 0.9261938333511353\n",
            "Train step - Step 670, Loss 1.113484263420105\n",
            "Train step - Step 680, Loss 1.0108399391174316\n",
            "Train step - Step 690, Loss 0.8310856223106384\n",
            "Train step - Step 700, Loss 0.9375761151313782\n",
            "Train step - Step 710, Loss 0.9667859673500061\n",
            "Train epoch - Accuracy: 0.7359712230215827 Loss: 0.9043755150527405 Corrects: 5115\n",
            "Starting epoch 14/30, LR = [0.01]\n",
            "Train step - Step 720, Loss 0.9702591896057129\n",
            "Train step - Step 730, Loss 0.7782442569732666\n",
            "Train step - Step 740, Loss 0.9739277958869934\n",
            "Train step - Step 750, Loss 0.8855606317520142\n",
            "Train step - Step 760, Loss 0.8821569681167603\n",
            "Train epoch - Accuracy: 0.7480575539568345 Loss: 0.8785973146150438 Corrects: 5199\n",
            "Starting epoch 15/30, LR = [0.01]\n",
            "Train step - Step 770, Loss 0.8387331962585449\n",
            "Train step - Step 780, Loss 0.8432906270027161\n",
            "Train step - Step 790, Loss 0.8719149827957153\n",
            "Train step - Step 800, Loss 0.7420621514320374\n",
            "Train step - Step 810, Loss 0.9465869665145874\n",
            "Train step - Step 820, Loss 1.120314359664917\n",
            "Train epoch - Accuracy: 0.7500719424460431 Loss: 0.8630646662917926 Corrects: 5213\n",
            "Starting epoch 16/30, LR = [0.01]\n",
            "Train step - Step 830, Loss 0.6610029935836792\n",
            "Train step - Step 840, Loss 0.7860026955604553\n",
            "Train step - Step 850, Loss 0.7878167629241943\n",
            "Train step - Step 860, Loss 0.9547661542892456\n",
            "Train step - Step 870, Loss 0.8229113817214966\n",
            "Train epoch - Accuracy: 0.7694964028776978 Loss: 0.8314526367873597 Corrects: 5348\n",
            "Starting epoch 17/30, LR = [0.01]\n",
            "Train step - Step 880, Loss 0.9079110026359558\n",
            "Train step - Step 890, Loss 0.929596483707428\n",
            "Train step - Step 900, Loss 0.7555102109909058\n",
            "Train step - Step 910, Loss 0.9204692840576172\n",
            "Train step - Step 920, Loss 0.8652746677398682\n",
            "Train step - Step 930, Loss 0.6983479261398315\n",
            "Train epoch - Accuracy: 0.7673381294964029 Loss: 0.8304813103881671 Corrects: 5333\n",
            "Starting epoch 18/30, LR = [0.01]\n",
            "Train step - Step 940, Loss 0.6409541964530945\n",
            "Train step - Step 950, Loss 0.6766501665115356\n",
            "Train step - Step 960, Loss 0.8544409275054932\n",
            "Train step - Step 970, Loss 0.9854723215103149\n",
            "Train step - Step 980, Loss 0.7637701034545898\n",
            "Train epoch - Accuracy: 0.7755395683453238 Loss: 0.8203857517928528 Corrects: 5390\n",
            "Starting epoch 19/30, LR = [0.01]\n",
            "Train step - Step 990, Loss 0.7750144004821777\n",
            "Train step - Step 1000, Loss 0.6753091812133789\n",
            "Train step - Step 1010, Loss 0.8337417244911194\n",
            "Train step - Step 1020, Loss 0.7769847512245178\n",
            "Train step - Step 1030, Loss 0.7287930250167847\n",
            "Train step - Step 1040, Loss 0.7456310987472534\n",
            "Train epoch - Accuracy: 0.7789928057553956 Loss: 0.7940515513900372 Corrects: 5414\n",
            "Starting epoch 20/30, LR = [0.01]\n",
            "Train step - Step 1050, Loss 0.6244581937789917\n",
            "Train step - Step 1060, Loss 0.6904581785202026\n",
            "Train step - Step 1070, Loss 0.8548627495765686\n",
            "Train step - Step 1080, Loss 0.8122953772544861\n",
            "Train step - Step 1090, Loss 0.9052270650863647\n",
            "Train epoch - Accuracy: 0.7889208633093525 Loss: 0.7727095096059841 Corrects: 5483\n",
            "Starting epoch 21/30, LR = [0.01]\n",
            "Train step - Step 1100, Loss 0.6477508544921875\n",
            "Train step - Step 1110, Loss 0.6449077725410461\n",
            "Train step - Step 1120, Loss 0.87875896692276\n",
            "Train step - Step 1130, Loss 0.792075514793396\n",
            "Train step - Step 1140, Loss 0.6628715991973877\n",
            "Train step - Step 1150, Loss 0.8317115306854248\n",
            "Train epoch - Accuracy: 0.7884892086330936 Loss: 0.7726753853379393 Corrects: 5480\n",
            "Starting epoch 22/30, LR = [0.01]\n",
            "Train step - Step 1160, Loss 0.8008781671524048\n",
            "Train step - Step 1170, Loss 0.7714864015579224\n",
            "Train step - Step 1180, Loss 0.914411187171936\n",
            "Train step - Step 1190, Loss 0.6919525861740112\n",
            "Train step - Step 1200, Loss 0.8263111114501953\n",
            "Train epoch - Accuracy: 0.7886330935251799 Loss: 0.7649670682708136 Corrects: 5481\n",
            "Starting epoch 23/30, LR = [0.01]\n",
            "Train step - Step 1210, Loss 0.596764087677002\n",
            "Train step - Step 1220, Loss 0.7654956579208374\n",
            "Train step - Step 1230, Loss 0.8272213935852051\n",
            "Train step - Step 1240, Loss 0.6821094155311584\n",
            "Train step - Step 1250, Loss 0.7177166938781738\n",
            "Train step - Step 1260, Loss 0.7275731563568115\n",
            "Train epoch - Accuracy: 0.7943884892086331 Loss: 0.7317334148352095 Corrects: 5521\n",
            "Starting epoch 24/30, LR = [0.01]\n",
            "Train step - Step 1270, Loss 0.542009711265564\n",
            "Train step - Step 1280, Loss 0.8237611055374146\n",
            "Train step - Step 1290, Loss 0.8073863983154297\n",
            "Train step - Step 1300, Loss 0.8075352311134338\n",
            "Train step - Step 1310, Loss 0.6915202140808105\n",
            "Train epoch - Accuracy: 0.8046043165467626 Loss: 0.7181012782947622 Corrects: 5592\n",
            "Starting epoch 25/30, LR = [0.01]\n",
            "Train step - Step 1320, Loss 0.5747316479682922\n",
            "Train step - Step 1330, Loss 0.7111999988555908\n",
            "Train step - Step 1340, Loss 0.5844724178314209\n",
            "Train step - Step 1350, Loss 0.7274206876754761\n",
            "Train step - Step 1360, Loss 0.5811049342155457\n",
            "Train step - Step 1370, Loss 0.7338644862174988\n",
            "Train epoch - Accuracy: 0.8050359712230216 Loss: 0.7170237246177179 Corrects: 5595\n",
            "Starting epoch 26/30, LR = [0.01]\n",
            "Train step - Step 1380, Loss 0.7273574471473694\n",
            "Train step - Step 1390, Loss 0.6479874849319458\n",
            "Train step - Step 1400, Loss 0.7598196268081665\n",
            "Train step - Step 1410, Loss 0.8673583269119263\n",
            "Train step - Step 1420, Loss 0.5899009108543396\n",
            "Train epoch - Accuracy: 0.8126618705035972 Loss: 0.6911369339167643 Corrects: 5648\n",
            "Starting epoch 27/30, LR = [0.01]\n",
            "Train step - Step 1430, Loss 0.6078215837478638\n",
            "Train step - Step 1440, Loss 0.6580583453178406\n",
            "Train step - Step 1450, Loss 0.6271231770515442\n",
            "Train step - Step 1460, Loss 0.5013284683227539\n",
            "Train step - Step 1470, Loss 0.7275189757347107\n",
            "Train step - Step 1480, Loss 0.7090433239936829\n",
            "Train epoch - Accuracy: 0.8191366906474821 Loss: 0.6646915727725132 Corrects: 5693\n",
            "Starting epoch 28/30, LR = [0.01]\n",
            "Train step - Step 1490, Loss 0.7200340032577515\n",
            "Train step - Step 1500, Loss 0.6318979263305664\n",
            "Train step - Step 1510, Loss 0.6198676228523254\n",
            "Train step - Step 1520, Loss 0.9739547967910767\n",
            "Train step - Step 1530, Loss 0.840785026550293\n",
            "Train epoch - Accuracy: 0.8148201438848921 Loss: 0.6774651106312978 Corrects: 5663\n",
            "Starting epoch 29/30, LR = [0.01]\n",
            "Train step - Step 1540, Loss 0.5414789915084839\n",
            "Train step - Step 1550, Loss 0.6398706436157227\n",
            "Train step - Step 1560, Loss 0.6190019249916077\n",
            "Train step - Step 1570, Loss 0.7406870722770691\n",
            "Train step - Step 1580, Loss 0.5681089162826538\n",
            "Train step - Step 1590, Loss 0.7832542061805725\n",
            "Train epoch - Accuracy: 0.8246043165467626 Loss: 0.6462338696452353 Corrects: 5731\n",
            "Starting epoch 30/30, LR = [0.01]\n",
            "Train step - Step 1600, Loss 0.643498957157135\n",
            "Train step - Step 1610, Loss 0.5929344296455383\n",
            "Train step - Step 1620, Loss 0.6756241321563721\n",
            "Train step - Step 1630, Loss 0.6412524580955505\n",
            "Train step - Step 1640, Loss 0.5476375222206116\n",
            "Train epoch - Accuracy: 0.8303597122302159 Loss: 0.634643756468519 Corrects: 5771\n",
            "Training finished in 272.013742685318 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d1391dd0>\n",
            "Constructing exemplars of class 79\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [45005, 4225, 34636, 8939, 44728, 26021, 48347, 21744, 40529, 43431, 38411, 8007, 32653, 48600, 49496, 32632, 45756, 16109, 27100, 44109, 30808, 11087, 34970, 28188, 45438, 28204, 12336, 5485, 38273, 14441, 42275, 32547, 34523, 23535, 29058, 15305, 20711, 11692, 16069, 7758, 30381, 47739, 758, 26811, 45472, 1710, 11863, 8293, 24260, 38515, 36145, 863, 3493, 45994, 9767, 28549, 14661, 8336, 26911, 13207, 6513, 40529, 32010, 12510, 38514, 22421, 38479, 771, 38829, 34361, 42275, 44724, 43923, 47453, 34116, 36438, 23672, 184, 35060, 1977, 42104, 9661, 4873, 44005, 7775, 47278, 5347, 35987, 5485, 4399, 15449, 12646, 10631, 48869, 20711, 3333, 44350, 40441, 24547, 16341]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d2427c90>\n",
            "Constructing exemplars of class 47\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [48953, 5363, 21027, 1765, 25919, 22495, 20152, 41102, 26632, 10889, 40637, 1620, 10052, 2118, 9876, 31553, 17534, 48623, 42765, 3189, 3048, 34621, 40763, 40197, 39864, 48254, 24663, 45365, 8896, 49531, 24315, 18803, 22954, 36131, 34063, 1121, 5345, 36730, 16027, 28601, 23919, 33001, 8653, 10323, 10889, 44843, 15845, 5134, 32828, 49764, 27543, 23463, 3281, 33474, 38194, 19646, 46425, 17926, 18407, 44251, 37871, 9769, 23919, 41521, 41456, 24190, 14505, 26494, 23245, 36842, 8193, 26342, 25024, 14666, 26437, 22837, 10894, 16066, 7519, 41055, 36181, 17883, 23326, 5598, 13857, 38194, 12721, 16654, 29713, 11960, 47305, 44092, 47236, 27741, 3797, 45703, 27674, 48716, 23577, 38323]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d132bd50>\n",
            "Constructing exemplars of class 7\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [41100, 38531, 31658, 39555, 7672, 27778, 49460, 11197, 37565, 33138, 43643, 10590, 29697, 5516, 38804, 25813, 45289, 34181, 16537, 47914, 40805, 48861, 26112, 25690, 26886, 22006, 13476, 49164, 18441, 15631, 1015, 35159, 10755, 28243, 27563, 12780, 29732, 12134, 4572, 482, 35079, 30165, 9501, 38132, 7226, 23066, 41071, 27874, 12202, 26886, 26443, 46653, 290, 27712, 31684, 12448, 13928, 42461, 47101, 19549, 40099, 28235, 9816, 7237, 3723, 10376, 28363, 13985, 29222, 49119, 22332, 25539, 19432, 16035, 29732, 41588, 18513, 40131, 25690, 34424, 25989, 35781, 26886, 29789, 1219, 18599, 36663, 16029, 29222, 42461, 8716, 4498, 15097, 5995, 24877, 11952, 38804, 36222, 9226, 38408]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d640d5d0>\n",
            "Constructing exemplars of class 82\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [14992, 23071, 18702, 46951, 3555, 7671, 26895, 28888, 30468, 16437, 42783, 10699, 12252, 25265, 45482, 10757, 44928, 3438, 6961, 8580, 28096, 25771, 274, 15939, 40408, 8099, 30911, 41968, 11449, 10505, 48289, 23431, 24715, 41672, 46768, 44928, 27407, 29103, 16467, 8331, 625, 12801, 34883, 42541, 35967, 29229, 28888, 10901, 14057, 10519, 4269, 871, 7415, 11501, 6570, 42626, 43606, 40656, 6057, 24901, 32644, 7027, 1136, 45366, 28254, 49708, 47319, 1854, 36727, 3438, 38486, 32602, 49631, 32944, 8331, 8553, 19657, 1919, 7586, 6271, 27707, 44697, 27487, 13458, 26440, 16643, 33090, 14807, 6761, 10699, 26976, 30203, 39637, 8253, 44213, 14057, 48703, 8580, 15113, 36727]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6426ed0>\n",
            "Constructing exemplars of class 34\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [48527, 48783, 1472, 30789, 42407, 40584, 35219, 7194, 11804, 15614, 47121, 17047, 17954, 16038, 3680, 13748, 3181, 42407, 3499, 43775, 31527, 25062, 40004, 20767, 13748, 24884, 14596, 16735, 41853, 13823, 22548, 16183, 19814, 43775, 17581, 39095, 34859, 49721, 18632, 35334, 23782, 42357, 26238, 42158, 29468, 21349, 49973, 22132, 42778, 34707, 13335, 21858, 11594, 26238, 28754, 4875, 23405, 8074, 48940, 911, 19440, 28938, 17634, 40584, 43805, 38325, 20724, 42357, 41853, 14824, 14233, 28754, 28262, 32901, 33581, 29129, 41290, 30393, 25992, 7515, 35024, 27664, 33777, 20874, 40019, 40584, 12666, 5248, 24929, 1550, 31783, 4577, 5287, 29853, 4292, 42036, 22089, 20724, 32312, 18397]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d8026b10>\n",
            "Constructing exemplars of class 81\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [37864, 41333, 14660, 39398, 14621, 37465, 41192, 8276, 37741, 18330, 22025, 46266, 26745, 14600, 29751, 21965, 34026, 8799, 15624, 31393, 40259, 18771, 46119, 6998, 38594, 41058, 186, 28913, 25500, 12751, 7023, 46637, 4538, 34777, 24640, 8321, 6209, 20400, 22504, 11675, 40563, 40218, 6214, 26308, 47035, 34261, 49479, 2596, 20504, 25857, 18363, 27776, 12683, 45200, 30059, 21021, 20470, 20783, 29471, 489, 20185, 29807, 36433, 1395, 46366, 3232, 19916, 16499, 6997, 33428, 18735, 13510, 36433, 11566, 31611, 40920, 196, 17469, 1481, 30362, 21025, 31782, 36046, 47989, 25550, 40631, 11885, 19430, 45865, 39868, 4599, 20504, 4904, 34777, 34156, 2132, 49343, 2156, 47627, 16318]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d712cb50>\n",
            "Constructing exemplars of class 21\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [32344, 17657, 24826, 9256, 14213, 4372, 8394, 34870, 45390, 47810, 41428, 354, 19977, 15256, 31387, 47107, 2555, 19647, 15584, 43531, 16783, 28523, 6046, 34948, 39144, 44550, 37664, 1672, 46074, 40977, 44393, 45167, 40466, 31206, 6663, 6650, 6432, 25124, 2738, 3692, 5105, 39222, 8688, 9709, 15320, 41690, 43407, 5105, 16105, 13623, 44233, 27308, 9955, 26154, 45961, 14614, 18399, 39499, 6507, 9433, 34, 37522, 12784, 9177, 44887, 17807, 3692, 15103, 40557, 6979, 2335, 37144, 9256, 43714, 11529, 45753, 3722, 1361, 27947, 14579, 1333, 32021, 20694, 19977, 30804, 40451, 24295, 45457, 28841, 5945, 37667, 27171, 20365, 12272, 44393, 23522, 4096, 47654, 5263, 22884]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6bccc50>\n",
            "Constructing exemplars of class 80\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [43611, 35376, 23697, 43266, 11768, 4360, 43720, 36788, 48082, 45030, 25618, 12741, 8076, 8806, 5735, 38312, 22939, 20105, 28928, 31266, 6819, 34060, 28894, 4893, 32587, 22939, 15026, 3095, 25986, 45437, 31171, 14576, 32123, 27162, 49446, 43204, 39906, 18, 9215, 2791, 2385, 27436, 14954, 1066, 31010, 11991, 1223, 12905, 26146, 40393, 48490, 2045, 3641, 3429, 14476, 8094, 7599, 38761, 10133, 6699, 40844, 24684, 19088, 22265, 40095, 31171, 40896, 20882, 35071, 23697, 4360, 46917, 15106, 12119, 23042, 30112, 42990, 8806, 31804, 43277, 32698, 34049, 30933, 42962, 43609, 14576, 47220, 30641, 42720, 1253, 39537, 49814, 11115, 8240, 33894, 17170, 4349, 9582, 39480, 25263]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d2427950>\n",
            "Constructing exemplars of class 68\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [29117, 36856, 45013, 19543, 7944, 34470, 8412, 23007, 28074, 40312, 43445, 27101, 16427, 12103, 18711, 14896, 31919, 27599, 45019, 19543, 8864, 12826, 49464, 30088, 28074, 22100, 34905, 39734, 17060, 5462, 21731, 42387, 8971, 32740, 16758, 49312, 12331, 37246, 9742, 22591, 209, 14068, 25899, 35105, 3836, 30088, 37930, 13044, 3533, 16762, 20363, 19692, 13154, 43263, 12826, 30803, 44121, 16146, 27262, 37548, 18711, 47878, 40600, 44988, 20363, 25540, 30803, 39734, 43445, 26044, 28443, 18690, 8710, 36461, 48206, 37930, 34905, 32918, 14068, 13154, 22905, 19905, 37785, 28217, 1673, 21643, 10361, 31953, 39234, 30803, 15772, 21729, 28074, 45172, 17003, 42939, 44458, 16762, 14793, 16812]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6bcc890>\n",
            "Constructing exemplars of class 16\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [49965, 46134, 29321, 12709, 31774, 15439, 19760, 22380, 37324, 7826, 22084, 6793, 2616, 35320, 36421, 44380, 41919, 41085, 4567, 29271, 29373, 14037, 13639, 41893, 45983, 3382, 9477, 8282, 24725, 49118, 9261, 35546, 2658, 30494, 49928, 28120, 23857, 33012, 12044, 21595, 1300, 23122, 16704, 12340, 23899, 48588, 38192, 41535, 9621, 19939, 11284, 8702, 38743, 20051, 17771, 20134, 14019, 6225, 1743, 49260, 22248, 13689, 41484, 18225, 28892, 34546, 12985, 1833, 14182, 1252, 12044, 17899, 9389, 30423, 29847, 7822, 16704, 47601, 9075, 47233, 38192, 14942, 41893, 37388, 4046, 37520, 26400, 18377, 7199, 4199, 30215, 38148, 1898, 40618, 2644, 22248, 43404, 48090, 20855, 46643]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.72 1.1019980907440186\n",
            "TEST GROUP:  0.742\n",
            "TEST ALL:  0.665\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  3000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [81, 79, 4, 10, 16, 18, 20, 22, 24, 32, 34, 56, 64, 68, 76, 80, 82, 90, 7, 21, 23, 39, 47, 49, 59, 61, 65, 67, 75, 0]\n",
            "TRAIN_SET CLASSES:  [75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "VALIDATION CLASSES:  [61, 32, 90, 24, 23, 76, 75, 10, 0, 64]\n",
            "GROUP:  3\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "Starting epoch 1/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 4.6670732498168945\n",
            "Train step - Step 10, Loss 3.0528290271759033\n",
            "Train step - Step 20, Loss 2.2620739936828613\n",
            "Train step - Step 30, Loss 1.8556950092315674\n",
            "Train step - Step 40, Loss 1.5130665302276611\n",
            "Train step - Step 50, Loss 1.3362675905227661\n",
            "Train epoch - Accuracy: 0.4339568345323741 Loss: 2.362730324354103 Corrects: 3016\n",
            "Starting epoch 2/30, LR = [0.01]\n",
            "Train step - Step 60, Loss 1.4264951944351196\n",
            "Train step - Step 70, Loss 1.4185466766357422\n",
            "Train step - Step 80, Loss 1.284545660018921\n",
            "Train step - Step 90, Loss 1.5529266595840454\n",
            "Train step - Step 100, Loss 1.3179564476013184\n",
            "Train epoch - Accuracy: 0.636546762589928 Loss: 1.2939295938889757 Corrects: 4424\n",
            "Starting epoch 3/30, LR = [0.01]\n",
            "Train step - Step 110, Loss 1.1830023527145386\n",
            "Train step - Step 120, Loss 1.345017433166504\n",
            "Train step - Step 130, Loss 1.1524721384048462\n",
            "Train step - Step 140, Loss 1.2016264200210571\n",
            "Train step - Step 150, Loss 1.3002625703811646\n",
            "Train step - Step 160, Loss 0.9975773692131042\n",
            "Train epoch - Accuracy: 0.6706474820143885 Loss: 1.1748267003615125 Corrects: 4661\n",
            "Starting epoch 4/30, LR = [0.01]\n",
            "Train step - Step 170, Loss 1.0338971614837646\n",
            "Train step - Step 180, Loss 1.1768397092819214\n",
            "Train step - Step 190, Loss 1.1144694089889526\n",
            "Train step - Step 200, Loss 1.1643966436386108\n",
            "Train step - Step 210, Loss 1.0027052164077759\n",
            "Train epoch - Accuracy: 0.6915107913669065 Loss: 1.1005343150063385 Corrects: 4806\n",
            "Starting epoch 5/30, LR = [0.01]\n",
            "Train step - Step 220, Loss 1.116358995437622\n",
            "Train step - Step 230, Loss 1.0439066886901855\n",
            "Train step - Step 240, Loss 0.9678201675415039\n",
            "Train step - Step 250, Loss 0.9963330030441284\n",
            "Train step - Step 260, Loss 1.0143519639968872\n",
            "Train step - Step 270, Loss 1.0117521286010742\n",
            "Train epoch - Accuracy: 0.7025899280575539 Loss: 1.0604707375876337 Corrects: 4883\n",
            "Starting epoch 6/30, LR = [0.01]\n",
            "Train step - Step 280, Loss 1.2013380527496338\n",
            "Train step - Step 290, Loss 1.2087833881378174\n",
            "Train step - Step 300, Loss 0.8518954515457153\n",
            "Train step - Step 310, Loss 0.9197553992271423\n",
            "Train step - Step 320, Loss 1.0405919551849365\n",
            "Train epoch - Accuracy: 0.719568345323741 Loss: 1.0169733735297224 Corrects: 5001\n",
            "Starting epoch 7/30, LR = [0.01]\n",
            "Train step - Step 330, Loss 1.0131006240844727\n",
            "Train step - Step 340, Loss 0.9189038276672363\n",
            "Train step - Step 350, Loss 0.9049680233001709\n",
            "Train step - Step 360, Loss 1.0823180675506592\n",
            "Train step - Step 370, Loss 1.279456615447998\n",
            "Train step - Step 380, Loss 1.072801947593689\n",
            "Train epoch - Accuracy: 0.720431654676259 Loss: 0.9984151175725374 Corrects: 5007\n",
            "Starting epoch 8/30, LR = [0.01]\n",
            "Train step - Step 390, Loss 0.8260331153869629\n",
            "Train step - Step 400, Loss 0.9810353517532349\n",
            "Train step - Step 410, Loss 1.0193812847137451\n",
            "Train step - Step 420, Loss 0.824249267578125\n",
            "Train step - Step 430, Loss 0.9172950983047485\n",
            "Train epoch - Accuracy: 0.7362589928057554 Loss: 0.9574067960711692 Corrects: 5117\n",
            "Starting epoch 9/30, LR = [0.01]\n",
            "Train step - Step 440, Loss 0.8807328939437866\n",
            "Train step - Step 450, Loss 0.96818608045578\n",
            "Train step - Step 460, Loss 0.9006687998771667\n",
            "Train step - Step 470, Loss 0.7883772253990173\n",
            "Train step - Step 480, Loss 1.069122552871704\n",
            "Train step - Step 490, Loss 0.792389988899231\n",
            "Train epoch - Accuracy: 0.7506474820143885 Loss: 0.9218526003858168 Corrects: 5217\n",
            "Starting epoch 10/30, LR = [0.01]\n",
            "Train step - Step 500, Loss 0.7781778573989868\n",
            "Train step - Step 510, Loss 0.7899582386016846\n",
            "Train step - Step 520, Loss 0.8671483397483826\n",
            "Train step - Step 530, Loss 0.712059497833252\n",
            "Train step - Step 540, Loss 0.9297217726707458\n",
            "Train epoch - Accuracy: 0.763453237410072 Loss: 0.8809184908695358 Corrects: 5306\n",
            "Starting epoch 11/30, LR = [0.01]\n",
            "Train step - Step 550, Loss 0.8690075874328613\n",
            "Train step - Step 560, Loss 0.9249552488327026\n",
            "Train step - Step 570, Loss 0.8284206986427307\n",
            "Train step - Step 580, Loss 0.8937262296676636\n",
            "Train step - Step 590, Loss 0.9369953274726868\n",
            "Train step - Step 600, Loss 0.8558499217033386\n",
            "Train epoch - Accuracy: 0.7667625899280576 Loss: 0.8587316372754763 Corrects: 5329\n",
            "Starting epoch 12/30, LR = [0.01]\n",
            "Train step - Step 610, Loss 0.7417939901351929\n",
            "Train step - Step 620, Loss 0.9295120239257812\n",
            "Train step - Step 630, Loss 0.9394307136535645\n",
            "Train step - Step 640, Loss 0.9102281332015991\n",
            "Train step - Step 650, Loss 1.0407710075378418\n",
            "Train epoch - Accuracy: 0.7687769784172662 Loss: 0.8574521259095171 Corrects: 5343\n",
            "Starting epoch 13/30, LR = [0.01]\n",
            "Train step - Step 660, Loss 0.8778231739997864\n",
            "Train step - Step 670, Loss 0.891282320022583\n",
            "Train step - Step 680, Loss 0.8105255961418152\n",
            "Train step - Step 690, Loss 0.9456382989883423\n",
            "Train step - Step 700, Loss 0.7934908866882324\n",
            "Train step - Step 710, Loss 0.9268454909324646\n",
            "Train epoch - Accuracy: 0.7753956834532374 Loss: 0.8369808598902586 Corrects: 5389\n",
            "Starting epoch 14/30, LR = [0.01]\n",
            "Train step - Step 720, Loss 0.7386971116065979\n",
            "Train step - Step 730, Loss 0.8317655324935913\n",
            "Train step - Step 740, Loss 0.7912489175796509\n",
            "Train step - Step 750, Loss 0.7670291066169739\n",
            "Train step - Step 760, Loss 0.7601689696311951\n",
            "Train epoch - Accuracy: 0.7802877697841727 Loss: 0.813114132692488 Corrects: 5423\n",
            "Starting epoch 15/30, LR = [0.01]\n",
            "Train step - Step 770, Loss 0.7754078507423401\n",
            "Train step - Step 780, Loss 0.8275821208953857\n",
            "Train step - Step 790, Loss 0.7573620080947876\n",
            "Train step - Step 800, Loss 0.8410055637359619\n",
            "Train step - Step 810, Loss 0.7338979244232178\n",
            "Train step - Step 820, Loss 0.810412585735321\n",
            "Train epoch - Accuracy: 0.7889208633093525 Loss: 0.7885042755552333 Corrects: 5483\n",
            "Starting epoch 16/30, LR = [0.01]\n",
            "Train step - Step 830, Loss 0.7712568044662476\n",
            "Train step - Step 840, Loss 0.9630728363990784\n",
            "Train step - Step 850, Loss 0.8401895761489868\n",
            "Train step - Step 860, Loss 0.5686521530151367\n",
            "Train step - Step 870, Loss 0.8055528402328491\n",
            "Train epoch - Accuracy: 0.7938129496402878 Loss: 0.7835746592240368 Corrects: 5517\n",
            "Starting epoch 17/30, LR = [0.01]\n",
            "Train step - Step 880, Loss 0.7377225160598755\n",
            "Train step - Step 890, Loss 0.6264883279800415\n",
            "Train step - Step 900, Loss 0.7753914594650269\n",
            "Train step - Step 910, Loss 0.7630677819252014\n",
            "Train step - Step 920, Loss 0.6572617292404175\n",
            "Train step - Step 930, Loss 0.7804292440414429\n",
            "Train epoch - Accuracy: 0.8076258992805755 Loss: 0.7516524191726026 Corrects: 5613\n",
            "Starting epoch 18/30, LR = [0.01]\n",
            "Train step - Step 940, Loss 0.7739548683166504\n",
            "Train step - Step 950, Loss 0.8483638763427734\n",
            "Train step - Step 960, Loss 0.8022617697715759\n",
            "Train step - Step 970, Loss 0.6921427249908447\n",
            "Train step - Step 980, Loss 0.8179528713226318\n",
            "Train epoch - Accuracy: 0.7975539568345323 Loss: 0.7500310613097046 Corrects: 5543\n",
            "Starting epoch 19/30, LR = [0.01]\n",
            "Train step - Step 990, Loss 0.7107634544372559\n",
            "Train step - Step 1000, Loss 0.6526304483413696\n",
            "Train step - Step 1010, Loss 0.597783088684082\n",
            "Train step - Step 1020, Loss 0.7551207542419434\n",
            "Train step - Step 1030, Loss 0.8901469111442566\n",
            "Train step - Step 1040, Loss 0.7047901749610901\n",
            "Train epoch - Accuracy: 0.8073381294964028 Loss: 0.7379777395467964 Corrects: 5611\n",
            "Starting epoch 20/30, LR = [0.01]\n",
            "Train step - Step 1050, Loss 0.6836426258087158\n",
            "Train step - Step 1060, Loss 0.8601969480514526\n",
            "Train step - Step 1070, Loss 0.7720678448677063\n",
            "Train step - Step 1080, Loss 0.6501148343086243\n",
            "Train step - Step 1090, Loss 0.7860913276672363\n",
            "Train epoch - Accuracy: 0.8129496402877698 Loss: 0.7109802330483636 Corrects: 5650\n",
            "Starting epoch 21/30, LR = [0.01]\n",
            "Train step - Step 1100, Loss 0.7243831753730774\n",
            "Train step - Step 1110, Loss 0.7441617250442505\n",
            "Train step - Step 1120, Loss 0.7509796023368835\n",
            "Train step - Step 1130, Loss 0.8755756616592407\n",
            "Train step - Step 1140, Loss 0.6726685762405396\n",
            "Train step - Step 1150, Loss 0.497554749250412\n",
            "Train epoch - Accuracy: 0.8231654676258993 Loss: 0.7097494814378752 Corrects: 5721\n",
            "Starting epoch 22/30, LR = [0.01]\n",
            "Train step - Step 1160, Loss 0.5509389638900757\n",
            "Train step - Step 1170, Loss 0.5426546931266785\n",
            "Train step - Step 1180, Loss 0.6766176223754883\n",
            "Train step - Step 1190, Loss 0.7234722971916199\n",
            "Train step - Step 1200, Loss 0.645133912563324\n",
            "Train epoch - Accuracy: 0.8345323741007195 Loss: 0.6663754489095949 Corrects: 5800\n",
            "Starting epoch 23/30, LR = [0.01]\n",
            "Train step - Step 1210, Loss 0.6788451075553894\n",
            "Train step - Step 1220, Loss 0.5659817457199097\n",
            "Train step - Step 1230, Loss 0.7142058610916138\n",
            "Train step - Step 1240, Loss 0.6637071967124939\n",
            "Train step - Step 1250, Loss 0.6196231245994568\n",
            "Train step - Step 1260, Loss 0.8136732578277588\n",
            "Train epoch - Accuracy: 0.8306474820143885 Loss: 0.6712639426155914 Corrects: 5773\n",
            "Starting epoch 24/30, LR = [0.01]\n",
            "Train step - Step 1270, Loss 0.7060744762420654\n",
            "Train step - Step 1280, Loss 0.6900229454040527\n",
            "Train step - Step 1290, Loss 0.7009823322296143\n",
            "Train step - Step 1300, Loss 0.5424100756645203\n",
            "Train step - Step 1310, Loss 0.7632062435150146\n",
            "Train epoch - Accuracy: 0.8317985611510791 Loss: 0.6658222153375475 Corrects: 5781\n",
            "Starting epoch 25/30, LR = [0.01]\n",
            "Train step - Step 1320, Loss 0.7296679615974426\n",
            "Train step - Step 1330, Loss 0.590924859046936\n",
            "Train step - Step 1340, Loss 0.6494251489639282\n",
            "Train step - Step 1350, Loss 0.7765036821365356\n",
            "Train step - Step 1360, Loss 0.627314031124115\n",
            "Train step - Step 1370, Loss 0.6214892864227295\n",
            "Train epoch - Accuracy: 0.8410071942446044 Loss: 0.6453893274712048 Corrects: 5845\n",
            "Starting epoch 26/30, LR = [0.01]\n",
            "Train step - Step 1380, Loss 0.6349061131477356\n",
            "Train step - Step 1390, Loss 0.5946342945098877\n",
            "Train step - Step 1400, Loss 0.6140395402908325\n",
            "Train step - Step 1410, Loss 0.5632243156433105\n",
            "Train step - Step 1420, Loss 0.6850942969322205\n",
            "Train epoch - Accuracy: 0.8376978417266187 Loss: 0.6470781713080921 Corrects: 5822\n",
            "Starting epoch 27/30, LR = [0.01]\n",
            "Train step - Step 1430, Loss 0.6235817670822144\n",
            "Train step - Step 1440, Loss 0.6360162496566772\n",
            "Train step - Step 1450, Loss 0.7586610317230225\n",
            "Train step - Step 1460, Loss 0.6309113502502441\n",
            "Train step - Step 1470, Loss 0.6524089574813843\n",
            "Train step - Step 1480, Loss 0.42894527316093445\n",
            "Train epoch - Accuracy: 0.8499280575539568 Loss: 0.626865383515255 Corrects: 5907\n",
            "Starting epoch 28/30, LR = [0.01]\n",
            "Train step - Step 1490, Loss 0.5679376125335693\n",
            "Train step - Step 1500, Loss 0.6097000241279602\n",
            "Train step - Step 1510, Loss 0.7398855090141296\n",
            "Train step - Step 1520, Loss 0.6349247694015503\n",
            "Train step - Step 1530, Loss 0.5808964967727661\n",
            "Train epoch - Accuracy: 0.8559712230215827 Loss: 0.5926986434819888 Corrects: 5949\n",
            "Starting epoch 29/30, LR = [0.01]\n",
            "Train step - Step 1540, Loss 0.5489786863327026\n",
            "Train step - Step 1550, Loss 0.5610836148262024\n",
            "Train step - Step 1560, Loss 0.6380710601806641\n",
            "Train step - Step 1570, Loss 0.6350624561309814\n",
            "Train step - Step 1580, Loss 0.5485703945159912\n",
            "Train step - Step 1590, Loss 0.46147963404655457\n",
            "Train epoch - Accuracy: 0.862589928057554 Loss: 0.5864714926438366 Corrects: 5995\n",
            "Starting epoch 30/30, LR = [0.01]\n",
            "Train step - Step 1600, Loss 0.5533796548843384\n",
            "Train step - Step 1610, Loss 0.6184084415435791\n",
            "Train step - Step 1620, Loss 0.45766252279281616\n",
            "Train step - Step 1630, Loss 0.6179746389389038\n",
            "Train step - Step 1640, Loss 0.6185343265533447\n",
            "Train epoch - Accuracy: 0.865179856115108 Loss: 0.5692286362922448 Corrects: 6013\n",
            "Training finished in 272.91529965400696 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d23e5390>\n",
            "Constructing exemplars of class 75\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [29359, 39461, 19729, 8879, 16112, 48685, 26584, 13120, 17592, 38316, 13782, 29627, 34141, 48824, 4013, 35116, 24051, 18869, 32525, 44235, 42680, 38388, 44809, 38177, 48685, 30895, 14355, 49738, 11513, 11650, 35359, 12568, 15616, 35116, 44935, 45706, 11235, 24380, 27322, 33792, 27323, 866, 18869, 28903, 5354, 30905, 6162, 37472, 33722, 19242, 37366, 1086, 36740, 42096, 22309, 32075, 9048, 32230, 24063, 48584, 9609, 40184, 40059, 12935, 17991, 13120]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d643f450>\n",
            "Constructing exemplars of class 23\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [15508, 41211, 14482, 39197, 7483, 48386, 31916, 15556, 4242, 30283, 34004, 46838, 45446, 14313, 49319, 23623, 534, 30782, 27917, 17618, 47077, 36855, 3241, 11703, 5482, 19910, 48501, 14645, 17475, 40278, 9569, 34296, 17618, 27917, 42773, 13078, 16132, 24642, 19449, 14857, 23635, 35170, 16241, 45346, 42947, 43498, 11755, 534, 42374, 42279, 33803, 36281, 29057, 26110, 8, 4464, 33198, 40522, 31679, 45473, 40986, 15019, 27917, 26404, 17678, 46614]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d13190d0>\n",
            "Constructing exemplars of class 90\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [22385, 30405, 15048, 5522, 40599, 44403, 3932, 4214, 29166, 7369, 10515, 1476, 32236, 47333, 35065, 34463, 2825, 39126, 42425, 37141, 40943, 20775, 11327, 21664, 33154, 19408, 17938, 37412, 6110, 16872, 37862, 38923, 45708, 26196, 10515, 49007, 3485, 42982, 13515, 41630, 10082, 7717, 6643, 5179, 9383, 41586, 40811, 24838, 15415, 36107, 10106, 10810, 32100, 4116, 6452, 49691, 24838, 11102, 39315, 1607, 41586, 46249, 27488, 3140, 5179, 25505]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d8bf2310>\n",
            "Constructing exemplars of class 10\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [3059, 244, 20793, 46245, 42810, 25639, 29099, 21164, 20374, 44562, 13032, 33315, 7596, 40268, 15759, 22611, 40049, 10577, 14231, 36990, 42544, 12472, 22069, 49077, 6426, 14072, 25845, 42860, 2237, 8707, 2559, 39239, 1587, 16740, 38032, 49077, 8048, 47341, 45738, 24639, 41067, 13343, 12759, 8612, 18605, 25426, 9250, 41296, 48258, 29346, 21720, 13948, 29868, 24614, 23295, 3655, 22136, 10639, 17980, 35787, 29406, 2387, 2078, 39263, 7299, 4956]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d640dcd0>\n",
            "Constructing exemplars of class 61\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [3462, 3290, 33294, 24396, 18425, 19259, 46709, 46714, 39044, 7581, 31471, 22050, 30414, 47524, 8980, 5376, 3169, 7488, 29475, 3249, 28747, 43885, 21161, 38599, 49230, 8454, 3950, 7658, 17532, 24396, 4619, 30244, 22334, 13468, 49039, 12737, 37185, 7590, 45243, 23015, 11666, 28304, 1742, 26641, 27389, 26370, 25154, 9613, 8121, 22269, 8177, 49846, 25744, 14014, 16273, 46714, 34404, 17053, 1742, 13110, 4619, 19893, 39329, 34062, 43257, 11216]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d1391490>\n",
            "Constructing exemplars of class 76\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [1022, 827, 13572, 43104, 21864, 33764, 26669, 35859, 16345, 3245, 608, 15210, 11599, 5817, 49740, 36458, 6129, 25054, 18913, 40164, 20258, 46486, 33448, 43527, 32774, 36994, 35418, 32173, 32606, 34025, 10287, 25394, 22767, 4781, 40506, 41051, 25324, 35857, 20632, 46267, 44302, 9628, 8210, 18437, 43875, 25164, 17031, 2086, 30238, 28994, 33448, 40623, 29249, 46936, 22299, 9750, 42923, 23756, 29987, 10456, 8907, 36496, 43104, 17523, 1475, 34764]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d640cc10>\n",
            "Constructing exemplars of class 64\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [9922, 2528, 12388, 28582, 38549, 40172, 41769, 24496, 32059, 48015, 39846, 8438, 17453, 18510, 29860, 28887, 16206, 47076, 32096, 129, 22855, 11000, 7477, 42940, 6866, 2528, 38436, 25709, 44603, 40089, 9646, 38320, 111, 37114, 16312, 46137, 9474, 38302, 7380, 26083, 22931, 8905, 42744, 28041, 48364, 28582, 2809, 3188, 42394, 19694, 45657, 45914, 44078, 10418, 21170, 34549, 6366, 8438, 27856, 21786, 9606, 4009, 23338, 20056, 38609, 46137]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12e2c90>\n",
            "Constructing exemplars of class 32\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [15399, 42454, 43693, 14494, 24589, 25001, 31746, 29839, 10492, 33763, 48637, 19236, 21300, 19992, 35583, 13853, 30068, 21743, 8787, 33857, 21788, 7444, 25684, 26100, 17946, 36127, 24595, 691, 23182, 4048, 46415, 33534, 44672, 33178, 8480, 42432, 41039, 25816, 24359, 45055, 25722, 31175, 30767, 26100, 36464, 38389, 45286, 37500, 7455, 31879, 15127, 38307, 33712, 46813, 16007, 22172, 41039, 32194, 36017, 43169, 7556, 36464, 13965, 45947, 19249, 9123]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6bcc710>\n",
            "Constructing exemplars of class 24\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [21372, 17638, 35409, 20718, 24107, 49776, 12752, 37375, 29112, 5926, 3518, 41965, 27618, 16214, 43138, 37130, 24949, 37576, 31327, 1348, 16435, 36053, 19200, 37247, 29828, 37375, 34207, 48163, 21255, 23506, 41463, 1601, 20598, 13264, 44578, 46596, 20366, 23302, 14370, 12892, 16478, 23931, 21318, 4668, 35205, 24366, 7558, 8440, 49776, 23061, 36752, 25147, 21166, 16810, 41717, 47356, 23802, 47405, 34263, 14370, 5023, 25739, 15911, 35571, 12028, 1348]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d64039d0>\n",
            "Constructing exemplars of class 0\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [25084, 49677, 37763, 15753, 17151, 7577, 2076, 44450, 35908, 40274, 8045, 48994, 48100, 31301, 13409, 658, 42117, 47676, 10221, 25902, 30621, 32124, 29399, 33563, 18746, 16785, 27350, 45045, 33492, 43216, 11314, 7577, 18223, 25979, 13852, 24053, 23886, 25902, 34205, 3757, 3936, 32681, 24134, 3520, 27759, 18223, 43219, 16352, 31148, 16785, 5649, 48751, 33632, 32233, 8512, 7452, 19745, 5153, 7498, 28316, 30249, 17882, 5239, 441, 14082, 20889]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.8 1.2112764120101929\n",
            "TEST GROUP:  0.783\n",
            "TEST ALL:  0.6383333333333333\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  4000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 95, 68, 64, 56, 42, 36, 34, 32, 30, 24, 22, 20, 18, 16, 10, 6, 4, 2, 72, 76, 80, 61, 83, 81, 79, 75, 67, 65, 63, 59, 82, 49, 47, 39, 23, 21, 7, 90, 0]\n",
            "TRAIN_SET CLASSES:  [95, 83, 63, 42, 30, 6, 2, 97, 72, 36]\n",
            "VALIDATION CLASSES:  [63, 42, 36, 97, 95, 30, 83, 72, 6, 2]\n",
            "GROUP:  4\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [95, 83, 63, 42, 30, 6, 2, 97, 72, 36]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "Starting epoch 1/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 5.373459815979004\n",
            "Train step - Step 10, Loss 3.653614044189453\n",
            "Train step - Step 20, Loss 2.6724929809570312\n",
            "Train step - Step 30, Loss 2.1039834022521973\n",
            "Train step - Step 40, Loss 1.7480379343032837\n",
            "Train step - Step 50, Loss 1.478896975517273\n",
            "Train epoch - Accuracy: 0.39855699855699855 Loss: 2.6673777741615217 Corrects: 2762\n",
            "Starting epoch 2/30, LR = [0.01]\n",
            "Train step - Step 60, Loss 1.387542486190796\n",
            "Train step - Step 70, Loss 1.5025217533111572\n",
            "Train step - Step 80, Loss 1.356264352798462\n",
            "Train step - Step 90, Loss 1.4262875318527222\n",
            "Train step - Step 100, Loss 1.3217846155166626\n",
            "Train epoch - Accuracy: 0.604040404040404 Loss: 1.3846301941644579 Corrects: 4186\n",
            "Starting epoch 3/30, LR = [0.01]\n",
            "Train step - Step 110, Loss 1.354332685470581\n",
            "Train step - Step 120, Loss 1.242668628692627\n",
            "Train step - Step 130, Loss 1.052735447883606\n",
            "Train step - Step 140, Loss 1.3680340051651\n",
            "Train step - Step 150, Loss 1.3086333274841309\n",
            "Train step - Step 160, Loss 1.2408236265182495\n",
            "Train epoch - Accuracy: 0.6316017316017316 Loss: 1.27190563647778 Corrects: 4377\n",
            "Starting epoch 4/30, LR = [0.01]\n",
            "Train step - Step 170, Loss 1.2836793661117554\n",
            "Train step - Step 180, Loss 1.229285478591919\n",
            "Train step - Step 190, Loss 1.2296357154846191\n",
            "Train step - Step 200, Loss 1.2307398319244385\n",
            "Train step - Step 210, Loss 1.165184497833252\n",
            "Train epoch - Accuracy: 0.6662337662337663 Loss: 1.1789102884942146 Corrects: 4617\n",
            "Starting epoch 5/30, LR = [0.01]\n",
            "Train step - Step 220, Loss 1.0985991954803467\n",
            "Train step - Step 230, Loss 1.003084659576416\n",
            "Train step - Step 240, Loss 1.098886251449585\n",
            "Train step - Step 250, Loss 1.1503547430038452\n",
            "Train step - Step 260, Loss 1.285318374633789\n",
            "Train step - Step 270, Loss 0.9319337010383606\n",
            "Train epoch - Accuracy: 0.6858585858585858 Loss: 1.1195635898116691 Corrects: 4753\n",
            "Starting epoch 6/30, LR = [0.01]\n",
            "Train step - Step 280, Loss 1.1539335250854492\n",
            "Train step - Step 290, Loss 1.0396171808242798\n",
            "Train step - Step 300, Loss 1.0878541469573975\n",
            "Train step - Step 310, Loss 1.2280646562576294\n",
            "Train step - Step 320, Loss 1.1792778968811035\n",
            "Train epoch - Accuracy: 0.6974025974025974 Loss: 1.084567426327847 Corrects: 4833\n",
            "Starting epoch 7/30, LR = [0.01]\n",
            "Train step - Step 330, Loss 1.0210223197937012\n",
            "Train step - Step 340, Loss 1.1564263105392456\n",
            "Train step - Step 350, Loss 1.07413911819458\n",
            "Train step - Step 360, Loss 1.1499837636947632\n",
            "Train step - Step 370, Loss 1.0244475603103638\n",
            "Train step - Step 380, Loss 1.040183663368225\n",
            "Train epoch - Accuracy: 0.7082251082251082 Loss: 1.06336592850403 Corrects: 4908\n",
            "Starting epoch 8/30, LR = [0.01]\n",
            "Train step - Step 390, Loss 0.9570107460021973\n",
            "Train step - Step 400, Loss 0.9763588905334473\n",
            "Train step - Step 410, Loss 1.0187314748764038\n",
            "Train step - Step 420, Loss 1.031327724456787\n",
            "Train step - Step 430, Loss 0.9701124429702759\n",
            "Train epoch - Accuracy: 0.7213564213564213 Loss: 1.014131194482118 Corrects: 4999\n",
            "Starting epoch 9/30, LR = [0.01]\n",
            "Train step - Step 440, Loss 0.9091742038726807\n",
            "Train step - Step 450, Loss 0.9317408204078674\n",
            "Train step - Step 460, Loss 0.9648092985153198\n",
            "Train step - Step 470, Loss 1.1618643999099731\n",
            "Train step - Step 480, Loss 1.101855993270874\n",
            "Train step - Step 490, Loss 0.849875807762146\n",
            "Train epoch - Accuracy: 0.7314574314574315 Loss: 1.0058841024470364 Corrects: 5069\n",
            "Starting epoch 10/30, LR = [0.01]\n",
            "Train step - Step 500, Loss 0.9269999265670776\n",
            "Train step - Step 510, Loss 1.083396315574646\n",
            "Train step - Step 520, Loss 0.9397149682044983\n",
            "Train step - Step 530, Loss 0.9377438426017761\n",
            "Train step - Step 540, Loss 0.9340618848800659\n",
            "Train epoch - Accuracy: 0.7428571428571429 Loss: 0.9597189410596355 Corrects: 5148\n",
            "Starting epoch 11/30, LR = [0.01]\n",
            "Train step - Step 550, Loss 0.8677008152008057\n",
            "Train step - Step 560, Loss 0.9952486753463745\n",
            "Train step - Step 570, Loss 0.9869943857192993\n",
            "Train step - Step 580, Loss 0.9457355737686157\n",
            "Train step - Step 590, Loss 0.8985718488693237\n",
            "Train step - Step 600, Loss 0.8718477487564087\n",
            "Train epoch - Accuracy: 0.7479076479076479 Loss: 0.9330213535227645 Corrects: 5183\n",
            "Starting epoch 12/30, LR = [0.01]\n",
            "Train step - Step 610, Loss 1.0364359617233276\n",
            "Train step - Step 620, Loss 0.8320323824882507\n",
            "Train step - Step 630, Loss 0.7559041380882263\n",
            "Train step - Step 640, Loss 0.8192717432975769\n",
            "Train step - Step 650, Loss 1.0929664373397827\n",
            "Train epoch - Accuracy: 0.7591630591630592 Loss: 0.9050508225635017 Corrects: 5261\n",
            "Starting epoch 13/30, LR = [0.01]\n",
            "Train step - Step 660, Loss 0.8110795021057129\n",
            "Train step - Step 670, Loss 1.1452836990356445\n",
            "Train step - Step 680, Loss 0.7334111928939819\n",
            "Train step - Step 690, Loss 0.778319239616394\n",
            "Train step - Step 700, Loss 0.9019972681999207\n",
            "Train step - Step 710, Loss 1.0627061128616333\n",
            "Train epoch - Accuracy: 0.7610389610389611 Loss: 0.9082293752640013 Corrects: 5274\n",
            "Starting epoch 14/30, LR = [0.01]\n",
            "Train step - Step 720, Loss 1.0181081295013428\n",
            "Train step - Step 730, Loss 0.7825557589530945\n",
            "Train step - Step 740, Loss 0.9294903874397278\n",
            "Train step - Step 750, Loss 0.8069837093353271\n",
            "Train step - Step 760, Loss 0.873007595539093\n",
            "Train epoch - Accuracy: 0.7630591630591631 Loss: 0.8899111115743243 Corrects: 5288\n",
            "Starting epoch 15/30, LR = [0.01]\n",
            "Train step - Step 770, Loss 0.96539705991745\n",
            "Train step - Step 780, Loss 0.6649055480957031\n",
            "Train step - Step 790, Loss 0.7622959613800049\n",
            "Train step - Step 800, Loss 1.0704597234725952\n",
            "Train step - Step 810, Loss 0.8840996026992798\n",
            "Train step - Step 820, Loss 0.8077667951583862\n",
            "Train epoch - Accuracy: 0.7759018759018759 Loss: 0.8661241450179019 Corrects: 5377\n",
            "Starting epoch 16/30, LR = [0.01]\n",
            "Train step - Step 830, Loss 0.8833562135696411\n",
            "Train step - Step 840, Loss 0.9500991702079773\n",
            "Train step - Step 850, Loss 0.7949070930480957\n",
            "Train step - Step 860, Loss 0.8197616338729858\n",
            "Train step - Step 870, Loss 0.8297187089920044\n",
            "Train epoch - Accuracy: 0.7871572871572872 Loss: 0.8349832626648279 Corrects: 5455\n",
            "Starting epoch 17/30, LR = [0.01]\n",
            "Train step - Step 880, Loss 0.8567399978637695\n",
            "Train step - Step 890, Loss 0.9926919341087341\n",
            "Train step - Step 900, Loss 0.8427734375\n",
            "Train step - Step 910, Loss 0.7669001817703247\n",
            "Train step - Step 920, Loss 0.6879523992538452\n",
            "Train step - Step 930, Loss 0.8040944337844849\n",
            "Train epoch - Accuracy: 0.7862914862914863 Loss: 0.8257738793567145 Corrects: 5449\n",
            "Starting epoch 18/30, LR = [0.01]\n",
            "Train step - Step 940, Loss 0.7716056108474731\n",
            "Train step - Step 950, Loss 0.9843724370002747\n",
            "Train step - Step 960, Loss 0.8155698180198669\n",
            "Train step - Step 970, Loss 0.931485652923584\n",
            "Train step - Step 980, Loss 0.7643011808395386\n",
            "Train epoch - Accuracy: 0.791053391053391 Loss: 0.8062286234177208 Corrects: 5482\n",
            "Starting epoch 19/30, LR = [0.01]\n",
            "Train step - Step 990, Loss 0.6075915098190308\n",
            "Train step - Step 1000, Loss 0.7785089015960693\n",
            "Train step - Step 1010, Loss 0.7621622085571289\n",
            "Train step - Step 1020, Loss 0.7106366157531738\n",
            "Train step - Step 1030, Loss 0.8782079815864563\n",
            "Train step - Step 1040, Loss 0.656812310218811\n",
            "Train epoch - Accuracy: 0.8025974025974026 Loss: 0.7799999500937964 Corrects: 5562\n",
            "Starting epoch 20/30, LR = [0.01]\n",
            "Train step - Step 1050, Loss 0.8052394986152649\n",
            "Train step - Step 1060, Loss 0.7035557627677917\n",
            "Train step - Step 1070, Loss 0.8360937237739563\n",
            "Train step - Step 1080, Loss 0.6439281105995178\n",
            "Train step - Step 1090, Loss 0.8198180198669434\n",
            "Train epoch - Accuracy: 0.8005772005772006 Loss: 0.7828820853006272 Corrects: 5548\n",
            "Starting epoch 21/30, LR = [0.01]\n",
            "Train step - Step 1100, Loss 0.6568119525909424\n",
            "Train step - Step 1110, Loss 0.7843469977378845\n",
            "Train step - Step 1120, Loss 0.8137409687042236\n",
            "Train step - Step 1130, Loss 0.8152614831924438\n",
            "Train step - Step 1140, Loss 0.7150024771690369\n",
            "Train step - Step 1150, Loss 0.7445759773254395\n",
            "Train epoch - Accuracy: 0.8111111111111111 Loss: 0.7650534327756103 Corrects: 5621\n",
            "Starting epoch 22/30, LR = [0.01]\n",
            "Train step - Step 1160, Loss 0.8161139488220215\n",
            "Train step - Step 1170, Loss 0.753247857093811\n",
            "Train step - Step 1180, Loss 0.648632287979126\n",
            "Train step - Step 1190, Loss 0.8463607430458069\n",
            "Train step - Step 1200, Loss 0.7003319263458252\n",
            "Train epoch - Accuracy: 0.8194805194805195 Loss: 0.733241744096489 Corrects: 5679\n",
            "Starting epoch 23/30, LR = [0.01]\n",
            "Train step - Step 1210, Loss 0.6300621032714844\n",
            "Train step - Step 1220, Loss 0.6815181970596313\n",
            "Train step - Step 1230, Loss 0.7596320509910583\n",
            "Train step - Step 1240, Loss 0.6829662322998047\n",
            "Train step - Step 1250, Loss 0.8108996748924255\n",
            "Train step - Step 1260, Loss 0.628944993019104\n",
            "Train epoch - Accuracy: 0.8090909090909091 Loss: 0.7682251658102479 Corrects: 5607\n",
            "Starting epoch 24/30, LR = [0.01]\n",
            "Train step - Step 1270, Loss 0.8099077343940735\n",
            "Train step - Step 1280, Loss 0.780350923538208\n",
            "Train step - Step 1290, Loss 0.6467756032943726\n",
            "Train step - Step 1300, Loss 0.9516135454177856\n",
            "Train step - Step 1310, Loss 0.629253625869751\n",
            "Train epoch - Accuracy: 0.8243867243867243 Loss: 0.7302117289918841 Corrects: 5713\n",
            "Starting epoch 25/30, LR = [0.01]\n",
            "Train step - Step 1320, Loss 0.7028462886810303\n",
            "Train step - Step 1330, Loss 0.8782973289489746\n",
            "Train step - Step 1340, Loss 0.6824581027030945\n",
            "Train step - Step 1350, Loss 0.6477843523025513\n",
            "Train step - Step 1360, Loss 0.5872334241867065\n",
            "Train step - Step 1370, Loss 0.6669888496398926\n",
            "Train epoch - Accuracy: 0.8262626262626263 Loss: 0.6995085017161624 Corrects: 5726\n",
            "Starting epoch 26/30, LR = [0.01]\n",
            "Train step - Step 1380, Loss 0.7063368558883667\n",
            "Train step - Step 1390, Loss 0.5719773769378662\n",
            "Train step - Step 1400, Loss 0.6435918211936951\n",
            "Train step - Step 1410, Loss 0.6294741630554199\n",
            "Train step - Step 1420, Loss 0.5776089429855347\n",
            "Train epoch - Accuracy: 0.8347763347763347 Loss: 0.6860147769144948 Corrects: 5785\n",
            "Starting epoch 27/30, LR = [0.01]\n",
            "Train step - Step 1430, Loss 0.7378178834915161\n",
            "Train step - Step 1440, Loss 0.5730424523353577\n",
            "Train step - Step 1450, Loss 0.6980646848678589\n",
            "Train step - Step 1460, Loss 0.6225231885910034\n",
            "Train step - Step 1470, Loss 0.7157799005508423\n",
            "Train step - Step 1480, Loss 0.7486373782157898\n",
            "Train epoch - Accuracy: 0.8350649350649351 Loss: 0.6841609691472624 Corrects: 5787\n",
            "Starting epoch 28/30, LR = [0.01]\n",
            "Train step - Step 1490, Loss 0.7262923121452332\n",
            "Train step - Step 1500, Loss 0.7129498720169067\n",
            "Train step - Step 1510, Loss 0.6150256991386414\n",
            "Train step - Step 1520, Loss 0.6982236504554749\n",
            "Train step - Step 1530, Loss 0.7275844812393188\n",
            "Train epoch - Accuracy: 0.8455988455988456 Loss: 0.6573326583548542 Corrects: 5860\n",
            "Starting epoch 29/30, LR = [0.01]\n",
            "Train step - Step 1540, Loss 0.7451562285423279\n",
            "Train step - Step 1550, Loss 0.6137784123420715\n",
            "Train step - Step 1560, Loss 0.5978531837463379\n",
            "Train step - Step 1570, Loss 0.5855374336242676\n",
            "Train step - Step 1580, Loss 0.6571690440177917\n",
            "Train step - Step 1590, Loss 0.5837622284889221\n",
            "Train epoch - Accuracy: 0.8425685425685425 Loss: 0.6516014188227027 Corrects: 5839\n",
            "Starting epoch 30/30, LR = [0.01]\n",
            "Train step - Step 1600, Loss 0.7826125621795654\n",
            "Train step - Step 1610, Loss 0.6034569144248962\n",
            "Train step - Step 1620, Loss 0.6002374887466431\n",
            "Train step - Step 1630, Loss 0.6160006523132324\n",
            "Train step - Step 1640, Loss 0.652415931224823\n",
            "Train epoch - Accuracy: 0.845021645021645 Loss: 0.6452331741134841 Corrects: 5856\n",
            "Training finished in 272.35325360298157 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0, 95, 83, 63, 42, 30, 6, 2, 97, 72, 36]\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d125fed0>\n",
            "Constructing exemplars of class 95\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [6109, 18410, 38920, 28993, 20478, 16946, 14251, 43178, 18560, 3190, 38855, 4736, 47545, 26558, 25996, 39309, 15787, 14821, 24552, 42567, 38952, 545, 14502, 27078, 30239, 38072, 4145, 31424, 5579, 31984, 14626, 38920, 1568, 29012, 46933, 11454, 39105, 36988, 22104, 17158, 18784, 32897, 26713, 36621, 47663, 23702, 21287, 33476, 13049, 37946]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12c5910>\n",
            "Constructing exemplars of class 83\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [45057, 26681, 32425, 45576, 34291, 42896, 21546, 24175, 20382, 40088, 35374, 3183, 42061, 27481, 43177, 23406, 22137, 48695, 37455, 14953, 23548, 3884, 40403, 47910, 47209, 15579, 3199, 22542, 21599, 17004, 19142, 4920, 26721, 5527, 37455, 26997, 14723, 13645, 15169, 32336, 37410, 3199, 44340, 44996, 11917, 25975, 44151, 19173, 3069, 37489]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12fe290>\n",
            "Constructing exemplars of class 63\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [29740, 37373, 3065, 45755, 6009, 33774, 1049, 37482, 39817, 6018, 35809, 30795, 2735, 2900, 13430, 4066, 24543, 16026, 3065, 28941, 47674, 13390, 2462, 47486, 43262, 49120, 8887, 15682, 31824, 3339, 22056, 37307, 29967, 31570, 28602, 41040, 17865, 6267, 6787, 44922, 46240, 25158, 27977, 4906, 49011, 27268, 44665, 38967, 8003, 38067]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6bcc890>\n",
            "Constructing exemplars of class 42\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [11588, 18046, 22536, 38984, 1780, 37854, 26109, 3743, 48431, 13914, 48144, 19455, 31743, 35340, 11695, 31761, 18899, 12600, 18090, 40134, 7937, 22092, 21852, 42563, 1780, 15505, 17755, 42965, 37323, 41357, 14780, 25242, 11196, 42816, 5065, 44616, 6000, 39728, 14558, 1515, 316, 29861, 7623, 3828, 14603, 37318, 8214, 40022, 15115, 6887]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6ec8210>\n",
            "Constructing exemplars of class 30\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [41101, 40831, 7451, 11290, 4883, 22244, 10526, 8726, 29270, 15776, 42265, 34360, 12161, 17463, 10434, 16088, 28423, 36587, 1612, 46979, 38775, 28039, 47038, 21934, 10589, 4057, 12458, 32680, 20381, 35421, 14471, 44160, 40798, 41554, 46184, 1612, 7451, 48707, 2238, 24811, 35592, 34771, 9913, 3367, 5370, 22281, 22991, 34693, 12458, 20545]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d132bad0>\n",
            "Constructing exemplars of class 6\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [17177, 36545, 43822, 21593, 25499, 18979, 12302, 16428, 24106, 22511, 8015, 19560, 9929, 25473, 25874, 2121, 34991, 18269, 31914, 46819, 20370, 43666, 5877, 11088, 4286, 40740, 618, 41529, 20878, 48404, 3919, 38848, 6816, 31817, 39806, 19542, 1762, 37309, 42605, 32290, 30566, 17234, 21456, 24106, 26872, 36254, 26268, 7154, 48404, 13721]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d2426890>\n",
            "Constructing exemplars of class 2\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [795, 41632, 7066, 27331, 3821, 37926, 12363, 33952, 21199, 13524, 49707, 48645, 15670, 11654, 26483, 18725, 33218, 3613, 14087, 16339, 47261, 33218, 30200, 33986, 27595, 37530, 8720, 9242, 7474, 28575, 15946, 4621, 11267, 30122, 2192, 13008, 18029, 45522, 41095, 38437, 31300, 11042, 1511, 8720, 32941, 33361, 27627, 13969, 14428, 18763]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d23e6250>\n",
            "Constructing exemplars of class 97\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [45500, 22369, 35970, 37424, 26191, 22091, 29182, 28762, 33770, 18898, 9207, 28610, 1687, 20073, 37541, 13319, 24866, 44273, 6930, 5300, 48042, 30822, 46756, 47811, 19680, 8568, 22424, 12109, 15542, 21311, 39785, 45858, 36498, 6765, 39897, 22956, 46630, 32465, 23369, 49451, 38222, 22433, 36866, 8417, 5786, 19829, 8376, 9207, 30581, 46998]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6ec8210>\n",
            "Constructing exemplars of class 72\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [20113, 37279, 33192, 17951, 40880, 15072, 5278, 5144, 31199, 32773, 42269, 7837, 11477, 41885, 41208, 30465, 48053, 11477, 39982, 27914, 22333, 4829, 3938, 49292, 21453, 22707, 32650, 43386, 33989, 23304, 62, 3899, 486, 5225, 37519, 32275, 8629, 7918, 15816, 43771, 38358, 25619, 27894, 40471, 24686, 10555, 6724, 19984, 33212, 4337]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d8026b10>\n",
            "Constructing exemplars of class 36\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [4102, 10187, 24960, 36456, 26336, 19104, 11191, 14113, 42092, 6080, 22506, 23256, 35573, 10465, 26779, 25165, 2022, 4118, 16090, 16169, 38917, 27450, 14508, 22228, 34405, 38828, 36849, 17929, 24152, 28995, 35913, 31632, 42996, 42239, 24767, 49056, 33779, 1886, 4758, 21761, 9740, 48424, 17816, 11563, 48956, 37125, 15907, 4967, 3610, 26336]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.7 1.5203022956848145\n",
            "TEST GROUP:  0.711\n",
            "TEST ALL:  0.557\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  5000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 80, 97, 93, 85, 81, 65, 61, 49, 21, 9, 96, 76, 83, 72, 68, 64, 56, 36, 32, 24, 20, 16, 4, 2, 6, 10, 18, 79, 75, 67, 63, 59, 55, 47, 39, 31, 23, 19, 7, 98, 94, 90, 82, 54, 42, 34, 30, 22, 0]\n",
            "TRAIN_SET CLASSES:  [55, 31, 19, 98, 94, 54, 93, 85, 9, 96]\n",
            "VALIDATION CLASSES:  [55, 54, 98, 96, 31, 94, 93, 85, 19, 9]\n",
            "GROUP:  5\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [55, 31, 19, 98, 94, 54, 93, 85, 9, 96]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "Starting epoch 1/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 6.180552959442139\n",
            "Train step - Step 10, Loss 4.11382532119751\n",
            "Train step - Step 20, Loss 2.8454530239105225\n",
            "Train step - Step 30, Loss 2.263998508453369\n",
            "Train step - Step 40, Loss 1.8148572444915771\n",
            "Train step - Step 50, Loss 1.3579148054122925\n",
            "Train epoch - Accuracy: 0.4184172661870504 Loss: 2.9181377218781615 Corrects: 2908\n",
            "Starting epoch 2/30, LR = [0.01]\n",
            "Train step - Step 60, Loss 1.4831132888793945\n",
            "Train step - Step 70, Loss 1.2848678827285767\n",
            "Train step - Step 80, Loss 1.155157208442688\n",
            "Train step - Step 90, Loss 1.1972081661224365\n",
            "Train step - Step 100, Loss 1.0737388134002686\n",
            "Train epoch - Accuracy: 0.6505035971223022 Loss: 1.2914642517172175 Corrects: 4521\n",
            "Starting epoch 3/30, LR = [0.01]\n",
            "Train step - Step 110, Loss 1.1146924495697021\n",
            "Train step - Step 120, Loss 0.9328364133834839\n",
            "Train step - Step 130, Loss 1.0873386859893799\n",
            "Train step - Step 140, Loss 1.188591480255127\n",
            "Train step - Step 150, Loss 1.2495837211608887\n",
            "Train step - Step 160, Loss 0.946648120880127\n",
            "Train epoch - Accuracy: 0.6949640287769784 Loss: 1.138410220918038 Corrects: 4830\n",
            "Starting epoch 4/30, LR = [0.01]\n",
            "Train step - Step 170, Loss 1.032278299331665\n",
            "Train step - Step 180, Loss 1.110520839691162\n",
            "Train step - Step 190, Loss 1.0941071510314941\n",
            "Train step - Step 200, Loss 1.100771427154541\n",
            "Train step - Step 210, Loss 1.0913364887237549\n",
            "Train epoch - Accuracy: 0.7148201438848921 Loss: 1.0467071692377543 Corrects: 4968\n",
            "Starting epoch 5/30, LR = [0.01]\n",
            "Train step - Step 220, Loss 0.9271912574768066\n",
            "Train step - Step 230, Loss 0.9727569818496704\n",
            "Train step - Step 240, Loss 0.9325953125953674\n",
            "Train step - Step 250, Loss 0.8766757845878601\n",
            "Train step - Step 260, Loss 1.0741970539093018\n",
            "Train step - Step 270, Loss 0.8784998655319214\n",
            "Train epoch - Accuracy: 0.7438848920863309 Loss: 0.9893245972489282 Corrects: 5170\n",
            "Starting epoch 6/30, LR = [0.01]\n",
            "Train step - Step 280, Loss 0.8641347885131836\n",
            "Train step - Step 290, Loss 0.916816771030426\n",
            "Train step - Step 300, Loss 0.9822704792022705\n",
            "Train step - Step 310, Loss 0.922368049621582\n",
            "Train step - Step 320, Loss 0.9913311004638672\n",
            "Train epoch - Accuracy: 0.7506474820143885 Loss: 0.950895621210551 Corrects: 5217\n",
            "Starting epoch 7/30, LR = [0.01]\n",
            "Train step - Step 330, Loss 0.9080662727355957\n",
            "Train step - Step 340, Loss 0.9149391651153564\n",
            "Train step - Step 350, Loss 0.8460832238197327\n",
            "Train step - Step 360, Loss 0.8868631720542908\n",
            "Train step - Step 370, Loss 0.950232982635498\n",
            "Train step - Step 380, Loss 0.7718467712402344\n",
            "Train epoch - Accuracy: 0.7594244604316547 Loss: 0.9234525584659988 Corrects: 5278\n",
            "Starting epoch 8/30, LR = [0.01]\n",
            "Train step - Step 390, Loss 0.822440505027771\n",
            "Train step - Step 400, Loss 0.7619167566299438\n",
            "Train step - Step 410, Loss 0.9533591270446777\n",
            "Train step - Step 420, Loss 1.0017517805099487\n",
            "Train step - Step 430, Loss 0.8435707092285156\n",
            "Train epoch - Accuracy: 0.7628776978417267 Loss: 0.9102222404205542 Corrects: 5302\n",
            "Starting epoch 9/30, LR = [0.01]\n",
            "Train step - Step 440, Loss 0.8011837005615234\n",
            "Train step - Step 450, Loss 0.892141580581665\n",
            "Train step - Step 460, Loss 0.7334936857223511\n",
            "Train step - Step 470, Loss 1.030297040939331\n",
            "Train step - Step 480, Loss 0.7728725075721741\n",
            "Train step - Step 490, Loss 0.8238397836685181\n",
            "Train epoch - Accuracy: 0.7785611510791367 Loss: 0.8596691747706571 Corrects: 5411\n",
            "Starting epoch 10/30, LR = [0.01]\n",
            "Train step - Step 500, Loss 0.7531023025512695\n",
            "Train step - Step 510, Loss 0.9778285026550293\n",
            "Train step - Step 520, Loss 0.9528743028640747\n",
            "Train step - Step 530, Loss 0.730466365814209\n",
            "Train step - Step 540, Loss 0.7657778263092041\n",
            "Train epoch - Accuracy: 0.7877697841726619 Loss: 0.8336554539975503 Corrects: 5475\n",
            "Starting epoch 11/30, LR = [0.01]\n",
            "Train step - Step 550, Loss 0.7816402316093445\n",
            "Train step - Step 560, Loss 0.6031080484390259\n",
            "Train step - Step 570, Loss 0.8633819222450256\n",
            "Train step - Step 580, Loss 0.7482743859291077\n",
            "Train step - Step 590, Loss 0.7983417510986328\n",
            "Train step - Step 600, Loss 0.8686378002166748\n",
            "Train epoch - Accuracy: 0.8024460431654676 Loss: 0.7980199087952539 Corrects: 5577\n",
            "Starting epoch 12/30, LR = [0.01]\n",
            "Train step - Step 610, Loss 0.8273664712905884\n",
            "Train step - Step 620, Loss 0.8484533429145813\n",
            "Train step - Step 630, Loss 0.7464349269866943\n",
            "Train step - Step 640, Loss 0.7566976547241211\n",
            "Train step - Step 650, Loss 0.7951521873474121\n",
            "Train epoch - Accuracy: 0.8024460431654676 Loss: 0.7931170127717712 Corrects: 5577\n",
            "Starting epoch 13/30, LR = [0.01]\n",
            "Train step - Step 660, Loss 0.6888726949691772\n",
            "Train step - Step 670, Loss 0.7731896638870239\n",
            "Train step - Step 680, Loss 0.8806660771369934\n",
            "Train step - Step 690, Loss 0.7800257802009583\n",
            "Train step - Step 700, Loss 0.8368983864784241\n",
            "Train step - Step 710, Loss 0.7145943641662598\n",
            "Train epoch - Accuracy: 0.8093525179856115 Loss: 0.7638972119461718 Corrects: 5625\n",
            "Starting epoch 14/30, LR = [0.01]\n",
            "Train step - Step 720, Loss 0.742104709148407\n",
            "Train step - Step 730, Loss 0.7022074460983276\n",
            "Train step - Step 740, Loss 0.7103356122970581\n",
            "Train step - Step 750, Loss 0.7516812086105347\n",
            "Train step - Step 760, Loss 0.6871962547302246\n",
            "Train epoch - Accuracy: 0.8223021582733813 Loss: 0.7318818573814502 Corrects: 5715\n",
            "Starting epoch 15/30, LR = [0.01]\n",
            "Train step - Step 770, Loss 0.7128425240516663\n",
            "Train step - Step 780, Loss 0.6519647240638733\n",
            "Train step - Step 790, Loss 0.620906412601471\n",
            "Train step - Step 800, Loss 0.6658846139907837\n",
            "Train step - Step 810, Loss 0.7045897245407104\n",
            "Train step - Step 820, Loss 0.6555362939834595\n",
            "Train epoch - Accuracy: 0.8282014388489208 Loss: 0.7137736065267659 Corrects: 5756\n",
            "Starting epoch 16/30, LR = [0.01]\n",
            "Train step - Step 830, Loss 0.7272439002990723\n",
            "Train step - Step 840, Loss 0.7445436716079712\n",
            "Train step - Step 850, Loss 0.7262676954269409\n",
            "Train step - Step 860, Loss 0.8102220296859741\n",
            "Train step - Step 870, Loss 0.735119640827179\n",
            "Train epoch - Accuracy: 0.8276258992805755 Loss: 0.7152696906405387 Corrects: 5752\n",
            "Starting epoch 17/30, LR = [0.01]\n",
            "Train step - Step 880, Loss 0.5785982608795166\n",
            "Train step - Step 890, Loss 0.5328409671783447\n",
            "Train step - Step 900, Loss 0.5860658884048462\n",
            "Train step - Step 910, Loss 0.7318811416625977\n",
            "Train step - Step 920, Loss 0.8144853115081787\n",
            "Train step - Step 930, Loss 0.8996670246124268\n",
            "Train epoch - Accuracy: 0.8446043165467626 Loss: 0.6784752238054069 Corrects: 5870\n",
            "Starting epoch 18/30, LR = [0.01]\n",
            "Train step - Step 940, Loss 0.6197095513343811\n",
            "Train step - Step 950, Loss 0.6019865274429321\n",
            "Train step - Step 960, Loss 0.6747769713401794\n",
            "Train step - Step 970, Loss 0.8167290687561035\n",
            "Train step - Step 980, Loss 0.5990289449691772\n",
            "Train epoch - Accuracy: 0.8441726618705035 Loss: 0.6722451548439136 Corrects: 5867\n",
            "Starting epoch 19/30, LR = [0.01]\n",
            "Train step - Step 990, Loss 0.5945720076560974\n",
            "Train step - Step 1000, Loss 0.7535305619239807\n",
            "Train step - Step 1010, Loss 0.8028831481933594\n",
            "Train step - Step 1020, Loss 0.6150467395782471\n",
            "Train step - Step 1030, Loss 0.6266103386878967\n",
            "Train step - Step 1040, Loss 0.6599474549293518\n",
            "Train epoch - Accuracy: 0.8538129496402878 Loss: 0.6443842205383795 Corrects: 5934\n",
            "Starting epoch 20/30, LR = [0.01]\n",
            "Train step - Step 1050, Loss 0.5159139633178711\n",
            "Train step - Step 1060, Loss 0.6509897112846375\n",
            "Train step - Step 1070, Loss 0.7357747554779053\n",
            "Train step - Step 1080, Loss 0.51738440990448\n",
            "Train step - Step 1090, Loss 0.7243077754974365\n",
            "Train epoch - Accuracy: 0.8506474820143884 Loss: 0.6578476814914951 Corrects: 5912\n",
            "Starting epoch 21/30, LR = [0.01]\n",
            "Train step - Step 1100, Loss 0.48862096667289734\n",
            "Train step - Step 1110, Loss 0.5739309787750244\n",
            "Train step - Step 1120, Loss 0.5721084475517273\n",
            "Train step - Step 1130, Loss 0.5902057886123657\n",
            "Train step - Step 1140, Loss 0.6127101182937622\n",
            "Train step - Step 1150, Loss 0.651599645614624\n",
            "Train epoch - Accuracy: 0.8671942446043166 Loss: 0.6122411286916665 Corrects: 6027\n",
            "Starting epoch 22/30, LR = [0.01]\n",
            "Train step - Step 1160, Loss 0.6583128571510315\n",
            "Train step - Step 1170, Loss 0.7544227242469788\n",
            "Train step - Step 1180, Loss 0.512643575668335\n",
            "Train step - Step 1190, Loss 0.6223374605178833\n",
            "Train step - Step 1200, Loss 0.4738810062408447\n",
            "Train epoch - Accuracy: 0.8624460431654676 Loss: 0.632820485581597 Corrects: 5994\n",
            "Starting epoch 23/30, LR = [0.01]\n",
            "Train step - Step 1210, Loss 0.6501903533935547\n",
            "Train step - Step 1220, Loss 0.48336732387542725\n",
            "Train step - Step 1230, Loss 0.5872403979301453\n",
            "Train step - Step 1240, Loss 0.5245871543884277\n",
            "Train step - Step 1250, Loss 0.5665591955184937\n",
            "Train step - Step 1260, Loss 0.6571460366249084\n",
            "Train epoch - Accuracy: 0.8602877697841727 Loss: 0.6116043926657533 Corrects: 5979\n",
            "Starting epoch 24/30, LR = [0.01]\n",
            "Train step - Step 1270, Loss 0.5521925687789917\n",
            "Train step - Step 1280, Loss 0.4805809557437897\n",
            "Train step - Step 1290, Loss 0.5107150673866272\n",
            "Train step - Step 1300, Loss 0.6722164154052734\n",
            "Train step - Step 1310, Loss 0.7114623785018921\n",
            "Train epoch - Accuracy: 0.8705035971223022 Loss: 0.5928250806794749 Corrects: 6050\n",
            "Starting epoch 25/30, LR = [0.01]\n",
            "Train step - Step 1320, Loss 0.4450770318508148\n",
            "Train step - Step 1330, Loss 0.5812860727310181\n",
            "Train step - Step 1340, Loss 0.4278104305267334\n",
            "Train step - Step 1350, Loss 0.5200986862182617\n",
            "Train step - Step 1360, Loss 0.6035352349281311\n",
            "Train step - Step 1370, Loss 0.5681424140930176\n",
            "Train epoch - Accuracy: 0.8776978417266187 Loss: 0.5900800133094514 Corrects: 6100\n",
            "Starting epoch 26/30, LR = [0.01]\n",
            "Train step - Step 1380, Loss 0.5372207760810852\n",
            "Train step - Step 1390, Loss 0.7309240698814392\n",
            "Train step - Step 1400, Loss 0.5762280821800232\n",
            "Train step - Step 1410, Loss 0.5406845808029175\n",
            "Train step - Step 1420, Loss 0.5968708992004395\n",
            "Train epoch - Accuracy: 0.8805755395683453 Loss: 0.565193058638264 Corrects: 6120\n",
            "Starting epoch 27/30, LR = [0.01]\n",
            "Train step - Step 1430, Loss 0.5766466856002808\n",
            "Train step - Step 1440, Loss 0.6628041863441467\n",
            "Train step - Step 1450, Loss 0.5314490795135498\n",
            "Train step - Step 1460, Loss 0.5137850642204285\n",
            "Train step - Step 1470, Loss 0.5146821141242981\n",
            "Train step - Step 1480, Loss 0.6256611943244934\n",
            "Train epoch - Accuracy: 0.8837410071942446 Loss: 0.5546175637348093 Corrects: 6142\n",
            "Starting epoch 28/30, LR = [0.01]\n",
            "Train step - Step 1490, Loss 0.527906596660614\n",
            "Train step - Step 1500, Loss 0.4586052894592285\n",
            "Train step - Step 1510, Loss 0.47058603167533875\n",
            "Train step - Step 1520, Loss 0.6954258680343628\n",
            "Train step - Step 1530, Loss 0.5907109975814819\n",
            "Train epoch - Accuracy: 0.8853237410071942 Loss: 0.5473627015669569 Corrects: 6153\n",
            "Starting epoch 29/30, LR = [0.01]\n",
            "Train step - Step 1540, Loss 0.49298104643821716\n",
            "Train step - Step 1550, Loss 0.5689568519592285\n",
            "Train step - Step 1560, Loss 0.46001023054122925\n",
            "Train step - Step 1570, Loss 0.4646874666213989\n",
            "Train step - Step 1580, Loss 0.44306308031082153\n",
            "Train step - Step 1590, Loss 0.5271376371383667\n",
            "Train epoch - Accuracy: 0.8896402877697842 Loss: 0.532117500562462 Corrects: 6183\n",
            "Starting epoch 30/30, LR = [0.01]\n",
            "Train step - Step 1600, Loss 0.5374148488044739\n",
            "Train step - Step 1610, Loss 0.5493297576904297\n",
            "Train step - Step 1620, Loss 0.5058349370956421\n",
            "Train step - Step 1630, Loss 0.5871375799179077\n",
            "Train step - Step 1640, Loss 0.5365266799926758\n",
            "Train epoch - Accuracy: 0.8984172661870503 Loss: 0.5172275579404488 Corrects: 6244\n",
            "Training finished in 274.9055700302124 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0, 95, 83, 63, 42, 30, 6, 2, 97, 72, 36, 55, 31, 19, 98, 94, 54, 93, 85, 9, 96]\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d23ee050>\n",
            "Constructing exemplars of class 55\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [42977, 3480, 12017, 42884, 43645, 12474, 7463, 30662, 48661, 24186, 7463, 30662, 8039, 10318, 5014, 36319, 23706, 1874, 46861, 8039, 49510, 19451, 17341, 47591, 3146, 46343, 35896, 38832, 40380, 21810, 45731, 45027, 34547, 23485, 36414, 14118, 39270, 8483, 24230, 43304]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d139a4d0>\n",
            "Constructing exemplars of class 31\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [3956, 1183, 38882, 13305, 26118, 21853, 5329, 7632, 36597, 5719, 42873, 29841, 21101, 7055, 729, 23628, 28294, 44547, 28600, 8073, 18367, 9283, 33313, 41426, 16631, 23412, 3898, 36413, 28227, 27279, 27559, 38085, 35039, 30862, 27391, 1632, 42530, 47117, 41502, 27652]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d66fe210>\n",
            "Constructing exemplars of class 19\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [18457, 22978, 18120, 29592, 3416, 7574, 40751, 9220, 33059, 47945, 31690, 224, 9420, 8687, 20489, 29808, 43499, 30947, 23346, 40063, 5494, 37885, 42182, 6658, 31885, 32371, 16010, 15495, 16642, 48061, 16162, 27164, 9224, 37317, 20440, 49453, 30072, 2454, 9027, 38846]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12f37d0>\n",
            "Constructing exemplars of class 98\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [26654, 28937, 9334, 6159, 21280, 39073, 35640, 11825, 43399, 39330, 46207, 21863, 1311, 33124, 20594, 35711, 9784, 37273, 40633, 888, 42841, 38365, 12513, 40126, 41617, 21863, 48263, 44278, 13666, 33834, 32017, 45343, 3677, 13588, 26296, 2026, 11607, 43556, 49807, 1328]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12fe5d0>\n",
            "Constructing exemplars of class 94\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [29603, 16813, 43131, 12406, 48316, 45232, 21961, 5106, 47933, 27952, 17302, 32861, 30951, 2562, 2486, 36318, 12137, 11579, 34467, 27025, 46096, 35248, 18882, 39092, 42599, 7307, 29501, 13284, 8822, 47292, 7409, 15951, 46995, 31280, 5346, 8001, 21239, 7641, 2160, 23058]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d8bfe250>\n",
            "Constructing exemplars of class 54\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [15562, 44079, 31507, 36206, 15609, 37811, 2701, 1577, 41107, 23478, 46437, 34619, 29032, 30030, 22656, 16858, 18721, 864, 22349, 39116, 32812, 21344, 8910, 14049, 19730, 18291, 31849, 14855, 9564, 510, 18056, 11407, 1231, 27780, 28202, 14184, 35051, 13428, 40242, 13070]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71df4f8590>\n",
            "Constructing exemplars of class 93\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [48452, 6769, 30747, 14657, 24762, 38606, 49709, 41870, 39927, 39295, 43772, 17819, 47219, 6631, 5781, 16419, 30525, 46092, 32354, 8311, 34843, 32790, 44569, 12776, 49958, 30427, 38430, 40766, 42654, 17719, 25452, 19352, 47562, 6769, 47762, 22985, 21451, 31441, 18352, 33849]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d23ee5d0>\n",
            "Constructing exemplars of class 85\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [8231, 46466, 5553, 46988, 31442, 38280, 38942, 21768, 33919, 2199, 14097, 39150, 13613, 46331, 6983, 35958, 23855, 45654, 34615, 32831, 28094, 38961, 27919, 43184, 17407, 7618, 35517, 13206, 43254, 3028, 23434, 39226, 11452, 25835, 2158, 49501, 25928, 17487, 14554, 29349]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d3cc8c90>\n",
            "Constructing exemplars of class 9\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [48666, 19838, 220, 11705, 9178, 26881, 48002, 18408, 22930, 17218, 40119, 21840, 13866, 8837, 35083, 34658, 2647, 12691, 40555, 45242, 20643, 14612, 7998, 660, 398, 36495, 40902, 39391, 25570, 9016, 19148, 31573, 27026, 38983, 44906, 30542, 48338, 25184, 8576, 13632]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d248ca50>\n",
            "Constructing exemplars of class 96\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [25297, 40701, 31046, 35249, 23118, 13491, 2624, 14073, 10063, 30270, 47766, 42314, 29435, 14720, 2359, 2056, 27135, 19457, 24681, 10454, 9244, 17398, 37342, 41602, 33277, 23665, 44895, 36814, 14915, 2392, 37082, 25886, 10056, 2951, 20234, 6421, 2108, 47881, 6033, 41214]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.7 1.5228768587112427\n",
            "TEST GROUP:  0.779\n",
            "TEST ALL:  0.5016\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  6000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [99, 95, 85, 81, 65, 61, 57, 49, 45, 21, 13, 9, 96, 88, 80, 76, 72, 68, 64, 60, 56, 40, 36, 32, 24, 20, 16, 8, 4, 93, 97, 2, 15, 83, 79, 75, 67, 63, 59, 55, 47, 39, 31, 23, 19, 7, 6, 98, 94, 90, 82, 54, 42, 34, 30, 22, 18, 14, 10, 0]\n",
            "TRAIN_SET CLASSES:  [99, 15, 14, 57, 45, 13, 88, 60, 40, 8]\n",
            "VALIDATION CLASSES:  [60, 57, 45, 40, 99, 88, 15, 14, 13, 8]\n",
            "GROUP:  6\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [99, 15, 14, 57, 45, 13, 88, 60, 40, 8]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "Starting epoch 1/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 6.787918567657471\n",
            "Train step - Step 10, Loss 4.424331188201904\n",
            "Train step - Step 20, Loss 3.144716739654541\n",
            "Train step - Step 30, Loss 2.0135467052459717\n",
            "Train step - Step 40, Loss 1.5907247066497803\n",
            "Train step - Step 50, Loss 1.7038624286651611\n",
            "Train epoch - Accuracy: 0.442158273381295 Loss: 2.9502545069276 Corrects: 3073\n",
            "Starting epoch 2/30, LR = [0.01]\n",
            "Train step - Step 60, Loss 1.3100835084915161\n",
            "Train step - Step 70, Loss 1.4555895328521729\n",
            "Train step - Step 80, Loss 1.2876752614974976\n",
            "Train step - Step 90, Loss 1.197856068611145\n",
            "Train step - Step 100, Loss 1.1382505893707275\n",
            "Train epoch - Accuracy: 0.6874820143884892 Loss: 1.2404114015496892 Corrects: 4778\n",
            "Starting epoch 3/30, LR = [0.01]\n",
            "Train step - Step 110, Loss 0.9584310054779053\n",
            "Train step - Step 120, Loss 1.1061019897460938\n",
            "Train step - Step 130, Loss 1.1163464784622192\n",
            "Train step - Step 140, Loss 1.089888334274292\n",
            "Train step - Step 150, Loss 1.0838165283203125\n",
            "Train step - Step 160, Loss 0.9973484873771667\n",
            "Train epoch - Accuracy: 0.722589928057554 Loss: 1.0707021023043626 Corrects: 5022\n",
            "Starting epoch 4/30, LR = [0.01]\n",
            "Train step - Step 170, Loss 1.0825361013412476\n",
            "Train step - Step 180, Loss 0.9612179398536682\n",
            "Train step - Step 190, Loss 0.9317951202392578\n",
            "Train step - Step 200, Loss 1.0559055805206299\n",
            "Train step - Step 210, Loss 0.9650425314903259\n",
            "Train epoch - Accuracy: 0.7496402877697842 Loss: 1.0002798546646996 Corrects: 5210\n",
            "Starting epoch 5/30, LR = [0.01]\n",
            "Train step - Step 220, Loss 0.8356165885925293\n",
            "Train step - Step 230, Loss 0.9013256430625916\n",
            "Train step - Step 240, Loss 0.925748884677887\n",
            "Train step - Step 250, Loss 0.8714450597763062\n",
            "Train step - Step 260, Loss 0.9120398759841919\n",
            "Train step - Step 270, Loss 0.868653416633606\n",
            "Train epoch - Accuracy: 0.7712230215827338 Loss: 0.9236788975763663 Corrects: 5360\n",
            "Starting epoch 6/30, LR = [0.01]\n",
            "Train step - Step 280, Loss 0.9360050559043884\n",
            "Train step - Step 290, Loss 0.7952862977981567\n",
            "Train step - Step 300, Loss 0.974802553653717\n",
            "Train step - Step 310, Loss 0.7329683899879456\n",
            "Train step - Step 320, Loss 0.8462543487548828\n",
            "Train epoch - Accuracy: 0.78 Loss: 0.8826397167178367 Corrects: 5421\n",
            "Starting epoch 7/30, LR = [0.01]\n",
            "Train step - Step 330, Loss 1.0008374452590942\n",
            "Train step - Step 340, Loss 0.8785548806190491\n",
            "Train step - Step 350, Loss 0.7191640734672546\n",
            "Train step - Step 360, Loss 0.9873504638671875\n",
            "Train step - Step 370, Loss 0.925977885723114\n",
            "Train step - Step 380, Loss 0.7484988570213318\n",
            "Train epoch - Accuracy: 0.7971223021582734 Loss: 0.8368748658338039 Corrects: 5540\n",
            "Starting epoch 8/30, LR = [0.01]\n",
            "Train step - Step 390, Loss 0.8035355806350708\n",
            "Train step - Step 400, Loss 0.6652087569236755\n",
            "Train step - Step 410, Loss 0.690735399723053\n",
            "Train step - Step 420, Loss 0.8830136060714722\n",
            "Train step - Step 430, Loss 0.8301740884780884\n",
            "Train epoch - Accuracy: 0.8070503597122303 Loss: 0.7973386471563106 Corrects: 5609\n",
            "Starting epoch 9/30, LR = [0.01]\n",
            "Train step - Step 440, Loss 0.7399466037750244\n",
            "Train step - Step 450, Loss 0.7234505414962769\n",
            "Train step - Step 460, Loss 0.7431851625442505\n",
            "Train step - Step 470, Loss 0.8172450065612793\n",
            "Train step - Step 480, Loss 0.884239673614502\n",
            "Train step - Step 490, Loss 0.821925699710846\n",
            "Train epoch - Accuracy: 0.8207194244604317 Loss: 0.772523332225333 Corrects: 5704\n",
            "Starting epoch 10/30, LR = [0.01]\n",
            "Train step - Step 500, Loss 0.6504123210906982\n",
            "Train step - Step 510, Loss 0.7979073524475098\n",
            "Train step - Step 520, Loss 0.9055379629135132\n",
            "Train step - Step 530, Loss 0.750931978225708\n",
            "Train step - Step 540, Loss 0.8461602926254272\n",
            "Train epoch - Accuracy: 0.8171223021582734 Loss: 0.7532705398257687 Corrects: 5679\n",
            "Starting epoch 11/30, LR = [0.01]\n",
            "Train step - Step 550, Loss 0.7771134972572327\n",
            "Train step - Step 560, Loss 0.7297630310058594\n",
            "Train step - Step 570, Loss 0.7394497394561768\n",
            "Train step - Step 580, Loss 0.7744042873382568\n",
            "Train step - Step 590, Loss 0.7812889814376831\n",
            "Train step - Step 600, Loss 0.6578109860420227\n",
            "Train epoch - Accuracy: 0.8420143884892086 Loss: 0.7049750582948863 Corrects: 5852\n",
            "Starting epoch 12/30, LR = [0.01]\n",
            "Train step - Step 610, Loss 0.5926120281219482\n",
            "Train step - Step 620, Loss 0.7285479307174683\n",
            "Train step - Step 630, Loss 0.6636748909950256\n",
            "Train step - Step 640, Loss 0.7295128703117371\n",
            "Train step - Step 650, Loss 0.7040489315986633\n",
            "Train epoch - Accuracy: 0.8411510791366906 Loss: 0.7006945243327738 Corrects: 5846\n",
            "Starting epoch 13/30, LR = [0.01]\n",
            "Train step - Step 660, Loss 0.6294412612915039\n",
            "Train step - Step 670, Loss 0.6786963939666748\n",
            "Train step - Step 680, Loss 0.669562578201294\n",
            "Train step - Step 690, Loss 0.6979638338088989\n",
            "Train step - Step 700, Loss 0.6789965629577637\n",
            "Train step - Step 710, Loss 0.6582666039466858\n",
            "Train epoch - Accuracy: 0.8489208633093526 Loss: 0.6812456878483725 Corrects: 5900\n",
            "Starting epoch 14/30, LR = [0.01]\n",
            "Train step - Step 720, Loss 0.789017915725708\n",
            "Train step - Step 730, Loss 0.6793102025985718\n",
            "Train step - Step 740, Loss 0.6135924458503723\n",
            "Train step - Step 750, Loss 0.8094404935836792\n",
            "Train step - Step 760, Loss 0.6406263113021851\n",
            "Train epoch - Accuracy: 0.8532374100719424 Loss: 0.6719857767674563 Corrects: 5930\n",
            "Starting epoch 15/30, LR = [0.01]\n",
            "Train step - Step 770, Loss 0.592258870601654\n",
            "Train step - Step 780, Loss 0.718337893486023\n",
            "Train step - Step 790, Loss 0.6490455865859985\n",
            "Train step - Step 800, Loss 0.7609212398529053\n",
            "Train step - Step 810, Loss 0.6618881225585938\n",
            "Train step - Step 820, Loss 0.651237428188324\n",
            "Train epoch - Accuracy: 0.8618705035971223 Loss: 0.6500895893487999 Corrects: 5990\n",
            "Starting epoch 16/30, LR = [0.01]\n",
            "Train step - Step 830, Loss 0.6692168712615967\n",
            "Train step - Step 840, Loss 0.6677001118659973\n",
            "Train step - Step 850, Loss 0.6988793015480042\n",
            "Train step - Step 860, Loss 0.575808048248291\n",
            "Train step - Step 870, Loss 0.7309134602546692\n",
            "Train epoch - Accuracy: 0.8664748201438849 Loss: 0.6251561474457061 Corrects: 6022\n",
            "Starting epoch 17/30, LR = [0.01]\n",
            "Train step - Step 880, Loss 0.6301944255828857\n",
            "Train step - Step 890, Loss 0.5936545133590698\n",
            "Train step - Step 900, Loss 0.7274457812309265\n",
            "Train step - Step 910, Loss 0.6633331775665283\n",
            "Train step - Step 920, Loss 0.7837902903556824\n",
            "Train step - Step 930, Loss 0.5018724799156189\n",
            "Train epoch - Accuracy: 0.8673381294964029 Loss: 0.6109866941232476 Corrects: 6028\n",
            "Starting epoch 18/30, LR = [0.01]\n",
            "Train step - Step 940, Loss 0.720506489276886\n",
            "Train step - Step 950, Loss 0.5460237860679626\n",
            "Train step - Step 960, Loss 0.4992417097091675\n",
            "Train step - Step 970, Loss 0.5855810046195984\n",
            "Train step - Step 980, Loss 0.6391980648040771\n",
            "Train epoch - Accuracy: 0.8762589928057554 Loss: 0.5967640070606479 Corrects: 6090\n",
            "Starting epoch 19/30, LR = [0.01]\n",
            "Train step - Step 990, Loss 0.6595994830131531\n",
            "Train step - Step 1000, Loss 0.530761182308197\n",
            "Train step - Step 1010, Loss 0.7441499829292297\n",
            "Train step - Step 1020, Loss 0.5727093815803528\n",
            "Train step - Step 1030, Loss 0.43201178312301636\n",
            "Train step - Step 1040, Loss 0.4162161946296692\n",
            "Train epoch - Accuracy: 0.8863309352517985 Loss: 0.5718656656210371 Corrects: 6160\n",
            "Starting epoch 20/30, LR = [0.01]\n",
            "Train step - Step 1050, Loss 0.44999489188194275\n",
            "Train step - Step 1060, Loss 0.6177610754966736\n",
            "Train step - Step 1070, Loss 0.5659977197647095\n",
            "Train step - Step 1080, Loss 0.5383581519126892\n",
            "Train step - Step 1090, Loss 0.5729156136512756\n",
            "Train epoch - Accuracy: 0.8932374100719425 Loss: 0.5481410671309601 Corrects: 6208\n",
            "Starting epoch 21/30, LR = [0.01]\n",
            "Train step - Step 1100, Loss 0.554743766784668\n",
            "Train step - Step 1110, Loss 0.5587043166160583\n",
            "Train step - Step 1120, Loss 0.5512394905090332\n",
            "Train step - Step 1130, Loss 0.6379868388175964\n",
            "Train step - Step 1140, Loss 0.6784927845001221\n",
            "Train step - Step 1150, Loss 0.5510408878326416\n",
            "Train epoch - Accuracy: 0.8926618705035971 Loss: 0.5467800871416819 Corrects: 6204\n",
            "Starting epoch 22/30, LR = [0.01]\n",
            "Train step - Step 1160, Loss 0.5436893105506897\n",
            "Train step - Step 1170, Loss 0.5624305009841919\n",
            "Train step - Step 1180, Loss 0.5015948414802551\n",
            "Train step - Step 1190, Loss 0.5332781076431274\n",
            "Train step - Step 1200, Loss 0.5759992599487305\n",
            "Train epoch - Accuracy: 0.8890647482014389 Loss: 0.5413685505853283 Corrects: 6179\n",
            "Starting epoch 23/30, LR = [0.01]\n",
            "Train step - Step 1210, Loss 0.4909706711769104\n",
            "Train step - Step 1220, Loss 0.48332589864730835\n",
            "Train step - Step 1230, Loss 0.5646758079528809\n",
            "Train step - Step 1240, Loss 0.47255733609199524\n",
            "Train step - Step 1250, Loss 0.5695775747299194\n",
            "Train step - Step 1260, Loss 0.5760276317596436\n",
            "Train epoch - Accuracy: 0.9027338129496403 Loss: 0.5311630241990947 Corrects: 6274\n",
            "Starting epoch 24/30, LR = [0.01]\n",
            "Train step - Step 1270, Loss 0.4883810579776764\n",
            "Train step - Step 1280, Loss 0.493015855550766\n",
            "Train step - Step 1290, Loss 0.5391418933868408\n",
            "Train step - Step 1300, Loss 0.5298431515693665\n",
            "Train step - Step 1310, Loss 0.522720217704773\n",
            "Train epoch - Accuracy: 0.9079136690647482 Loss: 0.5042285586261063 Corrects: 6310\n",
            "Starting epoch 25/30, LR = [0.01]\n",
            "Train step - Step 1320, Loss 0.5305840969085693\n",
            "Train step - Step 1330, Loss 0.5131083130836487\n",
            "Train step - Step 1340, Loss 0.5176154971122742\n",
            "Train step - Step 1350, Loss 0.4857015013694763\n",
            "Train step - Step 1360, Loss 0.4549964666366577\n",
            "Train step - Step 1370, Loss 0.5035089254379272\n",
            "Train epoch - Accuracy: 0.9130935251798561 Loss: 0.4907531504665347 Corrects: 6346\n",
            "Starting epoch 26/30, LR = [0.01]\n",
            "Train step - Step 1380, Loss 0.46782878041267395\n",
            "Train step - Step 1390, Loss 0.5192040205001831\n",
            "Train step - Step 1400, Loss 0.45179593563079834\n",
            "Train step - Step 1410, Loss 0.46911147236824036\n",
            "Train step - Step 1420, Loss 0.4623604416847229\n",
            "Train epoch - Accuracy: 0.916978417266187 Loss: 0.48308732983019714 Corrects: 6373\n",
            "Starting epoch 27/30, LR = [0.01]\n",
            "Train step - Step 1430, Loss 0.46467894315719604\n",
            "Train step - Step 1440, Loss 0.44558823108673096\n",
            "Train step - Step 1450, Loss 0.45713862776756287\n",
            "Train step - Step 1460, Loss 0.46069321036338806\n",
            "Train step - Step 1470, Loss 0.5672146081924438\n",
            "Train step - Step 1480, Loss 0.5120254755020142\n",
            "Train epoch - Accuracy: 0.9181294964028777 Loss: 0.47517334360012903 Corrects: 6381\n",
            "Starting epoch 28/30, LR = [0.01]\n",
            "Train step - Step 1490, Loss 0.4478272795677185\n",
            "Train step - Step 1500, Loss 0.5244534015655518\n",
            "Train step - Step 1510, Loss 0.427509069442749\n",
            "Train step - Step 1520, Loss 0.44935572147369385\n",
            "Train step - Step 1530, Loss 0.4901108145713806\n",
            "Train epoch - Accuracy: 0.9237410071942446 Loss: 0.44914721697354487 Corrects: 6420\n",
            "Starting epoch 29/30, LR = [0.01]\n",
            "Train step - Step 1540, Loss 0.49480873346328735\n",
            "Train step - Step 1550, Loss 0.43079280853271484\n",
            "Train step - Step 1560, Loss 0.5105654001235962\n",
            "Train step - Step 1570, Loss 0.4634122848510742\n",
            "Train step - Step 1580, Loss 0.4681711196899414\n",
            "Train step - Step 1590, Loss 0.4460357427597046\n",
            "Train epoch - Accuracy: 0.9273381294964029 Loss: 0.44576226634087324 Corrects: 6445\n",
            "Starting epoch 30/30, LR = [0.01]\n",
            "Train step - Step 1600, Loss 0.46382859349250793\n",
            "Train step - Step 1610, Loss 0.47678977251052856\n",
            "Train step - Step 1620, Loss 0.41907382011413574\n",
            "Train step - Step 1630, Loss 0.4090370535850525\n",
            "Train step - Step 1640, Loss 0.36074984073638916\n",
            "Train epoch - Accuracy: 0.9235971223021583 Loss: 0.4538296286665278 Corrects: 6419\n",
            "Training finished in 275.4227845668793 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0, 95, 83, 63, 42, 30, 6, 2, 97, 72, 36, 55, 31, 19, 98, 94, 54, 93, 85, 9, 96, 99, 15, 14, 57, 45, 13, 88, 60, 40, 8]\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  33\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d1069950>\n",
            "Constructing exemplars of class 99\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [8602, 3306, 12496, 16454, 28457, 1944, 45926, 35670, 4887, 8911, 10040, 5651, 37408, 21940, 4770, 33250, 5310, 26337, 21629, 37717, 10142, 4254, 34216, 7673, 23921, 4029, 20745, 37589, 29838, 8620, 2565, 20550, 44243]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12f37d0>\n",
            "Constructing exemplars of class 15\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [7144, 3513, 25466, 3312, 21376, 4670, 14571, 35373, 35943, 13537, 31418, 23529, 44628, 36867, 43953, 43240, 13553, 20060, 29078, 28738, 12704, 18780, 11997, 33400, 34879, 3166, 6024, 11820, 21501, 22550, 22399, 7532, 25964]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d1253510>\n",
            "Constructing exemplars of class 14\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [25643, 30640, 5628, 1077, 12899, 33542, 44850, 31006, 22912, 8794, 38608, 15290, 41179, 27146, 42966, 7787, 5733, 5464, 3395, 42124, 32616, 4167, 15906, 2749, 10717, 49565, 5668, 25683, 10520, 45685, 16930, 36683, 37613]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d1391a50>\n",
            "Constructing exemplars of class 57\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [2679, 32832, 45281, 17734, 12430, 33426, 32456, 35347, 44222, 5703, 18747, 30607, 7855, 27302, 48653, 41608, 283, 47398, 44495, 41810, 13188, 22909, 42621, 6441, 270, 23240, 38939, 27491, 26481, 24184, 7094, 20996, 3713]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d1069850>\n",
            "Constructing exemplars of class 45\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [39674, 41700, 5271, 29459, 31203, 7747, 8912, 46928, 37669, 3276, 7254, 10949, 4468, 27683, 41460, 1381, 19699, 24407, 46593, 24070, 40060, 44226, 20668, 213, 16959, 48201, 33612, 4608, 22194, 41950, 42678, 29442, 15856]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12f37d0>\n",
            "Constructing exemplars of class 13\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [2654, 29916, 47089, 23165, 31347, 22962, 12937, 48124, 14025, 25819, 23227, 16602, 27663, 25063, 8090, 39174, 24585, 3720, 49296, 1161, 1138, 39277, 45203, 240, 6354, 786, 3536, 8892, 41875, 36139, 49052, 27236, 31109]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12e2c10>\n",
            "Constructing exemplars of class 88\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [48977, 12613, 6567, 46956, 1766, 11746, 47139, 30074, 16340, 43636, 30652, 30961, 49131, 531, 44312, 25151, 7691, 23996, 26847, 6567, 19775, 32367, 1638, 21473, 17295, 18961, 10503, 33164, 39507, 6016, 40743, 27540, 13976]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d123dfd0>\n",
            "Constructing exemplars of class 60\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [34435, 24862, 45514, 35593, 28612, 48122, 19179, 19951, 20752, 48156, 31119, 42187, 35697, 3872, 3748, 47370, 30095, 26295, 48419, 18379, 3748, 2824, 10917, 26452, 43050, 4268, 26781, 14970, 11602, 20729, 46285, 16305, 29410]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d24286d0>\n",
            "Constructing exemplars of class 40\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [1203, 48663, 18370, 14337, 48679, 15257, 27993, 7743, 10745, 39257, 27721, 34529, 9158, 18142, 23284, 46477, 16553, 3542, 41178, 23410, 3483, 10618, 22975, 22531, 22419, 42812, 41863, 20629, 14620, 44471, 37768, 41668, 484]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d24214d0>\n",
            "Constructing exemplars of class 8\n",
            "lunghezza exemplar set:  33\n",
            "exemplar set:  [16448, 15361, 8744, 5757, 16285, 48198, 6070, 36188, 10477, 7517, 34500, 14485, 23351, 22852, 39445, 19628, 49151, 47175, 9972, 42613, 43468, 8715, 35617, 32999, 1290, 31209, 3584, 33226, 31476, 26549, 42892, 22237, 5213]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.82 1.5972262620925903\n",
            "TEST GROUP:  0.777\n",
            "TEST ALL:  0.43383333333333335\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  7000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [99, 49, 96, 9, 13, 17, 21, 45, 53, 84, 57, 61, 65, 69, 81, 85, 88, 80, 95, 36, 4, 8, 16, 20, 24, 32, 40, 76, 52, 56, 60, 64, 68, 72, 93, 97, 2, 47, 19, 23, 27, 31, 35, 39, 55, 6, 59, 63, 67, 75, 79, 83, 15, 7, 98, 94, 90, 86, 82, 70, 54, 50, 42, 34, 30, 22, 18, 14, 10, 0]\n",
            "TRAIN_SET CLASSES:  [35, 27, 86, 70, 50, 69, 53, 17, 84, 52]\n",
            "VALIDATION CLASSES:  [53, 52, 50, 35, 27, 86, 84, 17, 70, 69]\n",
            "GROUP:  7\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [35, 27, 86, 70, 50, 69, 53, 17, 84, 52]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "Starting epoch 1/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 7.925084114074707\n",
            "Train step - Step 10, Loss 4.6725945472717285\n",
            "Train step - Step 20, Loss 2.985973596572876\n",
            "Train step - Step 30, Loss 1.7723114490509033\n",
            "Train step - Step 40, Loss 1.6917378902435303\n",
            "Train step - Step 50, Loss 1.5356619358062744\n",
            "Train epoch - Accuracy: 0.4782106782106782 Loss: 3.103228416174521 Corrects: 3314\n",
            "Starting epoch 2/30, LR = [0.01]\n",
            "Train step - Step 60, Loss 0.9457136392593384\n",
            "Train step - Step 70, Loss 1.2175344228744507\n",
            "Train step - Step 80, Loss 1.0217299461364746\n",
            "Train step - Step 90, Loss 1.0234814882278442\n",
            "Train step - Step 100, Loss 1.0879366397857666\n",
            "Train epoch - Accuracy: 0.7323232323232324 Loss: 1.0897311335862285 Corrects: 5075\n",
            "Starting epoch 3/30, LR = [0.01]\n",
            "Train step - Step 110, Loss 0.8022093176841736\n",
            "Train step - Step 120, Loss 1.0828039646148682\n",
            "Train step - Step 130, Loss 1.0216299295425415\n",
            "Train step - Step 140, Loss 0.9397149085998535\n",
            "Train step - Step 150, Loss 1.054039716720581\n",
            "Train step - Step 160, Loss 1.0058008432388306\n",
            "Train epoch - Accuracy: 0.7682539682539683 Loss: 0.9383752520810302 Corrects: 5324\n",
            "Starting epoch 4/30, LR = [0.01]\n",
            "Train step - Step 170, Loss 0.9069880843162537\n",
            "Train step - Step 180, Loss 0.8470042943954468\n",
            "Train step - Step 190, Loss 0.9118011593818665\n",
            "Train step - Step 200, Loss 0.8307269215583801\n",
            "Train step - Step 210, Loss 0.9791883826255798\n",
            "Train epoch - Accuracy: 0.7831168831168831 Loss: 0.860338336736781 Corrects: 5427\n",
            "Starting epoch 5/30, LR = [0.01]\n",
            "Train step - Step 220, Loss 0.7824375033378601\n",
            "Train step - Step 230, Loss 0.7272000312805176\n",
            "Train step - Step 240, Loss 0.685932457447052\n",
            "Train step - Step 250, Loss 0.7935971617698669\n",
            "Train step - Step 260, Loss 0.8199551105499268\n",
            "Train step - Step 270, Loss 0.6324106454849243\n",
            "Train epoch - Accuracy: 0.810966810966811 Loss: 0.7849167999595102 Corrects: 5620\n",
            "Starting epoch 6/30, LR = [0.01]\n",
            "Train step - Step 280, Loss 0.7675297260284424\n",
            "Train step - Step 290, Loss 0.7831004858016968\n",
            "Train step - Step 300, Loss 0.7842934131622314\n",
            "Train step - Step 310, Loss 0.7227555513381958\n",
            "Train step - Step 320, Loss 0.6606191396713257\n",
            "Train epoch - Accuracy: 0.8209235209235209 Loss: 0.7507034165862663 Corrects: 5689\n",
            "Starting epoch 7/30, LR = [0.01]\n",
            "Train step - Step 330, Loss 0.622275710105896\n",
            "Train step - Step 340, Loss 0.7655258178710938\n",
            "Train step - Step 350, Loss 0.6726248860359192\n",
            "Train step - Step 360, Loss 0.5892672538757324\n",
            "Train step - Step 370, Loss 0.7685001492500305\n",
            "Train step - Step 380, Loss 0.7822625041007996\n",
            "Train epoch - Accuracy: 0.8362193362193362 Loss: 0.716000640512717 Corrects: 5795\n",
            "Starting epoch 8/30, LR = [0.01]\n",
            "Train step - Step 390, Loss 0.6797145009040833\n",
            "Train step - Step 400, Loss 0.7606601715087891\n",
            "Train step - Step 410, Loss 0.6097530722618103\n",
            "Train step - Step 420, Loss 0.5787034034729004\n",
            "Train step - Step 430, Loss 0.7754798531532288\n",
            "Train epoch - Accuracy: 0.8428571428571429 Loss: 0.6859782718476795 Corrects: 5841\n",
            "Starting epoch 9/30, LR = [0.01]\n",
            "Train step - Step 440, Loss 0.6433375477790833\n",
            "Train step - Step 450, Loss 0.6295425295829773\n",
            "Train step - Step 460, Loss 0.6019847989082336\n",
            "Train step - Step 470, Loss 0.7262553572654724\n",
            "Train step - Step 480, Loss 0.7276686429977417\n",
            "Train step - Step 490, Loss 0.5947411060333252\n",
            "Train epoch - Accuracy: 0.8496392496392496 Loss: 0.6686283685084201 Corrects: 5888\n",
            "Starting epoch 10/30, LR = [0.01]\n",
            "Train step - Step 500, Loss 0.5458847880363464\n",
            "Train step - Step 510, Loss 0.8611668348312378\n",
            "Train step - Step 520, Loss 0.6515793800354004\n",
            "Train step - Step 530, Loss 0.6505756974220276\n",
            "Train step - Step 540, Loss 0.5488032698631287\n",
            "Train epoch - Accuracy: 0.856998556998557 Loss: 0.6325549685077749 Corrects: 5939\n",
            "Starting epoch 11/30, LR = [0.01]\n",
            "Train step - Step 550, Loss 0.48603808879852295\n",
            "Train step - Step 560, Loss 0.6220542192459106\n",
            "Train step - Step 570, Loss 0.6169309616088867\n",
            "Train step - Step 580, Loss 0.682595431804657\n",
            "Train step - Step 590, Loss 0.6824045777320862\n",
            "Train step - Step 600, Loss 0.610420823097229\n",
            "Train epoch - Accuracy: 0.8591630591630591 Loss: 0.622037030401684 Corrects: 5954\n",
            "Starting epoch 12/30, LR = [0.01]\n",
            "Train step - Step 610, Loss 0.6509672403335571\n",
            "Train step - Step 620, Loss 0.5771400928497314\n",
            "Train step - Step 630, Loss 0.5331224203109741\n",
            "Train step - Step 640, Loss 0.6340673565864563\n",
            "Train step - Step 650, Loss 0.5352875590324402\n",
            "Train epoch - Accuracy: 0.8681096681096682 Loss: 0.603166340147434 Corrects: 6016\n",
            "Starting epoch 13/30, LR = [0.01]\n",
            "Train step - Step 660, Loss 0.512449324131012\n",
            "Train step - Step 670, Loss 0.5510694980621338\n",
            "Train step - Step 680, Loss 0.6644802093505859\n",
            "Train step - Step 690, Loss 0.467159628868103\n",
            "Train step - Step 700, Loss 0.6788771152496338\n",
            "Train step - Step 710, Loss 0.5869876742362976\n",
            "Train epoch - Accuracy: 0.8795093795093795 Loss: 0.5781056905377651 Corrects: 6095\n",
            "Starting epoch 14/30, LR = [0.01]\n",
            "Train step - Step 720, Loss 0.5271807909011841\n",
            "Train step - Step 730, Loss 0.5342026352882385\n",
            "Train step - Step 740, Loss 0.6102217435836792\n",
            "Train step - Step 750, Loss 0.48300230503082275\n",
            "Train step - Step 760, Loss 0.4852512776851654\n",
            "Train epoch - Accuracy: 0.888023088023088 Loss: 0.5640040679067416 Corrects: 6154\n",
            "Starting epoch 15/30, LR = [0.01]\n",
            "Train step - Step 770, Loss 0.6081277132034302\n",
            "Train step - Step 780, Loss 0.5269032716751099\n",
            "Train step - Step 790, Loss 0.706714391708374\n",
            "Train step - Step 800, Loss 0.582040548324585\n",
            "Train step - Step 810, Loss 0.5148882269859314\n",
            "Train step - Step 820, Loss 0.5097690224647522\n",
            "Train epoch - Accuracy: 0.8743145743145743 Loss: 0.5870602238229858 Corrects: 6059\n",
            "Starting epoch 16/30, LR = [0.01]\n",
            "Train step - Step 830, Loss 0.5060968995094299\n",
            "Train step - Step 840, Loss 0.5839705467224121\n",
            "Train step - Step 850, Loss 0.5460336208343506\n",
            "Train step - Step 860, Loss 0.5261605978012085\n",
            "Train step - Step 870, Loss 0.49265140295028687\n",
            "Train epoch - Accuracy: 0.8923520923520923 Loss: 0.5415527513532927 Corrects: 6184\n",
            "Starting epoch 17/30, LR = [0.01]\n",
            "Train step - Step 880, Loss 0.538645327091217\n",
            "Train step - Step 890, Loss 0.48920875787734985\n",
            "Train step - Step 900, Loss 0.48932480812072754\n",
            "Train step - Step 910, Loss 0.6406008005142212\n",
            "Train step - Step 920, Loss 0.6485048532485962\n",
            "Train step - Step 930, Loss 0.6243389844894409\n",
            "Train epoch - Accuracy: 0.8923520923520923 Loss: 0.5428557226668188 Corrects: 6184\n",
            "Starting epoch 18/30, LR = [0.01]\n",
            "Train step - Step 940, Loss 0.6427582502365112\n",
            "Train step - Step 950, Loss 0.4925520718097687\n",
            "Train step - Step 960, Loss 0.6036407947540283\n",
            "Train step - Step 970, Loss 0.43387681245803833\n",
            "Train step - Step 980, Loss 0.5205016136169434\n",
            "Train epoch - Accuracy: 0.9018759018759018 Loss: 0.5092507510990292 Corrects: 6250\n",
            "Starting epoch 19/30, LR = [0.01]\n",
            "Train step - Step 990, Loss 0.4151764512062073\n",
            "Train step - Step 1000, Loss 0.49092650413513184\n",
            "Train step - Step 1010, Loss 0.5625079870223999\n",
            "Train step - Step 1020, Loss 0.6120260953903198\n",
            "Train step - Step 1030, Loss 0.4512980282306671\n",
            "Train step - Step 1040, Loss 0.5963056683540344\n",
            "Train epoch - Accuracy: 0.898989898989899 Loss: 0.5113547578002468 Corrects: 6230\n",
            "Starting epoch 20/30, LR = [0.01]\n",
            "Train step - Step 1050, Loss 0.4204254746437073\n",
            "Train step - Step 1060, Loss 0.5305567979812622\n",
            "Train step - Step 1070, Loss 0.4706352651119232\n",
            "Train step - Step 1080, Loss 0.45928406715393066\n",
            "Train step - Step 1090, Loss 0.45514506101608276\n",
            "Train epoch - Accuracy: 0.9093795093795094 Loss: 0.4797231684859525 Corrects: 6302\n",
            "Starting epoch 21/30, LR = [0.01]\n",
            "Train step - Step 1100, Loss 0.5564592480659485\n",
            "Train step - Step 1110, Loss 0.5202901363372803\n",
            "Train step - Step 1120, Loss 0.5642085075378418\n",
            "Train step - Step 1130, Loss 0.4656539559364319\n",
            "Train step - Step 1140, Loss 0.5855019688606262\n",
            "Train step - Step 1150, Loss 0.4329906404018402\n",
            "Train epoch - Accuracy: 0.9124098124098124 Loss: 0.48485395357894345 Corrects: 6323\n",
            "Starting epoch 22/30, LR = [0.01]\n",
            "Train step - Step 1160, Loss 0.43743157386779785\n",
            "Train step - Step 1170, Loss 0.4231434464454651\n",
            "Train step - Step 1180, Loss 0.568202793598175\n",
            "Train step - Step 1190, Loss 0.45920687913894653\n",
            "Train step - Step 1200, Loss 0.4693208932876587\n",
            "Train epoch - Accuracy: 0.9158730158730158 Loss: 0.47303013196067206 Corrects: 6347\n",
            "Starting epoch 23/30, LR = [0.01]\n",
            "Train step - Step 1210, Loss 0.4740474224090576\n",
            "Train step - Step 1220, Loss 0.44031476974487305\n",
            "Train step - Step 1230, Loss 0.4320186376571655\n",
            "Train step - Step 1240, Loss 0.4793780446052551\n",
            "Train step - Step 1250, Loss 0.5226694345474243\n",
            "Train step - Step 1260, Loss 0.49508100748062134\n",
            "Train epoch - Accuracy: 0.922077922077922 Loss: 0.44955632260007433 Corrects: 6390\n",
            "Starting epoch 24/30, LR = [0.01]\n",
            "Train step - Step 1270, Loss 0.4429909586906433\n",
            "Train step - Step 1280, Loss 0.4056445360183716\n",
            "Train step - Step 1290, Loss 0.3573375344276428\n",
            "Train step - Step 1300, Loss 0.42326056957244873\n",
            "Train step - Step 1310, Loss 0.5334781408309937\n",
            "Train epoch - Accuracy: 0.9246753246753247 Loss: 0.43891314428090017 Corrects: 6408\n",
            "Starting epoch 25/30, LR = [0.01]\n",
            "Train step - Step 1320, Loss 0.40265488624572754\n",
            "Train step - Step 1330, Loss 0.488696813583374\n",
            "Train step - Step 1340, Loss 0.43284571170806885\n",
            "Train step - Step 1350, Loss 0.47992822527885437\n",
            "Train step - Step 1360, Loss 0.5779670476913452\n",
            "Train step - Step 1370, Loss 0.5242037177085876\n",
            "Train epoch - Accuracy: 0.9222222222222223 Loss: 0.4509662420030624 Corrects: 6391\n",
            "Starting epoch 26/30, LR = [0.01]\n",
            "Train step - Step 1380, Loss 0.4734489321708679\n",
            "Train step - Step 1390, Loss 0.42778053879737854\n",
            "Train step - Step 1400, Loss 0.43951812386512756\n",
            "Train step - Step 1410, Loss 0.46790608763694763\n",
            "Train step - Step 1420, Loss 0.48801401257514954\n",
            "Train epoch - Accuracy: 0.9196248196248197 Loss: 0.45471282032833127 Corrects: 6373\n",
            "Starting epoch 27/30, LR = [0.01]\n",
            "Train step - Step 1430, Loss 0.4256906509399414\n",
            "Train step - Step 1440, Loss 0.4713960587978363\n",
            "Train step - Step 1450, Loss 0.4740566313266754\n",
            "Train step - Step 1460, Loss 0.4883708953857422\n",
            "Train step - Step 1470, Loss 0.47270917892456055\n",
            "Train step - Step 1480, Loss 0.4499899446964264\n",
            "Train epoch - Accuracy: 0.9232323232323232 Loss: 0.4602817296465754 Corrects: 6398\n",
            "Starting epoch 28/30, LR = [0.01]\n",
            "Train step - Step 1490, Loss 0.40435123443603516\n",
            "Train step - Step 1500, Loss 0.37088972330093384\n",
            "Train step - Step 1510, Loss 0.4246799349784851\n",
            "Train step - Step 1520, Loss 0.4565895199775696\n",
            "Train step - Step 1530, Loss 0.36748021841049194\n",
            "Train epoch - Accuracy: 0.9363636363636364 Loss: 0.40672247843308884 Corrects: 6489\n",
            "Starting epoch 29/30, LR = [0.01]\n",
            "Train step - Step 1540, Loss 0.41304415464401245\n",
            "Train step - Step 1550, Loss 0.45840978622436523\n",
            "Train step - Step 1560, Loss 0.41898417472839355\n",
            "Train step - Step 1570, Loss 0.451416552066803\n",
            "Train step - Step 1580, Loss 0.4424492120742798\n",
            "Train step - Step 1590, Loss 0.43101561069488525\n",
            "Train epoch - Accuracy: 0.93001443001443 Loss: 0.4350583144596645 Corrects: 6445\n",
            "Starting epoch 30/30, LR = [0.01]\n",
            "Train step - Step 1600, Loss 0.5682071447372437\n",
            "Train step - Step 1610, Loss 0.4020410180091858\n",
            "Train step - Step 1620, Loss 0.5097488164901733\n",
            "Train step - Step 1630, Loss 0.3882589340209961\n",
            "Train step - Step 1640, Loss 0.4153895974159241\n",
            "Train epoch - Accuracy: 0.925974025974026 Loss: 0.444558070404361 Corrects: 6417\n",
            "Training finished in 276.4045321941376 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0, 95, 83, 63, 42, 30, 6, 2, 97, 72, 36, 55, 31, 19, 98, 94, 54, 93, 85, 9, 96, 99, 15, 14, 57, 45, 13, 88, 60, 40, 8, 35, 27, 86, 70, 50, 69, 53, 17, 84, 52]\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  28\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d1237f50>\n",
            "Constructing exemplars of class 35\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [2570, 35495, 13173, 12537, 24842, 17219, 41536, 2404, 17870, 44680, 19143, 13027, 2229, 2585, 4099, 7636, 30438, 24005, 29894, 29177, 9758, 28697, 21964, 46398, 2073, 44621, 2689, 40629]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71df4f8590>\n",
            "Constructing exemplars of class 27\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [29745, 35192, 13943, 23741, 1782, 499, 2526, 26590, 9910, 1055, 26692, 42469, 41821, 25834, 40086, 36965, 22950, 29748, 34645, 25823, 16275, 22683, 42258, 46693, 32691, 25307, 3545, 19172]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d3ce1d10>\n",
            "Constructing exemplars of class 86\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [48810, 28787, 1941, 21679, 8303, 38298, 18102, 8379, 45141, 22232, 263, 43676, 31440, 40573, 34543, 36755, 49425, 5767, 38135, 31202, 38089, 35748, 17240, 24875, 38734, 45951, 15501, 43891]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d248c2d0>\n",
            "Constructing exemplars of class 70\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [28386, 49522, 25525, 7500, 2321, 6823, 47404, 6154, 28348, 42355, 37948, 29996, 34840, 18834, 38241, 4053, 15131, 30080, 14278, 34135, 38943, 20634, 49208, 21097, 30080, 20517, 17315, 17746]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d1069a90>\n",
            "Constructing exemplars of class 50\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [27918, 14258, 29452, 14448, 30721, 16215, 17106, 32841, 15408, 44661, 918, 23294, 17335, 26655, 43906, 44664, 27719, 10544, 16021, 19954, 31598, 34148, 10528, 3827, 29953, 28960, 25418, 32917]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d246da50>\n",
            "Constructing exemplars of class 69\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [7966, 4201, 35173, 35437, 25881, 35869, 49700, 29833, 5581, 28234, 12861, 23762, 4217, 633, 20249, 20496, 39244, 47887, 7795, 39645, 42127, 38162, 29510, 48583, 12844, 15587, 5541, 49128]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12e2c10>\n",
            "Constructing exemplars of class 53\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [41697, 8497, 31909, 43937, 15426, 44868, 28505, 12223, 5665, 47235, 34320, 23848, 48925, 4802, 39324, 48743, 7993, 36804, 15346, 23513, 35479, 43170, 27081, 37812, 31517, 48159, 44656, 7495]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6ead410>\n",
            "Constructing exemplars of class 17\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [32476, 17058, 7653, 42012, 35977, 42458, 3579, 25327, 30520, 43475, 18255, 13136, 23805, 40430, 37675, 12104, 21011, 24846, 17579, 25335, 12242, 18329, 18264, 40232, 20803, 14976, 34259, 33972]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d63d6090>\n",
            "Constructing exemplars of class 84\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [22544, 11689, 38435, 27833, 30547, 27393, 2013, 21532, 9516, 14146, 12785, 48417, 5663, 38151, 23536, 6726, 49315, 49330, 17195, 571, 48508, 16767, 32093, 3387, 43130, 16070, 13705, 44365]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6b3c710>\n",
            "Constructing exemplars of class 52\n",
            "lunghezza exemplar set:  28\n",
            "exemplar set:  [44001, 40941, 31698, 23679, 21327, 35172, 18065, 40199, 34810, 8993, 32484, 48284, 1281, 49841, 18165, 43008, 17591, 40246, 18680, 34015, 43730, 47642, 41159, 39994, 12273, 42825, 1128, 35825]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.72 1.8168379068374634\n",
            "TEST GROUP:  0.815\n",
            "TEST ALL:  0.4095714285714286\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  8000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [99, 95, 96, 9, 13, 17, 21, 29, 37, 45, 49, 53, 57, 61, 65, 69, 81, 85, 93, 88, 84, 80, 40, 4, 8, 16, 20, 24, 32, 36, 44, 76, 48, 52, 56, 60, 64, 68, 72, 97, 2, 6, 51, 23, 27, 31, 35, 39, 43, 47, 55, 15, 59, 63, 67, 71, 75, 79, 83, 19, 7, 10, 50, 14, 18, 22, 30, 34, 38, 42, 54, 98, 70, 74, 78, 82, 86, 90, 94, 0]\n",
            "TRAIN_SET CLASSES:  [71, 51, 43, 78, 74, 38, 37, 29, 48, 44]\n",
            "VALIDATION CLASSES:  [51, 48, 44, 43, 38, 37, 29, 78, 74, 71]\n",
            "GROUP:  8\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [71, 51, 43, 78, 74, 38, 37, 29, 48, 44]\n",
            "Len TOTAL train susbset:  6910\n",
            "training\n",
            "Starting epoch 1/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 8.948784828186035\n",
            "Train step - Step 10, Loss 4.99345588684082\n",
            "Train step - Step 20, Loss 3.4873573780059814\n",
            "Train step - Step 30, Loss 2.542346239089966\n",
            "Train step - Step 40, Loss 1.759633183479309\n",
            "Train step - Step 50, Loss 1.4942342042922974\n",
            "Train epoch - Accuracy: 0.42793053545586107 Loss: 3.3812147493817872 Corrects: 2957\n",
            "Starting epoch 2/30, LR = [0.01]\n",
            "Train step - Step 60, Loss 1.4514940977096558\n",
            "Train step - Step 70, Loss 1.212231993675232\n",
            "Train step - Step 80, Loss 1.2464113235473633\n",
            "Train step - Step 90, Loss 1.0937281847000122\n",
            "Train step - Step 100, Loss 1.228204369544983\n",
            "Train epoch - Accuracy: 0.679739507959479 Loss: 1.239978724243672 Corrects: 4697\n",
            "Starting epoch 3/30, LR = [0.01]\n",
            "Train step - Step 110, Loss 1.1559407711029053\n",
            "Train step - Step 120, Loss 1.0557825565338135\n",
            "Train step - Step 130, Loss 0.9043934941291809\n",
            "Train step - Step 140, Loss 1.0290658473968506\n",
            "Train step - Step 150, Loss 0.8610188961029053\n",
            "Train step - Step 160, Loss 1.2116947174072266\n",
            "Train epoch - Accuracy: 0.7276410998552822 Loss: 1.051932055111732 Corrects: 5028\n",
            "Starting epoch 4/30, LR = [0.01]\n",
            "Train step - Step 170, Loss 0.9824513792991638\n",
            "Train step - Step 180, Loss 1.0111428499221802\n",
            "Train step - Step 190, Loss 0.9174865484237671\n",
            "Train step - Step 200, Loss 0.7869997620582581\n",
            "Train step - Step 210, Loss 0.9408568143844604\n",
            "Train epoch - Accuracy: 0.7535455861070912 Loss: 0.9521281249271288 Corrects: 5207\n",
            "Starting epoch 5/30, LR = [0.01]\n",
            "Train step - Step 220, Loss 0.774513840675354\n",
            "Train step - Step 230, Loss 0.8631430268287659\n",
            "Train step - Step 240, Loss 0.9433842301368713\n",
            "Train step - Step 250, Loss 1.0834341049194336\n",
            "Train step - Step 260, Loss 0.7879346609115601\n",
            "Train epoch - Accuracy: 0.7746743849493488 Loss: 0.8926651893752356 Corrects: 5353\n",
            "Starting epoch 6/30, LR = [0.01]\n",
            "Train step - Step 270, Loss 0.7458279132843018\n",
            "Train step - Step 280, Loss 0.8256284594535828\n",
            "Train step - Step 290, Loss 0.8871118426322937\n",
            "Train step - Step 300, Loss 0.656491756439209\n",
            "Train step - Step 310, Loss 0.9478110074996948\n",
            "Train step - Step 320, Loss 0.7640353441238403\n",
            "Train epoch - Accuracy: 0.7879884225759769 Loss: 0.846147616744559 Corrects: 5445\n",
            "Starting epoch 7/30, LR = [0.01]\n",
            "Train step - Step 330, Loss 0.7738202810287476\n",
            "Train step - Step 340, Loss 0.825255274772644\n",
            "Train step - Step 350, Loss 0.6544859409332275\n",
            "Train step - Step 360, Loss 0.8489903211593628\n",
            "Train step - Step 370, Loss 0.9185043573379517\n",
            "Train epoch - Accuracy: 0.7979739507959479 Loss: 0.8155861425330774 Corrects: 5514\n",
            "Starting epoch 8/30, LR = [0.01]\n",
            "Train step - Step 380, Loss 0.8425329923629761\n",
            "Train step - Step 390, Loss 0.8400379419326782\n",
            "Train step - Step 400, Loss 0.770064115524292\n",
            "Train step - Step 410, Loss 0.7072497010231018\n",
            "Train step - Step 420, Loss 0.7395132184028625\n",
            "Train step - Step 430, Loss 0.855675458908081\n",
            "Train epoch - Accuracy: 0.8118668596237337 Loss: 0.7844802310083778 Corrects: 5610\n",
            "Starting epoch 9/30, LR = [0.01]\n",
            "Train step - Step 440, Loss 0.7564467191696167\n",
            "Train step - Step 450, Loss 0.7317970991134644\n",
            "Train step - Step 460, Loss 0.6241980791091919\n",
            "Train step - Step 470, Loss 0.6803346276283264\n",
            "Train step - Step 480, Loss 0.8080669641494751\n",
            "Train epoch - Accuracy: 0.823589001447178 Loss: 0.7382616439707544 Corrects: 5691\n",
            "Starting epoch 10/30, LR = [0.01]\n",
            "Train step - Step 490, Loss 0.5917171239852905\n",
            "Train step - Step 500, Loss 0.8713827133178711\n",
            "Train step - Step 510, Loss 0.6702132821083069\n",
            "Train step - Step 520, Loss 0.7219880819320679\n",
            "Train step - Step 530, Loss 0.774440586566925\n",
            "Train epoch - Accuracy: 0.8311143270622287 Loss: 0.7175416964007874 Corrects: 5743\n",
            "Starting epoch 11/30, LR = [0.01]\n",
            "Train step - Step 540, Loss 0.6243430376052856\n",
            "Train step - Step 550, Loss 0.5975199937820435\n",
            "Train step - Step 560, Loss 0.7502139806747437\n",
            "Train step - Step 570, Loss 0.6028252840042114\n",
            "Train step - Step 580, Loss 0.7806745171546936\n",
            "Train step - Step 590, Loss 0.6084940433502197\n",
            "Train epoch - Accuracy: 0.8389290882778582 Loss: 0.7001218031698302 Corrects: 5797\n",
            "Starting epoch 12/30, LR = [0.01]\n",
            "Train step - Step 600, Loss 0.6126156449317932\n",
            "Train step - Step 610, Loss 0.608049213886261\n",
            "Train step - Step 620, Loss 0.6745009422302246\n",
            "Train step - Step 630, Loss 0.671744167804718\n",
            "Train step - Step 640, Loss 0.7609537839889526\n",
            "Train epoch - Accuracy: 0.8457308248914617 Loss: 0.676354943015296 Corrects: 5844\n",
            "Starting epoch 13/30, LR = [0.01]\n",
            "Train step - Step 650, Loss 0.6965570449829102\n",
            "Train step - Step 660, Loss 0.5461479425430298\n",
            "Train step - Step 670, Loss 0.7391166090965271\n",
            "Train step - Step 680, Loss 0.7481576204299927\n",
            "Train step - Step 690, Loss 0.6083688735961914\n",
            "Train step - Step 700, Loss 0.7242629528045654\n",
            "Train epoch - Accuracy: 0.8525325615050652 Loss: 0.6602532228409123 Corrects: 5891\n",
            "Starting epoch 14/30, LR = [0.01]\n",
            "Train step - Step 710, Loss 0.5774369835853577\n",
            "Train step - Step 720, Loss 0.5654707551002502\n",
            "Train step - Step 730, Loss 0.6404405236244202\n",
            "Train step - Step 740, Loss 0.5863838195800781\n",
            "Train step - Step 750, Loss 0.8006995916366577\n",
            "Train epoch - Accuracy: 0.8590448625180898 Loss: 0.6356379386657571 Corrects: 5936\n",
            "Starting epoch 15/30, LR = [0.01]\n",
            "Train step - Step 760, Loss 0.5297915935516357\n",
            "Train step - Step 770, Loss 0.6282538771629333\n",
            "Train step - Step 780, Loss 0.6728184223175049\n",
            "Train step - Step 790, Loss 0.6649811267852783\n",
            "Train step - Step 800, Loss 0.6077774167060852\n",
            "Train epoch - Accuracy: 0.8562952243125904 Loss: 0.6419885912610894 Corrects: 5917\n",
            "Starting epoch 16/30, LR = [0.01]\n",
            "Train step - Step 810, Loss 0.6187970638275146\n",
            "Train step - Step 820, Loss 0.5113142132759094\n",
            "Train step - Step 830, Loss 0.4275537133216858\n",
            "Train step - Step 840, Loss 0.6825718879699707\n",
            "Train step - Step 850, Loss 0.5889214873313904\n",
            "Train step - Step 860, Loss 0.6629810333251953\n",
            "Train epoch - Accuracy: 0.8709117221418234 Loss: 0.6044288695289843 Corrects: 6018\n",
            "Starting epoch 17/30, LR = [0.01]\n",
            "Train step - Step 870, Loss 0.6377502679824829\n",
            "Train step - Step 880, Loss 0.5410683751106262\n",
            "Train step - Step 890, Loss 0.5989995002746582\n",
            "Train step - Step 900, Loss 0.5991602540016174\n",
            "Train step - Step 910, Loss 0.6287819147109985\n",
            "Train epoch - Accuracy: 0.8800289435600579 Loss: 0.5881360345914981 Corrects: 6081\n",
            "Starting epoch 18/30, LR = [0.01]\n",
            "Train step - Step 920, Loss 0.5534435510635376\n",
            "Train step - Step 930, Loss 0.599258303642273\n",
            "Train step - Step 940, Loss 0.5898580551147461\n",
            "Train step - Step 950, Loss 0.7146482467651367\n",
            "Train step - Step 960, Loss 0.565606415271759\n",
            "Train step - Step 970, Loss 0.5310368537902832\n",
            "Train epoch - Accuracy: 0.8785817655571635 Loss: 0.5817043839591285 Corrects: 6071\n",
            "Starting epoch 19/30, LR = [0.01]\n",
            "Train step - Step 980, Loss 0.5710452795028687\n",
            "Train step - Step 990, Loss 0.5718134045600891\n",
            "Train step - Step 1000, Loss 0.6395509839057922\n",
            "Train step - Step 1010, Loss 0.5037586688995361\n",
            "Train step - Step 1020, Loss 0.592688798904419\n",
            "Train epoch - Accuracy: 0.8875542691751085 Loss: 0.563408406253489 Corrects: 6133\n",
            "Starting epoch 20/30, LR = [0.01]\n",
            "Train step - Step 1030, Loss 0.7381475567817688\n",
            "Train step - Step 1040, Loss 0.5154584050178528\n",
            "Train step - Step 1050, Loss 0.5095899105072021\n",
            "Train step - Step 1060, Loss 0.5197189450263977\n",
            "Train step - Step 1070, Loss 0.5490307807922363\n",
            "Train epoch - Accuracy: 0.8934876989869754 Loss: 0.5449371421906434 Corrects: 6174\n",
            "Starting epoch 21/30, LR = [0.01]\n",
            "Train step - Step 1080, Loss 0.43540945649147034\n",
            "Train step - Step 1090, Loss 0.4339841306209564\n",
            "Train step - Step 1100, Loss 0.5387653112411499\n",
            "Train step - Step 1110, Loss 0.5308579206466675\n",
            "Train step - Step 1120, Loss 0.5367008447647095\n",
            "Train step - Step 1130, Loss 0.6031032800674438\n",
            "Train epoch - Accuracy: 0.896671490593343 Loss: 0.5309259745560576 Corrects: 6196\n",
            "Starting epoch 22/30, LR = [0.01]\n",
            "Train step - Step 1140, Loss 0.4931905269622803\n",
            "Train step - Step 1150, Loss 0.5183359980583191\n",
            "Train step - Step 1160, Loss 0.43087369203567505\n",
            "Train step - Step 1170, Loss 0.5759325623512268\n",
            "Train step - Step 1180, Loss 0.49109604954719543\n",
            "Train epoch - Accuracy: 0.9065123010130246 Loss: 0.5150783929742021 Corrects: 6264\n",
            "Starting epoch 23/30, LR = [0.01]\n",
            "Train step - Step 1190, Loss 0.46144676208496094\n",
            "Train step - Step 1200, Loss 0.5437200665473938\n",
            "Train step - Step 1210, Loss 0.5184015035629272\n",
            "Train step - Step 1220, Loss 0.5239510536193848\n",
            "Train step - Step 1230, Loss 0.5661120414733887\n",
            "Train step - Step 1240, Loss 0.4696812629699707\n",
            "Train epoch - Accuracy: 0.8995658465991317 Loss: 0.5159587918418189 Corrects: 6216\n",
            "Starting epoch 24/30, LR = [0.01]\n",
            "Train step - Step 1250, Loss 0.48571354150772095\n",
            "Train step - Step 1260, Loss 0.49718043208122253\n",
            "Train step - Step 1270, Loss 0.5279003381729126\n",
            "Train step - Step 1280, Loss 0.48886555433273315\n",
            "Train step - Step 1290, Loss 0.5161123871803284\n",
            "Train epoch - Accuracy: 0.911287988422576 Loss: 0.4941960910812301 Corrects: 6297\n",
            "Starting epoch 25/30, LR = [0.01]\n",
            "Train step - Step 1300, Loss 0.5216609835624695\n",
            "Train step - Step 1310, Loss 0.5610335469245911\n",
            "Train step - Step 1320, Loss 0.5355522632598877\n",
            "Train step - Step 1330, Loss 0.5158504843711853\n",
            "Train step - Step 1340, Loss 0.6078797578811646\n",
            "Train epoch - Accuracy: 0.9133140376266281 Loss: 0.4833969047721664 Corrects: 6311\n",
            "Starting epoch 26/30, LR = [0.01]\n",
            "Train step - Step 1350, Loss 0.47793322801589966\n",
            "Train step - Step 1360, Loss 0.45238178968429565\n",
            "Train step - Step 1370, Loss 0.5276548862457275\n",
            "Train step - Step 1380, Loss 0.5603050589561462\n",
            "Train step - Step 1390, Loss 0.5128952264785767\n",
            "Train step - Step 1400, Loss 0.5319066643714905\n",
            "Train epoch - Accuracy: 0.9105643994211288 Loss: 0.48775341226119556 Corrects: 6292\n",
            "Starting epoch 27/30, LR = [0.01]\n",
            "Train step - Step 1410, Loss 0.4300575256347656\n",
            "Train step - Step 1420, Loss 0.5019392967224121\n",
            "Train step - Step 1430, Loss 0.4404504597187042\n",
            "Train step - Step 1440, Loss 0.5213887691497803\n",
            "Train step - Step 1450, Loss 0.46548783779144287\n",
            "Train epoch - Accuracy: 0.9151953690303908 Loss: 0.4754898305222198 Corrects: 6324\n",
            "Starting epoch 28/30, LR = [0.01]\n",
            "Train step - Step 1460, Loss 0.41826510429382324\n",
            "Train step - Step 1470, Loss 0.4713350534439087\n",
            "Train step - Step 1480, Loss 0.5306761264801025\n",
            "Train step - Step 1490, Loss 0.49168166518211365\n",
            "Train step - Step 1500, Loss 0.48498260974884033\n",
            "Train step - Step 1510, Loss 0.5194580554962158\n",
            "Train epoch - Accuracy: 0.9253256150506513 Loss: 0.45793512545515247 Corrects: 6394\n",
            "Starting epoch 29/30, LR = [0.01]\n",
            "Train step - Step 1520, Loss 0.3731660544872284\n",
            "Train step - Step 1530, Loss 0.4570215344429016\n",
            "Train step - Step 1540, Loss 0.4529609680175781\n",
            "Train step - Step 1550, Loss 0.37715989351272583\n",
            "Train step - Step 1560, Loss 0.540656328201294\n",
            "Train epoch - Accuracy: 0.9244573082489146 Loss: 0.4506680325299027 Corrects: 6388\n",
            "Starting epoch 30/30, LR = [0.01]\n",
            "Train step - Step 1570, Loss 0.4185623228549957\n",
            "Train step - Step 1580, Loss 0.38545793294906616\n",
            "Train step - Step 1590, Loss 0.3772276043891907\n",
            "Train step - Step 1600, Loss 0.5343316793441772\n",
            "Train step - Step 1610, Loss 0.4048638641834259\n",
            "Train epoch - Accuracy: 0.924602026049204 Loss: 0.4542182086173084 Corrects: 6389\n",
            "Training finished in 273.36671900749207 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0, 95, 83, 63, 42, 30, 6, 2, 97, 72, 36, 55, 31, 19, 98, 94, 54, 93, 85, 9, 96, 99, 15, 14, 57, 45, 13, 88, 60, 40, 8, 35, 27, 86, 70, 50, 69, 53, 17, 84, 52, 71, 51, 43, 78, 74, 38, 37, 29, 48, 44]\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  25\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12c0a90>\n",
            "Constructing exemplars of class 71\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [11493, 35701, 2643, 17377, 21723, 28103, 12984, 32559, 373, 24644, 35701, 568, 27452, 10754, 30972, 46573, 3615, 21986, 48425, 11968, 22840, 17365, 47656, 5409, 14365]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d124fa50>\n",
            "Constructing exemplars of class 51\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [35709, 38043, 16645, 10437, 19334, 3781, 16859, 2645, 24491, 44385, 33846, 20360, 33216, 15524, 46371, 33746, 17319, 33557, 40925, 7042, 17400, 20157, 48859, 42640, 4272]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d3ce1290>\n",
            "Constructing exemplars of class 43\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [2618, 12782, 48556, 45857, 17574, 23017, 18307, 15712, 21160, 31032, 18464, 4078, 2595, 42437, 1318, 49152, 14537, 7089, 16037, 18853, 13236, 344, 616, 17262, 10887]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d3ce1650>\n",
            "Constructing exemplars of class 78\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [44507, 36503, 28749, 19436, 48341, 9387, 33351, 165, 19907, 16264, 33671, 11476, 29851, 47918, 2916, 27253, 30672, 1248, 21492, 4495, 15532, 35515, 43630, 23175, 42102]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d640c390>\n",
            "Constructing exemplars of class 74\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [29609, 45798, 14798, 883, 35255, 49061, 28264, 34280, 20516, 24924, 37051, 44924, 49463, 36630, 194, 10908, 8523, 49523, 36298, 29484, 49386, 21759, 28511, 5736, 8782]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d1347f50>\n",
            "Constructing exemplars of class 38\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [42882, 35956, 25951, 21896, 37798, 48765, 29830, 9185, 8212, 22049, 44271, 48229, 11757, 22337, 24136, 2703, 37174, 48895, 12004, 35113, 39910, 31646, 25475, 6820, 28720]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6b213d0>\n",
            "Constructing exemplars of class 37\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [22090, 5458, 43441, 26751, 13018, 20260, 28064, 26271, 16893, 48330, 34689, 7544, 17442, 4335, 20423, 2704, 42151, 16883, 39421, 45600, 40493, 12416, 36519, 22723, 6730]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6bb7810>\n",
            "Constructing exemplars of class 29\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [44889, 24998, 30977, 18870, 3800, 24440, 15839, 42694, 42411, 14599, 33724, 3171, 10139, 5847, 15765, 16884, 44116, 14704, 3788, 39177, 24961, 29096, 41585, 35936, 44948]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d3ce1150>\n",
            "Constructing exemplars of class 48\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [41044, 32078, 28295, 43981, 48887, 41180, 15007, 7732, 34150, 35206, 30247, 20237, 21584, 10050, 14437, 39781, 12845, 45046, 49176, 17073, 42543, 34685, 31900, 44506, 34397]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d3ce1210>\n",
            "Constructing exemplars of class 44\n",
            "lunghezza exemplar set:  25\n",
            "exemplar set:  [31247, 43719, 14330, 20540, 14323, 39080, 9382, 17570, 42805, 16102, 30732, 48226, 35364, 35370, 41492, 49773, 19493, 11887, 14345, 14142, 42989, 29432, 39668, 23818, 36921]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.78 1.946778416633606\n",
            "TEST GROUP:  0.755\n",
            "TEST ALL:  0.377625\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  9000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 10, 26, 34, 42, 50, 58, 74, 82, 90, 98, 19, 27, 35, 43, 51, 59, 67, 75, 83, 99, 18, 2, 87, 97, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 9, 17, 41, 49, 57, 65, 81, 4, 12, 20, 28, 22, 30, 38, 46, 54, 70, 78, 86, 94, 7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 14, 6, 93, 5, 36, 44, 52, 60, 68, 76, 84, 92, 13, 85, 21, 29, 37, 45, 53, 61, 69, 77, 0]\n",
            "TRAIN_SET CLASSES:  [87, 58, 46, 26, 77, 41, 5, 92, 28, 12]\n",
            "VALIDATION CLASSES:  [28, 58, 46, 41, 92, 26, 87, 77, 12, 5]\n",
            "GROUP:  9\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [87, 58, 46, 26, 77, 41, 5, 92, 28, 12]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "Starting epoch 1/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 9.178956031799316\n",
            "Train step - Step 10, Loss 5.794130325317383\n",
            "Train step - Step 20, Loss 3.6998071670532227\n",
            "Train step - Step 30, Loss 2.25759220123291\n",
            "Train step - Step 40, Loss 1.69817054271698\n",
            "Train step - Step 50, Loss 1.4472994804382324\n",
            "Train epoch - Accuracy: 0.42776978417266187 Loss: 3.6256315599414086 Corrects: 2973\n",
            "Starting epoch 2/30, LR = [0.01]\n",
            "Train step - Step 60, Loss 1.3234872817993164\n",
            "Train step - Step 70, Loss 1.2879843711853027\n",
            "Train step - Step 80, Loss 1.1007277965545654\n",
            "Train step - Step 90, Loss 0.9847685098648071\n",
            "Train step - Step 100, Loss 1.0801342725753784\n",
            "Train epoch - Accuracy: 0.7132374100719424 Loss: 1.177217475321653 Corrects: 4957\n",
            "Starting epoch 3/30, LR = [0.01]\n",
            "Train step - Step 110, Loss 0.9221155643463135\n",
            "Train step - Step 120, Loss 0.799051821231842\n",
            "Train step - Step 130, Loss 1.1177895069122314\n",
            "Train step - Step 140, Loss 0.923631489276886\n",
            "Train step - Step 150, Loss 0.8420207500457764\n",
            "Train step - Step 160, Loss 0.9056873321533203\n",
            "Train epoch - Accuracy: 0.7638848920863309 Loss: 0.9439083131611776 Corrects: 5309\n",
            "Starting epoch 4/30, LR = [0.01]\n",
            "Train step - Step 170, Loss 0.8947983980178833\n",
            "Train step - Step 180, Loss 0.9168405532836914\n",
            "Train step - Step 190, Loss 0.8753941655158997\n",
            "Train step - Step 200, Loss 0.7869996428489685\n",
            "Train step - Step 210, Loss 0.7931436896324158\n",
            "Train epoch - Accuracy: 0.796115107913669 Loss: 0.8427264846143105 Corrects: 5533\n",
            "Starting epoch 5/30, LR = [0.01]\n",
            "Train step - Step 220, Loss 0.7114661931991577\n",
            "Train step - Step 230, Loss 0.952212929725647\n",
            "Train step - Step 240, Loss 0.7585980296134949\n",
            "Train step - Step 250, Loss 0.8103047609329224\n",
            "Train step - Step 260, Loss 0.7470149993896484\n",
            "Train step - Step 270, Loss 0.8488378524780273\n",
            "Train epoch - Accuracy: 0.8159712230215828 Loss: 0.7943077442114301 Corrects: 5671\n",
            "Starting epoch 6/30, LR = [0.01]\n",
            "Train step - Step 280, Loss 0.6563715934753418\n",
            "Train step - Step 290, Loss 0.6505995988845825\n",
            "Train step - Step 300, Loss 0.7971585988998413\n",
            "Train step - Step 310, Loss 0.7500357627868652\n",
            "Train step - Step 320, Loss 0.8918116688728333\n",
            "Train epoch - Accuracy: 0.8217266187050359 Loss: 0.7440904352990844 Corrects: 5711\n",
            "Starting epoch 7/30, LR = [0.01]\n",
            "Train step - Step 330, Loss 0.6784418225288391\n",
            "Train step - Step 340, Loss 0.6403279900550842\n",
            "Train step - Step 350, Loss 0.7356117963790894\n",
            "Train step - Step 360, Loss 0.6924269199371338\n",
            "Train step - Step 370, Loss 0.6969252228736877\n",
            "Train step - Step 380, Loss 0.8420515060424805\n",
            "Train epoch - Accuracy: 0.8359712230215828 Loss: 0.7231395636016517 Corrects: 5810\n",
            "Starting epoch 8/30, LR = [0.01]\n",
            "Train step - Step 390, Loss 0.5292291045188904\n",
            "Train step - Step 400, Loss 0.6176861524581909\n",
            "Train step - Step 410, Loss 0.5425536632537842\n",
            "Train step - Step 420, Loss 0.8116111159324646\n",
            "Train step - Step 430, Loss 0.672634482383728\n",
            "Train epoch - Accuracy: 0.8402877697841726 Loss: 0.6945918594504432 Corrects: 5840\n",
            "Starting epoch 9/30, LR = [0.01]\n",
            "Train step - Step 440, Loss 0.5888742208480835\n",
            "Train step - Step 450, Loss 0.7118898630142212\n",
            "Train step - Step 460, Loss 0.6401085257530212\n",
            "Train step - Step 470, Loss 0.7386770248413086\n",
            "Train step - Step 480, Loss 0.6192387342453003\n",
            "Train step - Step 490, Loss 0.7241134643554688\n",
            "Train epoch - Accuracy: 0.8553956834532375 Loss: 0.6463252763267902 Corrects: 5945\n",
            "Starting epoch 10/30, LR = [0.01]\n",
            "Train step - Step 500, Loss 0.7086529731750488\n",
            "Train step - Step 510, Loss 0.5746974945068359\n",
            "Train step - Step 520, Loss 0.741412341594696\n",
            "Train step - Step 530, Loss 0.6041694283485413\n",
            "Train step - Step 540, Loss 0.6592962741851807\n",
            "Train epoch - Accuracy: 0.8663309352517986 Loss: 0.6271554436958093 Corrects: 6021\n",
            "Starting epoch 11/30, LR = [0.01]\n",
            "Train step - Step 550, Loss 0.5530387163162231\n",
            "Train step - Step 560, Loss 0.62936931848526\n",
            "Train step - Step 570, Loss 0.6515132784843445\n",
            "Train step - Step 580, Loss 0.5537280440330505\n",
            "Train step - Step 590, Loss 0.596565842628479\n",
            "Train step - Step 600, Loss 0.6341041922569275\n",
            "Train epoch - Accuracy: 0.8682014388489209 Loss: 0.6177196672494463 Corrects: 6034\n",
            "Starting epoch 12/30, LR = [0.01]\n",
            "Train step - Step 610, Loss 0.6148593425750732\n",
            "Train step - Step 620, Loss 0.5953749418258667\n",
            "Train step - Step 630, Loss 0.5421485900878906\n",
            "Train step - Step 640, Loss 0.7252874970436096\n",
            "Train step - Step 650, Loss 0.5580439567565918\n",
            "Train epoch - Accuracy: 0.883453237410072 Loss: 0.5906742927153333 Corrects: 6140\n",
            "Starting epoch 13/30, LR = [0.01]\n",
            "Train step - Step 660, Loss 0.5265190005302429\n",
            "Train step - Step 670, Loss 0.5580346584320068\n",
            "Train step - Step 680, Loss 0.5992938876152039\n",
            "Train step - Step 690, Loss 0.6130492091178894\n",
            "Train step - Step 700, Loss 0.4973567724227905\n",
            "Train step - Step 710, Loss 0.5031783580780029\n",
            "Train epoch - Accuracy: 0.8866187050359712 Loss: 0.5600678125045282 Corrects: 6162\n",
            "Starting epoch 14/30, LR = [0.01]\n",
            "Train step - Step 720, Loss 0.5044861435890198\n",
            "Train step - Step 730, Loss 0.4858227074146271\n",
            "Train step - Step 740, Loss 0.46656835079193115\n",
            "Train step - Step 750, Loss 0.5351928472518921\n",
            "Train step - Step 760, Loss 0.5372371673583984\n",
            "Train epoch - Accuracy: 0.8969784172661871 Loss: 0.5420873919665384 Corrects: 6234\n",
            "Starting epoch 15/30, LR = [0.01]\n",
            "Train step - Step 770, Loss 0.5171308517456055\n",
            "Train step - Step 780, Loss 0.558151364326477\n",
            "Train step - Step 790, Loss 0.6308633089065552\n",
            "Train step - Step 800, Loss 0.5909850597381592\n",
            "Train step - Step 810, Loss 0.45850682258605957\n",
            "Train step - Step 820, Loss 0.6078512668609619\n",
            "Train epoch - Accuracy: 0.8935251798561151 Loss: 0.5430174083160839 Corrects: 6210\n",
            "Starting epoch 16/30, LR = [0.01]\n",
            "Train step - Step 830, Loss 0.43807893991470337\n",
            "Train step - Step 840, Loss 0.5079823136329651\n",
            "Train step - Step 850, Loss 0.5120669603347778\n",
            "Train step - Step 860, Loss 0.47562375664711\n",
            "Train step - Step 870, Loss 0.4923757314682007\n",
            "Train epoch - Accuracy: 0.9100719424460432 Loss: 0.504136702156753 Corrects: 6325\n",
            "Starting epoch 17/30, LR = [0.01]\n",
            "Train step - Step 880, Loss 0.42429861426353455\n",
            "Train step - Step 890, Loss 0.4553107023239136\n",
            "Train step - Step 900, Loss 0.5248022079467773\n",
            "Train step - Step 910, Loss 0.43542900681495667\n",
            "Train step - Step 920, Loss 0.5660829544067383\n",
            "Train step - Step 930, Loss 0.4599490761756897\n",
            "Train epoch - Accuracy: 0.9112230215827338 Loss: 0.49014037084236417 Corrects: 6333\n",
            "Starting epoch 18/30, LR = [0.01]\n",
            "Train step - Step 940, Loss 0.3922932744026184\n",
            "Train step - Step 950, Loss 0.44849494099617004\n",
            "Train step - Step 960, Loss 0.5300898551940918\n",
            "Train step - Step 970, Loss 0.5089747309684753\n",
            "Train step - Step 980, Loss 0.5428425669670105\n",
            "Train epoch - Accuracy: 0.9076258992805756 Loss: 0.4956382117511557 Corrects: 6308\n",
            "Starting epoch 19/30, LR = [0.01]\n",
            "Train step - Step 990, Loss 0.4311645030975342\n",
            "Train step - Step 1000, Loss 0.48061031103134155\n",
            "Train step - Step 1010, Loss 0.46772825717926025\n",
            "Train step - Step 1020, Loss 0.4628203511238098\n",
            "Train step - Step 1030, Loss 0.6771055459976196\n",
            "Train step - Step 1040, Loss 0.42866888642311096\n",
            "Train epoch - Accuracy: 0.92 Loss: 0.47119037046706935 Corrects: 6394\n",
            "Starting epoch 20/30, LR = [0.01]\n",
            "Train step - Step 1050, Loss 0.4800458550453186\n",
            "Train step - Step 1060, Loss 0.4333680272102356\n",
            "Train step - Step 1070, Loss 0.38176605105400085\n",
            "Train step - Step 1080, Loss 0.5300453305244446\n",
            "Train step - Step 1090, Loss 0.5151296257972717\n",
            "Train epoch - Accuracy: 0.9159712230215827 Loss: 0.46859237221505146 Corrects: 6366\n",
            "Starting epoch 21/30, LR = [0.01]\n",
            "Train step - Step 1100, Loss 0.4475792646408081\n",
            "Train step - Step 1110, Loss 0.4900037348270416\n",
            "Train step - Step 1120, Loss 0.4039865732192993\n",
            "Train step - Step 1130, Loss 0.5004958510398865\n",
            "Train step - Step 1140, Loss 0.5876137018203735\n",
            "Train step - Step 1150, Loss 0.471618115901947\n",
            "Train epoch - Accuracy: 0.9207194244604316 Loss: 0.4639907919245658 Corrects: 6399\n",
            "Starting epoch 22/30, LR = [0.01]\n",
            "Train step - Step 1160, Loss 0.4634963572025299\n",
            "Train step - Step 1170, Loss 0.4830687642097473\n",
            "Train step - Step 1180, Loss 0.4710139036178589\n",
            "Train step - Step 1190, Loss 0.39358755946159363\n",
            "Train step - Step 1200, Loss 0.40278002619743347\n",
            "Train epoch - Accuracy: 0.9310791366906475 Loss: 0.4318683492880073 Corrects: 6471\n",
            "Starting epoch 23/30, LR = [0.01]\n",
            "Train step - Step 1210, Loss 0.43863242864608765\n",
            "Train step - Step 1220, Loss 0.36745554208755493\n",
            "Train step - Step 1230, Loss 0.47098827362060547\n",
            "Train step - Step 1240, Loss 0.4479806423187256\n",
            "Train step - Step 1250, Loss 0.43502455949783325\n",
            "Train step - Step 1260, Loss 0.41480711102485657\n",
            "Train epoch - Accuracy: 0.9273381294964029 Loss: 0.4345427210725469 Corrects: 6445\n",
            "Starting epoch 24/30, LR = [0.01]\n",
            "Train step - Step 1270, Loss 0.37366336584091187\n",
            "Train step - Step 1280, Loss 0.41426676511764526\n",
            "Train step - Step 1290, Loss 0.45204591751098633\n",
            "Train step - Step 1300, Loss 0.42062729597091675\n",
            "Train step - Step 1310, Loss 0.39378657937049866\n",
            "Train epoch - Accuracy: 0.9329496402877698 Loss: 0.43431142426223207 Corrects: 6484\n",
            "Starting epoch 25/30, LR = [0.01]\n",
            "Train step - Step 1320, Loss 0.3648450970649719\n",
            "Train step - Step 1330, Loss 0.3864625096321106\n",
            "Train step - Step 1340, Loss 0.38314348459243774\n",
            "Train step - Step 1350, Loss 0.4197915494441986\n",
            "Train step - Step 1360, Loss 0.42679154872894287\n",
            "Train step - Step 1370, Loss 0.37911003828048706\n",
            "Train epoch - Accuracy: 0.9330935251798561 Loss: 0.417141122852298 Corrects: 6485\n",
            "Starting epoch 26/30, LR = [0.01]\n",
            "Train step - Step 1380, Loss 0.46178489923477173\n",
            "Train step - Step 1390, Loss 0.4185032844543457\n",
            "Train step - Step 1400, Loss 0.5373398065567017\n",
            "Train step - Step 1410, Loss 0.3650091886520386\n",
            "Train step - Step 1420, Loss 0.3686647415161133\n",
            "Train epoch - Accuracy: 0.9374100719424461 Loss: 0.41719070676419373 Corrects: 6515\n",
            "Starting epoch 27/30, LR = [0.01]\n",
            "Train step - Step 1430, Loss 0.4368923604488373\n",
            "Train step - Step 1440, Loss 0.2967362403869629\n",
            "Train step - Step 1450, Loss 0.356181800365448\n",
            "Train step - Step 1460, Loss 0.45605558156967163\n",
            "Train step - Step 1470, Loss 0.3890378177165985\n",
            "Train step - Step 1480, Loss 0.3221477270126343\n",
            "Train epoch - Accuracy: 0.9440287769784173 Loss: 0.39503797078304154 Corrects: 6561\n",
            "Starting epoch 28/30, LR = [0.01]\n",
            "Train step - Step 1490, Loss 0.44064173102378845\n",
            "Train step - Step 1500, Loss 0.3925122618675232\n",
            "Train step - Step 1510, Loss 0.36835625767707825\n",
            "Train step - Step 1520, Loss 0.40570691227912903\n",
            "Train step - Step 1530, Loss 0.415657103061676\n",
            "Train epoch - Accuracy: 0.9374100719424461 Loss: 0.40160995827304374 Corrects: 6515\n",
            "Starting epoch 29/30, LR = [0.01]\n",
            "Train step - Step 1540, Loss 0.3363177180290222\n",
            "Train step - Step 1550, Loss 0.42014822363853455\n",
            "Train step - Step 1560, Loss 0.3101350665092468\n",
            "Train step - Step 1570, Loss 0.3878331184387207\n",
            "Train step - Step 1580, Loss 0.3501206338405609\n",
            "Train step - Step 1590, Loss 0.41844141483306885\n",
            "Train epoch - Accuracy: 0.9487769784172662 Loss: 0.3723946769803548 Corrects: 6594\n",
            "Starting epoch 30/30, LR = [0.01]\n",
            "Train step - Step 1600, Loss 0.3605221211910248\n",
            "Train step - Step 1610, Loss 0.36451810598373413\n",
            "Train step - Step 1620, Loss 0.35933923721313477\n",
            "Train step - Step 1630, Loss 0.3850279152393341\n",
            "Train step - Step 1640, Loss 0.3502483069896698\n",
            "Train epoch - Accuracy: 0.9507913669064748 Loss: 0.3720366646231507 Corrects: 6608\n",
            "Training finished in 276.3279092311859 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0, 95, 83, 63, 42, 30, 6, 2, 97, 72, 36, 55, 31, 19, 98, 94, 54, 93, 85, 9, 96, 99, 15, 14, 57, 45, 13, 88, 60, 40, 8, 35, 27, 86, 70, 50, 69, 53, 17, 84, 52, 71, 51, 43, 78, 74, 38, 37, 29, 48, 44, 87, 58, 46, 26, 77, 41, 5, 92, 28, 12]\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  22\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d6bb7e90>\n",
            "Constructing exemplars of class 87\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [19583, 10595, 11967, 19726, 16515, 24711, 9484, 27684, 46448, 21052, 49062, 2615, 45803, 25161, 25637, 42821, 5096, 8328, 17375, 46361, 43074, 48963]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d249b050>\n",
            "Constructing exemplars of class 58\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [27665, 21529, 3450, 11821, 39162, 44027, 47072, 19550, 21944, 21254, 46080, 11966, 14801, 43421, 42277, 1832, 404, 49829, 3051, 29724, 22331, 48543]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d63d6a50>\n",
            "Constructing exemplars of class 46\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [44792, 43804, 4451, 42641, 29145, 46273, 5342, 1366, 11678, 45112, 13831, 11990, 17609, 44409, 5378, 40882, 885, 5997, 35976, 27062, 5028, 11240]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d123d3d0>\n",
            "Constructing exemplars of class 26\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [12830, 31444, 25795, 22893, 37514, 2473, 43252, 31807, 23562, 49629, 6487, 39284, 10986, 6862, 34020, 42229, 40879, 10850, 37303, 46941, 21561, 43258]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d248ca50>\n",
            "Constructing exemplars of class 77\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [375, 3167, 46115, 32540, 15086, 7391, 11604, 35366, 3099, 26912, 30801, 33941, 11750, 27851, 12079, 14631, 19697, 5636, 9098, 33282, 17139, 12576]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d8bfe250>\n",
            "Constructing exemplars of class 41\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [9991, 45459, 41898, 24615, 7774, 25203, 16375, 34264, 13296, 41474, 42589, 1060, 16085, 1152, 42927, 22570, 24871, 17864, 11605, 5085, 41749, 10896]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12f31d0>\n",
            "Constructing exemplars of class 5\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [48292, 706, 47511, 17924, 19751, 33624, 25066, 41793, 8724, 37170, 4594, 34557, 26717, 47955, 32879, 11119, 38419, 13463, 39023, 16721, 23315, 7667]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d640c390>\n",
            "Constructing exemplars of class 92\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [37859, 16347, 14277, 45395, 46322, 29265, 8272, 48153, 38742, 19710, 37479, 32113, 4033, 43106, 34778, 37511, 24076, 1815, 4310, 16846, 24851, 18926]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d119e150>\n",
            "Constructing exemplars of class 28\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [17281, 17617, 13445, 7683, 48307, 41611, 39922, 18871, 6547, 42041, 315, 42952, 3324, 43301, 11942, 12143, 39875, 32408, 5467, 16267, 12153, 41551]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d132b850>\n",
            "Constructing exemplars of class 12\n",
            "lunghezza exemplar set:  22\n",
            "exemplar set:  [38958, 27597, 6189, 41561, 33452, 22943, 45806, 1877, 11957, 25148, 5367, 39732, 35972, 33947, 5937, 15663, 28276, 8832, 11244, 6833, 24355, 21150]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.8 1.9521970748901367\n",
            "TEST GROUP:  0.837\n",
            "TEST ALL:  0.37322222222222223\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  10000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 82, 2, 10, 18, 26, 34, 42, 50, 58, 66, 74, 90, 87, 98, 3, 11, 19, 27, 35, 43, 51, 59, 67, 97, 89, 81, 73, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 1, 9, 17, 25, 33, 41, 49, 57, 65, 75, 83, 91, 93, 14, 22, 30, 38, 46, 54, 62, 70, 78, 86, 94, 7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 6, 85, 99, 77, 4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92, 5, 13, 21, 29, 37, 45, 53, 61, 69, 0]\n",
            "TRAIN_SET CLASSES:  [91, 11, 3, 66, 62, 89, 73, 33, 25, 1]\n",
            "VALIDATION CLASSES:  [62, 25, 33, 91, 89, 11, 73, 3, 66, 1]\n",
            "GROUP:  10\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [91, 11, 3, 66, 62, 89, 73, 33, 25, 1]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "Starting epoch 1/30, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2742: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 9.775483131408691\n",
            "Train step - Step 10, Loss 6.303915977478027\n",
            "Train step - Step 20, Loss 3.4255733489990234\n",
            "Train step - Step 30, Loss 2.0011751651763916\n",
            "Train step - Step 40, Loss 1.7836265563964844\n",
            "Train step - Step 50, Loss 1.3438512086868286\n",
            "Train epoch - Accuracy: 0.45396825396825397 Loss: 3.5264304550523193 Corrects: 3146\n",
            "Starting epoch 2/30, LR = [0.01]\n",
            "Train step - Step 60, Loss 1.3544520139694214\n",
            "Train step - Step 70, Loss 1.2062047719955444\n",
            "Train step - Step 80, Loss 1.25858736038208\n",
            "Train step - Step 90, Loss 1.309112548828125\n",
            "Train step - Step 100, Loss 1.1611347198486328\n",
            "Train epoch - Accuracy: 0.7063492063492064 Loss: 1.2184061281608813 Corrects: 4895\n",
            "Starting epoch 3/30, LR = [0.01]\n",
            "Train step - Step 110, Loss 1.049349308013916\n",
            "Train step - Step 120, Loss 1.0700749158859253\n",
            "Train step - Step 130, Loss 1.029944896697998\n",
            "Train step - Step 140, Loss 1.0674993991851807\n",
            "Train step - Step 150, Loss 1.0850838422775269\n",
            "Train step - Step 160, Loss 1.000741958618164\n",
            "Train epoch - Accuracy: 0.7578643578643579 Loss: 0.9696639045492396 Corrects: 5252\n",
            "Starting epoch 4/30, LR = [0.01]\n",
            "Train step - Step 170, Loss 0.8475964069366455\n",
            "Train step - Step 180, Loss 0.8053510189056396\n",
            "Train step - Step 190, Loss 0.7867026925086975\n",
            "Train step - Step 200, Loss 0.8081492781639099\n",
            "Train step - Step 210, Loss 0.7705689668655396\n",
            "Train epoch - Accuracy: 0.779076479076479 Loss: 0.8884621348387923 Corrects: 5399\n",
            "Starting epoch 5/30, LR = [0.01]\n",
            "Train step - Step 220, Loss 0.9759266972541809\n",
            "Train step - Step 230, Loss 0.9154915809631348\n",
            "Train step - Step 240, Loss 0.7618203163146973\n",
            "Train step - Step 250, Loss 0.7859057188034058\n",
            "Train step - Step 260, Loss 0.7740834951400757\n",
            "Train step - Step 270, Loss 0.7839337587356567\n",
            "Train epoch - Accuracy: 0.7991341991341991 Loss: 0.8344558295061406 Corrects: 5538\n",
            "Starting epoch 6/30, LR = [0.01]\n",
            "Train step - Step 280, Loss 0.7745245695114136\n",
            "Train step - Step 290, Loss 0.8634713888168335\n",
            "Train step - Step 300, Loss 0.7155903577804565\n",
            "Train step - Step 310, Loss 0.7770912051200867\n",
            "Train step - Step 320, Loss 0.7554197311401367\n",
            "Train epoch - Accuracy: 0.8088023088023089 Loss: 0.7837077198606549 Corrects: 5605\n",
            "Starting epoch 7/30, LR = [0.01]\n",
            "Train step - Step 330, Loss 0.6328693628311157\n",
            "Train step - Step 340, Loss 0.7444741725921631\n",
            "Train step - Step 350, Loss 0.6578630208969116\n",
            "Train step - Step 360, Loss 0.72353196144104\n",
            "Train step - Step 370, Loss 0.6286916732788086\n",
            "Train step - Step 380, Loss 0.680144190788269\n",
            "Train epoch - Accuracy: 0.8285714285714286 Loss: 0.7321991049022042 Corrects: 5742\n",
            "Starting epoch 8/30, LR = [0.01]\n",
            "Train step - Step 390, Loss 0.7627871632575989\n",
            "Train step - Step 400, Loss 0.7212303280830383\n",
            "Train step - Step 410, Loss 0.7689992785453796\n",
            "Train step - Step 420, Loss 0.6660546064376831\n",
            "Train step - Step 430, Loss 0.6043747663497925\n",
            "Train epoch - Accuracy: 0.8316017316017316 Loss: 0.7244270631933281 Corrects: 5763\n",
            "Starting epoch 9/30, LR = [0.01]\n",
            "Train step - Step 440, Loss 0.5490670204162598\n",
            "Train step - Step 450, Loss 0.7266191244125366\n",
            "Train step - Step 460, Loss 0.6590206623077393\n",
            "Train step - Step 470, Loss 0.6846872568130493\n",
            "Train step - Step 480, Loss 0.7149537801742554\n",
            "Train step - Step 490, Loss 0.7175877094268799\n",
            "Train epoch - Accuracy: 0.8417027417027417 Loss: 0.7202568154314379 Corrects: 5833\n",
            "Starting epoch 10/30, LR = [0.01]\n",
            "Train step - Step 500, Loss 0.6837344765663147\n",
            "Train step - Step 510, Loss 0.7291262745857239\n",
            "Train step - Step 520, Loss 0.5981805324554443\n",
            "Train step - Step 530, Loss 0.7462076544761658\n",
            "Train step - Step 540, Loss 0.6552469730377197\n",
            "Train epoch - Accuracy: 0.8510822510822511 Loss: 0.6509049530841228 Corrects: 5898\n",
            "Starting epoch 11/30, LR = [0.01]\n",
            "Train step - Step 550, Loss 0.5567063093185425\n",
            "Train step - Step 560, Loss 0.5501372814178467\n",
            "Train step - Step 570, Loss 0.6462144255638123\n",
            "Train step - Step 580, Loss 0.6177850365638733\n",
            "Train step - Step 590, Loss 0.6062198281288147\n",
            "Train step - Step 600, Loss 0.5359509587287903\n",
            "Train epoch - Accuracy: 0.870995670995671 Loss: 0.6238944888286948 Corrects: 6036\n",
            "Starting epoch 12/30, LR = [0.01]\n",
            "Train step - Step 610, Loss 0.5601303577423096\n",
            "Train step - Step 620, Loss 0.633254885673523\n",
            "Train step - Step 630, Loss 0.7820276021957397\n",
            "Train step - Step 640, Loss 0.5436699390411377\n",
            "Train step - Step 650, Loss 0.7492518424987793\n",
            "Train epoch - Accuracy: 0.8761904761904762 Loss: 0.5949417622316451 Corrects: 6072\n",
            "Starting epoch 13/30, LR = [0.01]\n",
            "Train step - Step 660, Loss 0.546196460723877\n",
            "Train step - Step 670, Loss 0.6719585657119751\n",
            "Train step - Step 680, Loss 0.6226023435592651\n",
            "Train step - Step 690, Loss 0.5073673725128174\n",
            "Train step - Step 700, Loss 0.5866298675537109\n",
            "Train step - Step 710, Loss 0.616933286190033\n",
            "Train epoch - Accuracy: 0.8831168831168831 Loss: 0.56620362799233 Corrects: 6120\n",
            "Starting epoch 14/30, LR = [0.01]\n",
            "Train step - Step 720, Loss 0.6407608985900879\n",
            "Train step - Step 730, Loss 0.664351761341095\n",
            "Train step - Step 740, Loss 0.7071537971496582\n",
            "Train step - Step 750, Loss 0.5934759378433228\n",
            "Train step - Step 760, Loss 0.5383747816085815\n",
            "Train epoch - Accuracy: 0.8772005772005772 Loss: 0.5936038506495488 Corrects: 6079\n",
            "Starting epoch 15/30, LR = [0.01]\n",
            "Train step - Step 770, Loss 0.4521346092224121\n",
            "Train step - Step 780, Loss 0.5332062244415283\n",
            "Train step - Step 790, Loss 0.5319038033485413\n",
            "Train step - Step 800, Loss 0.4887677729129791\n",
            "Train step - Step 810, Loss 0.6460756063461304\n",
            "Train step - Step 820, Loss 0.6052529811859131\n",
            "Train epoch - Accuracy: 0.8822510822510823 Loss: 0.5634978586117082 Corrects: 6114\n",
            "Starting epoch 16/30, LR = [0.01]\n",
            "Train step - Step 830, Loss 0.40969082713127136\n",
            "Train step - Step 840, Loss 0.4992905855178833\n",
            "Train step - Step 850, Loss 0.5101896524429321\n",
            "Train step - Step 860, Loss 0.5625845193862915\n",
            "Train step - Step 870, Loss 0.6904258131980896\n",
            "Train epoch - Accuracy: 0.8953823953823954 Loss: 0.5320379652158178 Corrects: 6205\n",
            "Starting epoch 17/30, LR = [0.01]\n",
            "Train step - Step 880, Loss 0.48323535919189453\n",
            "Train step - Step 890, Loss 0.4832659661769867\n",
            "Train step - Step 900, Loss 0.4999726116657257\n",
            "Train step - Step 910, Loss 0.425462543964386\n",
            "Train step - Step 920, Loss 0.42119526863098145\n",
            "Train step - Step 930, Loss 0.5419827699661255\n",
            "Train epoch - Accuracy: 0.8963924963924964 Loss: 0.5364327529120066 Corrects: 6212\n",
            "Starting epoch 18/30, LR = [0.01]\n",
            "Train step - Step 940, Loss 0.4421146512031555\n",
            "Train step - Step 950, Loss 0.4920848608016968\n",
            "Train step - Step 960, Loss 0.5051096677780151\n",
            "Train step - Step 970, Loss 0.40602952241897583\n",
            "Train step - Step 980, Loss 0.5033055543899536\n",
            "Train epoch - Accuracy: 0.8997113997113997 Loss: 0.5177392445257388 Corrects: 6235\n",
            "Starting epoch 19/30, LR = [0.01]\n",
            "Train step - Step 990, Loss 0.3696442246437073\n",
            "Train step - Step 1000, Loss 0.46888792514801025\n",
            "Train step - Step 1010, Loss 0.41648781299591064\n",
            "Train step - Step 1020, Loss 0.4083603024482727\n",
            "Train step - Step 1030, Loss 0.49942973256111145\n",
            "Train step - Step 1040, Loss 0.45281684398651123\n",
            "Train epoch - Accuracy: 0.9063492063492063 Loss: 0.5044927451861713 Corrects: 6281\n",
            "Starting epoch 20/30, LR = [0.01]\n",
            "Train step - Step 1050, Loss 0.5170458555221558\n",
            "Train step - Step 1060, Loss 0.4773440957069397\n",
            "Train step - Step 1070, Loss 0.5009070634841919\n",
            "Train step - Step 1080, Loss 0.5440614819526672\n",
            "Train step - Step 1090, Loss 0.41261595487594604\n",
            "Train epoch - Accuracy: 0.9181818181818182 Loss: 0.48626628350385853 Corrects: 6363\n",
            "Starting epoch 21/30, LR = [0.01]\n",
            "Train step - Step 1100, Loss 0.4310294985771179\n",
            "Train step - Step 1110, Loss 0.4853869676589966\n",
            "Train step - Step 1120, Loss 0.43772053718566895\n",
            "Train step - Step 1130, Loss 0.5103044509887695\n",
            "Train step - Step 1140, Loss 0.43858522176742554\n",
            "Train step - Step 1150, Loss 0.5004675388336182\n",
            "Train epoch - Accuracy: 0.9238095238095239 Loss: 0.4687557439955454 Corrects: 6402\n",
            "Starting epoch 22/30, LR = [0.01]\n",
            "Train step - Step 1160, Loss 0.3723049461841583\n",
            "Train step - Step 1170, Loss 0.43817102909088135\n",
            "Train step - Step 1180, Loss 0.48343387246131897\n",
            "Train step - Step 1190, Loss 0.4875819683074951\n",
            "Train step - Step 1200, Loss 0.3751603066921234\n",
            "Train epoch - Accuracy: 0.9246753246753247 Loss: 0.45000469576228747 Corrects: 6408\n",
            "Starting epoch 23/30, LR = [0.01]\n",
            "Train step - Step 1210, Loss 0.38672012090682983\n",
            "Train step - Step 1220, Loss 0.49199578166007996\n",
            "Train step - Step 1230, Loss 0.504474401473999\n",
            "Train step - Step 1240, Loss 0.4750756621360779\n",
            "Train step - Step 1250, Loss 0.49842241406440735\n",
            "Train step - Step 1260, Loss 0.4591515064239502\n",
            "Train epoch - Accuracy: 0.9196248196248197 Loss: 0.4822639813326826 Corrects: 6373\n",
            "Starting epoch 24/30, LR = [0.01]\n",
            "Train step - Step 1270, Loss 0.49419647455215454\n",
            "Train step - Step 1280, Loss 0.4827677607536316\n",
            "Train step - Step 1290, Loss 0.44014620780944824\n",
            "Train step - Step 1300, Loss 0.3870537281036377\n",
            "Train step - Step 1310, Loss 0.5360694527626038\n",
            "Train epoch - Accuracy: 0.9264069264069265 Loss: 0.4499696196931781 Corrects: 6420\n",
            "Starting epoch 25/30, LR = [0.01]\n",
            "Train step - Step 1320, Loss 0.44309455156326294\n",
            "Train step - Step 1330, Loss 0.6066538095474243\n",
            "Train step - Step 1340, Loss 0.6012752056121826\n",
            "Train step - Step 1350, Loss 0.4173238277435303\n",
            "Train step - Step 1360, Loss 0.44353049993515015\n",
            "Train step - Step 1370, Loss 0.48065370321273804\n",
            "Train epoch - Accuracy: 0.9111111111111111 Loss: 0.5105967195863159 Corrects: 6314\n",
            "Starting epoch 26/30, LR = [0.01]\n",
            "Train step - Step 1380, Loss 0.37401989102363586\n",
            "Train step - Step 1390, Loss 0.3985159397125244\n",
            "Train step - Step 1400, Loss 0.4288802742958069\n",
            "Train step - Step 1410, Loss 0.4477121829986572\n",
            "Train step - Step 1420, Loss 0.5043727159500122\n",
            "Train epoch - Accuracy: 0.9339105339105339 Loss: 0.4370296765715529 Corrects: 6472\n",
            "Starting epoch 27/30, LR = [0.01]\n",
            "Train step - Step 1430, Loss 0.4246068000793457\n",
            "Train step - Step 1440, Loss 0.4462548494338989\n",
            "Train step - Step 1450, Loss 0.5433899164199829\n",
            "Train step - Step 1460, Loss 0.3692432641983032\n",
            "Train step - Step 1470, Loss 0.40295255184173584\n",
            "Train step - Step 1480, Loss 0.4508839249610901\n",
            "Train epoch - Accuracy: 0.9251082251082251 Loss: 0.460889807648817 Corrects: 6411\n",
            "Starting epoch 28/30, LR = [0.01]\n",
            "Train step - Step 1490, Loss 0.4626125693321228\n",
            "Train step - Step 1500, Loss 0.4843768775463104\n",
            "Train step - Step 1510, Loss 0.4407016336917877\n",
            "Train step - Step 1520, Loss 0.5974003672599792\n",
            "Train step - Step 1530, Loss 0.4052797257900238\n",
            "Train epoch - Accuracy: 0.929004329004329 Loss: 0.4509851885736419 Corrects: 6438\n",
            "Starting epoch 29/30, LR = [0.01]\n",
            "Train step - Step 1540, Loss 0.41263920068740845\n",
            "Train step - Step 1550, Loss 0.4137347936630249\n",
            "Train step - Step 1560, Loss 0.4851597547531128\n",
            "Train step - Step 1570, Loss 0.40638086199760437\n",
            "Train step - Step 1580, Loss 0.3637416660785675\n",
            "Train step - Step 1590, Loss 0.379241943359375\n",
            "Train epoch - Accuracy: 0.9370851370851371 Loss: 0.42581618229548135 Corrects: 6494\n",
            "Starting epoch 30/30, LR = [0.01]\n",
            "Train step - Step 1600, Loss 0.41057151556015015\n",
            "Train step - Step 1610, Loss 0.4506934881210327\n",
            "Train step - Step 1620, Loss 0.42618998885154724\n",
            "Train step - Step 1630, Loss 0.3508020341396332\n",
            "Train step - Step 1640, Loss 0.46191006898880005\n",
            "Train epoch - Accuracy: 0.9378066378066378 Loss: 0.41908565819865523 Corrects: 6499\n",
            "Training finished in 276.8163959980011 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0, 95, 83, 63, 42, 30, 6, 2, 97, 72, 36, 55, 31, 19, 98, 94, 54, 93, 85, 9, 96, 99, 15, 14, 57, 45, 13, 88, 60, 40, 8, 35, 27, 86, 70, 50, 69, 53, 17, 84, 52, 71, 51, 43, 78, 74, 38, 37, 29, 48, 44, 87, 58, 46, 26, 77, 41, 5, 92, 28, 12, 91, 11, 3, 66, 62, 89, 73, 33, 25, 1]\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  20\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d11bc050>\n",
            "Constructing exemplars of class 91\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [5238, 1536, 19231, 44419, 13501, 8290, 46255, 42817, 41298, 12649, 40269, 14248, 17600, 24905, 34077, 22529, 18200, 15836, 47332, 15832]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d2476410>\n",
            "Constructing exemplars of class 11\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [9838, 28281, 26940, 3882, 33440, 33080, 114, 33486, 13383, 26462, 48825, 17486, 25131, 44178, 15831, 49950, 15981, 20085, 13508, 2011]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d123da10>\n",
            "Constructing exemplars of class 3\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [16998, 28410, 48197, 37630, 28473, 39517, 5964, 32575, 47588, 22961, 43990, 37825, 42451, 21860, 33800, 32967, 36102, 39650, 32428, 27383]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d24286d0>\n",
            "Constructing exemplars of class 66\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [5241, 2254, 2621, 49086, 32338, 14902, 8948, 44314, 35623, 45222, 12957, 34787, 12469, 36407, 15453, 23023, 16107, 2684, 13123, 268]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12a1c90>\n",
            "Constructing exemplars of class 62\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [26092, 49760, 13484, 39946, 45208, 46963, 30305, 29143, 16620, 18391, 22469, 10706, 4593, 42881, 45208, 26972, 7711, 30104, 38442, 4154]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d640de10>\n",
            "Constructing exemplars of class 89\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [41994, 14743, 22608, 10047, 17008, 2597, 37659, 25581, 14990, 24850, 19073, 24565, 25581, 3347, 29301, 630, 19338, 6201, 720, 42481]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d11adfd0>\n",
            "Constructing exemplars of class 73\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [33129, 14700, 23693, 41824, 48457, 49281, 24073, 49367, 23807, 34502, 9128, 5862, 3202, 46747, 11109, 15476, 22124, 28506, 37577, 5957]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12c53d0>\n",
            "Constructing exemplars of class 33\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [22410, 31856, 30831, 16311, 39447, 9476, 28459, 40011, 23437, 40337, 22448, 7924, 32833, 11351, 22640, 20276, 9284, 22215, 19137, 3200]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d1244850>\n",
            "Constructing exemplars of class 25\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [48228, 13276, 4674, 5373, 6655, 34978, 10044, 30530, 46144, 25546, 7967, 45458, 24141, 27616, 15289, 8478, 4570, 36398, 49060, 49575]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f71d12c53d0>\n",
            "Constructing exemplars of class 1\n",
            "lunghezza exemplar set:  20\n",
            "exemplar set:  [13789, 41352, 42038, 43477, 38182, 18248, 4987, 27954, 29302, 3168, 4995, 10293, 30118, 44352, 37742, 39770, 48456, 48041, 23459, 49445]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.8 2.0675785541534424\n",
            "TEST GROUP:  0.813\n",
            "TEST ALL:  0.3226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU1ONpIVBqKM"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6fD5peKBpWb",
        "outputId": "c3960de1-f817-4027-f59a-4195062fd169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "method = \"Learning Without Forgetting\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics FINETUNING for seed 66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAJcCAYAAAAb0rWEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdfbwd47338c8vCVJPSUUkSCJUSFWOkFC0gqa0xSkt1SoaT9Xqs7a3oj3Vp9Oj6m6rvVvulKqW46EeymlQ7rROOBUPIURFRQlJiITaCA0iv/uPmc2aa2avNXv2mllrr/19v17rlX3Nuuaaa2bNWmsy63ddP3N3RERERDrNoFZ3QERERKQMusgRERGRjqSLHBEREelIusgRERGRjqSLHBEREelIusgRERGRjqSLnA5mZm8xs/8ys+fN7Hd9aOdIM7upmX1rBTO7wcxmtLofRbVL/83sPDP7tzrPf8vMLq6yT/2RmZ1uZue3uh9lMbNjzOy2VvdDBjZd5LQBM/u4md1tZqvM7Kn4y+zdTWj6MGAUMMLdP1K0EXe/xN33b0J/EsxsHzNzM7smWL5TvPyWnO3k+lJ19w+4+0UF+tkWH9ZF+9+Imf3NzD5aU35XfPzDZS+a2RB3/7S7fzdevo+ZLW12n+r01c1s2zrPH2Nmr8fvpe7H/6mqfzX9SPQz6zi5+/fd/YSq+9auzGywmX3PzJ6Mz7V7zWx4zfPbmNkf4ueeMbOzWtlf6R90kdNiZvZl4CfA94kuSMYBvwAObkLzWwEPu/uaJrRVlpXAHmY2ombZDODhZm3AIm19rpvZkBZufg4wraY8DXgoY9ntbX4udbvd3TeseXyuNyv3h/OlQ30b2BPYA9gYOBpYDWBm6wI3A38CRgNjAN0tlMbcXY8WPYBhwCrgI3XqrEd0EfRk/PgJsF783D7AUuArwArgKeDY+LlvA68Cr8XbOB74FnBxTdvjAQeGxOVjgEeBF4HHgCNrlt9Ws96ewF3A8/G/e9Y8dwvwXeB/4nZuAjbtYd+6+38e8Nl42WBgGfBN4JaauucAS4AXgHnAXvHy9wf7eV9NP/497sc/gW3jZSfEz58LXFXT/g+A2YBl9DOx/8FzE4k+fP8B/A04vOa5A4F74z4vAb6VceyPB54gutA4BrgNOBt4Ln4NPhAc2xNq+1Sn7tZxmy8C/w/4ee1rH+zD0cCCmvL1cfvhsm/Ef/8a+B6wQXxs18bHfhWwBdF5dgXwm3j7fwWm1rT19nhfuuLnPpi1j+Gxj/fHgZfibX20l69Vo/M2PF/2j1/T54n+4/HfQd+OAxbGx/+PwFY99HNGneN0cXA+zIjPh2eAr9ds6y3ARfG2FgKnAEv78NlzDBnv9Xr7leN8HwFcR3S+30n0OZD5WmT0563xcXlbD8+fCNxadH/1GLiPlndgID+IvqDXEF9k9FDnO8BcYDNgJPAX4Lvxc/vE638HWAc4AHgZeGv8/Bsfoj2Uuz9YhxB9Yb0AbB8/tznwjvjvY3jzi2aT+MPv6Hi9I+LyiPj5W4C/A9vFH8y3AGf2sG/7EF3k7AncES87IP5gPYHkRc5R8YfoEKKLuuXA0Kz9qunHE8A74nXWIXmRsD7R3aJjgL2IvlTG9NDPN/Y/WL4B0cXLsfE2do7b2aFm/yYR3TH9F+Bp4JDg2P8mbuct8XZeAz5JdLF3EtGFrdXsU+1FTr26txNdAK0LvDt+bXu6yNmK6At4k7ivK+L+LKlZ9jwwLa7/a+B7ta9h0N63iP4HfkDct/8A5sbPrQM8Apwe9+09RF+024f7mHXs42O2bZ33S0+vVZ7ztvZ8GRkfsw/H5S/Gx7v7+B8c78fb4+e/Afylp37WOU7hRc4v42O/E/AK8Pb4+TOJLrLeSnQX4/6wvV587tR7r/e4XzQ+3y8jurjdANiR6D8rta/dH4BTe+jTNKKL3q8RvbcfJv6PT/z8r4DfAjfE27wFmFTWZ7MenfPQLdnWGgE84/V/AjgS+I67r3D3lUR3aI6uef61+PnX3P16ov8NbV+wP2uBHc3sLe7+lLv/NaPOgcAid/+tu69x90uJftr415o6F7r7w+7+T6IPvcn1NurufwE2MbPtgU8QffGHdS5292fjbf5vojtcjfbz1+7+13id14L2XiY6jj8iuu39eXfvbWzJQcBid78w3sa9wFXAR+Jt3OLuC9x9rbvfD1wK7B208S13fyk+VgCPu/sv3f11ov+5b070M2aWzLpmNg7YFfimu7/q7rcR/Q87k7s/TvQFvxfRl+uiuD//U7NsXeCOXhyb29z9+rhvv43bANgd2JDowvdVd/8T0ZffEb1ou5Hdzayr5rE7+c7bN84X4APAX9396rj8U6Iv326fBv7D3RfGz38fmGxmW/Wx799293+6+33Afbx53A4Hvu/uz8Xn6U/7uJ2e3uv19qvH893MBgOHEp1zL7n7A0Tn5Bvc/SB3P7OH/owhurO9HdFdyMOAb5nZfjXPfyze7y2AWcC18c9YIj3SRU5rPQts2iAeYwvg8Zry4/GyN9oILpJeJvoS6RV3fwn4KNGH3FNmNsvMJuboT3eftqwp134Z5O3Pb4HPAfsC14RPmtlXzWxhPFKsi+gDcdMGbS6p96S730F0y96ILsZ6ayvgnbVfqEQXpaPjPr/TzP5sZivN7HmiYxv2OezjG8cuvhCDno9fT3W3AP5RsyxrO6HuuJxpwK3xsttqlt3p7q80aCOzb0TnwND4PN8CWOLua2ueD8+fvprr7sNrHnPJd97WHqMtasvu7kR3HbttBZxT87r/g+g86ut+9PTeSfSHOq9nPGqrO+j6vPD5Bu/1evtV73wfSXR3p7Zf4fGup/si/zvxRd79RHeGDqh5/jZ3v8HdXyW6SzmC6I6TSI90kdNatxPdkj6kTp0niT5cuo2LlxXxEtHPNN1G1z7p7n909/2I7gg8RHTrvFF/uvu0rGCfuv0W+AxwffDljJntRRSDcDjRT3HDiX4+se6u99BmT8u72/0s0R2hJ+P2e2sJ8N/BF+qG7n5S/Px/Et1BGevuw4hijyxoo24fC3qK6M5Y7Ws9tsE63Rc5e/HmRc6tNcvm9LBeb/v/JDA2COytPX/qnqN9kOe8rd2Xp4juHgBRMHJtmei1/1Tw2r8lviuZpa+vc6I/1Hk9PRq11R10/eke6vT0Xq+3X/XO95VEP53X9mtcL/bv/u6u1XYzeL6M94p0OF3ktJC7P08UYPtzMzvEzNY3s3XM7AM1wyMvBb5hZiPNbNO4ftFRBfOBaWY2zsyGAad1P2Fmo8zsYDPbgOjCaxXRLe3Q9cB28bD3IfEw4x2IfnIozN0fI/op5+sZT29E9AG6EhhiZt8kGn3R7WlgfG9GxJjZdkTBs0cR/Wx1ipnV+1nNzGxo7YNon7czs6Pj120dM9vVzLr/d7kR0R2V1Wa2G/DxvP3ri/jnp7uJbveva2Z7kPxZJsscohiLaUQ/UwEsIPrpYF96vsh5GhgRn0953EF0h+KU+HjtE/ftsvj5+cCH4/fCtkSB2eH2tsm5rVq9PW9nAZPi9+UQ4LMkL7jOA04zs3cAmNkwM6udpiHsZ2+PU+iKeHtvNbMtie56FtLgvV5vv3o83+OfJa8mOufWN7MdiIKoc3H3vxNdVH/dzNaL30Mf483X52KinyHfG/809iWi2JyFRY+DDAy6yGmxOL7ky0QBfiuJ/rf0OeD3cZXvEX1h3U/0pXNPvKzItm4GLo/bmkfyA35Q3I8niW5R700UzBq28SzRb/NfIfq57RTgIHd/pkifgrZvc/esu1R/BG4kCkZ8nCiotfa2ePdEh8+a2T2NthN/aV0M/MDd73P3RUSBsL81s/V6WG1Polvm4WN/og/jJ4l+avgB0d0hiO5MfcfMXiS6OC3yk1hRRxINxX2W6Hy5nOgLLZO7P0x0/i1396542VqiUTIbEwW8Z633ENGF+KPxTxhbZNWrqf8q0UXNB4i+pH4BfCJuB+DHRKPlniaK6bgkaOJbwEXxtg6vt61gu706b+PlHwHOiuvvQPQ+fCV+/hqi1/oyM3sBeCDep8x+9vY4ZfgO0c9ljxGNlruSOq9nAz2+1+vtl7u/SP3z/XNEP68tJwpOv7B2oxbN/3V6nX4dQXS37Vmii8x/c/fZ8bb/RvQfkvOIAsYPJhqV92rBYyADRPdIDBHpYGZ2OfCQu5/R6r70R/FdwqVEQ63/3Ab9OQn4mLuHgewiUkN3ckQ6UPwzwtvMbJCZvZ/of76/b7SevMnM3mdmw+O7e6cTxVPNbVFfNrdo1ulBFo1C/AoZAfoiktTKWVZFpDyjiWIkRhDdgTgpHvIr+e1BFDy+LvAg0RxH/6y/SmnWBf4vUYxUF1EM0y9a1BeRfkM/V4mIiEhH0s9VIiIi0pFa8nNVHCNwDtGU7+fXmQUTgM1Pujdxu2nLu+4qsXf92+vrrJNaNvi11zJq1vfKRhullq334ouF+hQK+7h6+PBEeYOVK3vdBhTbz3Zrp1nC169Zr11/lPXahFr5WoWKnkvhes3ap2Z9FoT9WzN0aMN2m/W+emz69NSycXOSsyJk9SeUZ7//clfy15E9dw2nxoKXRo5MlJ/cbbdUnee/vWV6xXJV+bNOZftW+Z2ceI6DnxMNS9wBOCKeU0FERESkaVrxc9VuwCPu/mg8x8FlRCM/RERERJqmFRc5W5KcyG0pGflezOxEM7vbzO5++cGrKuuciIjIQLP29dcre1SpbQOP3X2mu09196nr73Boq7sjIiIi/UwrAo+XkUziNoYGyR3DQOMnLrgvUR53/E5N6VhWkFuRoNgsVQWCNivYsMxA1bCPRY5pkUDMrPWadbzaLVh5yOrViXKe4NF2C55ulir3oVHw77yzE7lnAZh8WjKdVdFzO0+dsO2ygoqztlXkvVf0tXt60qREeevZsxuukxV4HL6P8nxH7Llr8vNs3k+XEpryhWR5wqxZ6Q59+8SeulqKtWuzUhWWY9DgwdVtq7ItvekuYIKZbW1m6xLlQbmuBf0QEZESdMLFsXSGyu/kuPsaM/scUdLFwcCv3P2vVfdDREREIpXGyuS4E9ksLZknx92vB65vxbZFRERkYFDuKhERkQFu7dpqRz1VpW1HV4mIiIj0ReUJOs1sLPAbYBTRNNIz3f2ceutMnTqzbie3+91jqWUPf2TrPvSys3XqKJpQJ+xnq/ehrFQBzWo3Tztl7UMrhWkBoPjIz0bKOgez2g1HOGWN9MrzeoajxrJGTuU5Xs9OmJAob7h8eapO2PYzEycmykO7utLtfuE3ifJWp++fqrPwhg9Vmtbh5RdfrOxiYP2NNqps31rxc9Ua4Cvufo+ZbQTMM7Ob3f3BFvRFREREOlQrRlc9BTwV//2imS0kmvFYFzkiIiItoJicEpjZeGBn4I6M595I67By5ZzwaREREZG6WnaRY2YbAlcBX3L3F8Lna9M6jBw5rfoOioiISL/WkiHkZrYO0QXOJe5+dV/bywoy3uWqxxPlew7dKlWnEwMSq5Q1JXwYhFdWMGQeA/n1bNa5XdYxrDKdRlgnnPIfYNSCBU3pT1nC1zMrmLUsWcf4+bFjE+WsgNxwvTypKPKklMjzmoftdI0fn6oTBgiPnj8/VWfV6NGJ8vDFi1N1wtQPYTtZ+/SPG78XbOfhVJ2qVZ04syqV38kxMwMuABa6+4+q3r6IiJRrIP8HQ9pLK+7kvAs4GlhgZt2XvKfHsyCLiIhIxapM0FmlVoyuug2odPy/iIiIDDxK6yAiIjLAdWpMTsde5ISBxguOPDJVZ9Ill1TVnZRWzmTbrO1kBdTlCRzsb/rja5VnvTB4dNiSJak6nTCjcDg7cFaQcTvtQ6tnuQ4tOvDA1LIJs2Y1XK/RMQ3PP0gHMBc938JBEUunp7c15SfJCImsWaTD/mTNnNzIKxttxPLJkxPLNrv//kR58Xvf2et2JZ+WXeSY2WDgbmCZux/Uqn60i3b7YhARKSrPSKqBIrzAaVedeienlZMBfhFY2MLti4iISAdr1Tw5Y4ADgX8HvtyKPoiIiEikU0dXtepOzk+AU4Aej6rSOoiIiEhfVH4nx8wOAla4+zwz26eneu4+E5gJMHXqzMpSwIuIiAw0nRqTY+7VXj+Y2X8QTQa4BhgKbAxc7e5H9bTOzntcmOhkniDdPBH4z06YkCiPWLSoYbvN0qzRHGE7WdH/VY54avfROGUd97L2s91G2rSbdhoV1W6yRgsVSbNS1jn4+F57pZZtdeutiXLWPoTy7FOeYzHvvPtTdda/+duJ8nbXXddwW6uHD0+Uw/QRAGvXXTe5nRUrUnWWXLhHpfPJLX98cWUXA6O3Gl/ZvrViMsDTgNMA4js5X613gSMiIiLlWru2M+/ktHJ0lYiIiEhpWjoZoLvfAtzSyj6IiIgMdJ0ak6M7OSIiItKRKg88LiIcXVVWsO3MOS+nlp04bf1et9MJsqZcz5r2v5GyghYVkNv52i2ouKz+PD1pUqKclXaiWfLMRNys/Wqn12/hoYemlr39qqsS5TAVRJY83yvhtrKCisfMnZsod40fn6rz2KX7Vhp4vPSRhyu7GBiz7XadG3gMYGbDgfOBHQEHjnP321vRFxERaS6ldeh/OnUywFbF5JwD3Ojuh5nZusDAvF0iIiIipWnFZIDDgGnAMQDu/irwatX9EBERkYgCj5tna2AlcKGZ3Wtm55vZBmElpXUQERGRvmjFRc4QYBfgXHffGXgJODWs5O4z3X2qu08dOXJa1X0UEREZMNa+/npljyq1IiZnKbDU3e+Iy1eScZHTG3ki3vOMxskaSfVfV76QKP/rYRv3snfZ22/lSIM8x6LISKosZe1nmcevnV6rLGX1L2t0yZDVq0vZVh7tdtyLpJPJY/jixQV6U0yYciBPSoQiIxlb/dqFKXuy0jGE53vWKN3weOX5rhk3J/nLw6P775+qMzhIX7Hh8uUN25ViWpHWYbmZLTGz7d39b8B04MGq+yEiIiIRja5qrs8Dl8Qjqx4Fjm1RP0RERKRDteQix93nA1NbsW0RERFJ0ugqERERkX6kJWkdzOxk4ASi2Y4XAMe6++qe6odpHVpp/2uXpZbddPCWLeiJhMJAwiKpPUSaqd2D2IsK0740a6DCSyNHJsph4Duk39d5AqPDdrPaztpW2E6z0snM/mNyW9Pflw56vvvuEytN6/DQvDsr+56dOGW3yvat8js5ZrYl8AVgqrvvCAwGPlZ1P0RERKSztSrweAjwFjN7jSilw5Mt6oeIiMiAt/b1zhxdVfmdHHdfBpwNPAE8BTzv7jeF9TTjsYiIiPRFK36ueitwMFF6hy2ADczsqLCeZjwWERGpxtq1r1f2qFIrRle9F3jM3Ve6+2vA1cCeLeiHiIiIdLBWxOQ8AexuZusD/ySa8fjueiu00wiFrJFUZY00yKOdjk2rtftoqipfK50Xb8qTaqHd04+Eo4Oy0jEs23XXRHnLu+5qyrazjl+RNATh6MdnJk5M1Rk9f36inOf45akTpmcAGNrVlSjv99uFqTpXPvdcorzZsZMabiuUNbJr+vsap9OQ5mhFWoc7zOxK4B5gDXAvMLPqfoiIiEikUycDbNWMx2cAZ7Ri2yIiIjIwtGoIuYiIiLSJdkvQmTVpMLA5cBkwApgHHO3ur9ZrR2kdREREpG3UmTT4B8CP3X1b4Dng+EZtlXYnx8x+BRwErIg7iZltAlwOjAcWA4e7+3M9tdGtlUGTeQI4w0Djed9PT9E95fTkNN7NCgztxIDSZk2dXlRZgeTN2odWBtJmCQNK80yP3yx5AnL723sk6/XN2q9QswKNQ4/uv39q2TY3paY2S2l0noZBxgBrhiY/O/O8dnneD8MXL04tWzV6dKJ889FvT9VZctI7EuUR68xr2E7X+PGJcp4g7az3TNXaMCYnnDT4KeA9wMfj5y8CvgWcW6+RMu/k/Bp4f7DsVGC2u08AZsdlERHpIHkuPGTgqp3sN36cWPt81qTBRD9Pdbn7mrjaUqBh4sjS7uS4+xwzGx8sPhjYJ/77IuAW4Gtl9UFEREQaq/JOjrvPpM6o6mDS4C7gd6RvmuRSdUzOKHd/Kv57OTCqp4pK6yAiIjIgZU0a/C5guJl135wZAyxr1FDLAo/d3Ymipnt6XmkdREREKrB27drKHjm8MWmwmRnRpMEPAn8GDovrzACubdRQ1UPInzazzd39KTPbHFhRtKHa2T3LCrjrViRwMQw0Bnj5or++8fd6J0zuU586XVWzAfe0nSpnrS6itt+vr7NO045XGEScdxbp2npl9aenvuQJym0nu1z1eKJ8z6Fbpeq0+vwPY2omzJqVej5ct1G7a4YObXiehHXyyupLnvdIbVBwGPTcbcpZcxPtZKltp2v8eLa69dY3ys9OmMAT05L/Ud/uuutSbYQBywNZnUmDZwGXmdn34mUXNGqr6ouc64iuvs4k51VYlnD68jKVcYEj9VWZ7iBLf7rAySoXFV7g5BV+KVXZn/5+gZOl1ed/owscyL6oaCTPedKMC5ysZY0ucHpSe4HTk7Cd2gscIHWBk6UdLnDabXRVD5MGPwrs1pt2Svu5yswuBW4HtjezpWZ2PNHFzX5mtojoN7czy9q+iIiIDGxljq46ooenppe1TREREem9tWvb605Os2jGYxEREelIyl0lIiIywK19vb1yVzVL1Wkdfgj8K/Aq8HfgWHfv6m3bRUZTZQUxFglyyyOcah5ggxnJ6cEf/OWdifIOn9y5lL7kVSTNRFnpFzKPX5MCTPvbFP9VKvp+yJNaoYg8/clz3jYrhUozZI2kChV9X+XZzzwBw2GdpydNStUZtWBBw76E7Tw2PRmpsPETT6TWC9MbZA0CKPJ6Zn3+h6OpVg8fnqrzzMSJdfsHMLSrq275X37zm9Q6+hyqTtVpHW4GdnT3fwEeBk4rcfsiItIC+hKXdlFpWgd3r83qNpc3J/URERGRFlHgcfMdB9zQ05NK6yAiIiJ90ZLAYzP7OtEshpf0VKc2gdfUqTN7TP8gIiIifdNukwE2S+UXOWZ2DFFA8vQ4f1WvFQk8KyvIOEuewMusQOOZc15OlI+fPixRLvN37iJtl9WfVs9i206Bqv1BkderWQHDeV6bsl6/sgLvi7ZR1ns4K8i4yHtk3JzkHfkq9zPr8z8MIs4KPA4HuWz8nw+n6jx2xoF1t/XshAmpdUYsWpQohwHO0jyVXuSY2fuBU4C93f3lRvUHkvACR0REpCo5E2f2O1Wndfg/wEbAzWY238zOK2v7IiIiMrBVndahYcZQERERqVanxuQorYOIiIh0JKV1EBERGeA69U5OpWkdap77CnA2MNLdn+lt25042uXEaeunli3da/dEeatbb62qO6Vq99FLrexPkRQJZY3yKVMrR0U1S1n9y0pBsHzy5EQ5z2fB82PHppaFaRLynDvNOr+qfD3DPq8aPbrhOi+demVq2eIFyUn5J89IH4tVuyfbDkdODV+8uGH/Xhg3rmH/pJgy7+T8mijQOJG4w8zGAvsD6cQlIiIiUjmNruold58D/CPjqR8TDSPXBH8iIiJSmqrnyTkYWObu95lZo7onAicCjBt3JCNHTqughyIiIgNPp8bkVDa6yszWB04HvpmnvrvPdPep7j5VFzgiIiLSW1XeyXkbsDXQfRdnDHCPme3m7ssr7Ee/EQYXzjtl91SdKWfNrao7hYJis7R7QGkrFTmm/fF4Ltt110Q5nD5/IMtKQVBk0EEYZAzFgv7z1Fl04IGpZRNmzWq4Xlm6xo9PlMNg4CzDjt8ptWyDCcn1so7FE9/9U6K81z7p4ORQmMZh0iUZaRxPPrFhO83UqVnIK7vIcfcFwGbdZTNbDEwtMrpKRETaVysvcERqVZ3WQURERKQSVad1qH1+fFnbFhERkfwUeCwiIiLSjyitg4iIyADXqZMBVp7Wwcw+D3wWeB2Y5e6nlNWHTpM1kmqXqx5PlO85dKvStl90NJVISKOpWqOskXhZgcaPTZ+eKG89e3Yp286SZzRVONLsnJufS9XZ+y/JEWojTk+3k2c0VWjUggW9XkeKqTStg5ntCxwM7OTur5jZZj2sKyIi/VR4gSPtTzE5vdRDWoeTgDPd/ZW4zoqyti8iIiIDW9WBx9sBe5nZHWb232a2a08VzexEM7vbzO5euXJOhV0UEREZWNa+/npljypVfZEzBNgE2B34X8AV1kMSK6V1EBERkb6oenTVUuBqd3fgTjNbC2wKtCSi9ZWNNkqUs6ZTz6PIVOmN2sjbThhovPDQQ1N13n7VVb3uT6dqxmvVH7bdyv1sd3nea0Xfj1XJ6t/q4cPrliFfQG4e4WfnkNWrE+Vxc+akjle4zvLJk1PthukrwnWg2Of082PHppYN7epKlPe+O+P//OuGERfrp6oUSXcT9ifsSyt06uiqqu/k/B7YF8DMtgPWBZTWQUSkg7TTBaEMbGUOIb8U2AfY1MyWAmcAvwJ+ZWYPAK8CM+K7OiIiItIinTq6qhVpHY4qa5siIiIi3TTjsYiIyAC3dm1n3slR7ioRERHpSJWmdTCzycB5wFBgDfAZd7+zr9sqOhKi6GiqIttqRht59jNrJNVNc9ckyvvvXuxlLzJip91GqQyUbXdC4GdZI8TytFPl8QtH2gxbsqSHmm/K6l84qqfMNCxFPjvDdcKRVM3aDsC88+5PlCd/vvHn0JSvNh45lTUQOBwZFZ6380+eklpn4oV/T5TD0WmtsPZ1ja7qrV8D7w+WnQV8290nA9+MyyIiIiJNV2bg8RwzGx8uBjaO/x4GPFnW9kVERCQfxeQ0x5eAH5rZEuBs4LSeKiqtg4iIiPRF1Rc5JwEnu/tY4GTggp4qKq2DiIiI9EXVQ8hnAF+M//4dcH4zGs0Kwssz1fZAmfo+DDT+8B+Wp+pcfdDohu2ExycdlJcOwss6ps0IYM5ap1lpOtpds87bdj//y+pP1nlbZpBuI3kCjUNlBvTnOS+enjQpUR61YEFTtt2oL1n9yUrZMOXTyfIrGw1N1Vk1OvmZt+Hy9OdiWCcr/cLS3XdPlMOA6ilnzU2tc+/xxyfKO1/Q4//3K9OpkwFWfSfnSWDv+O/3AM1JpCIiIiISqDqtwyeBc8xsCLz5UaQAACAASURBVLAaOLGs7YuIiEg+nZqgsxVpHdKTBoiIiIg0mdI6iIiIDHCdGpPTsRc5eQIJ2y3QspFm9TcryHjeT5cmylO+MKZhO0WDNYvsR5UzWJclDIyGYn2ucubfThAGr7YyyDiPPOdJmbON56mz6UMPJcp5+pzVn2b0JU/gdhhADDBiUTIkdN7pO6bqzJp+W6L8zenDUnXGzE0GFuc57tvcdFPDdaQ5yozJGQv8BhhFNAngTHc/x8w2AS4HxgOLgcPd/bmy+iHSrtr9okykKJ3bb+ov/5no1Ds5ZY6uWgN8xd13AHYHPmtmOwCnArPdfQIwOy6LiIiINFWZgcdPAU/Ff79oZguBLYGDiUZdAVwE3AJ8rax+iIiISH2dOrqqknly4hxWOwN3AKPiCyCA5UQ/Z2Wto7QOIiIiUljpgcdmtiFwFfAld3/BzN54zt3dzDxrPXefCcwEmDp1ZmYdERER6bt2iskxs+2JYne7bQN8kyjOt1cxvaVe5JjZOkQXOJe4+9Xx4qfNbHN3f8rMNgdW9HU7zZri/NkJE1LLwgj8otp9Cv1wNNUdc9O3Lt+5e/LGX5lTy7e7Iq+ngjF7p1lpOtr9nJx30lGJ8pRzL25Ku2Xud9h2s0ZtFRlllOdzaPjixQ3bmfzDv6WWffP76dFUjbY175Rkmofx1zybWmfI6tUN2x3I3P1vwGQAMxsMLAOu4c2Y3jPN7NS4XDfcpbSfqyy6ZXMBsNDdf1Tz1HVEOayI/722rD6IiIhIY2vXvl7Zo5emA39398eJYnovipdfBBzSaOUyY3LeBRwNvMfM5sePA4Azgf3MbBHw3rgsIiIiA0BtzG38qJfi6WPApfHfuWJ6a5U5uuo2wHp4enpZ2xUREZHeWft6daOramNu6zGzdYEPAqdltNFjTG+tqrOQi4iIiOTxAeAed386Lj8dx/KSN6a3I9I6NCvArllBxlma0ceXRo5MLRva1dVwO0WCZMMgY4Bpv0+mfphzSOPUD3m0e1B2lv7Qx3YWnstZqRb6Y6B2kXO5WYHGeTQrmDts55mJE1N1trzrrl63Gx6vPEHFWcf4+bFjG24rTAexZujQVJ0wHURWCoknLrgvUZ786cbnQNi/1cOH1+/swHUEb/5UBW/G9J5JzpjeVqR1+CHwr8CrwN+BY929q+eWRESkPylygSOtVSAguFRmtgGwH/CpmsVnAleY2fHA48DhjdppRVqHm4Ed3f1fgIfJ+K1NREREBi53f8ndR7j78zXLnnX36e4+wd3f6+7/aNRO5Wkd3L02/epc4LCy+iAiIiKNtdNkgM3UirQOtY4DbuhhHaV1EBERkcIqT+tQs/zrRD9pXZK1ntI6iIiIVKNTE3S2Iq0DZnYMcBAw3d1LuYBp1iiCdpI1AiWPIiMWsoSjqeZ9a4tUnW0vGJwoZ41GKLLt/jgCa6AK33uQnsa+6LncTspKa5Kn3axjnOczrsjnYFZ/wtczT6Bx2E7WiKLwvMhzPB+bnp52bevZsxPlrNFWT0+alChnpX4IR1yF6wBscGYyHdDq4clxNFnn+iMH7Z0oj747nVJCmqPM0VWZaR3M7P3AKcDe7v5yWdsXERGRfDo1JqfMOzndaR0WmNn8eNnpwE+B9YCb44zkc9390yX2Q0RERAagVqR1uL6sbYqIiEjvdeqdHKV1EBERkY5U+YzHNc9/BTgbGOnuzzR7+w9/8IOJ8qRLMgdx9XtFAnLzBPzlEQYZA5z2y3sT5V+8f9NUHQUR9x9hwGaeQPKs4Nas4NVmKHIuFQ3aDZV13ma1G/Y5DPxtprLen2GKhKw0CkU+h7a4885ebxtg04ceSpSzzoGsPobC1Dp5TD7/8hy1du51u32h0VW91z3j8T1mthEwz8xudvcH4wug/YEnSty+iIiIDGCVz3gMPAj8mGiEVcPkWiIiIlIuxeT0Qe2Mx2Z2MLDM3e9rsI5mPBYREZHCKp3xmOgnrNOJfqqqSzMei4iIVKPdspA3S6l3cjJmPH4bsDVwn5ktBsYA95hZOipMREREpA8qnfHY3RcAm9XUWQxM7evoqqyRG0VGUzVr1EWViox8aNaU+lkjbcLRVB/+w/JUnasP6v017UAZgdVuI882XJ5+/YqociRSI0Xf0y+NHJkoZ72Pnp2QnOJ/xKJFhbYVqvJzqMgItaz+hXXyjMwrImvbCw89NFHe5qabUnXCEWrhawfp8z/PyMH5J09JlDe9Pf39tNWttybK4bklzVP5jMfurskARUQ6WNZ/GKW9uYaQ906dGY9r64wva/siIiIysJUeeCwiIiLtbdDgzkyA0Jl7JSIiIgNeS9I6mNnngc8CrwOz3P2UvmyrWUGNRYP72ilYNCsIu5X9yQoyfmz69ER53JzkPEitDrbNo6zXvN32vd36U0SR1yrrfZQnYL9ZgcahPEHPVWr0Wbneiy+m+hyus2zXXVPrbXnXXX3vHPD2q65KlLOCisPA48UfGpGqc/l+yZQRZ+331lSd8HyaeOHf624nqz9lnTe9MWhw3eiSfqvytA5EFz0HAzu5+ytmtlndVkREpF/RaCFpF61I6/BJ4Ex3fyV+bkVZfRAREZHGBg3qzDs5lad1ALYD9jKzO8zsv80sfc8SpXUQERGRvqk0rYO7v2BmQ4BNgN2BXYErzGwbd0+kblBaBxERkWooJqeAjLQOAEuBq+OLmjvNbC2wKdBjJF2e2TVbqZ2CM1vdlzxBnlvPnp0oP77XXqk64YygRWTFBTQrYLPVx1nyK/JatdvrG5637TbAILTBypWp91/4Ob7pQw+xevjwSvqz+L3vTC17yy5fSZTXveXsVJ2vfWa3RHn4OotTdcJ9yAo0DpetGTo0UX560iRe3kzhqWWoNK1D7PfAvsCfzWw7YF2gT2kdpLM04wJHRFonT+BxVRc4rZZ10RNqhwucTo3JqTytA/Ar4Fdm9gDwKjAj/KlKREREpK9aldbhqLK2KyIiIr3TqTE5mvFYREREOpJyV4mIiAxwisnppZ7SOpjZZOA8YCjRrMifcfc7e26p/UZTNRKOIoD0Piw68MBEecKsWaX2qSpFRniEgcZX/+XVVJ0P77luw3bCESdFR1K1U5qO/iBrpE+oGcew3UcUValZ+53ntcuz/bCdoV1dqTrhKMoxc+c27E/R/Qzb2fSBx1N1tjp350T5+bHp+dgajYrKqpMn0HjD5csT5eGLF2fUelvDdqSxVqR1OAv4trvfYGYHxOV9SuyHiIhUaKBefEr7aUVaBwc2jqsNA54sqw8iIiLSmAKP+yBI6/Al4IdmtgQ4Gzith3WU1kFEREQKK/0iJ0zrAJwEnOzuY4GTiSYMTHH3me4+1d2njhw5rexuioiIDFiDBllljypZmfPwxWkd/gD8sXvWYzN7Hhju7h7Pivy8u29crx3lrpIzTnowUf72uTuk6jw/dmyiPGzJklL7JNmW7ZrOubvlXXe1oCflamUgdCcEYRfdh3A25WYNMMjSKHgamjND+7wvHZBa5keNqfRq4GdfnVHZ9+znz76osn1rRVqHJ4G9gVuA9wCLyuqDiIiINNapMTmtSOvwSeCcOBv5auDEEvsgIiIiA1Sr0jpMKWu7IiIi0judeidHaR1ERESkIymtg4iIyACntA69ZGZDgTnAevF2rnT3M8xsa+AyYAQwDzja3dPz+NfQNPsSjqZ6+aK/puoMm9H7dp+dMCG1bMSicmLhw3Qf/S1dSV5ljaRq1oiiZrXTys+hrG0X+ZzMk4KmLEWPX57RVOEIv6xUC5s+9FCivHzy5FSd8FzOGkkVHvdjr12WKP/mgM1S64QjxCb//OZUHY46Nr1Meq3Mn6teAd7j7jsBk4H3m9nuwA+AH7v7tsBzwPEl9kFEREQaGDTYKntUul9lNeyRVXFxnfjhRMPGr4yXXwQcUlYfREREZOAqNfDYzAbHw8dXADcDfwe63H1NXGUpUT6rrHXfSOvwzNO3lNlNERGRAW3QoEGVPSrdrzIbd/fX3X0yMAbYDZjYi3XfSOuw6ah9yuqiiIiIdKhKRle5e5eZ/RnYAxhuZkPiuzljgGX1124coJZneu41Q4emlnVq4Gcj/XFK+DBAeERGkPG8U3ZPlKecNbdhu2UFGWfJc751QpB9WWkdmnUsmtVOGDwKxVMMNEOR/SpyThbddrPaCc+v0fPnp+qE51uebWe1E9rlqsdTy2799NRE+TcHNN6HoV1difL9n/hEw3XKpnlyesnMRprZ8PjvtwD7AQuBPwOHxdVmANeW1QcREREZuMq8k7M5cJGZDSa6mLrC3f9gZg8Cl5nZ94B76SELuYiIiEhflJnW4X5g54zljxLF54iIiEgb6NTJAJXWQURERDqS0jqIiIgMcJ0aeNyKtA6XAFOB14A7gU+5e5+GO+SJyC9zlEo7jYjJM3Kq1SN2nh87NlEetmRJw3XyjIIKR1Pde3x6Mu2dL2jvELBWvzbNUFZah6LKen+WNZKq6KitcATi8MWLU3XCfS860rIZaTCKplQJz6+s1BThtrrGj0/VCY/P/P/9WKrO24c9nyjf9bF06ocNXqv/2mT1L0wzMW7OnPSKJ+1bt91OFw9cOh/YkWgi4eOAvwGXA+OBxcDh7v5cvXZakdbhEqL5ciYBbwFOKLEPIiIi0kAbpnU4B7jR3ScCOxGNzj4VmO3uE4DZcbn+fhU8Hg31lNbB3a+Pn3OiOzljyuqDiIiI9C9mNgyYRjz62t1fdfcu4GCidFCQMy1UpWkd3P2OmufWAY4Gbuxh3TfSOqxcmXErT0RERJpi0CCr7FH7/R4/Tgy6szWwErjQzO41s/PNbANglLs/FddZDoxquF/NPUxJYVoHM9ux5ulfAHPcPZ27nmRah5Ejp5XZTREREalI7fd7/JgZVBkC7AKc6+47Ay8R/DQV/xrkjbZVdVqH9wMPmNkZwEjgU1Vsv2zNCuYrIs905e1mw+XLK9lOVpDxg7+8N1He4ZOpqZw6QjsFw5cpDOrshFQt4ZT/kA5GzqoTBu3mCcjNEp47WSlxmnE+NSulShjEm9fq4cMT5U1/d0SqzqrjfpEor5+x3+FxDvvz4OGHp9YJP5uyj0W1gcdtNrpqKbC05tefK4kucp42s83d/Skz25zoV6K6qk7r8JCZnQC8DzjC3deWtX0RERHpf9x9ObDEzLaPF00HHgSuI0oHBTnTQrUircMa4HHgdjMDuNrdv1NiP0RERKSONpzx+PPAJWa2LvAocCzxtYSZHU90HZG+TRZoRVoHTUAoIiIiPXL3+URz6oWm96YdXXCIiIgMcG0Wk9M0yl0lIiIiHanytA41z/8UOM7dNyyrD61S5UiWKkdtNWtbzWinaP/C0VTzTtk9VSdMD9EfdepoqlCe0VTtfizyjIQrkkIiz7HJGjkVrldkRFbe9ZohazvhaLTFHxqRqrPjuclRnps+9FCqzpAv7BksSR/T8HhdesEzifIRxzdOJdMOo2TbMCanKcr8uao7rcOqeOK/28zsBnefa2ZTgbeWuG0REWmRdr+wlIGj8rQO8WirHwKnlLVtERERkVakdfgccF3N1Mw9rau0DiIiIhVowwSdzdmvMhvPSOswDfgI8LMc6yqtg4iIiBRWdVqHfYFtgUfiiQDXN7NH3H3bKvpRlTxBeO0+HX27/6berP5lBRm/fNFfE+X1Z7wjVaestAntno4h69wOg1ezzuV2369WauWxyJMS4fmxY1PLhi1Zkihn7UP4GffkbrslylvceWdqnWZ9DoaB2pN/nE6DEfb5iWnp/0yHx6dr/PhUnR2uuCJRPuL4vL18U1bqh6p1auBx1Wkd5rn7aHcf7+7jgZc77QJHRGSgy8qbJdIKlad1KHF7IiIiUsCgwZ05bV7laR2COh03R46IiIi0B6V1EBERGeA6Na2DLnJKkCeQsFkBdu0W0Nlu/SlivRMmJ8ovjRyeqlNkBto82v14ZfUvXNbK2W+ld/K8LmGQcR7rvfhi6jzYevbsRDmclbh7vUaa9RnzoZ3+lCj/+v50gHUYeJwVLB3WydO/MJh7s/vvz+jhHhnLpLcqT+tg0bCq7xENJX8dONfdf1pWP0REpFrtkKZAeqdTR1dVntYBeDswFpjo7mvNbLMS+yAiIiIDVJmBxw6k0joAJwEfd/e1cb0VZfVBREREGuvUmJxWpHV4G/DROGXDDWY2oYd1ldZBRERECqs6rcOORDE6q919KvBL4Fc9rKu0DiIiIhUYNMgqe1Sp6rQO7weWAlfHT10DXFjGNjthlE8e7TayJc+2wpEFRUZvlCnch6yRVAsPPTRRfvtVV5Xap96q8vxvxrayZsgtMgKx1ed/K7XyM69IoPHQrnSqhTyKfOatHp4eIXnZowcnyium/Uuqzi3v+WWi/MmvbNGwf+G5nPU6bLh8eaKcfSw0uqoZyhxdNRJ4Lb7A6U7r8APg90Q5rB4D9gYeLqsPIiIi0linxuRUntbBzG4DLjGzk4kCk08osQ8iIiIyQFWe1sHdu4ADy9quiIiICGjGYxERkQFPkwH2Up0Zj6cDPyT6CWsVcIy7P9Ls7bcy6G6gbLuoRoHGVQaPFt1WGGicJ3A2nMY+K9iwWfvZbsHmjTQrzUlWX/rje6SIdtuvRv3pGj8+tWzEokVN2U74fnxm4sRUndHz5yfKj/3bten+XHRKojyFi1N1wvPryd12S5TDdBaQDoQuGoQtjbVixuNzgYPdfaGZfQb4BnBMif0QEZEKtdsFlzSmwONeqjPjsQMbx8uHAU+W1QcREREZuEqNyYlHVs0DtgV+7u53mNkJwPVm9k/gBWD3HtY9ETgRYNy4I9GEgCIiIuXo1JicVsx4fDJwgLuPIZoI8Ec9rKsZj0VERKSwqmc8/gCwU5zDCuBy4MYq+iAiIiLZFJPTS3VmPB5mZtu5+8PxsoVlbD/PiIpmjbpoZZBdK9M6FN1WOPIhHFnTbiOD8oycyhodNO/7QxPlKaen00OE2n0kUFnnV5nnbZ52wu0/MS199zhrlEwZss63cCTSqAULGraTJ9VCK0fzDV+8uGGdovsQvh/DkVSQHgU14uqXUnV2vDg94qrR9vOcJ+Hr+cK4cQ3XkWJaMePxJ4GrzGwt8BxwXIl9EBERkQZ0J6eX6sx4fA1RYk4RERGR0mjGYxERkQFOo6tERERE+pHS7+TEMTl3A8vc/SAz2xq4DBhBNIfO0e7+arO3mycQrt2COotoZaBq0W01Ywr/ZgWq5mmnaH+nnL46UX5+7NhEOSu9Rbufk2X1r8z9bhTonrX9qoKMs2T1b9OHHmpK22W9R4oIUxsAbLCycXB+kcECWSkktrjzzkT5jK/8PVXni5ttmChPPm1Yw/4s23XXRDkr6Dk0bs6c9MLTPtRwvWbq1JicKu7kfJHkCKofAD92922JAo+Pr6APIiIiMsCUepFjZmOAA4Hz47IB7wGujKtcBBxSZh9ERESkvkGDBlX2qHS/Sm7/J8ApwNq4PALocvc1cXkpsGXWimZ2opndbWZ3r1yZcStPREREpI7SLnLM7CBghbvPK7K+0jqIiIhIX5QZePwu4INmdgAwlCjz+DnAcDMbEt/NGQMsK7EPIiIi0kCnBh6XORngacBpAGa2D/BVdz/SzH4HHEY0wmoG0HjebOlRu4/GKUuZ09GXNWJtw+XLE+V5p++YqrPjzx5PlJsxEq2Zqkwb0ixDVq9uXKlJ8ozkKqLIMW5WypKyXt9mjaTKEr7mWWkwnp40KVE++B9PpOp899r3Bf25NVUnPIbhaKpVo0en1gnrtPt7qD9rxWSAXwMuM7PvAfcCF7SgDyIiIhLr1MkAq8pCfgtwS/z3o8Bu9eqLiIiI9JXSOoiIiAxwnRqTo7QOIiIi0pFakdbhEmAq8BpwJ/Apd29J1FUrUyKUpT8GhrabqlIXhEHGAA+c9I5EecpZc0vpS1FZQZRhQHW7nW9V9icM2m33z5g8Qdl5PlOKfO5krdPbNvLKCrAevnhxonzsoHQg9DN7JLc/Zm66z09MS05xMmHWrER5aFdXap0wzUT4HmoF3ckpLkzrcAkwEZgEvAU4oYI+iIhIRdrtYk4GrkrTOgC4+/UeI7qTM6bMPoiIiEh9gwZZZY9K96vk9sO0Dm8ws3WAo4Ebs1ZUWgcRERHpi1amdfgFMMfd07MrobQOIiIiVRk02Cp7VKnStA5mdrG7H2VmZwAjgU+VuP2GOuF343YPbJQ3ha/VkNWrU69XGGj80siRqXbyzBRblqwAySLn3EAJkC9rn5p1/PKsU7TOgiOPTJQnXXJJr9stKmw7a1ufOvCORPnEO7+dqrPjBcn/g2e1s81NN9XtS9as1+02k3knqzqtw1FmdgLwPmC6u6d+xhIZKDrxS70oHYvOEl7gSPvr1BmPWzFPznnAKOB2M5tvZt9sQR9ERESkTZnZYjNbEF8n3B0v28TMbjazRfG/b23UTivSOmiWZRERkTbSpvPk7Ovuz9SUTwVmu/uZZnZqXP5avQY047GIiIj0BwcDF8V/XwQc0mgFXeSIiIhIZWqniIkfJ2ZUc+AmM5tX8/wod38q/ns5UehLXZWndahZ/lPgOHffsOw+NFO7jQopsu2sKc4V7V++Iq9V1kiqcFTI/531zsJ96q1mnesDJdC4rNGPrT5+j02fnihvPXt2ohyOpCqqrJQSAD+686BEecqs61N1sj4re7utrDbaMX1LlYHH7j4TmNmg2rvdfZmZbQbcbGYPBW24mXmjbbUirQNmNhVoGDAkIiIiA4+7L4v/XQFcA+wGPG1mmwPE/65o1E7laR3iOzs/JJoJWURERFqsnSYDNLMNzGyj7r+B/YEHgOuAGXG1GcC1Dfer8BHJJyutw+eA62p+V8uktA4iIiID0ijgNjO7jyjH5Sx3vxE4E9jPzBYB743LdZUWk1Ob1iGeDBAz2wL4CLBPo/Vrf7ObOnVmw9/dREREpJh2mgzQ3R8FdspY/iwwPb1GzypN6wD8FXgFeMTMANY3s0fcfdsS+9FUrQ74awYFGfdvYaDxvJ8uTdWZ8oUxVXWnI4TBoc16j3RCyous/oSBxll1QquHD0+U86QnydrvRQcemChPmDWrYTtZ/Xtm4sSG660aPTpRfvYLv0nV2f4zu9VtY8jq1allO57710R53klHNeyLFFN1WodEOLuZrepPFzgiIiKdqE0nA+wzzZMjIiIiHanytA7B8n41R46IiEgnGjS4M+95dOZeiYiIyICnZJkiIiIDXDuNrmqmHi9yzOxnRLkjMrn7F/JsIEzrYNGwqu8RDSV/HTjX3X/aq14XUOaIhbKmbhfJI2skVVmjhYpq9/dIq49PraLHpshr3qwUL3n6HI4yemnkyFSdoV1dDdvNM5oq3K8HPr9Vqs6OP5ufKD94+OGpOhsuX54oZ42kCvv4xAX3JcqjvvTu1DrhSLNNHs6aNu5tGcukt+rdybm7SdvoTuuwcVw+BhgLTHT3tXFeChEREWmRTh1d1eNFjrtfVFs2s/Xd/eXeNF6T1uHfgS/Hi08CPu7ua+PtNMw9ISIiItJbDQOPzWwPM3sQeCgu72Rmv8jZflZah7cBH41TNtxgZhN62K7SOoiIiFRg0CCr7FHpfuWo8xPgfcCzAO5+HzCt0Uq1aR2Cp9YDVrv7VOCXwK+y1nf3me4+1d2njhzZcHMiIiIiCblGV7n7kjgNQ7fXc6yWSutgZhcDS4Gr4zrXABfm725xZQY6VhVE2W7TvQ9k/S2Q9rHp6XQv4dT8ZQqPT5XHr91fq2bJEzAcHos86+Q5XmGqBUgHCIfbytp2nvQQeXSNH58ob3rr8FSdIav/lihvdv/9GXWSwdJZx+KUm59LlM/4+HsT5fVeTKevCPs3bk7WrxUKPG6GPBc5S8xsT8DNbB3eDCSuq4e0DkeZ2ZnAvsBjwN7AwwX7LiIibSjPCChpLwMu8LjGp4FzgC2BJ4E/Ap/twzbPBC4xs5OBVcAJfWhLREREJFPDixx3fwY4si8bqU3r4O5dRCOuREREpA106mSAeUZXbWNm/2VmK81shZlda2bbVNE5ERERkaLy/Fz1n8DPgQ/F5Y8BlwLvLKtTIiIiUp2BHJOzvrv/tqZ8sZn9r7wbyEjrMB34IdFdpFXAMe7+SG863QmKjPjIqlOknWcnpKcmGrFoUcP12l2VqQz62widrJFU036/NFGec0g6PUSo6OiX8HgVOX5FRxc2GtmVt50851e7j+Qq0p+s1A/h6KCsQOMi78c1Q4c2rJNnHzZ96KGGdeZ/PZlWYuLPF6fqbLAyPTIq9MmfHJUob9N1U8N1wnQRUp56uas2if+8wcxOBS4jymX1UeD6XmwjTOtwLnCwuy80s88A3yBK9SAiIh0g68JI2lunxuTUu5Mzj+iipnvPP1XznBMPD6+nh7QOzpsXPMOIRmyJiIiINFW93FVbN6H97rQOtZf1JwDXm9k/gReA3bNWNLMTgRMBxo07Es16LCIiUo5OjcnJk9YBM9vRzA43s090P3Ks01Nah5OBA9x9DNFsxz/KWl9pHURERKQvzN3rVzA7A9gH2IEoFucDwG3ufliD9f4DOBpYQ5zWAfgzMNHd3xbXGQfc6O471Gtr6tSZiU6+NDIZMJYnOCyPPL8jFw1mfXrSpER51IIFhdqRzlFmmo4iAbDzTknfVJ1y1tw+bzvv9hu1025BvFka9fne449PrbPzBRf0ud2idbL0h+PcW3nOyWallAjbnTnn5VSdIw/dKlEe2tWVqnPv7cdWemvltVfPrX8x0ETrrHtSZfuW507OYcB0YLm7HwvsRBRLU5e7n+buY9x9PNGw8z8BBwPDzGy7uNp+5EgRISIiItJbeYaQ/9Pd15rZGjPbGFgBjC2yMXdfY2afBK4ys7XAc8BxRdoSERGR5hiIo6u63W1mw4FfEo24WgXc3puNBGkdriHKPi4iIiJSmjy5qz4T/3memd0IbOzu6Zz0IiIiK3UBdQAAIABJREFU0i916uiqepMB7lLvOXe/p5wuiYiIiPRdj6OrzOzPddZzd39Pw8bNFgMvAq8Da9x9ajyT8uXAeGAxcLi7P1evnXB0VajdRgyUOWqmLM0ayfL82GS41rAlSwr3qa/64+vQbj5x/YpE+TcHbNainpQ3aiurnXY/d8L3GZT3XssadTpk9epEOc/xWzV6dKLcrP7m+fx/ZuLE1LIw9UPW6xse56x0DKuHD6+7ra1uvTW1zsJDD02UN7s//ePIY5fuW+mtlbXU/55tpkGcWNm+1ZsMcN8mbWNfd3+mpnwqMNvdz4zTRZwKfK1J2xIREREBck4G2GQHAxfFf18EHNKCPoiIiEiHK/six4GbzGxenKYBYJS7PxX/vRwYlbWimZ1oZneb2d0rV84puZsiIiID1xr3yh5VyjOEvC/e7e7LzGwz4GYzS/wI6u5uZpl77O4zgZnQOCZHREREJNTwIsfMDDgS2MbdvxOnYhjt7nc2Wtfdl8X/rjCza4DdgKfNbHN3f8rMNieaXLBP2ikgEFrbn6IBk83qcysDjUPtdl6Esl6rNUOHJspF04gU2X7W8QoDjeednZ6ifspX129ux2KtTOvQbudOeCzyvM/C9DeQToGT5/OiyDk4+LXXUtsP+9ys4O486wxfvDi1LAwYnvCz9PRv8+5JpjqZclb6uHeNH58oZwUah7a4M/n1GQZyt0KVd1jWrTCkOs/PVb8A9gCOiMsvAj9vtJKZbWBmG3X/DewPPABcB8yIq80Aru1ln0VEpI1lXWCJtEKen6ve6e67mNm9AO7+nJmtm2O9UcA10Y0ghgD/6e43mtldwBVmdjzwOHB4wb6LiIhIE1QdK1OVPBc5r5nZYKIgYsxsJLC20Uru/ihRMs9w+bNECT9FRERESpPnIuenRLmmNjOzfyfKSv6NUnslIiIilRmwd3Lc/RIzm0d098WAQ9x9Yek9a7J2n8G0WTpxn/JqZaBqEVn9a/fg2qwg48emJ2/Mbj17dq/brfL92e7nRZYifS4azBq+FmFgLcCIRYvqtrHBypWpdsLZg7P6FwbeZwVY55lVPdx2GGSctf3/+eWnUnXWDWZpXrbr66k6r268cd1th/sE6ZmT55/w0VQdaY48o6vGAS8D/1W7zN2fyLHuYtJpHX4I/CvwKvB34Fh37yrWfRERaTd5Ui1Ie1nT6g6UJM/PVbOI4nEMGApsDfwNeEfObYRpHW4GTnP3NWb2A+A0lNZBREREmizPz1WTastxdvLPFN2gu99UU5xLFOMjIiIiLdKpMTm9Tuvg7vcA78xbnXRah1rHATdkrai0DiIiItIXeWJyvlxTHATsAjyZs/1UWgd3nxO3+3WinwEvyVpRaR1ERESq0al3cvLE5GxU8/caohidq/I03kNahzlmdgxwEDDdvZoj224jKvrbSKBWe3bChEQ5a3RHOIqhE45pmaOO8pyDeQJIw9FUf7kr/Zbec9f687j3x9cqPCeh8aijKuVJx5DnuBfZp6x2w1FQWccvHHVUVLj9oV3psS3h50WYagHSxzCrz5s+lEjJmOuYrgpGbW3y8FMZtd7WsB1prO5FTjwJ4Ebu/tXeNhynchjk7i/WpHX4jpm9HzgF2Nvd04lwREREpFID7k6OmQ2JR0C9q2DbPaV1eARYj+jnK4C57v7pgtsQERERyVTvTs6dRPE3883sOuB3wEvdT7r71fUarpPWYdtiXRURERHJL09MzlDgWeA9vDlfjgN1L3JERESkfxhwP1cR5ar6MvAAb17cdMt1NLJmPK557ivA2cDIYLLAfi9PsGh/DLRspTzBj3kCLfuboudJnqDiPG3nqRNua89dGwcwl3X+V5keopVBxi+NHJlaFgbXFg0kD9dr1jGdd9JRifLk8y9P1clKgRDKCiJu5JmJExtuKyvo+Ylp0xLlTR5+OFXngaMOTpR3vPjaRDkrpUSYKmP9FStSdRR43Bz1LnIGAxuSvLjp1ptLvnDGY8xsLFEgcsPUECIiIlKugZjW4Sl3/05J2/0x0QiraxtVFBERESmi3ozH9Se2yCc147GZHQwsc/f76q2oGY9FRESqsca9skdeZjbYzO41sz/E5a3N7A4ze8TMLjezdRu1Ue8iZ3runvTs3e6+C/AB4LNmNg04HfhmoxXdfaa7T3X3qSNHTmtUXURERDrLF4GFNeUfAD+OR2k/BxzfqIEeL3Lc/R997V3tjMfANcDeRFnM74uDkscA95jZ6B4bERERkVK1250cMxsDHAicH5eNaJT3lXGVi4BDGrWTZwh5IT3NeOzum9XUWQxM7bTRVUVGHuQZwfD82LGpOuFU6WV6ZaONEuWio5mU0qJ8VR7TIqO05p2+Y6I85fsPVNaXTrDBypVNaafoCLtwdNdLZ/0pVWezYyclylPOvbgp/cnzuRN+xgxfvDhVJ88xHLJ6daKc9Xk76bvJHNNPnpucCi6rv+HoqjA1RGSPhv3rr+Lwldqk3TPjfJW1fkIUu9v9xTMC6HL37hjppcCWjbZV2kUOPcx4XOL2RESkDYQXONL+qpwnpzYBdxYzOwhY4e7zzGyfvmyrtIucnmY8DuqML2v7IiIi0i+9C/igmR1ANCHxxsA5wPDulFNE4S7LGjVUL/BYREREBoB2islx99PcfUx8I+RjwJ/c/Ujgz8BhcbUZ5JiGRhc5IiIi0h98DfhynOh7BHBBoxXMS/wdrqe0Dmb2eeCz8fJZ7n5KvXamTp3Z1kk1ikyV3kpVTn2fZ/tVbjtP8HS7968ThYHI0Lxg5FCrz/8iwkEHWakN2v1cCc/tMKgXynsdwm1nefDww1PLtp/25UR5vRMmp+os3X33RHnM3LmJclZah1BWnccu3bcZc9XlNmfVjyr7np224Zcr27cyA4+7JdI6mNm+wMHATu7+iplt1vOqIiIiIsVUcZETOgk4091fgTfm0BEREZEW6dQs5GXH5KTSOgDbAXvFUzP/t5ntmrWi0jqIiIhIX5R9J+fd7r4s/knqZjN7KN7mJsDuwK7AFWa2jQfBQbXj6Ns9JkdERETaT6kXObVpHczsGmA3olkKr44vau40s7XApkBzpvAUERGRXunUn6tKG12VkdbhZuA7wHhgC3f/ppltB8wGxoV3cmo1405OfxxR0SxFRuwM5ONVFqWz6J2Lfv50ojzjs6Na1JNIWa/fvFOSo3OmnDW3h5qSV5h2Ims0WrNev0aja+efPCW1bOKFf0+Us/p37+3HVjq66qYXzq7sKmf/jb/aEaOrMtM6xKnRf2VmDwCvAjPqXeCIiIhIuTr1Tk7laR3c/VXgqLK2KyIiIgKtGUIuIiIibaRT7+QorYOIiIh0pFLv5GSldTCzycB5RJlF1wCfcfc7y+wHDOwgzyLTvWcdr6rSV/S3NBl5lRXoWLTddg+EDgON531ri1Sdyf+eHJSZZx+ypvjP8x5ZM3Ror7cVpmMYtmRJqk4YaJwn6L/KgQFZ2wrTEGSlaAiFdfL0N89+Zr2eG6xMnhfLdk1Px7Z86vaJ8vj/d0eqzobLlzfsY7hfz0ycmChP/vG81Dr3f+ITifIOV1zRcDtlW9PqDpSk8rQOwFnAt939hjiN+lnAPhX0Q0RERAaQVsTkOLBx/Pcw4MkW9EFERERiiskpJiutw5eAH5rZEuBs4LSsFZXWQURERPqiFWkdDgNOdverzOxw4ALgveGKSusgIiJSjU69k9OKtA4zgC/GVX4HnF9mH6R5qgpMbbcA2HbTrOPT347zlG+lf9k+5ebnEuWz9ntrw3aKBOIXXS8r0LiRPK9LGASdd708np0wIVEesWhRqk4Y2NsseQYdhLJelzAYefT8+ak6YcDw8MWLU3VWjR6dKGe9nh/+QzI4+eqD0q9D2J/trruubl+keUq7yMlI67A/UVqHJ4G9gVuA9wDpd5CIiEgHyBr91Y50J6f3ekrrsAo4x8yGAKuBE+u0ISIiIlJIK9I63AakM5aJiIhIS3TqnRzNeCwiIiIdSRc5IiIi0pHKTuswnGj01I5Ec+YcB/wNuBwYDywGDnf353poQkTaSJXpBPIIR1OFI4Mge3RQf1d0hFge4fEqmgajUUqLoudSnnQkYf+ytvXJtT9JlH858UupOmH6iqw0D5cekTzn1iO57axjVSTFRdk6Na1D2XdyzgFudPeJRPE5C4FTgdnuPgGYHZdFRKRDhBc4Iq1S5hDyYcA04BgAd38VeNXMDubNXFUXEQ0l/1pZ/RAREZH6FHjce1sDK4ELzexeMzs/ni9nlLs/FddZTjTUPEVpHURERKQvyrzIGQLsApzr7jsDLxH8NOXuThSrk+LuM919qrtPHTlyWondFBERGdjWuFf2qFKZgcdLgaXufkdcvpLoIudpM9vc3Z8ys82BFSX2QSQhT9BiJyhrP9v9eGUFGc876ahEecq5F1fVnY6wfPLk1LKtbr217jpF0lnkVSQ4edfLHknVue7Q6YnykAnp1ArP7JUcE7PVrelth8HJXePHJ9vNkbIhK6BZmqPMyQCXm9kSM9ve3f8GTAcejB8zgDPjf68tqw8iIiLSWKfG5JSdhfzzwCVmti7wKHAs0U9kV5jZ8cDjwOEl90FEREQGoLKzkP//9u48Xo6qzvv45weBhC1EIGENBiQRGJYgF56IsmZEwCU6Aiogq0YQUSKKAvOwOAMioNHREZ8IKA6gsimMCASRdV5CFghJ2BIYwpIQEpGwJ5Dk9/xx6pruququ6k5Xdd++3/fr1a+kqs+pc6r6dHXdqnPObwbQk/LW2JR1IiIi0gbdeidHMx6LiIhIVyr6cZWIiIh0uG6d8bgdYR3+BfgE8A7wNHCcuy8psh5S7c2hQxPr1lu8uLTy2znCqdNHB5Wpv4w0i4+mWuOqJxJpVh61fcPbzROWoNOPcZ59SBtJNf30MVXLoydOz9xOnvebOV559uG+E5O9JpYPH1S1PGhJys/QKx+NrZidSBI/d8a3k7YP08/cqWp5+18MSqSR1mhHWIc7gJ3cfRdgDnBGwXUQEZESddrFnGTr1nlyCrvIqQjrcDmEsA7uvsTdJ7t7752xB4CtiqqDiIiI9F/tCOtQ6Xjg1rTMCusgIiJSDt3JaVzdsA5mdhahr9PVaZkV1kFERERWRzvCOmBmxwIfB8ZG8aukRGV2Mk7T1zpn9kV5jmE3Huc8nVDTOhm/Onx41XKesATtPMZ59jOPZuu3+0UPNJwnz/c8T33m77FH1fKWU6dm5skTWiHtvBjvtP7ZPe9JpPndlH2rlrPObwCjL36yannm0Udn1k+aU3pYBzM7CDgd2Nfd3yqqfBEREcmnWycDbEdYh6nAQOAOMwN4wN1PLLgeIiIi0s+0I6zDdkWWKSIiIo3p1js5CusgIiIiXUlhHURERPo5hXVoQlpYB3f/a/TeacAlwFB3/1uR9ailG0f1LNtgg8S6ga+/3oaa1BYPKxEf1dCqkSNpuvEzb5VWHfciP78szZYTH031zNixiTTb3HlnU9tuVNrxi0vbz6zvVZo854tWnVOWD6oOXfDCmDGJNPEQEmnHYrMZMxoue+Ho0Yl1Q+bNq1qOh1pIdUF2kvjxShvZtWTEiKrlLaZMSW7opA9mFyaZir6T0xvW4dCo8/G6AGY2HDgQeK7g8kVERCSD+uQ0qFZYh+jtiYRh5N15VEVERKTtSg/rYGbjgPnu/ki9zArrICIiUg6FdWhcWliHc4EzgbOzMiusg4iIiKyOssM6nEu4w/NINBHgVsBDZranuy8ssC6purHTaad3MobsDpHNfi6tmja+v2rVsSmqk3i8sybAxnPntqSsuLROxr++v7oD6dEfHpRI00zH9lZ1hm8mXEuz54t459o824mniXcyblZa5+R4p+ZNnngikSbeIXjEdcnPM945OU/5s095b9VyPIQDwPoLq3/u0o9fuR2P1SenQdFFy/Nm9v5o1VjgIXcf5u4j3H0E4ULoA+24wBEREZHu1o6wDiIiItJBuvVOTjvCOlS+P6LI8kVERKT/0ozHIiIi/ZxmPBZpQjOdIdPkmXVVnYo7w9yPfSyxbuQttzS8nfjn2apOxs3OyBzvaDzx7GcSaSZ8d5uG69Pps0q3czBD2j69tPPOVcubzpqVSNNMp+a0mYnj5U8/PTlL8+4XPVC9fMHsRJr455V2TOfvsUeuevYHZjYIuBcYSLhOud7dzzGzbYDfAhsD04EvuPs79bZVaIBOMxtiZteb2RNm9riZfTBaf0q07lEzu6jIOkh36rRRZCIiafKE6dAFTsIy4AB33xUYDRxkZmOA7wMT3X074BXghKwNlR7Wwcz2B8YBu7r7MjMbVnAdREREpI5O6njs7g68ES2uFb0cOAA4Ilp/JWFamkvrbasdYR1OAi5092XR+kVF1UFEREQ6S2VEg+g1PiXNmmY2A1gE3AE8DSxx997uQy8AW2aVVXpYB2AUsLeZPWhm95hZ6n06hXUQEREpR5lhHSojGkSvSfH6uPsKdx9NmDR4T2D7Zvar7LAO34nWbwSMAb4FXGvR9MeVFNZBRESkf4ueAN1FmAJ6iJn1drPZCpiflb/ssA7fidbfGD1zm2JmK4FNCHd92qrMEQvNyFO/IvehVdPPN6O/dDQu6hiX2babGUlVplbtd9pIqkn3vlW1PH6fdVtSVlyz+xAfHRQfGdQXpI2miot35N1y6tTMPG9stllmmrTjNf2ko6rTXHpV1XJaOJJ4WIe0sBOwW2Z9WqmT+uSY2VDgXXdfYmbrAB8hdDq+CziUMMLqGOCmrG2VHdbhMeAPwP4AZjYKWBv4W1H1EBERkT5lc+AuM5sJTAXucPc/At8GvmFmTxGGkV+etaF2hHV4E7jCzGYD7wDHRHd1REREpA066U6Ou88k5VaWu/8voX9Obu0K63BUyjoRERGRltGMxyIiIv2cwjr0A3k68706fHhi3YbPP19EdRLy1K/IzsBldnLur4o6ft3wueQJ7dFu8Y7Gs448smp556uvztxGkd+rVnU0Tvss4uKfTTOd6vPMFrx0yJDEunhH3jeHDk2keW6f6lG7O9xwQyJNfD/T9nvwc6/UTZMWjuT0O6rzfO8QzYlblEIvcsxsCHAZsBNhtsLjgbeBnwODCBePX3H3KUXWQ0REytNpF5+SrZP65LRS6WEdgGuB89z9VjM7BLgI2K/geoiIiEg/U9hFTkVYh2MhhHUA3jEzBwZHyTYEFhRVBxEREcnWrXdy2hHW4VTgYjN7HrgEOCMts8I6iIiIyOpoR1iHk4AJ7j4cmECNyXwU1kFERKQcZcauKpMVNQ+fmW0GPODuI6LlvQkXOR8Ghri7RzGrXnX3wbW3BD09k6oqGR/hVNboptXx8siRVctpPe6L0s5wDO2k0V+NaefIwf7i1MOT0Wt+dG1y5E8r5BmZlKa/fkeKOl88u/feiXXvve++zHzTpo1PxHQs0j5Pnlba1ce97/9BafvWjrAOC4B9o3UHAOX92ouIiEi/0Y6wDjcBP44iiS4FxhdcBxEREamjWzsetyOsw/3A7kWWKyIiIqIZj0VERPo5hXVoUNQX53cVq7YFzgZ+Ha0fAcwDDnf3V+L56ymqM2SrOp6lbafMjsZxzexDX5hCP0t/7UDZrE7rZNzOzvpp4t+JZr4PaZ2Mz58wp2r5rImjGt5umv7c/uNtZ9CSJYk06y1OdgJvhfl77FG1PGDp0kSaZ8aOrVre5s47C6mLFNvx+El3H+3uowmPp94Cfk8YYXWnu48E7oyWRUREpE26dQh5kfPkVBoLPO3uzwLjgCuj9VcCnyqpDiIiItKPlNUn53PAb6L/b+ruL0b/XwhsmpbBzMYTjbzaeusj0YSAIiIixejW0VWF38mJho9/Ergu/p6HmQhTj6xmPBYREZHVUcadnIOBh9z9pWj5JTPb3N1fNLPNgUUl1EFERERq6NY7OWVc5HyeVY+qAG4GjgEujP69qYQ65Bo51arRCN0wqqGvjaTqFt0QgqNVoxTbPZoqrqjvRHw01eBr5iTSvHZEa0ZcFSXPZx5Ps3TIkESePCOe8nxH4m3n4RNOSKQZdfPNVcsL9twzs+xhM2cm1sVHJW45dWrV8uOf+Uxm2WlppDUKvciJoo5/BPhyxeoLgWvN7ATgWeDwIusgIiLl6osX5/2d7uQ0wd3fBDaOrXuZMNpKREREpDCa8VhERKSf69YZj8uaJ0dERESkVO0I67Al8AngHeBp4Dh3T8653WJlPiNuVcdL6VytmOK/L8oT7qMbwoi0sz5pnYy/ctvfqpZ/dtAmpdQlr7TPPKuDcLNhFZppX7tdfnliXfwzfv/nf5BIM3XaN6qWt528MHM78Xay7eTJiTzxfdhiypREmjAwuTzd2ienHWEd7gB2cvddgDnAGUXVQUREypf2h55IO5Qe1sHdJ7t77+O/B4CtSqqDiIiI9CNlXeRUhnWodDxwa1oGMxtvZtPMbNrixfcWWjkREZH+TAE6m1QrrIOZnUXo0H11Wj6FdRAREZHV0Y6wDpjZscDHgbFR/CoRERFpk27teFx6WAczOwg4HdjX3d9qRQHtHs3Uzqn4uyEMQF9U1EibTvv83hw6tGq52RExWTptdFqn1Sc+mqrI0A/x0ULLBw1KpMlqB2u++27i3PTSzjtnlr3JE08kttMKz4xNzj8bD9Hw9kl7JDOeP6lqcfmgHRJJBixdWrUcP37x9wFeHjmyannIvHnJsqUl2hHW4afAQOAOMwN4wN1PLLIeIiJSHo2u6nt0J6cJNcI6bFdkmSIiIiKgsA4iIiL9nsI6iIiIiPQhpYd1cPcfRe+fBlwCDHX3v6VsIrd2d9ZsZ/nt3vdmqLN0kKffQrPHplXHuJmOxu0eCBDXTAiOTgszEZfWyXj66WOqlkdPnJ5Ik+dziO9nM/udVs6ms2ZVLc/fI9nRt6h2su6iRYl1g5ZURxOaMWH3RJpPDHu4anlBE8fib9tvn1gX72iclqZs6pPTIHd/EhgNYGZrAvMJYR0ws+HAgcBzRZUvIiIi/VtZfXL+EdYhWp5IGEZ+U0nli4iISA3deien9LAOZjYOmO/uj9TLoLAOIiIisjoKv5NTEdbhDDNbFziT8KiqLnefBEwC6OmZ1J2XmCIiIh2gW+/klBrWwcx2BrYBHokmAtwKeMjM9nT3hbU2oI6q0irNdEItyprvvpuYUTjeGXJ1tl2EIjtLFyVtxtksndTJGPKdA+MdjZddNiORZt1j/qnudtO2XVQn7Pjsxmn1aVXH+/UXJn9e4jM5D35040QaPpBd1qvDh1ctx9tb2mzG8TTxTtnBB7MLl0ylhnVw91nAsN43zGwe0LO6o6tE+qL4BY6ISLu4d+cs1YX2yakI63BjkeWIiIiIxJUe1iH2/ogiyxcREZH+S2EdRERE+ruVa7e7BoVQWAcRERHpSm0J62BmpwAnAyuAW9z99HrbKmu0RqdNRx+X1lG1mWn3W1V+s2W385h20qiZMj+7VmnVZ1fmiMky21t8v+IjePK0v2bPQ/E08ZFUAKOue6Zqec5h22RuN63OrRilWGR4jTmf/GTV8rCZMxNpNp47t2p58HPJCfhn/d+Dq/MwN5HmqY/vW7W8+6VXVS2nnbcXjh5dtbzZjORIuNJ16Z2c0sM6mNn+wDhgV3dfZmbD6mxGREREpCmlh3Uws4uBC919GYC7JyOniYiISHm69E5O6WEdgFHA3mb2oJndY2bJULQorIOIiIisnlLDOlSUuREwBtgDuNbMtnWvnlNaYR1ERERK0qV3ckoN6xAtvwDcGF3UTDGzlcAmQNt7YaZ17uukkBLt7qja7vLL0kmfeadpVef8bj2m8f3qtGMT72j82C8eTqTZ8Uu7VS3HwxYAbPj88w2X3czAhWYHCoy6+eaq5RknfySRZsh/zqtafmFscj/XeH3zquV4Z2WAjea8WLWcJ/RJPKTFkhEjMvNIc0oN6xD5A7A/cJeZjQLWBhTWQUREpF066E6OmQ0Hfg1sCjgwyd1/bGYbEUZtjwDmAYe7+yv1ttWOsA5XANua2Wzgt8Ax8UdVIiIi0m8tB05z9x0JXVtONrMdge8Ad7r7SODOaLmu0sM6uPs7wFFFlisiIiIN6KA7Oe7+IvBi9P/XzexxYEvC9DP7RcmuBO4Gvl1vW5rxWEREREpTOXo6eo2vk3YEsBvwILBpdAEEsJDwOKsuxa4SERHp70q8k1M5eroeM1sfuAE41d1fM7PKbbiZZXZ1saK6w9QK60C4vfRzYBDhudtX3H1KvW1pCLmIpGlmxE6nh29pt4lnV4d+mPDd7NAPrRqB1WkjG+P7NWjJkkSabWIj1BZ8bruGy0lrkw//9ThLSVoY+8vPSvud9QO+krlvZrYW8Efgdnf/YbTuSWA/d3/RzDYH7nb399fbTulhHYBfAOe5+61mdghwEauesYmISB/XzAWOtFkH9cmxcMvmcuDx3gucyM3AMcCF0b83ZW2rHWEdHBgcrd8QWFBSHURERKTzfQj4AjDLzHqjl55JuLi51sxOAJ4FDs/aUFkXOZVhHU4FbjezSwgdn/dKyxB1RBoPsPXWRzJ06D5l1FNERETayN3vB2o90hrbyLYKH11VEdbhumjVScAEdx8OTCDckkpw90nu3uPuPbrAERERKdDKtct7laiwjsf/KMBsHHCyux8YLb8KDIl6RhvwqrsPrreNvtbxeNkGGyTWNTs9eaOK7FTZqk6B7exc2ExHVenb4p1Hy+wvUlRbL7Pz9PSfz0ys2/3EXRreTvy7l9aJt1X7EP/M/z5qVCLNRnPmVC2nhVaI59vt8uTf5PH9WjpkSNVyWiiIl3beuW4egMUTdyi34/HkX5fX8fjAo0vbt3aEdVgA7EsYZXUAkGwBIiIiUp4VndPxuJUKvcipCOvw5YrVXwJ+bGYDgKVE/W5EREREWqkdYR3uB3YvslwRERFpgHfnnRyFdRAREZGupLAOIiIi/V0HTQbYSkVGUYy6AAAgAElEQVT3yZkAfBFwYBZwHLA58FvCY6zpwBeiyORdo6yRVGmKHKnUqm23c6r2bhhNFR+918721he0c/bdotp6md+htJFU50+oHpl01sTk6KW4Mr978c88rQ28PHJk1fImTzyRSJO2Li4+Mmr9hQsz87xw8tVVy6NPSevBsUPmdiRbYY+rzGxL4GtAj7vvBKxJmBTw+8BEd98OeAU4oag6iIiISA5dOk9O0X1yBgDrRCOp1gVeJAwbvz56/0rgUwXXQURERPqhIgN0zo9CNzwHvA1MJjyeWuLuy6NkLwBbpuVXWAcREZGSdGmfnCIfV70HGAdsA2wBrAcclDe/wjqIiIjI6iiy4/E/A8+4+2IAM7uREFl0iJkNiO7mbAXML7AOIl2nGzoalxnao51hRPLo9PqliXc0PuP4WYk037ti58S6suQJIZEWbiEuLURP3Iozb6xaHnhc9n6P+MFnqpaXD8rurFw43clp2HPAGDNbN4pRNRZ4DLgLODRKcwxwU4F1EBGRkrXzAkekUpF9ch40s+uBh4DlwMPAJOAW4Ldm9u/RutQo5CIiIlKSLr2TU3RYh3OAc2Kr/xfYs8hyRURERDTjsYiISH+nOznSieId4wYsXZpI02kdGftiR0tprTI/805vX51ev/j3FZJ1TuuDM/3UQ6qWd//Rn1pbsTryzK786V3/UrV81YLPJtLEz6dpx2K90w+IpUl2co6Lz4qcdt6W1ih0MkAzm2Bmj5rZbDP7jZkNMrOrzezJaN0VZpZsNSIi0mfFL3BE2qUdYR2uBrYHdgbWIcS2EhERkXZZsXZ5rxIV/biqN6zDu4SwDgvcfXLvm2Y2hTBXjoiIiEhLFXYnx93nA71hHV4EXo1d4KwFfAG4LS2/mY03s2lmNm3x4nuLqqaIiIj42uW9SlRqWAczO6oiyc+Ae939vrT8CusgIiIiq6PssA57AVeZ2TnAUODLeTak0Ti19cUp/vvr56d2LK2SFm6gqHPB8kGDEuuy2m7aSKqj/7SoavknX949kWbD559vsHb5PHzCCYl1y66tPobLhyT3c9Euu1QtD37uuUSa+EipuLRj9cZmm1UtP/WZDpghWkPIG/aPsA6EKORjgWlm9kXgo8BYd19ZYPkiIiLSj7UjrMObwLPAX0NIK2509+8WVQ8RERHJoDs5jasR1kETEIqIiEjhdMEhIiLS35U8f01Z+sRFjjpo1pY2zXhcXzt+3bhP0DfrnKezdCtCi+QJHdBfvDp8eGJdvENu2jGOfw6dNijh0mP+qWp52lVzE2nGfjTZ+bcVRt18c2JdvPNvWgfruCHz5iXWxdtpnvPXoCXVoR9G3JI8FhylKeRaodCLHDObQJjR2IFZwHHuvjR67z+A4919/SLrICIiIhm6tE9OO8I6YGY9wHuKKltERESk9LAOZrYmcDFwBPDpgssXERGRLCXPRFyWdoR1+Cpws7u/WC+/wjqIiIjI6ijsTk4srMMS4DozOxo4DNgvK7+7TyLMq0NPzyQvqp4iIiL93oo1212DQpQd1uE8YB3gqWgiwHXN7Cl3367AenS1bhyB0o371Ffl+SxaMYpHn/kqeUIbpB2voo5hns83z4iw9RYvrlpOG0l12hEzqpZ/cM3oPFXMlDZyauO51SOa0kZFLUkZxRY3/ZK3qpZ3/+a6DdcnKzSENK+wx1VUhHWwcEUzFvihu2/m7iPcfQTwli5wRES6S1ExqEQa1Y6wDiIiItJB1lhZZijJ8h6NtSOsQ+X7miNHRERECtEnZjwWERGR4tiKFSWW1iV3ctJmPAaWAf9OGGW1ArjU3f+jyHq0kqafl3bLE2qhnfQdWaXTP6tmxUNItKoPTryj8ajrnkmkmXPYNlXLecIopHXsjedbOmRIIk28c3Kanc7bNLYmu6P2ryZMr1o+8Zz3ZeaR5hQ5hLx3xuMd3f1tM7uWMOOxAcOB7d19pZkNK6oOIiIikq3cOznlKX3GY8JdnCPcfSWAuy8quA4iIiLSD7VjxuP3AZ+NZjO+1cxGpuXXjMciIiLlWGPlytJepe5XURuOzXi8BbCemR0FDASWunsP8AvgirT87j7J3XvcvWfo0H2KqqaIiIh0qbJnPN4LeAG4MUrze+CXBdZBREREMqhPTuP+MeMx8DZhxuNpwGvA/sAzwL7AnALr0HLdMjqir8kzSqUbR7LkGamk0UyrLx6WoFWjhfri55BntFIzoTya+X7GR1IBTL+gOiTC7mdmh15I26d4aIVBS5Zk5pt8fTLNweMaL/vkM7euWp5x6kfqb0Sa1o4Zj9cBro6Gl79BGGIuIiIibaI7OU2oMePxMuBjRZYrIiIiohmPRURE+rmyRz2Vpcgo5CIiIiJt046wDh8CLiZcYL0BHOvuTxVZD6n25tChiXXrLV7c8HbK7PCaZ7t9sZNnlv6635Bv2v1m2m2aPB2Nm+k42xc7wzezX3m206rjFe9o/J8XPJdIE+/YG+9knCatfcXXHXjovESaNamuY55O7A+fcELV8k6XX5us0FGfr1XVQnRrn5wi58npDevQ4+47ESJyfQ64FDjS3UcD1wD/WlQdRESkfH3hYk76h6IfV/WGdRjAqrAODgyO3t8wWiciIiICgJldYWaLzGx2xbqNzOwOM5sb/fuerO20I6zDF4E/mdkLwBeAC9PyK6yDiIhIOWzFitJeOf0KOCi27jvAne4+ErgzWq6rHWEdJgCHuPtWhNmOf5iWX2EdRERE+id3vxf4e2z1OODK6P9XAp/K2k7ZYR0+BOzq7g9GaX4H3FZgHURERCRDmUPIzWw8ML5i1SR3n5Qj66bu/mL0/4XAplkZ2hHW4TAzG+Xuc4CPAI8XWAdJkTZ9eR59caSIlK9V7SK+nVaNpGpWM/uVJ8+yDTaoWm4mZEKzmh0h+cZmm1Utr79wYWK7rThe8WMDyeMTH0kFcNoRM6qWL/jvvTO3k2bjuXMz0zy7d/W2N5tRXXZ8tBXAsJkzq5bzjP7qJtEFTZ6LmnrbcDPzrHTtCOvwAnCDma0EXgGOL6oOIiJSPv0B1Pf0kSHkL5nZ5u7+opltDizKytCOsA6/j14iIiIied0MHEMYsHQMcFNWBoV1EBER6ec67U6Omf0G2A/YJBqNfQ7h4uZaMzsBeBY4PGs7usgRERGRjuLutaZ8HtvIdooO6/B14EuAAb9w9x+Z2UaEUVUjgHnA4e7+SpH1kGrNPi/Xc3aR1iuzo3Fcs9/pPGEwWqHZY/ODa0ZXLd/5hw8n0nz0sD9nlhUPgZPW+f29991XN0/asYoP/lgyYkQiTdkUoLNBZrYT4QJnT2BX4ONmth1NTOYjIiIi0qgi7+TsADzo7m8BmNk9wL8QJvPZL0pzJXA38O0C6yEiIiJ1dFqfnFYpMnbVbGBvM9s4mivnEGA4OSfzUVgHERERWR1FzpPzuJl9H5gMvAnMAFbE0tSczKdysqCenkmZE/6IiIhIc7r1Tk7R8+RcDlwOYGYXECYCbHgyH6lNsxAXL8+sqyKt0uwsxJ0mPtNvvANus/vZzDkv3skY4NJzhlUtf+n7ryXSDFi6NHPbWZ2T0/YzbtNZs1LWfjAzn2QrenTVMHdfZGZbE/rjjCEE7GxoMh8REek70kIZSGfr1tFVRc+Tc4OZbQy8C5zs7kvMrOHJfEREREQaVfTjqkRUNHd/mQYn8xERERFplGY8FhER6ee6teNxkUPIRURERNqmHWEdLgY+AbwDPA0c5+5L6mxG6lg+aFBmmr44MqMozYzM0EgqKVOz39f4KMB2ttu0UAatGgnaTL608+RJ51UP7L36kvcl0nzmkjerltOOaXw0VZ7PIV6fPCOwiqY7OQ2qE9bhDmAnd98FmAOcUVQdREREpP8qPayDu19UkeYB4NAC6yAiIiIZunUIeTvCOlQ6Hrg1LbPCOoiIiMjqaFtYBzM7C1gOXF0jv8I6iIiIlKBb++S0I6wDZnYs8HFgrLu37QKmG0Ii9MVOse087p3+GXdDm+wGfTG0QqefC9p5/PKEZxh7/dDEutu/Xl3n8d9M5mumw3e8PkuHDMnMI80pPayDmR0EnA7s29tfR0RERNpHd3KakxbW4afAQOAOMwN4wN1PLLgeIiIi0s+0I6zDdkWWKSIiIo3R6CoRERGRPkSxq0RERPo59clpQlpYh4r3TgMuAYa6+9+KrEctnT5aolt1+nHva6O/+uJIoE7XzuPXrZ9nq75XzWwnLU18VNR777svkWZ8bNW0aeMTaXp6JmWWn1WfPOF5pDmFXeTEwjq8A9xmZn9096fMbDhwIPBcUeWLiIhIPt16J6fIPjn/COvg7suBewjDyAEmEoaRa5I/ERERKUTpYR3MbBww390fqZdZYR1ERERkdZQd1mEgcCbhUVVWfoV1EBERKUG3DiEvO6zDS8CngEeiiQC3Ah4ysz3dfWGRdRHJq6918uxr9ZX6ivw8uyFsSJ46p3XejssTfuHV4dUxpdM6Gd971+erlvfZ/zeZdYmHcVh/oX7+ilJ6WAd3/3HF+/OAnnaNrhIREZHu7XhceliHgssTERERAdoQ1iH2/ogiyxcREZFs3XonR2EdREREpCsprIOIiEg/p9FVTagV1sHMTgFOBlYAt7j76UXWQ7pPnqnvu3V6/E6n494eeUZOtfNzaGZUVLP1jed7ZuzYRJqt762efy0+4glgw+efzyzrQwdeX7V80sFPVy1PuvV9mdtYOHp0ZhppTulhHYDhwDhgV3dfZmbDiqqDiIiIZOvWPjlF3sn5R1gHADPrDevQA1zo7ssA3H1RgXUQERGRfqr0sA7AqGj9g2Z2j5ntkZZZYR1ERETKYStWlPYqU2EXOe7+ONAb1uE2QliHFYS7RxsBY4BvAddaNP1xLP8kd+9x956hQ/cpqpoiIiLSpcoO6/ACsD1wo7s7MMXMVgKbAIubLadVHR1fHjkysW7juXObqpPk10xnQ3Vk7VxlfjbdEKagqM62na7MDupbTJmSWdbyQYMSad4cOrRqeb3FyZ+peIflS/+8fdXy+ackRy2d/ZPq7aRtF3ZLWVccja5qQlpYB2AlsD9wl5mNAtYGFNZBREREWqr0sA5mdgVwhZnNJoy6Oia6qyMiIiJtoNFVTUgL6+Du7wBHFVmuiIiIiMI6iIiISFfqirAOreqslqeTsWZzbb2ijl+3fi7d0Nm2VZqZRbfZ7TQjz/miL35+yzbYoGp54OuvN7yNPPv96vDhiXV5ZiGOH/cBS5dm5ll/4cLMNHnE9+vsnyT348enLKha/vav1m1J2aujWx9XFXonx8y+bmazzexRMzs1WjfazB4wsxnRPDh7FlkHEZFO0RcvaGT1pF2oSXnaEdbhIuA8d7/VzA6Jlvcrqh4iIiJSn4aQN65WWAcHBkdpNgQWpGcXERERaV6RFzmzgfOjIeRvE8I6TANOBW43s0sIj8v2SstsZuOB8QBbb30kmvVYRESkGOqT06A6YR1OAia4+3BgAtGMyCn5FdZBREREmmZlzcNXEdbhe8AQd/coZtWr7j64Xt6enkmrXUmNihIRaVyec2enn1/jo8EgOSIszz7k2U48TdrIrvh2p00bn0gDJGI6Fmm3D/6ytEl5H/7rcaXtW9Gjq4ZF//aGdbiG0Adn3yjJAYCCQ4mIdJFOusCR/q0dYR2+BPzYzAYAS4n63YiIiEh7aHRVE2qEdbgf2L3IckVERKTvMrODgB8DawKXufuFzWynK2Y8FhERkeZ10ugqM1sT+E/gI4S+vFPN7GZ3f6zRbSl2lYiItFSeUBoidewJPOXu/xsF9f4tMK6pLbl7n3kB48vKV1aebi1L9es7ZXV6/XQsdCzaXVan16+vvQh9cadVvMbH3j+U8Iiqd/kLwE+bKqvdO9vggZlWVr6y8nRrWapf3ymr0+unY6Fj0e6yOr1+3fZq5UWOHleJiIhIJ5kPVEY23Spa1zBd5IiIiEgnmQqMNLNtzGxt4HPAzc1sqK+NrppUYr6y8nRrWapf3ymr0+tXZlmdXr8yy+r0+pVZVqfXr6u4+3Iz+ypwO2EI+RXu/mgz2yotrIOIiIhImfS4SkRERLqSLnJERESkO7V7qFgDQ8oOAp4EngK+kyP9IGAK8AjwKHBeA2UNAa4HngAeBz6YI8/XgdlRWafWSXcFsAiYXbHu4qismcDvCVHas/KcS+htPiN6HZIjz2jggSj9NGDPWJ7hwF3AY9F+fD1af1i0vBLoSdmn1HwV758GOLBJjrJ+V7FP84AZeT5XYBvgwah9/A5YO0eey6N1M6PPe/0ceQw4H5gTtY2v5azfAcBDURu5EhiQchzXBB4G/hgtX01o87Ojz3OtHHl+BTxTcQxH12iH8Xxjo/rNAO4HtoulnwfM6m07edpFrXz12kWdsrLaReI7C2wE3EEIAnwH8J4833Xg36I2MQOYDGyR5/wAnBKtexS4KGdZuwJ/jfb3v4HBFenfX7HPM4DXgFOpc76ok+dc6p8vauXLOmdMiPZ3NvAbQvv/KuF7mPhs6+WreO8/gDfy5AHuq6jzAuAPWeflnO0iLV9Wu0j9DajXLmqUU7NN6NX4q+0VyFXJcEJ+GtgWWJvwA7JjRh4j+tEC1iL8AI7JWd6VwBej/69N7KIjJf1OUUNdl9CZ+8/EfiQq0u4DfIDqi48DiX70gO8D38+R51zgm3XqlJZnMnBw9P9DgLtjeTYHPhD9fwPCD/mOwA6Ek+DdpF/kpOaLlocTOo89S/VFTs08FWl+AJyd53MFrgU+F63/OXBSjjyVPyg/pOLiuU6e44BfA2tE7w3LUb+9gOeBUdH67wInpBzHbwDXsOrC45Boe0Y4qZ+UI8+vgENztPF4vjnADtH/vwL8KpZ+HsmLkbrtola+eu2iXp6MdpH4zgIX9X6mwHeIfa/q5KtsF18Dfp4jz/6E7/3AtHZRJ99UYN9o3fHAv9XY5zWBhcB7yThf1MhzLnXOF3Xy1TxnAFsSLqjXiZavBY4FdgNG1PnsU/NF/+8B/ovYRU69PBVpbgCOrlhOPS9ntYs6+Wq2izp5araLOnlytQm98r36yuOqhqd49uCNaHGt6OVZBZnZhoQLhMuj7bzj7ksysu0APOjub7n7cuAe4F9q1Ote4O+xdZOjfBD+atoqK0+WGnkcGBz9f0PCXz6VeV5094ei/79O+GtzS3d/3N2frFNWar7o7YnA6cSOfUYezMyAwwk/7pX5an2uBxD+SobwY/KprDzu/lpFWetU1rFOOScB33X3lVG6RTnqtwJ4x93nROvvAD5Tmc/MtgI+BlxWsa0/Rdtzwt2hrbLy5FEjX922kSarXWRIbRdZ0tpFne/sOEJbgFibqJevt11E1qusY52yTgIudPdl0fqqdlEn3yjg3ihZol1UGAs87e7PZp0v0vLUeL+WynxZ7WIAsI6ZDSD8WC9w94fdfV5GGYl8UbyiiwntIlee3jfMbDDhHPCHivS1zst120WtfPXaRZ2y6rWLWnnytgnJoa9c5GxJ+Eu41wtU/CDWYmZrmtkMwmObO9z9wRxlbQMsBn5pZg+b2WVmtl5GntnA3ma2sZmtS/iLZ3hGnlqOB27NmfarZjbTzK4ws/fkSH8qcLGZPQ9cApxRK6GZjSD8RZbnmKXmM7NxwHx3fyRvnorVewMvufvclPRVnyvhLt+SihN/on3Uagtm9kvCX63bAz/Jked9wGfNbJqZ3WpmI3PUbwowwMx6oiSHkmwfPyKc3FembG8twoyft+XMc37ULiaa2cD49mrk+yLwJzN7ISorHvHXgclmNt3Mxqdss5ZEvhztol5Zae2i1nd2U3d/MUqzENg0tq2a33UzOz/6nhwJnJ0jzyjCOeBBM7vHzPbIWdajrPqD7TBqnzc+R+yCP1LvfBHPk/d8UZmv5jnD3edH654DXgRedffJdbable+rwM0Vn1mePL0+BdwZuxCpdV7Oahc1z+d12kWtPPXaRa08eduE5NBXLnKa4u4r3H004S+dPc1spxzZBhAe81zq7rsBbxJuadYr53HCbePJhB+iGYS/3htiZmcBywl9MbJcSvjBHU340v8gR56TgAnuPpzwfPvyGvVYn3Dr99TYSaOuynyE/TiT6hNBI2V9nvSTeuJzJVyg1FWrLbj7ccAWhDtJn82RZyCw1N17gF8Q+spk1e+fCD8cE81sCvA6Fe3DzD4OLHL36TWq/zPgXne/L0eeM6LjsQeh78G3K9+sk28CoZ/GVsAvCY/vKn3Y3T8AHAycbGb71KhrXFq+rHZRr6y0dpH5nY3uhsXvGtXM5+5nRd+Tqwk/vll5BhCO9xjgW8C10V2nrHzHA18xs+mEx7bvxA9GNBnaJ4HrYutrni9S8uQ6X6Tkq3nOiC6UxhEu4LYA1jOzo9K2GysjLd/RhB/0nzSQp7KsRLvIc15Oaxf18tVqF3Xy1GwXdfJktglpgHfAM7OsF6GD3u0Vy2cAZzS4jbPJ8Uwa2AyYV7G8N3BLg2VdAHylzvsjqOgrE607ltDZbN28ebLei68HXmXV3EgGvJaSZy1CX4lvpLx3N7X7XlTlA3Ym3MmYF72WE/4K2yyrLMKJ4SVgq5yf67eAv7Gqn0JVe8nTFgiPEv6YlYfQgXCbimP4aqPtjtCn4tqK5e8R7j7NI/xl+RZwVfTeOYRb8GvEtlEzT0Wa/eL7VCPfLYTHE71ptgYeq7NP51buU712kZLv/2a1i1pl1WoX1PjOEjptbx6t2xx4Mk++WJqtqf4O1SrrNmD/ivVPA0MbLGsUMCXlGIwDJsfWHUv980UiT8V7I6h9LqnKR51zBuGi5PKK5aOBn1UszyO9T05avmei9tjbLlYSuihklgVsArxMReflGvt2AaG/Wd12UStfvXZRp6y67SJHOaltQq/8r75yJ6fhKZ7NbKiZDYn+vw7wEcIPVF3uvhB43szeH60aSxgBVJeZDYv+3ZrwXPWarDwVeQ8iPD74pLu/lTPP5hWLnybc+syyANg3+v8BhNEFlds0wl9qj7t7/C/5enVJ5HP3We4+zN1HuPsIwg/rB6Ljm1XWPwNPuPsLKWWlfa6PE0ZqHRolOwa4KSPPk2a2XUVdPklF+6jTfv5A6EwI4Vj29rOpm6+ifQwk3F35eW8edz/D3beKjtPngL+4+1Fm9kXgo8DnPeoDlCPP5hX79Cli7SItH+GHbUMzGxUl6z2mvfu0nplt0Pt/wkVaZnurkW9qRruoV1Zqu6jznb2Z0BYg1ibq5bPqR5DjqGgXdcr6R7uIjuPahAvvrLJ628UawL9S0S4qVN2lyHm+iOfJe76I3xGpd854DhhjZutG7W0sFe2mjrR8P3T3zSraxVvuvl3Osg4lXMwvjRdU47xct13UylevXdQpq267qFFOnjYhebX7Kivvi/C8cg7hSvisHOl3IQyRnUn4Qp/dQFmjCcMlZxIaaWKIYUqe+wgnu0eAsXXS/YZwu/hdwgn+BMJwy+dZNRQyPpojLc9/EYYYziR8aTfPkefDwPSojg8Cu8fyfJhw63ZmRV0OIZwUXwCWEf6Svj1PvliaeVSPrqqZhzBC6MRGPlfCyLsp0bG8jmg0Q608hEe1/xMdw9mE28+Dc5QzhPCX+yzCX9K75qzfxYST8pPUn2JgP1aNeFpOaO+9xye1Dcfy/KVin66iYlh8Rr5PR/keIdyZ2bYi3bbR+t5h8WdV5KnXLlLzZbSLmnky2kXiOwtsDNxJ+GH+M7BRznw3RMdvJmEI75Y58qwdHe/ZhKH4B+Qs6+uE89ocQj8oi+VZj3CXYsOKdVnni7Q8dc8XdfJlnTPOI/zYz47KGEgYefQCof0uoCKadL18sffThpCn5iG014Pynpdztou0fFntIi1P3XZRI0/dNqFXYy+FdRAREZGu1FceV4mIiIg0RBc5IiIi0pV0kSMiIiJdSRc5IiIi0pV0kSMiIiJdSRc5Ih3EzFaY2Qwzm21m10XTvTe7rV+Z2aHR/y8zsx3rpN3PzPZqoox5ZrZJ3vWxNG/Uez8l/blm9s1G6ygi/ZcuckQ6y9vuPtrddyJM535i5ZsWghM2zN2/6O71JrXcjxAtXUSka+giR6Rz3QdsF91luc/MbibMkrummV1sZlMtBFz8MoRZjs3sp2b2pJn9GRjWuyEzu9uiAKFmdpCZPWRmj5jZnRYCpJ4ITIjuIu0dzdx8Q1TGVDP7UJR3YzObbGaPmtllhKn+6zKzP1gItvmoxQJuWggi+mhUj6HRuveZ2W1RnvvMLDM2mYhImqb+KhSRYkV3bA5mVeTxDwA7ufsz0YXCq+6+RxQm4n/MbDIhkvv7gR0JkZUfIxZANLqQ+AWwT7Stjdz972b2c8Iss5dE6a4BJrr7/dGU87cDOxBiad3v7t81s48RZtLOcnxUxjrAVDO7wd1fJsywO83dJ5jZ2dG2vwpMIsxsPNfM/g8hQOkBTRxGEenndJEj0lnWMbMZ0f/vI8T32osQpO+ZaP2BwC69/W2ADYGRhCCjv3H3FcACM/tLyvbHECKaPwPg7n+vUY9/Bna0VYG0B1uIGL8PIcYO7n6Lmb2SY5++Zmafjv4/PKrry4QgjL+L1l8F3BiVsRdwXUXZA3OUISKSoIsckc7ytruPrlwR/di/WbkKOMXdb4+lO6SF9VgDGOOxoIcVFx65mNl+hAumD7r7W2Z2NzCoRnKPyl0SPwYiIs1QnxyRvud24CQzWwtCdOMoYve9wGejPjubsypaeqUHgH3MbJso70bR+teBDSrSTQZO6V0ws96LjnuBI6J1BxOCTNazIfBKdIGzPeFOUq81WBU5/gjCY7DXgGfM7LCoDDOzXTPKEBFJpYsckb7nMkJ/m4fMbDbw/wh3ZX9PiKz8GPBrQpT0Ku6+GBhPeDT0CKseF/038OnejseESNI9Ucfmx1g1yus8wkXSo4THVs9l1PU2YICZPU6IqPxAxXtvAntG+3AA8N1o/ZHACVH9HgXG5TgmIklpBpgAAABESURBVCIJikIuIiIiXUl3ckRERKQr6SJHREREupIuckRERKQr6SJHREREupIuckRERKQr6SJHREREupIuckRERKQr/X+Tji5txAlmhAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e48000f7a7ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# write down json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mwriteMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusionMatrixData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracies' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-o1n4skl-77"
      },
      "source": [
        "##CLOSED AND OPEN WORLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lUUjCewmNLo"
      },
      "source": [
        "#Dataset divided into 2 halves, 50 for closed 50 for open (choose five different random division)\n",
        "#1) closed world\n",
        "#  1.1)without rejection -> standard incremental scenario (train and test using selected 10 classes) but with 50 classes\n",
        "#      iter = 0 -> 10 or 20 (he does so in BDOC) classes ? ask Dario\n",
        "#      next iters -> add 10 until 50\n",
        "#      result expected -> equal to incremental \n",
        "#\n",
        "#  1.2)with rejection -> same procedure of above but we implement a rejection technique that \n",
        "#      classify as unknown an object that doesn't belong to the classes seen in the training (for the alg follow BDOC)\n",
        "#      result expected -> idealistic the model should not reject any of the object because we've tested the model with classes seen in the training\n",
        "#\n",
        "#2) open world\n",
        "#    at each step -> test the model only on unknown samples (the second half of dataset)\n",
        "#    \n",
        "#    result expected -> idealist the model should reject all of the test objects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFHiLXyLmaWu"
      },
      "source": [
        "###download and dividing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OJxXhGZmdDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9165f481-cd20-4279-c304-0fe65df85521"
      },
      "source": [
        "# Import dataset and apply transformations \n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# Check datasets length \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZFLVy0Tmt3G"
      },
      "source": [
        "#closed and open world\n",
        "splits_of_10 = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "#first 5 splits to closed world\n",
        "closed = {k:splits_of_10[k] for k in range(5)}\n",
        "\n",
        "#last 5 to open (removing the train val splits)\n",
        "open = []\n",
        "for k in range(5,10):\n",
        "  for j in[\"train\", \"val\"]:\n",
        "    open += splits_of_10[k][j]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMyXZhVemn8q"
      },
      "source": [
        "### Closed world without rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyhNorNImn9E",
        "outputId": "b3eff72d-7e3f-4af2-f4f8-60eb6f5ab110"
      },
      "source": [
        "# Reverse indexing for closed world\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, closed)\n",
        "print(outputs_labels_mapping.getGroups())\n",
        "\n",
        "# TEST split\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Int64Index([0, 1, 2, 3, 4], dtype='int64')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W6-U42kmn9F"
      },
      "source": [
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,5):\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwfGtPNUoSdC"
      },
      "source": [
        "def sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 5\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      print(\"TEST SET LENGHT: \", len(test_set))\n",
        "      print(\"TEST CURRENT GROUP SET LENGHT: \", len(test_subset))\n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      test_classes = list(test_dataset.df.loc[test_set.indices, 'labels'].value_counts().index)\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TEST_SET CLASSES: \", test_classes)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl8IKzFsocw5"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI36G5hjodD8"
      },
      "source": [
        "method = \"Closed world without Rejection\"\n",
        "print(\"metrics ClosedWord for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,5):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}