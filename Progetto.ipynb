{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LWF funzionante.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoManca-FM/Project_MLDL/blob/main/Progetto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAfp19qRVa7l"
      },
      "source": [
        "### GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag2iq0PaVWC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7633fd2-1943-4c9a-ca26-a3e297871c2b"
      },
      "source": [
        "# Check GPU assigned\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jun  8 08:13:46 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   78C    P0    34W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OWtqytyzmMF"
      },
      "source": [
        "## Network, dataset, functions and parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itMjhanGVTUp"
      },
      "source": [
        "### LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6tCA_s2rru"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nHVORYbiZRx"
      },
      "source": [
        "### Resnet32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIdSy6PnibZA"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\"\"\"\n",
        "Credits to @hshustc\n",
        "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from torch.nn import init\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_relu=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.stride = stride\n",
        "        self.use_relu = use_relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "        if self.use_relu:\n",
        "            out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        assert stride == 2\n",
        "        self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat((x, x.mul(0)), 1)\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block=BasicBlock, depth=32, num_classes=0, channels=3):\n",
        "\n",
        "        super(CifarResNet, self).__init__()\n",
        "\n",
        "        # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
        "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
        "        layer_blocks = (depth - 2) // 6\n",
        "        bn = nn.BatchNorm2d\n",
        "        self.inplanes = 16\n",
        "\n",
        "        self.conv_1_3x3 = nn.Conv2d(channels, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn_1 = bn(self.inplanes)\n",
        "\n",
        "        self.stage_1 = self._make_layer(block, 16, layer_blocks, 1)\n",
        "        self.stage_2 = self._make_layer(block, 32, layer_blocks, 2)\n",
        "        self.stage_3 = self._make_layer(block, 64, layer_blocks, 2, last=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.relu = nn.ReLU()\n",
        "        #self.linear = nn.Linear(64, num_classes)\n",
        "        self.fcs = nn.ModuleList([nn.Linear(64, num_classes)])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, math.sqrt(1. / 64.))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, last=False):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = Downsample(self.\n",
        "inplanes, planes * block.expansion, stride)\n",
        "\n",
        "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        if last:\n",
        "            for i in range(1, blocks - 1):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            layers.append(block(self.inplanes, planes, use_relu=False))\n",
        "\n",
        "        else:\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv_1_3x3(x)\n",
        "\n",
        "        x = self.bn_1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.stage_1(x)\n",
        "        x = self.stage_2(x)\n",
        "        x = self.stage_3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        out = []\n",
        "        for fc in self.fcs:\n",
        "            out.append(fc(x))\n",
        "        out = torch.cat(out, dim=1)\n",
        "        return out\n",
        "\n",
        "    def addOutputNodes(self, num_classes):\n",
        "        self.fcs.append(nn.Linear(64, num_classes))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ga-SaCsjkYV"
      },
      "source": [
        "### Cifar100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHAyljuejmkd"
      },
      "source": [
        "# ref:\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py\n",
        "# https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR10\n",
        "# homework2 (caltech)\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# This is an handler class for the Cifar dataset\n",
        "class CIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    `CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "    This is a subclass of the `CIFAR100` Dataset.\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar-100-python'\n",
        "    train_file = 'train'\n",
        "    test_file = 'test'\n",
        "    meta_file = 'meta'\n",
        "\n",
        "    def __init__(self, root, split = 'train', transform = None):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              root (string): Root directory of the dataset where directory\n",
        "                  cifar-100-python exists.\n",
        "              split (string, optional): If 'train', creates dataset from training\n",
        "                  set, otherwise creates from test set.\n",
        "              transform (callable, optional): A function/transform that takes in a\n",
        "                  PIL image and returns a transformed version.\n",
        "        \"\"\"\n",
        "        super(CIFAR100, self).__init__(root, transform=transform)\n",
        "\n",
        "        self.split = split\n",
        "        if split == 'train':\n",
        "            filename = self.train_file\n",
        "        else:\n",
        "            filename = self.test_file\n",
        "\n",
        "        data_path = os.path.join(self.root, self.base_folder, filename)\n",
        "        data = None\n",
        "        labels = None\n",
        "\n",
        "        with open(data_path, 'rb') as f:\n",
        "            entry = pickle.load(f, encoding='latin1')\n",
        "            data = entry['data']\n",
        "            labels = entry['fine_labels']\n",
        "        \n",
        "        data = np.vstack(data).reshape(-1, 3, 32, 32)\n",
        "        data = data.transpose((0, 2, 3, 1))  # Convert to HWC\n",
        "        \n",
        "        labels = np.array(labels)\n",
        "\n",
        "        self.df = pd.DataFrame()\n",
        "        self.df['data'] = pd.Series(list(data))\n",
        "        self.df['labels'] = labels\n",
        "\n",
        "        self.data = self.df['data']\n",
        "        self.labels = self.df['labels']\n",
        "\n",
        "        self._load_meta()\n",
        "\n",
        "    def _load_meta(self):\n",
        "        meta_path = os.path.join(self.root, self.base_folder, self.meta_file)\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f, encoding='latin1')\n",
        "            self.label_names = meta['fine_label_names']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.df.loc[index, 'data'], self.df.loc[index, 'labels']\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img) # Return a PIL image\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return index, img, target\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def getTargets(self):\n",
        "        return set(self.labels)\n",
        "     \n",
        "    # test\n",
        "    def get_indices(self, labels):\n",
        "        return list(self.df[self.df['labels'].isin(labels)].index)\n",
        "\n",
        "    def split_classes(self, n_splits=10, seed=None, dictionary_of='dataframes'):\n",
        "        if dictionary_of not in ['dataframes','indices']:\n",
        "            raise ValueError(\"'dictionary_of' must be equal to 'dataframes' or 'indices'\")\n",
        "\n",
        "        all_classes = list(self.df['labels'].value_counts().index)\n",
        "        dictionary = {}\n",
        "        random.seed(seed)\n",
        "        random.shuffle(all_classes)\n",
        "        split_size = int(len(all_classes)/n_splits)\n",
        "        for j in range(n_splits):\n",
        "            if ((j+1)*split_size < len(all_classes)):\n",
        "                split_end = (j+1)*split_size\n",
        "            else:\n",
        "                split_end = None\n",
        "            subgroup = all_classes[j*split_size:split_end]\n",
        "            if dictionary_of == 'dataframes':\n",
        "                dictionary[j] = self.df[self.df['labels'].isin(subgroup)]\n",
        "            elif dictionary_of == 'indices':\n",
        "                dictionary[j] = list(self.df[self.df['labels'].isin(subgroup)].index)\n",
        "        return dictionary\n",
        "\n",
        "    def split_groups_in_train_validation(self, groups, ratio=0.5, seed=None):\n",
        "        groups_train_val = dict()\n",
        "        for k, subdf in groups.items():\n",
        "            train_indexes = []\n",
        "            val_indexes = []\n",
        "            split_labels = list(subdf['labels'].value_counts().index)\n",
        "            for l in split_labels:\n",
        "                indexes_to_sample = list(subdf[subdf['labels'] == l].index)\n",
        "                random.seed(seed)\n",
        "                train_samples = random.sample(indexes_to_sample, int(len(indexes_to_sample)*ratio))\n",
        "                train_indexes = train_indexes + train_samples\n",
        "                val_indexes = val_indexes + list(set(indexes_to_sample).difference(set(train_samples)))\n",
        "            groups_train_val[k] = {\n",
        "                'train': train_indexes,\n",
        "                'val': val_indexes\n",
        "            }\n",
        "        return groups_train_val\n",
        "    \n",
        "    def split_in_train_val_groups(self, n_splits=10, ratio=0.5, seed=None):\n",
        "        groups = self.split_classes(n_splits=n_splits, seed=seed)\n",
        "        return self.split_groups_in_train_validation(groups, ratio=ratio, seed=seed)\n",
        "\n",
        "    # given a tensors returns an image (used in exemplars)\n",
        "    #def tensorToImg(self, tensor):\n",
        "    #   return Variable(transform(Image.fromarray(img)), volatile=True)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_M1anVYkRnF"
      },
      "source": [
        "### Reverse Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j66mUOXekTlM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "__all__ = ['ReverseIndex']\n",
        "\n",
        "class ReverseIndex():\n",
        "\n",
        "    def __init__(self, dataset, splits, device='cuda'):\n",
        "\n",
        "        self.df = pd.DataFrame(columns=['group', 'labels'])\n",
        "\n",
        "        for k in splits.keys():\n",
        "            labels = list(dataset.df.loc[splits[k]['train'],'labels'].value_counts().index)\n",
        "            group = [k for i in range(len(labels))]\n",
        "            data = pd.DataFrame(np.array([group, labels]).T, columns=['group', 'labels'])\n",
        "            self.df = self.df.append(data, ignore_index=True)\n",
        "\n",
        "        self.df['nodes'] = self.df.index\n",
        "        self.device = device\n",
        "    \n",
        "    def _changeIndex(self, reverse_index, column):\n",
        "        reverse_index = reverse_index.set_index(column)\n",
        "        reverse_index[column] = reverse_index.index\n",
        "        return reverse_index\n",
        "\n",
        "    def getLabels(self, outputs):\n",
        "        outs = outputs.cpu().numpy()\n",
        "        reverse_index = self._changeIndex(self.df, 'nodes')\n",
        "        labels = reverse_index.loc[outs, 'labels']\n",
        "\n",
        "        labels = torch.tensor(list(labels))\n",
        "        return labels.to(self.device)\n",
        "\n",
        "    def getNodes(self, labels):\n",
        "        labels = labels.cpu().numpy()\n",
        "       \n",
        "        reverse_index = self._changeIndex(self.df, 'labels')\n",
        "\n",
        "        nodes = reverse_index.loc[labels, 'nodes']\n",
        "\n",
        "        nodes = torch.tensor(list(nodes))\n",
        "        return nodes.to(self.device)\n",
        "\n",
        "    def getGroups(self, distinct=True):\n",
        "        return self.df['group'].value_counts().index.sort_values()\n",
        "    \n",
        "    def getLabelsOfGroup(self, group):\n",
        "        return self.df.loc[self.df['group'] == group, 'labels']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVfOeOCuknB8"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QpAHzFkoW0"
      },
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Module\n",
        "\n",
        "# These are the default iCaRL hyper-parameters\n",
        "def getHyperparams():\n",
        "\tdictHyperparams = {\n",
        "\t\t\"LR\": 2,\n",
        "\t\t\"MOMENTUM\": 0.9,\n",
        "\t\t\"WEIGHT_DECAY\": 1e-5,\n",
        "\t\t\"NUM_EPOCHS\": 70,\n",
        "\t\t\"MILESTONES\": [49, 63],\n",
        "\t\t\"BATCH_SIZE\": 128,\n",
        "\t\t\"DEVICE\": 'cuda',\n",
        "\t\t\"GAMMA\": 0.2,\n",
        "\t\t\"SEED\": 66, #use 30, 42, 16\n",
        "\t\t\"LOG_FREQUENCY\": 10,\n",
        "\t\t\"NUM_CLASSES\": 100\n",
        "\t}\n",
        "\treturn dictHyperparams\n",
        "\n",
        "def getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize):\n",
        "\toptimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\tscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA, last_epoch=-1) \n",
        "\treturn optimizer, scheduler\n",
        "\n",
        "# the mean and the std have been found on the web as mean and std of cifar100\n",
        "# alternative (realistic): compute mean and std for the dataset\n",
        "def getTransformations():\n",
        "\t# Define transforms for training phase\n",
        "\ttrain_transform = transforms.Compose([transforms.RandomHorizontalFlip(), # Randomly flip the image with probability of 0.5\n",
        "\t                                      transforms.Pad(4), # Add padding\n",
        "\t                                      transforms.RandomCrop(32),# Crops a random squares of the image\n",
        "\t                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\t# Define transforms for the evaluation phase\n",
        "\teval_transform = transforms.Compose([\n",
        "\t                                      transforms.ToTensor(),\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\treturn train_transform, eval_transform\n",
        "\n",
        "# BCEWithLogits = Sigmoid + BCE, is the loss used in iCaRL\n",
        "def getLossCriterion():\n",
        "\tcriterion = nn.BCEWithLogitsLoss(reduction = 'mean') # for classification: Cross Entropy\n",
        "\treturn criterion\n",
        "\n",
        "def computeLoss(criterion, outputs, labels):\n",
        "\treturn criterion(outputs, labels)\n",
        "\n",
        "# support BCE\n",
        "def _one_hot_encode(labels, n_classes, reverse_index, dtype=None, device='cuda'):\n",
        "\tbatch_size = len(labels)\n",
        "\tenconded = torch.zeros(batch_size, n_classes, dtype=dtype, device=device)\n",
        "\tlabels=map_to_outputs(labels, reverse_index)\n",
        "\tfor i, l in enumerate(labels):\n",
        "\t  enconded[i, l] = 1\n",
        "\treturn enconded\n",
        "\n",
        "def map_to_outputs(labels, reverse_index):\n",
        "\tif reverse_index is None:\n",
        "\t  return labels\n",
        "\tif type(labels) == int:\n",
        "\t  return int(reverse_index.getNodes(torch.tensor([labels])))\n",
        "\telif type(labels) == torch.Tensor:\n",
        "\t\treturn reverse_index.getNodes(labels)\n",
        "\n",
        "\n",
        "def plotAccuracyTrend(method, data_plot_line, seed):\n",
        "\tplt.figure(figsize=(20,7))\n",
        "\taccuracyDF=pd.DataFrame(data_plot_line, columns = ['Classes','Accuracy'])\n",
        "\tax = sns.lineplot(x=\"Classes\", y=\"Accuracy\",data=accuracyDF, marker = 'o')\n",
        "\tax.minorticks_on()\n",
        "\tax.set_xticks(np.arange(10,110,10))\n",
        "\tax.set_xlim(xmin=9, xmax=101)\n",
        "\tax.set_ylim(ymin=0, ymax=1)\n",
        "\tplt.legend(['Accuracy {}'.format(method)])\n",
        "\tax.grid(axis='y')\n",
        "\tplt.title(\"Accuracies against seen classes {} - seed: {}\".format(method, seed))\n",
        "\t\n",
        "\tfilename = \"acc_{}_{}.jpg\".format(method, seed) # ex. acc_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "def plotConfusionMatrix(method, confusionMatrixData, seed):\n",
        "\tfig,ax=plt.subplots(figsize=(10,10))\n",
        "\tsns.heatmap(confusionMatrixData,cmap='terrain',ax=ax)\n",
        "\tplt.ylabel('True label')\n",
        "\tplt.xlabel('Predicted label')\n",
        "\tplt.title(\"Confusion Matrix {} - seed: {}\".format(method, seed))\n",
        "\n",
        "\tfilename = \"cm_{}_{}.jpg\".format(method, seed) # ex. cm_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "# Write down the metrics (accuracy trand and confusion matrix)\n",
        "# this method is a shortcut when perfoming multiple tests with different splits (random_seed)\n",
        "# and allow us to plot on the same graph the data from multiple models (accuracy)\n",
        "def writeMetrics(method, seed, accuracies, confusionMatrixData):\n",
        "  data = {}\n",
        "  data['accuracies'] = []\n",
        "  data['cm'] = [] #cm line\n",
        "  i = 0\n",
        "  for classes_seen in range(10, 110, 10): #x axis on the plot\n",
        "    data['accuracies'].append({classes_seen : accuracies[i]}) \n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  for class_num in range(0,len(confusionMatrixData)): #rows of the cm\n",
        "    data['cm'].append({class_num : confusionMatrixData[i].tolist()}) \n",
        "    i += 1\n",
        "  \n",
        "  # dump to file\n",
        "  aus = method + '_' + str(seed)\n",
        "  filename = 'data_{}.json'.format(aus)\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "\n",
        "# Functions\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getResNet32():\n",
        "    net = CifarResNet()\n",
        "    # net.fc = nn.Linear(net.fc.in_features, output_size) # embedded in the class\n",
        "\n",
        "    criterion = getLossCriterion()\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return net, criterion, optimizer, scheduler\n",
        "\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getNet():\n",
        "    return getResNet32()\n",
        "\n",
        "def getSchedulerOptimizer(net):\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return optimizer, scheduler"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfJj1z2LSxp0"
      },
      "source": [
        "### DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysghtAWOPYZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0410d64-2e18-43ef-a874-e9e386faf137"
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "if not os.path.isdir('./{}'.format(\"$DATA_DIR/cifar-100-python\")):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-08 08:13:47--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  40.9MB/s    in 4.4s    \n",
            "\n",
            "2021-06-08 08:13:52 (36.6 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n",
            "mv: cannot move 'cifar-100-python' to 'DATA/cifar-100-python/cifar-100-python': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY10HfpTS1lq"
      },
      "source": [
        "### HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-pqSNg4_Ris"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100\n",
        "BATCH_SIZE = 128 \n",
        "LR = 0.1         \n",
        "MOMENTUM = 0.9     \n",
        "WEIGHT_DECAY = 1e-05\n",
        "NUM_EPOCHS = 70\n",
        "GAMMA = 0.2\n",
        "LOG_FREQUENCY = 10\n",
        "MILESTONES = [49,63]\n",
        "RANDOM_SEED = 66"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW7WxV_QS4re"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_jHycn_1kk"
      },
      "source": [
        "train_transform, eval_transform = getTransformations()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPrwhTcIqUbA"
      },
      "source": [
        "### train & test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJKwvGljJj2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a890e0d7-9c60-43b6-f370-d58a01c196c0"
      },
      "source": [
        "# Import dataset and apply transformations \n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# Check datasets length \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnooWwcyS_YN"
      },
      "source": [
        "### SPLIT DATA IN CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckn3H69iJj2X"
      },
      "source": [
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpcJvhxhJOLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece158c6-b229-483e-b11f-dd01d192c4b0"
      },
      "source": [
        "# TRAIN / VAL split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "print(outputs_labels_mapping)\n",
        "\n",
        "# TEST split\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.ReverseIndex object at 0x7f6fb56d2050>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MSItI0QVpn"
      },
      "source": [
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10):\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpWv5ZkhxTPJ"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4NQa-wNxWL1"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pisadUTPxcRP"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            # if (group_id > 1):\n",
        "            #   old_features = old_net.forward(images)\n",
        "            #   old_outputs = old_net.predict(old_features)\n",
        "            #   labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX_61nSuxh3v"
      },
      "source": [
        "### sequential learning fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM0GysHZxk1B"
      },
      "source": [
        "### Fine tuning\n",
        "def sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    old_net = None\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    #confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2X2IVZ-xrVT"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DGpd8lslxtUW",
        "outputId": "2a7a5171-8688-4b1f-fe52-f88f4b532a6a"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.7173749208450317\n",
            "Train step - Step 10, Loss 0.31387636065483093\n",
            "Train step - Step 20, Loss 0.2874338626861572\n",
            "Train step - Step 30, Loss 0.2654590606689453\n",
            "Train epoch - Accuracy: 0.318989898989899 Loss: 0.34560690457772725 Corrects: 1579\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.26789265871047974\n",
            "Train step - Step 50, Loss 0.258944034576416\n",
            "Train step - Step 60, Loss 0.2350231260061264\n",
            "Train step - Step 70, Loss 0.23865528404712677\n",
            "Train epoch - Accuracy: 0.44363636363636366 Loss: 0.24301170718790305 Corrects: 2196\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.23784291744232178\n",
            "Train step - Step 90, Loss 0.22167158126831055\n",
            "Train step - Step 100, Loss 0.22836807370185852\n",
            "Train step - Step 110, Loss 0.21694694459438324\n",
            "Train epoch - Accuracy: 0.48707070707070704 Loss: 0.22610946089330344 Corrects: 2411\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.21196965873241425\n",
            "Train step - Step 130, Loss 0.20575585961341858\n",
            "Train step - Step 140, Loss 0.22211872041225433\n",
            "Train step - Step 150, Loss 0.20619133114814758\n",
            "Train epoch - Accuracy: 0.5016161616161616 Loss: 0.21912972303954037 Corrects: 2483\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.2055768072605133\n",
            "Train step - Step 170, Loss 0.20553207397460938\n",
            "Train step - Step 180, Loss 0.22383907437324524\n",
            "Train step - Step 190, Loss 0.19269728660583496\n",
            "Train epoch - Accuracy: 0.5418181818181819 Loss: 0.20608459960932685 Corrects: 2682\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.19962547719478607\n",
            "Train step - Step 210, Loss 0.1956397444009781\n",
            "Train step - Step 220, Loss 0.23317204415798187\n",
            "Train step - Step 230, Loss 0.1870502084493637\n",
            "Train epoch - Accuracy: 0.5523232323232323 Loss: 0.19966931800047558 Corrects: 2734\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.19728612899780273\n",
            "Train step - Step 250, Loss 0.17967957258224487\n",
            "Train step - Step 260, Loss 0.1929226964712143\n",
            "Train step - Step 270, Loss 0.19188766181468964\n",
            "Train epoch - Accuracy: 0.5834343434343434 Loss: 0.19277908541939476 Corrects: 2888\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1998911201953888\n",
            "Train step - Step 290, Loss 0.18329913914203644\n",
            "Train step - Step 300, Loss 0.16586630046367645\n",
            "Train step - Step 310, Loss 0.17675089836120605\n",
            "Train epoch - Accuracy: 0.598989898989899 Loss: 0.1864176897809963 Corrects: 2965\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.17738604545593262\n",
            "Train step - Step 330, Loss 0.18055176734924316\n",
            "Train step - Step 340, Loss 0.17641305923461914\n",
            "Train step - Step 350, Loss 0.14969894289970398\n",
            "Train epoch - Accuracy: 0.6155555555555555 Loss: 0.17903075204955207 Corrects: 3047\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.1671830266714096\n",
            "Train step - Step 370, Loss 0.1687958985567093\n",
            "Train step - Step 380, Loss 0.159011110663414\n",
            "Train epoch - Accuracy: 0.6309090909090909 Loss: 0.1738420210462628 Corrects: 3123\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.15419088304042816\n",
            "Train step - Step 400, Loss 0.14775967597961426\n",
            "Train step - Step 410, Loss 0.16979841887950897\n",
            "Train step - Step 420, Loss 0.1525963395833969\n",
            "Train epoch - Accuracy: 0.6539393939393939 Loss: 0.1639658623993999 Corrects: 3237\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.19196446239948273\n",
            "Train step - Step 440, Loss 0.1466265171766281\n",
            "Train step - Step 450, Loss 0.17056730389595032\n",
            "Train step - Step 460, Loss 0.18163080513477325\n",
            "Train epoch - Accuracy: 0.6608080808080808 Loss: 0.16202940798167026 Corrects: 3271\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.176211878657341\n",
            "Train step - Step 480, Loss 0.1588747799396515\n",
            "Train step - Step 490, Loss 0.1342204362154007\n",
            "Train step - Step 500, Loss 0.14058950543403625\n",
            "Train epoch - Accuracy: 0.6767676767676768 Loss: 0.15383696049752862 Corrects: 3350\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.1555892676115036\n",
            "Train step - Step 520, Loss 0.13571906089782715\n",
            "Train step - Step 530, Loss 0.1515876054763794\n",
            "Train step - Step 540, Loss 0.1359780877828598\n",
            "Train epoch - Accuracy: 0.702020202020202 Loss: 0.14594818502363532 Corrects: 3475\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.15486030280590057\n",
            "Train step - Step 560, Loss 0.13412602245807648\n",
            "Train step - Step 570, Loss 0.15869688987731934\n",
            "Train step - Step 580, Loss 0.13754160702228546\n",
            "Train epoch - Accuracy: 0.7034343434343434 Loss: 0.14330182038172326 Corrects: 3482\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.13797789812088013\n",
            "Train step - Step 600, Loss 0.14471013844013214\n",
            "Train step - Step 610, Loss 0.1659015417098999\n",
            "Train step - Step 620, Loss 0.13035106658935547\n",
            "Train epoch - Accuracy: 0.712929292929293 Loss: 0.1402043603164981 Corrects: 3529\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.10688119381666183\n",
            "Train step - Step 640, Loss 0.10551120340824127\n",
            "Train step - Step 650, Loss 0.11231940239667892\n",
            "Train step - Step 660, Loss 0.12159333378076553\n",
            "Train epoch - Accuracy: 0.7434343434343434 Loss: 0.1296956703397963 Corrects: 3680\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.13172531127929688\n",
            "Train step - Step 680, Loss 0.142404243350029\n",
            "Train step - Step 690, Loss 0.12234494835138321\n",
            "Train step - Step 700, Loss 0.14414963126182556\n",
            "Train epoch - Accuracy: 0.756969696969697 Loss: 0.12469814313180519 Corrects: 3747\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.1333446353673935\n",
            "Train step - Step 720, Loss 0.15927483141422272\n",
            "Train step - Step 730, Loss 0.11001887172460556\n",
            "Train step - Step 740, Loss 0.12100914865732193\n",
            "Train epoch - Accuracy: 0.7450505050505051 Loss: 0.12714890332836093 Corrects: 3688\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.11506552994251251\n",
            "Train step - Step 760, Loss 0.10524898767471313\n",
            "Train step - Step 770, Loss 0.0984029471874237\n",
            "Train epoch - Accuracy: 0.7668686868686869 Loss: 0.11733168877435453 Corrects: 3796\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.12441835552453995\n",
            "Train step - Step 790, Loss 0.11353017389774323\n",
            "Train step - Step 800, Loss 0.13449843227863312\n",
            "Train step - Step 810, Loss 0.12670955061912537\n",
            "Train epoch - Accuracy: 0.7739393939393939 Loss: 0.11266988289175611 Corrects: 3831\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.10420570522546768\n",
            "Train step - Step 830, Loss 0.10050588846206665\n",
            "Train step - Step 840, Loss 0.09052333980798721\n",
            "Train step - Step 850, Loss 0.07840898633003235\n",
            "Train epoch - Accuracy: 0.7808080808080808 Loss: 0.11037116950509523 Corrects: 3865\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.13141851127147675\n",
            "Train step - Step 870, Loss 0.11617157608270645\n",
            "Train step - Step 880, Loss 0.11001942306756973\n",
            "Train step - Step 890, Loss 0.08741968870162964\n",
            "Train epoch - Accuracy: 0.7844444444444445 Loss: 0.10794938291564132 Corrects: 3883\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.11431687325239182\n",
            "Train step - Step 910, Loss 0.0919078066945076\n",
            "Train step - Step 920, Loss 0.1131000742316246\n",
            "Train step - Step 930, Loss 0.12487182766199112\n",
            "Train epoch - Accuracy: 0.7935353535353535 Loss: 0.10410453104310566 Corrects: 3928\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.0851389467716217\n",
            "Train step - Step 950, Loss 0.10327544063329697\n",
            "Train step - Step 960, Loss 0.09313042461872101\n",
            "Train step - Step 970, Loss 0.12224413454532623\n",
            "Train epoch - Accuracy: 0.8014141414141415 Loss: 0.10208582940125706 Corrects: 3967\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.10216508060693741\n",
            "Train step - Step 990, Loss 0.10625173896551132\n",
            "Train step - Step 1000, Loss 0.12439626455307007\n",
            "Train step - Step 1010, Loss 0.09492490440607071\n",
            "Train epoch - Accuracy: 0.805050505050505 Loss: 0.10121105768764863 Corrects: 3985\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.1075296625494957\n",
            "Train step - Step 1030, Loss 0.08558668196201324\n",
            "Train step - Step 1040, Loss 0.10448060184717178\n",
            "Train step - Step 1050, Loss 0.0987623855471611\n",
            "Train epoch - Accuracy: 0.8115151515151515 Loss: 0.09598487866647316 Corrects: 4017\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.11706822365522385\n",
            "Train step - Step 1070, Loss 0.08957835286855698\n",
            "Train step - Step 1080, Loss 0.1004997119307518\n",
            "Train step - Step 1090, Loss 0.10269360989332199\n",
            "Train epoch - Accuracy: 0.8151515151515152 Loss: 0.093790872918837 Corrects: 4035\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10780717432498932\n",
            "Train step - Step 1110, Loss 0.0829559937119484\n",
            "Train step - Step 1120, Loss 0.06946247071027756\n",
            "Train step - Step 1130, Loss 0.08679612725973129\n",
            "Train epoch - Accuracy: 0.8232323232323232 Loss: 0.09224864899811118 Corrects: 4075\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.0877915769815445\n",
            "Train step - Step 1150, Loss 0.07081689685583115\n",
            "Train step - Step 1160, Loss 0.09684779495000839\n",
            "Train epoch - Accuracy: 0.8305050505050505 Loss: 0.08737428996298048 Corrects: 4111\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.08674424141645432\n",
            "Train step - Step 1180, Loss 0.08422601222991943\n",
            "Train step - Step 1190, Loss 0.09191498905420303\n",
            "Train step - Step 1200, Loss 0.09191594272851944\n",
            "Train epoch - Accuracy: 0.834949494949495 Loss: 0.08547621402174535 Corrects: 4133\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.07304417341947556\n",
            "Train step - Step 1220, Loss 0.09637052565813065\n",
            "Train step - Step 1230, Loss 0.08423800766468048\n",
            "Train step - Step 1240, Loss 0.09754743427038193\n",
            "Train epoch - Accuracy: 0.8418181818181818 Loss: 0.0811596822106477 Corrects: 4167\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.08752617239952087\n",
            "Train step - Step 1260, Loss 0.08612791448831558\n",
            "Train step - Step 1270, Loss 0.09345199167728424\n",
            "Train step - Step 1280, Loss 0.06988094002008438\n",
            "Train epoch - Accuracy: 0.8432323232323232 Loss: 0.08195995536717501 Corrects: 4174\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.063926100730896\n",
            "Train step - Step 1300, Loss 0.09519979357719421\n",
            "Train step - Step 1310, Loss 0.0794561430811882\n",
            "Train step - Step 1320, Loss 0.09330239146947861\n",
            "Train epoch - Accuracy: 0.8434343434343434 Loss: 0.08210123597973525 Corrects: 4175\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.06706669181585312\n",
            "Train step - Step 1340, Loss 0.07936250418424606\n",
            "Train step - Step 1350, Loss 0.08014490455389023\n",
            "Train step - Step 1360, Loss 0.08957503736019135\n",
            "Train epoch - Accuracy: 0.8488888888888889 Loss: 0.07934777613541093 Corrects: 4202\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.08020991086959839\n",
            "Train step - Step 1380, Loss 0.09724964946508408\n",
            "Train step - Step 1390, Loss 0.06381719559431076\n",
            "Train step - Step 1400, Loss 0.06528539955615997\n",
            "Train epoch - Accuracy: 0.8549494949494949 Loss: 0.07585879642855037 Corrects: 4232\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.07746966183185577\n",
            "Train step - Step 1420, Loss 0.08617367595434189\n",
            "Train step - Step 1430, Loss 0.06970410794019699\n",
            "Train step - Step 1440, Loss 0.08160582929849625\n",
            "Train epoch - Accuracy: 0.8527272727272728 Loss: 0.07876499318715298 Corrects: 4221\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.06916437298059464\n",
            "Train step - Step 1460, Loss 0.06720501184463501\n",
            "Train step - Step 1470, Loss 0.06595558673143387\n",
            "Train step - Step 1480, Loss 0.0707656592130661\n",
            "Train epoch - Accuracy: 0.8654545454545455 Loss: 0.07132082007449082 Corrects: 4284\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.05659304931759834\n",
            "Train step - Step 1500, Loss 0.06974741071462631\n",
            "Train step - Step 1510, Loss 0.0784800574183464\n",
            "Train step - Step 1520, Loss 0.06901217252016068\n",
            "Train epoch - Accuracy: 0.8672727272727273 Loss: 0.07315080337753199 Corrects: 4293\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.07148046046495438\n",
            "Train step - Step 1540, Loss 0.08015324175357819\n",
            "Train step - Step 1550, Loss 0.07519340515136719\n",
            "Train epoch - Accuracy: 0.8682828282828283 Loss: 0.06976768631826748 Corrects: 4298\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.061872996389865875\n",
            "Train step - Step 1570, Loss 0.08952672779560089\n",
            "Train step - Step 1580, Loss 0.0770217701792717\n",
            "Train step - Step 1590, Loss 0.06781128793954849\n",
            "Train epoch - Accuracy: 0.8719191919191919 Loss: 0.06902587638659911 Corrects: 4316\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.0734851062297821\n",
            "Train step - Step 1610, Loss 0.055662430822849274\n",
            "Train step - Step 1620, Loss 0.08112241327762604\n",
            "Train step - Step 1630, Loss 0.07834818959236145\n",
            "Train epoch - Accuracy: 0.8739393939393939 Loss: 0.06590250587222551 Corrects: 4326\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.05307411774992943\n",
            "Train step - Step 1650, Loss 0.04475534334778786\n",
            "Train step - Step 1660, Loss 0.06450628489255905\n",
            "Train step - Step 1670, Loss 0.06469791382551193\n",
            "Train epoch - Accuracy: 0.8838383838383839 Loss: 0.06405000432874217 Corrects: 4375\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.055307772010564804\n",
            "Train step - Step 1690, Loss 0.06018352508544922\n",
            "Train step - Step 1700, Loss 0.05738571286201477\n",
            "Train step - Step 1710, Loss 0.07564640045166016\n",
            "Train epoch - Accuracy: 0.883030303030303 Loss: 0.06291912011124871 Corrects: 4371\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.0579797700047493\n",
            "Train step - Step 1730, Loss 0.05599530413746834\n",
            "Train step - Step 1740, Loss 0.07791357487440109\n",
            "Train step - Step 1750, Loss 0.07570113986730576\n",
            "Train epoch - Accuracy: 0.8860606060606061 Loss: 0.06122048602411241 Corrects: 4386\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.046970587223768234\n",
            "Train step - Step 1770, Loss 0.05448814854025841\n",
            "Train step - Step 1780, Loss 0.0570836178958416\n",
            "Train step - Step 1790, Loss 0.052846431732177734\n",
            "Train epoch - Accuracy: 0.8921212121212121 Loss: 0.05948092536342264 Corrects: 4416\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.0464363619685173\n",
            "Train step - Step 1810, Loss 0.056273628026247025\n",
            "Train step - Step 1820, Loss 0.0658160001039505\n",
            "Train step - Step 1830, Loss 0.0690997764468193\n",
            "Train epoch - Accuracy: 0.8955555555555555 Loss: 0.05688923456151076 Corrects: 4433\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.059149112552404404\n",
            "Train step - Step 1850, Loss 0.05253974348306656\n",
            "Train step - Step 1860, Loss 0.07070215791463852\n",
            "Train step - Step 1870, Loss 0.05269395932555199\n",
            "Train epoch - Accuracy: 0.8927272727272727 Loss: 0.05812157820571553 Corrects: 4419\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.04611649736762047\n",
            "Train step - Step 1890, Loss 0.06521129608154297\n",
            "Train step - Step 1900, Loss 0.0597667470574379\n",
            "Train step - Step 1910, Loss 0.07057886570692062\n",
            "Train epoch - Accuracy: 0.9002020202020202 Loss: 0.05608734115506663 Corrects: 4456\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.042010191828012466\n",
            "Train step - Step 1930, Loss 0.043184537440538406\n",
            "Train step - Step 1940, Loss 0.0397011898458004\n",
            "Train epoch - Accuracy: 0.9242424242424242 Loss: 0.04325935698518849 Corrects: 4575\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.039822064340114594\n",
            "Train step - Step 1960, Loss 0.036546554416418076\n",
            "Train step - Step 1970, Loss 0.04635452851653099\n",
            "Train step - Step 1980, Loss 0.033635109663009644\n",
            "Train epoch - Accuracy: 0.9432323232323232 Loss: 0.037418713830035144 Corrects: 4669\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.03440694510936737\n",
            "Train step - Step 2000, Loss 0.04546314477920532\n",
            "Train step - Step 2010, Loss 0.03715860843658447\n",
            "Train step - Step 2020, Loss 0.050867773592472076\n",
            "Train epoch - Accuracy: 0.9412121212121212 Loss: 0.03600090804876703 Corrects: 4659\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.02928098477423191\n",
            "Train step - Step 2040, Loss 0.03901729732751846\n",
            "Train step - Step 2050, Loss 0.037106212228536606\n",
            "Train step - Step 2060, Loss 0.03669671341776848\n",
            "Train epoch - Accuracy: 0.9452525252525252 Loss: 0.03400145272564406 Corrects: 4679\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.0314776748418808\n",
            "Train step - Step 2080, Loss 0.031674575060606\n",
            "Train step - Step 2090, Loss 0.018039535731077194\n",
            "Train step - Step 2100, Loss 0.0348820686340332\n",
            "Train epoch - Accuracy: 0.9468686868686869 Loss: 0.033380321872354755 Corrects: 4687\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.02144073322415352\n",
            "Train step - Step 2120, Loss 0.029142146930098534\n",
            "Train step - Step 2130, Loss 0.03788841888308525\n",
            "Train step - Step 2140, Loss 0.032097745686769485\n",
            "Train epoch - Accuracy: 0.9496969696969697 Loss: 0.03260654991022264 Corrects: 4701\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.027982238680124283\n",
            "Train step - Step 2160, Loss 0.020777106285095215\n",
            "Train step - Step 2170, Loss 0.030742371454834938\n",
            "Train step - Step 2180, Loss 0.0357779823243618\n",
            "Train epoch - Accuracy: 0.9543434343434344 Loss: 0.03025592699345916 Corrects: 4724\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.031895961612463\n",
            "Train step - Step 2200, Loss 0.025660132989287376\n",
            "Train step - Step 2210, Loss 0.038197994232177734\n",
            "Train step - Step 2220, Loss 0.025359375402331352\n",
            "Train epoch - Accuracy: 0.9525252525252526 Loss: 0.03145596713730783 Corrects: 4715\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.03523494675755501\n",
            "Train step - Step 2240, Loss 0.02871292270720005\n",
            "Train step - Step 2250, Loss 0.028518855571746826\n",
            "Train step - Step 2260, Loss 0.028394445776939392\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.031200148419599342 Corrects: 4703\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.026044586673378944\n",
            "Train step - Step 2280, Loss 0.03078872337937355\n",
            "Train step - Step 2290, Loss 0.03193800523877144\n",
            "Train step - Step 2300, Loss 0.05611391365528107\n",
            "Train epoch - Accuracy: 0.9555555555555556 Loss: 0.030103796654277377 Corrects: 4730\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.02573627233505249\n",
            "Train step - Step 2320, Loss 0.026890838518738747\n",
            "Train step - Step 2330, Loss 0.03133704885840416\n",
            "Train epoch - Accuracy: 0.953939393939394 Loss: 0.0305143504032884 Corrects: 4722\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.029058242216706276\n",
            "Train step - Step 2350, Loss 0.02426040545105934\n",
            "Train step - Step 2360, Loss 0.03160322457551956\n",
            "Train step - Step 2370, Loss 0.04511835798621178\n",
            "Train epoch - Accuracy: 0.9563636363636364 Loss: 0.029337084354324774 Corrects: 4734\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.031107379123568535\n",
            "Train step - Step 2390, Loss 0.029924381524324417\n",
            "Train step - Step 2400, Loss 0.024590641260147095\n",
            "Train step - Step 2410, Loss 0.020830800756812096\n",
            "Train epoch - Accuracy: 0.961010101010101 Loss: 0.02768125159346094 Corrects: 4757\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.02306819148361683\n",
            "Train step - Step 2430, Loss 0.05020679160952568\n",
            "Train step - Step 2440, Loss 0.032019324600696564\n",
            "Train step - Step 2450, Loss 0.035698115825653076\n",
            "Train epoch - Accuracy: 0.9587878787878787 Loss: 0.028257839812172784 Corrects: 4746\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.03360297158360481\n",
            "Train step - Step 2470, Loss 0.02808537892997265\n",
            "Train step - Step 2480, Loss 0.01882505789399147\n",
            "Train step - Step 2490, Loss 0.01958288811147213\n",
            "Train epoch - Accuracy: 0.96 Loss: 0.026811572033347504 Corrects: 4752\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.024240998551249504\n",
            "Train step - Step 2510, Loss 0.015550089068710804\n",
            "Train step - Step 2520, Loss 0.01995779201388359\n",
            "Train step - Step 2530, Loss 0.02513287402689457\n",
            "Train epoch - Accuracy: 0.962020202020202 Loss: 0.025361865840174934 Corrects: 4762\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.021166345104575157\n",
            "Train step - Step 2550, Loss 0.018641367554664612\n",
            "Train step - Step 2560, Loss 0.021410910412669182\n",
            "Train step - Step 2570, Loss 0.03057834319770336\n",
            "Train epoch - Accuracy: 0.9646464646464646 Loss: 0.023534335130167127 Corrects: 4775\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.029833734035491943\n",
            "Train step - Step 2590, Loss 0.018363453447818756\n",
            "Train step - Step 2600, Loss 0.026480138301849365\n",
            "Train step - Step 2610, Loss 0.028178563341498375\n",
            "Train epoch - Accuracy: 0.9660606060606061 Loss: 0.024363452725187695 Corrects: 4782\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.0342327281832695\n",
            "Train step - Step 2630, Loss 0.02055048942565918\n",
            "Train step - Step 2640, Loss 0.020426509901881218\n",
            "Train step - Step 2650, Loss 0.021581510081887245\n",
            "Train epoch - Accuracy: 0.9648484848484848 Loss: 0.025252315486320343 Corrects: 4776\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.03008689358830452\n",
            "Train step - Step 2670, Loss 0.0230241846293211\n",
            "Train step - Step 2680, Loss 0.019735518842935562\n",
            "Train step - Step 2690, Loss 0.01372586376965046\n",
            "Train epoch - Accuracy: 0.9644444444444444 Loss: 0.024579011685769966 Corrects: 4774\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.02005911059677601\n",
            "Train step - Step 2710, Loss 0.023675382137298584\n",
            "Train step - Step 2720, Loss 0.02118927799165249\n",
            "Train epoch - Accuracy: 0.9634343434343434 Loss: 0.024567253339772274 Corrects: 4769\n",
            "Training finished in 203.4930591583252 seconds\n",
            "EVALUATION:  0.78 0.12797264754772186\n",
            "TEST GROUP:  0.836\n",
            "TEST ALL:  0.836\n",
            "GROUP:  2\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.5198947787284851\n",
            "Train step - Step 10, Loss 0.18119554221630096\n",
            "Train step - Step 20, Loss 0.1347174048423767\n",
            "Train step - Step 30, Loss 0.10982789844274521\n",
            "Train epoch - Accuracy: 0.36808080808080806 Loss: 0.17154767145111102 Corrects: 1822\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.09604242444038391\n",
            "Train step - Step 50, Loss 0.09236812591552734\n",
            "Train step - Step 60, Loss 0.08287163078784943\n",
            "Train step - Step 70, Loss 0.08627394586801529\n",
            "Train epoch - Accuracy: 0.6482828282828282 Loss: 0.08684955637563359 Corrects: 3209\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.08100271224975586\n",
            "Train step - Step 90, Loss 0.07603686302900314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-754cea9f07ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequentialLearningFineTuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_subsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-31e6b3e7dcf7>\u001b[0m in \u001b[0;36msequentialLearningFineTuning\u001b[0;34m(train_subsets, val_subsets, test_subsets)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSchedulerOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset learning rate and step_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes_seen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# Validate on current group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-20cff53f0192>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Bring images and labels to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aakVopSdyeGr"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ0VbtGQygRH"
      },
      "source": [
        "method = \"finetuning\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ-J8aC_xx-c"
      },
      "source": [
        "## LWF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZBNivOix1FB"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdfIllqBx59v"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkzFiDVDyAm_"
      },
      "source": [
        "### sequential learning LWF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf7REFkIyEiQ"
      },
      "source": [
        "### LWF\n",
        "def sequentialLearningLWF(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZTwTS_4TcES"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkrMQy2TuUAb"
      },
      "source": [
        "# train\n",
        "# net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningLWF(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2qUxVyMTfB4"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_4xLfwcpDz"
      },
      "source": [
        "method = \"Learning Without Forgetting\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGHBWaLNXGeI"
      },
      "source": [
        "\"\"\"num_classes_seen = 100\n",
        "dif_accuracies=printAccuracyDifference(net,old_accuracies, num_classes_seen)\n",
        "dif_accuracies\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjGRIqDtNQP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nX10znUi6qd"
      },
      "source": [
        "## iCaRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKBdrJ8Csy2a"
      },
      "source": [
        "### classify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAPTOU9bszTY"
      },
      "source": [
        "def classify(batch_img, net, exemplar_sets):\n",
        "  \"\"\" classify images by nearest mean-of-exemplars \"\"\" \n",
        "  \"\"\" \n",
        "  class1: list_of_indices for images that represent class1\n",
        "  class2: list_of_indices for images that represent class2\n",
        "  ...\n",
        "  class100: list_of_indices for images that represent class 100\n",
        "  with Subset I retrieve the images from the train_dataset\n",
        "  iterate over it to calculate mean for each class\n",
        "  \"\"\"\n",
        "  for k, exemplars_indices in exemplar_sets.items():\n",
        "    features = []\n",
        "    classes_mean = []\n",
        "    class_images_set = Subset(train_dataset, exemplars_indices)\n",
        "    class_images = DataLoader(class_images_set, batch_size=BATCH_SIZE, num_workers=2)\n",
        "    for _, images, labels in class_images:\n",
        "      # for each class (paper from y=1...t) calculate features and then mean\n",
        "      feature = net.forward(images)\n",
        "      features.append(feature)\n",
        "    features_s = torch.cat(features)\n",
        "    class_mean = features_s.mean(0)\n",
        "    classes_mean.append(class_mean)\n",
        "    means_exemplars = torch.cat(classes_mean, dim=0)\n",
        "    means_exemplars = torch.stack([means_exemplars] * BATCH_SIZE)\n",
        "    means_exemplars = means_exemplars.transpose(1,2)\n",
        "  feature_images_to_classify = net.forward(batch_img)\n",
        "  # sono da normalizzare?\n",
        "  feature_images_to_classify = feature_images_to_classify.unsqueeze(2)\n",
        "  feature_images_to_classify = feature_images_to_classify.expand_as(means_exemplars)\n",
        "  preds = torch.argmin((feature - means_exemplars).pow(2).sum(1), dim=1)\n",
        "  return preds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5HRagcujHgM"
      },
      "source": [
        "### construct exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blwBqG6BjNsM"
      },
      "source": [
        "import sys\n",
        "\n",
        "def constructExemplarSet(net, Xclass, m):\n",
        "#Xclass contiene immagini e label della classe X\n",
        "  exemplars_set = []\n",
        "  feature_exemplars = []\n",
        "  indexes = []\n",
        "  features = [] \n",
        "  class_images = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    XtrainLoader = torch.utils.data.DataLoader(Xclass, shuffle = True, batch_size=1, num_workers=2)\n",
        "    for _, image, labels in XtrainLoader:\n",
        "      image = image.to(DEVICE)\n",
        "      class_images.append(image)\n",
        "      # per ogni immagine della classe x, prendiamo le rispettive feature e le uniamo nel vettore features, che contiene tutte quelle delle immagini della classe x\n",
        "      feature = net.forward(image)\n",
        "      #feature = net.predict(featuretmp)\n",
        "      feature = feature/np.linalg.norm(feature.cpu())\n",
        "      features.append(feature)\n",
        "\n",
        "    features = torch.cat(features, dim=0) #cat solve the problem of inequal size of tensors \n",
        "    current_class_mean = features.mean(0) # mu = media delle features delle immagini della classe\n",
        "\n",
        "    for k in range(1, m+1):\n",
        "      min = 100000\n",
        "      sum = 0\n",
        "      for j in range(k-1):\n",
        "        sum += feature_exemplars[j]\n",
        "      for x in range(len(Xclass)): \n",
        "        if (x not in indexes):\n",
        "          phiX = features[x]\n",
        "          val = current_class_mean - ((phiX + sum)/k)\n",
        "          val = np.linalg.norm(val.cpu().numpy()) ## NORMA \n",
        "          if (val < min):\n",
        "            min = val\n",
        "            feature_min = phiX\n",
        "            index_min = x\n",
        "      feature_exemplars.append(feature_min)\n",
        "      print(Xclass[index_min][0])\n",
        "      exemplars_set.append(Xclass[index_min][0])\n",
        "\n",
        "    print(\"lunghezza exemplar set: \", len(exemplars_set))\n",
        "    return exemplars_set"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy_HVL-tjZcb"
      },
      "source": [
        "### reduce exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7kuQ5DnjcCP"
      },
      "source": [
        "# DA CHIAMARE PER OGNI CLASSE k: riduce il numero di exemplars all'interno dell'exemplar_set della classe k \n",
        "# EXEMPLAR SET è una lista di indici che rappresentano la posizione dell'immagine selezionata per l'exemplar della classe corrente nel dataset di partenza\n",
        "def reduceExemplarSet(m, exemplars_set):\n",
        "  exemplars_new = []\n",
        "  for i in range(m):\n",
        "    exemplars_new.append(exemplars_set[i])\n",
        "\n",
        "  return exemplars_new"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMMlEpLsjfaj"
      },
      "source": [
        "### update representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRdA9RtDjhak"
      },
      "source": [
        "def updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot, old_net):\n",
        "  #exemplars_set_tot contiene tutti gli exemplars set ottenuti fino ad ora\n",
        "  #train_iter contiene tutti i dati (immagini + labels) delle classi nuove (s, .., t)\n",
        "  exemplars_subset = []\n",
        "  exemplars_indices = []\n",
        "  total_exemplars = []\n",
        "  labels_tot = []\n",
        "\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "    if (exemplar_set_class_k != []):\n",
        "      #print(\"List: \", exemplar_set_class_k)\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      #print(\"Exemplar subset length: \", len(exemplars_subset))\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "\n",
        "  train_subset_total = torch.utils.data.ConcatDataset([train_subset, total_exemplars])\n",
        "  print(\"Len TOTAL train susbset: \", len(train_subset_total))\n",
        "  train_loader = torch.utils.data.DataLoader(train_subset_total, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  # train_loader è la concatenazione delle nuove classi con gli exemplar_sets calcolati fino a questo punto \n",
        "  print(\"training\")\n",
        "  train(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac65-6QGjm9O"
      },
      "source": [
        "### incremental train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPU_32ArjpPH"
      },
      "source": [
        "import copy\n",
        "def incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net):\n",
        "  print(\"Starting the update representation\")\n",
        "  exemplar_indices = None\n",
        "  num_classes = 10\n",
        "  classes_until_now = []\n",
        "  new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "  print(\"NEW CLASSES: \", new_classes_examined)\n",
        "\n",
        "  updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net)\n",
        "                \n",
        "  iteration = group_id - 1\n",
        "  t = (num_classes * iteration) + num_classes # num_classes ricevute fino a questo momento \n",
        "  m = int(K/t) #casto ad intero ? per difetto o eccesso?\n",
        "  #s = num_classes * iteration\n",
        "\n",
        "  # REDUCING EXEMPLAR SET FOR EXISTING CLASSES\n",
        "  print(\"reducing exemplars for each class\")\n",
        "  for y in classes_until_now: #ci serve un set con tutte le classi fino ad ora viste\n",
        "    exemplar_y_new = reduceExemplarSet(m, exemplars_set_tot[y]) # valore associato alla chiave y che rappresenta il label della classe \n",
        "    exemplars_set_tot[y] = exemplar_y_new\n",
        "  \n",
        "  classes_until_now.append(new_classes_examined)\n",
        "  \n",
        "  # CONSTRUCTION EXEMPLAR SET FOR NEW CLASSES\n",
        "  for y in new_classes_examined: # nuovi classi in arrivo di cui vogliamo costruire il set rappresentativo\n",
        "    images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data']\n",
        "    imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "    class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "    print(\"class train: \", class_train_subset)\n",
        "    print(\"Constructing exemplars of class\", y)\n",
        "    exemplars_set = constructExemplarSet(net, class_train_subset, m) # exemplar set è un set di indici\n",
        "    #devo recuperare dal dataset iniziale l'indice delle immagini dell'exemplar set creato \n",
        "    #for image in exemplars_set:\n",
        "     # exemplars_set = train_dataset.df.index[train_dataset.df['data'] == image].tolist()\n",
        "    exemplars_set_tot[y] = exemplars_set\n",
        "    print(\"exemplar set: \", exemplars_set)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDxmsQlWjp5D"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dQznGoEju9J"
      },
      "source": [
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes_till_now, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    print(\"num classes till now: \", num_classes_till_now)\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            #print(\"labels: \", set(labels))\n",
        "            labels_enc = _one_hot_encode(labels, num_classes_till_now, outputs_labels_mapping)\n",
        "            #print(labels_enc)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            #if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHpi410ljzWD"
      },
      "source": [
        "### test and validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQYB0-cLj0sP"
      },
      "source": [
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTv6zBetj3Cb"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fl5mJrUj4fU"
      },
      "source": [
        "# # MAIN quello che facevamo prima\n",
        "# K = 2000\n",
        "# iterations = 10\n",
        "# num_classes = 10\n",
        "# test_set = [] #initialized here because we test over all the classes not only those one in which I train\n",
        "# exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "# for i in range(iterations):\n",
        "#   # train_subsets, val_subsets, test_subsets\n",
        "\n",
        "#   valid_loader = torch.utils.data.DataLoader(val_subsets, shuffle = True, batch_size = batch_size, num_workers=2) \n",
        "#   print(\"Train the network, iteration: \", i, \" on classes: \", classes_current_iter)\n",
        "#   incrementalTrain(train_subsets, i, net, device, epochs, num_classes, K, exemplars_set_tot) # Train the network with 10 classes at a time\n",
        "#   test(test_subsets, i, net) # Test the network with all classes seen until this iteration"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_cnELYdkap4"
      },
      "source": [
        "def sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 10\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    \n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      for _, images, labels in train_loader:\n",
        "        labels_train.append(labels)\n",
        "      \n",
        "      #print(\"TRAIN LABELS: \", set(labels_train))\n",
        "\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      # train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      # train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTV1Gxq6yF5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799f00ca-fdc8-4977-d51f-b78ec39f5a49"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN:  4950\n",
            "GROUP:  1\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "Len TOTAL train susbset:  4950\n",
            "training\n",
            "num classes till now:  10\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.7203276753425598\n",
            "Train step - Step 10, Loss 0.308976411819458\n",
            "Train step - Step 20, Loss 0.2790014445781708\n",
            "Train step - Step 30, Loss 0.26507875323295593\n",
            "Train epoch - Accuracy: 0.2905050505050505 Loss: 0.34349234314879984 Corrects: 1438\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.2639125883579254\n",
            "Train step - Step 50, Loss 0.2555479109287262\n",
            "Train step - Step 60, Loss 0.23188185691833496\n",
            "Train step - Step 70, Loss 0.22223280370235443\n",
            "Train epoch - Accuracy: 0.43777777777777777 Loss: 0.24562838882508903 Corrects: 2167\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.2104872763156891\n",
            "Train step - Step 90, Loss 0.24342218041419983\n",
            "Train step - Step 100, Loss 0.24665389955043793\n",
            "Train step - Step 110, Loss 0.23541812598705292\n",
            "Train epoch - Accuracy: 0.4888888888888889 Loss: 0.2267482519812054 Corrects: 2420\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.20797789096832275\n",
            "Train step - Step 130, Loss 0.22148862481117249\n",
            "Train step - Step 140, Loss 0.20483723282814026\n",
            "Train step - Step 150, Loss 0.23181715607643127\n",
            "Train epoch - Accuracy: 0.4995959595959596 Loss: 0.21952438111859138 Corrects: 2473\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.22152531147003174\n",
            "Train step - Step 170, Loss 0.21537671983242035\n",
            "Train step - Step 180, Loss 0.2184302806854248\n",
            "Train step - Step 190, Loss 0.2121490091085434\n",
            "Train epoch - Accuracy: 0.5361616161616162 Loss: 0.21018849045339258 Corrects: 2654\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.18365073204040527\n",
            "Train step - Step 210, Loss 0.21026258170604706\n",
            "Train step - Step 220, Loss 0.21422560513019562\n",
            "Train step - Step 230, Loss 0.20016424357891083\n",
            "Train epoch - Accuracy: 0.561010101010101 Loss: 0.20021107240156694 Corrects: 2777\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.19342069327831268\n",
            "Train step - Step 250, Loss 0.1709129810333252\n",
            "Train step - Step 260, Loss 0.1817905157804489\n",
            "Train step - Step 270, Loss 0.2106257528066635\n",
            "Train epoch - Accuracy: 0.5717171717171717 Loss: 0.19421480173414404 Corrects: 2830\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.18172626197338104\n",
            "Train step - Step 290, Loss 0.18946151435375214\n",
            "Train step - Step 300, Loss 0.1786566525697708\n",
            "Train step - Step 310, Loss 0.17116263508796692\n",
            "Train epoch - Accuracy: 0.5872727272727273 Loss: 0.18882393488378235 Corrects: 2907\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.17903496325016022\n",
            "Train step - Step 330, Loss 0.177634596824646\n",
            "Train step - Step 340, Loss 0.18320466578006744\n",
            "Train step - Step 350, Loss 0.16203324496746063\n",
            "Train epoch - Accuracy: 0.6197979797979798 Loss: 0.1790639069646296 Corrects: 3068\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.16612885892391205\n",
            "Train step - Step 370, Loss 0.14235123991966248\n",
            "Train step - Step 380, Loss 0.1873885840177536\n",
            "Train epoch - Accuracy: 0.6363636363636364 Loss: 0.17206004112055806 Corrects: 3150\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.1599283367395401\n",
            "Train step - Step 400, Loss 0.17335975170135498\n",
            "Train step - Step 410, Loss 0.15224899351596832\n",
            "Train step - Step 420, Loss 0.19194713234901428\n",
            "Train epoch - Accuracy: 0.6478787878787878 Loss: 0.16723304004982265 Corrects: 3207\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.17058764398097992\n",
            "Train step - Step 440, Loss 0.16052410006523132\n",
            "Train step - Step 450, Loss 0.1573724001646042\n",
            "Train step - Step 460, Loss 0.19085471332073212\n",
            "Train epoch - Accuracy: 0.6543434343434343 Loss: 0.16424894125774653 Corrects: 3239\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.15074720978736877\n",
            "Train step - Step 480, Loss 0.1546257883310318\n",
            "Train step - Step 490, Loss 0.1789395660161972\n",
            "Train step - Step 500, Loss 0.14428405463695526\n",
            "Train epoch - Accuracy: 0.6686868686868687 Loss: 0.15829440157822888 Corrects: 3310\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.15610723197460175\n",
            "Train step - Step 520, Loss 0.13515740633010864\n",
            "Train step - Step 530, Loss 0.13567659258842468\n",
            "Train step - Step 540, Loss 0.1403159350156784\n",
            "Train epoch - Accuracy: 0.6937373737373738 Loss: 0.15113115189051388 Corrects: 3434\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.16012945771217346\n",
            "Train step - Step 560, Loss 0.1791778802871704\n",
            "Train step - Step 570, Loss 0.14320476353168488\n",
            "Train step - Step 580, Loss 0.13099204003810883\n",
            "Train epoch - Accuracy: 0.6925252525252525 Loss: 0.14780399560326277 Corrects: 3428\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.13884146511554718\n",
            "Train step - Step 600, Loss 0.13439629971981049\n",
            "Train step - Step 610, Loss 0.1447247713804245\n",
            "Train step - Step 620, Loss 0.14181213080883026\n",
            "Train epoch - Accuracy: 0.7185858585858586 Loss: 0.1396551275012469 Corrects: 3557\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.14005017280578613\n",
            "Train step - Step 640, Loss 0.12293918430805206\n",
            "Train step - Step 650, Loss 0.14610141515731812\n",
            "Train step - Step 660, Loss 0.12851713597774506\n",
            "Train epoch - Accuracy: 0.7385858585858586 Loss: 0.13420277905584585 Corrects: 3656\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.13377659022808075\n",
            "Train step - Step 680, Loss 0.14283569157123566\n",
            "Train step - Step 690, Loss 0.1504344791173935\n",
            "Train step - Step 700, Loss 0.14374658465385437\n",
            "Train epoch - Accuracy: 0.7323232323232324 Loss: 0.1330397994229288 Corrects: 3625\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.12394215911626816\n",
            "Train step - Step 720, Loss 0.12399353832006454\n",
            "Train step - Step 730, Loss 0.1315901279449463\n",
            "Train step - Step 740, Loss 0.1175580769777298\n",
            "Train epoch - Accuracy: 0.7434343434343434 Loss: 0.12773436261547935 Corrects: 3680\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.12554912269115448\n",
            "Train step - Step 760, Loss 0.10858737677335739\n",
            "Train step - Step 770, Loss 0.1124696359038353\n",
            "Train epoch - Accuracy: 0.7591919191919192 Loss: 0.12214129562329765 Corrects: 3758\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.12008623033761978\n",
            "Train step - Step 790, Loss 0.12202470749616623\n",
            "Train step - Step 800, Loss 0.11877763271331787\n",
            "Train step - Step 810, Loss 0.10077822208404541\n",
            "Train epoch - Accuracy: 0.7741414141414141 Loss: 0.11765119133874623 Corrects: 3832\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.13955946266651154\n",
            "Train step - Step 830, Loss 0.10209381580352783\n",
            "Train step - Step 840, Loss 0.11292562633752823\n",
            "Train step - Step 850, Loss 0.11657728999853134\n",
            "Train epoch - Accuracy: 0.7713131313131313 Loss: 0.11646852990593573 Corrects: 3818\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.07196363061666489\n",
            "Train step - Step 870, Loss 0.10666649788618088\n",
            "Train step - Step 880, Loss 0.10819939523935318\n",
            "Train step - Step 890, Loss 0.1166655644774437\n",
            "Train epoch - Accuracy: 0.794949494949495 Loss: 0.10860349157542894 Corrects: 3935\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.118799589574337\n",
            "Train step - Step 910, Loss 0.11323487758636475\n",
            "Train step - Step 920, Loss 0.09419720619916916\n",
            "Train step - Step 930, Loss 0.10676167160272598\n",
            "Train epoch - Accuracy: 0.7886868686868687 Loss: 0.10711914352997384 Corrects: 3904\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.1264331340789795\n",
            "Train step - Step 950, Loss 0.09399125725030899\n",
            "Train step - Step 960, Loss 0.10278697311878204\n",
            "Train step - Step 970, Loss 0.11096946150064468\n",
            "Train epoch - Accuracy: 0.7923232323232323 Loss: 0.10559247948304572 Corrects: 3922\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.09456651657819748\n",
            "Train step - Step 990, Loss 0.10823845863342285\n",
            "Train step - Step 1000, Loss 0.12995779514312744\n",
            "Train step - Step 1010, Loss 0.10100918263196945\n",
            "Train epoch - Accuracy: 0.7967676767676768 Loss: 0.10349090497000049 Corrects: 3944\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.0836942121386528\n",
            "Train step - Step 1030, Loss 0.09950803965330124\n",
            "Train step - Step 1040, Loss 0.12417973577976227\n",
            "Train step - Step 1050, Loss 0.10245814174413681\n",
            "Train epoch - Accuracy: 0.8143434343434344 Loss: 0.09843584813854911 Corrects: 4031\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.07050906866788864\n",
            "Train step - Step 1070, Loss 0.10645895451307297\n",
            "Train step - Step 1080, Loss 0.10492844879627228\n",
            "Train step - Step 1090, Loss 0.09894537180662155\n",
            "Train epoch - Accuracy: 0.8153535353535354 Loss: 0.09711994327680029 Corrects: 4036\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10060695558786392\n",
            "Train step - Step 1110, Loss 0.08655154705047607\n",
            "Train step - Step 1120, Loss 0.10479213297367096\n",
            "Train step - Step 1130, Loss 0.10731282830238342\n",
            "Train epoch - Accuracy: 0.8218181818181818 Loss: 0.09531060063477718 Corrects: 4068\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.09571563452482224\n",
            "Train step - Step 1150, Loss 0.10137248039245605\n",
            "Train step - Step 1160, Loss 0.10942398756742477\n",
            "Train epoch - Accuracy: 0.8274747474747475 Loss: 0.09146802643934886 Corrects: 4096\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.08294562995433807\n",
            "Train step - Step 1180, Loss 0.07950039952993393\n",
            "Train step - Step 1190, Loss 0.09689628332853317\n",
            "Train step - Step 1200, Loss 0.08559267967939377\n",
            "Train epoch - Accuracy: 0.8341414141414142 Loss: 0.08749228288428952 Corrects: 4129\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.08523052930831909\n",
            "Train step - Step 1220, Loss 0.08941002190113068\n",
            "Train step - Step 1230, Loss 0.07999062538146973\n",
            "Train step - Step 1240, Loss 0.08822818845510483\n",
            "Train epoch - Accuracy: 0.8343434343434344 Loss: 0.08597939337443823 Corrects: 4130\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.0858485996723175\n",
            "Train step - Step 1260, Loss 0.08151984214782715\n",
            "Train step - Step 1270, Loss 0.0896592065691948\n",
            "Train step - Step 1280, Loss 0.09517906606197357\n",
            "Train epoch - Accuracy: 0.8276767676767677 Loss: 0.09002138588163588 Corrects: 4097\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.0697714239358902\n",
            "Train step - Step 1300, Loss 0.06918876618146896\n",
            "Train step - Step 1310, Loss 0.08795073628425598\n",
            "Train step - Step 1320, Loss 0.1062576100230217\n",
            "Train epoch - Accuracy: 0.8343434343434344 Loss: 0.08694502220912413 Corrects: 4130\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.08063419908285141\n",
            "Train step - Step 1340, Loss 0.08771377056837082\n",
            "Train step - Step 1350, Loss 0.06819571554660797\n",
            "Train step - Step 1360, Loss 0.0690365806221962\n",
            "Train epoch - Accuracy: 0.8434343434343434 Loss: 0.08045656429095702 Corrects: 4175\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.06645452231168747\n",
            "Train step - Step 1380, Loss 0.06913169473409653\n",
            "Train step - Step 1390, Loss 0.0713866576552391\n",
            "Train step - Step 1400, Loss 0.06887006014585495\n",
            "Train epoch - Accuracy: 0.862020202020202 Loss: 0.0752880018467855 Corrects: 4267\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.06950412690639496\n",
            "Train step - Step 1420, Loss 0.09586217254400253\n",
            "Train step - Step 1430, Loss 0.08041895925998688\n",
            "Train step - Step 1440, Loss 0.08222146332263947\n",
            "Train epoch - Accuracy: 0.8587878787878788 Loss: 0.07479353222883109 Corrects: 4251\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.06414385885000229\n",
            "Train step - Step 1460, Loss 0.07889880239963531\n",
            "Train step - Step 1470, Loss 0.07380550354719162\n",
            "Train step - Step 1480, Loss 0.09824664890766144\n",
            "Train epoch - Accuracy: 0.8571717171717171 Loss: 0.07468223452568054 Corrects: 4243\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.0700611025094986\n",
            "Train step - Step 1500, Loss 0.06206626817584038\n",
            "Train step - Step 1510, Loss 0.08438392728567123\n",
            "Train step - Step 1520, Loss 0.07845465838909149\n",
            "Train epoch - Accuracy: 0.864040404040404 Loss: 0.07454840588449228 Corrects: 4277\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.06221665069460869\n",
            "Train step - Step 1540, Loss 0.07122745364904404\n",
            "Train step - Step 1550, Loss 0.0653262510895729\n",
            "Train epoch - Accuracy: 0.8723232323232323 Loss: 0.06967057518886798 Corrects: 4318\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.06687653064727783\n",
            "Train step - Step 1570, Loss 0.06625133007764816\n",
            "Train step - Step 1580, Loss 0.05757639557123184\n",
            "Train step - Step 1590, Loss 0.08429103344678879\n",
            "Train epoch - Accuracy: 0.8719191919191919 Loss: 0.07007015089344497 Corrects: 4316\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.07719677686691284\n",
            "Train step - Step 1610, Loss 0.0713912695646286\n",
            "Train step - Step 1620, Loss 0.06358744204044342\n",
            "Train step - Step 1630, Loss 0.08355032652616501\n",
            "Train epoch - Accuracy: 0.8783838383838384 Loss: 0.0653227171241635 Corrects: 4348\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.08794642984867096\n",
            "Train step - Step 1650, Loss 0.06182081252336502\n",
            "Train step - Step 1660, Loss 0.05332310125231743\n",
            "Train step - Step 1670, Loss 0.07828081399202347\n",
            "Train epoch - Accuracy: 0.8777777777777778 Loss: 0.06577657637271014 Corrects: 4345\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.04730772599577904\n",
            "Train step - Step 1690, Loss 0.05462014675140381\n",
            "Train step - Step 1700, Loss 0.09004992246627808\n",
            "Train step - Step 1710, Loss 0.07151152938604355\n",
            "Train epoch - Accuracy: 0.8777777777777778 Loss: 0.06678049798565681 Corrects: 4345\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.06144821271300316\n",
            "Train step - Step 1730, Loss 0.06110958382487297\n",
            "Train step - Step 1740, Loss 0.08300915360450745\n",
            "Train step - Step 1750, Loss 0.0620819590985775\n",
            "Train epoch - Accuracy: 0.883030303030303 Loss: 0.06481014678875606 Corrects: 4371\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.03997661918401718\n",
            "Train step - Step 1770, Loss 0.04969089850783348\n",
            "Train step - Step 1780, Loss 0.06657799333333969\n",
            "Train step - Step 1790, Loss 0.049639344215393066\n",
            "Train epoch - Accuracy: 0.8846464646464647 Loss: 0.06255031233484094 Corrects: 4379\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.06567586958408356\n",
            "Train step - Step 1810, Loss 0.06682932376861572\n",
            "Train step - Step 1820, Loss 0.06690429896116257\n",
            "Train step - Step 1830, Loss 0.04208202287554741\n",
            "Train epoch - Accuracy: 0.8951515151515151 Loss: 0.05994385074786466 Corrects: 4431\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.06439796835184097\n",
            "Train step - Step 1850, Loss 0.06845609098672867\n",
            "Train step - Step 1860, Loss 0.04582394286990166\n",
            "Train step - Step 1870, Loss 0.05882657319307327\n",
            "Train epoch - Accuracy: 0.8947474747474747 Loss: 0.056908094106298504 Corrects: 4429\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.062101688235998154\n",
            "Train step - Step 1890, Loss 0.049524933099746704\n",
            "Train step - Step 1900, Loss 0.04594596475362778\n",
            "Train step - Step 1910, Loss 0.07558047026395798\n",
            "Train epoch - Accuracy: 0.9022222222222223 Loss: 0.05413343238409119 Corrects: 4466\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.0341094434261322\n",
            "Train step - Step 1930, Loss 0.033670615404844284\n",
            "Train step - Step 1940, Loss 0.03665536269545555\n",
            "Train epoch - Accuracy: 0.9280808080808081 Loss: 0.04350198537261799 Corrects: 4594\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.03161663934588432\n",
            "Train step - Step 1960, Loss 0.04468941688537598\n",
            "Train step - Step 1970, Loss 0.03224421292543411\n",
            "Train step - Step 1980, Loss 0.044585052877664566\n",
            "Train epoch - Accuracy: 0.9365656565656566 Loss: 0.03673063160795154 Corrects: 4636\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.035249706357717514\n",
            "Train step - Step 2000, Loss 0.02344611845910549\n",
            "Train step - Step 2010, Loss 0.0416758731007576\n",
            "Train step - Step 2020, Loss 0.02915826439857483\n",
            "Train epoch - Accuracy: 0.944040404040404 Loss: 0.035554638342423873 Corrects: 4673\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.044321537017822266\n",
            "Train step - Step 2040, Loss 0.035701822489500046\n",
            "Train step - Step 2050, Loss 0.03927234932780266\n",
            "Train step - Step 2060, Loss 0.033074285835027695\n",
            "Train epoch - Accuracy: 0.944040404040404 Loss: 0.03419981337481677 Corrects: 4673\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.03443395718932152\n",
            "Train step - Step 2080, Loss 0.03697459027171135\n",
            "Train step - Step 2090, Loss 0.03271481394767761\n",
            "Train step - Step 2100, Loss 0.04863820597529411\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.033462131714279 Corrects: 4703\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.021725324913859367\n",
            "Train step - Step 2120, Loss 0.03952132165431976\n",
            "Train step - Step 2130, Loss 0.02114788629114628\n",
            "Train step - Step 2140, Loss 0.03956834226846695\n",
            "Train epoch - Accuracy: 0.9486868686868687 Loss: 0.03225163756446405 Corrects: 4696\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.020193351432681084\n",
            "Train step - Step 2160, Loss 0.02818901836872101\n",
            "Train step - Step 2170, Loss 0.032638322561979294\n",
            "Train step - Step 2180, Loss 0.021662404760718346\n",
            "Train epoch - Accuracy: 0.9464646464646465 Loss: 0.03252230226843044 Corrects: 4685\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.02794475667178631\n",
            "Train step - Step 2200, Loss 0.03512785583734512\n",
            "Train step - Step 2210, Loss 0.039696332067251205\n",
            "Train step - Step 2220, Loss 0.031599011272192\n",
            "Train epoch - Accuracy: 0.9486868686868687 Loss: 0.03317998454877825 Corrects: 4696\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.031843218952417374\n",
            "Train step - Step 2240, Loss 0.029032690450549126\n",
            "Train step - Step 2250, Loss 0.02872380055487156\n",
            "Train step - Step 2260, Loss 0.020960181951522827\n",
            "Train epoch - Accuracy: 0.9555555555555556 Loss: 0.030218343847628795 Corrects: 4730\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.02458878420293331\n",
            "Train step - Step 2280, Loss 0.03202703222632408\n",
            "Train step - Step 2290, Loss 0.02487115189433098\n",
            "Train step - Step 2300, Loss 0.05196870118379593\n",
            "Train epoch - Accuracy: 0.9551515151515152 Loss: 0.030112360255284742 Corrects: 4728\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.027782952412962914\n",
            "Train step - Step 2320, Loss 0.025936609134078026\n",
            "Train step - Step 2330, Loss 0.03334102779626846\n",
            "Train epoch - Accuracy: 0.9507070707070707 Loss: 0.030746784970314817 Corrects: 4706\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.03192359209060669\n",
            "Train step - Step 2350, Loss 0.03210363909602165\n",
            "Train step - Step 2360, Loss 0.03745369240641594\n",
            "Train step - Step 2370, Loss 0.036363471299409866\n",
            "Train epoch - Accuracy: 0.9583838383838383 Loss: 0.028646007477484567 Corrects: 4744\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.026604831218719482\n",
            "Train step - Step 2390, Loss 0.026300249621272087\n",
            "Train step - Step 2400, Loss 0.020079853013157845\n",
            "Train step - Step 2410, Loss 0.03943519666790962\n",
            "Train epoch - Accuracy: 0.9531313131313132 Loss: 0.02901437116391731 Corrects: 4718\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.01629612408578396\n",
            "Train step - Step 2430, Loss 0.037557702511548996\n",
            "Train step - Step 2440, Loss 0.03401618078351021\n",
            "Train step - Step 2450, Loss 0.018831372261047363\n",
            "Train epoch - Accuracy: 0.9531313131313132 Loss: 0.029825416539955622 Corrects: 4718\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.026815935969352722\n",
            "Train step - Step 2470, Loss 0.03313649818301201\n",
            "Train step - Step 2480, Loss 0.031236549839377403\n",
            "Train step - Step 2490, Loss 0.023376135155558586\n",
            "Train epoch - Accuracy: 0.9622222222222222 Loss: 0.025703371540464537 Corrects: 4763\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.04353566840291023\n",
            "Train step - Step 2510, Loss 0.0227393489331007\n",
            "Train step - Step 2520, Loss 0.03149541839957237\n",
            "Train step - Step 2530, Loss 0.025942891836166382\n",
            "Train epoch - Accuracy: 0.9612121212121212 Loss: 0.026917859364037563 Corrects: 4758\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.016631579026579857\n",
            "Train step - Step 2550, Loss 0.023329690098762512\n",
            "Train step - Step 2560, Loss 0.032747264951467514\n",
            "Train step - Step 2570, Loss 0.01927151158452034\n",
            "Train epoch - Accuracy: 0.9674747474747475 Loss: 0.025234362762233224 Corrects: 4789\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.022758465260267258\n",
            "Train step - Step 2590, Loss 0.03335212171077728\n",
            "Train step - Step 2600, Loss 0.02508629858493805\n",
            "Train step - Step 2610, Loss 0.02072058990597725\n",
            "Train epoch - Accuracy: 0.964040404040404 Loss: 0.024675468514212453 Corrects: 4772\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.030907073989510536\n",
            "Train step - Step 2630, Loss 0.020333413034677505\n",
            "Train step - Step 2640, Loss 0.03157395124435425\n",
            "Train step - Step 2650, Loss 0.023988714441657066\n",
            "Train epoch - Accuracy: 0.9680808080808081 Loss: 0.02337346481178144 Corrects: 4792\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.026042116805911064\n",
            "Train step - Step 2670, Loss 0.02774999849498272\n",
            "Train step - Step 2680, Loss 0.02380615472793579\n",
            "Train step - Step 2690, Loss 0.02048693783581257\n",
            "Train epoch - Accuracy: 0.9652525252525253 Loss: 0.023941898432494415 Corrects: 4778\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.020494163036346436\n",
            "Train step - Step 2710, Loss 0.02165835350751877\n",
            "Train step - Step 2720, Loss 0.03561672195792198\n",
            "Train epoch - Accuracy: 0.9696969696969697 Loss: 0.023424555437281877 Corrects: 4800\n",
            "Training finished in 191.3082253932953 seconds\n",
            "reducing exemplars for each class\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f77650c5d90>\n",
            "Constructing exemplars of class 67\n",
            "12575\n",
            "18540\n",
            "9996\n",
            "13704\n",
            "17070\n",
            "33761\n",
            "8812\n",
            "46574\n",
            "23070\n",
            "43250\n",
            "44686\n",
            "22239\n",
            "26910\n",
            "43844\n",
            "9702\n",
            "6666\n",
            "10946\n",
            "2860\n",
            "8206\n",
            "13704\n",
            "5543\n",
            "39355\n",
            "27043\n",
            "42180\n",
            "44686\n",
            "32943\n",
            "14637\n",
            "5399\n",
            "18701\n",
            "40605\n",
            "17422\n",
            "43164\n",
            "12575\n",
            "44686\n",
            "9853\n",
            "41643\n",
            "29655\n",
            "965\n",
            "6815\n",
            "30015\n",
            "18540\n",
            "47057\n",
            "41721\n",
            "46553\n",
            "37255\n",
            "18324\n",
            "9092\n",
            "12995\n",
            "13704\n",
            "23105\n",
            "2798\n",
            "40465\n",
            "20057\n",
            "33036\n",
            "27183\n",
            "19900\n",
            "18981\n",
            "40605\n",
            "37145\n",
            "7164\n",
            "6919\n",
            "33169\n",
            "8345\n",
            "32541\n",
            "3017\n",
            "9653\n",
            "48574\n",
            "27071\n",
            "17422\n",
            "9853\n",
            "43164\n",
            "2546\n",
            "14999\n",
            "33169\n",
            "41721\n",
            "23070\n",
            "23608\n",
            "43880\n",
            "35199\n",
            "8206\n",
            "5807\n",
            "38172\n",
            "42180\n",
            "38166\n",
            "40465\n",
            "27071\n",
            "13704\n",
            "27612\n",
            "14999\n",
            "33761\n",
            "49244\n",
            "43042\n",
            "18540\n",
            "12995\n",
            "23728\n",
            "48250\n",
            "21582\n",
            "13208\n",
            "18324\n",
            "17422\n",
            "39682\n",
            "42180\n",
            "11131\n",
            "17070\n",
            "4076\n",
            "4609\n",
            "48574\n",
            "1973\n",
            "12437\n",
            "46644\n",
            "15324\n",
            "49783\n",
            "33169\n",
            "15332\n",
            "42399\n",
            "42295\n",
            "39682\n",
            "8206\n",
            "2860\n",
            "38052\n",
            "12995\n",
            "48523\n",
            "19900\n",
            "13704\n",
            "25966\n",
            "24148\n",
            "1247\n",
            "42180\n",
            "12575\n",
            "14515\n",
            "26671\n",
            "3847\n",
            "12908\n",
            "17305\n",
            "46553\n",
            "16346\n",
            "6978\n",
            "39089\n",
            "426\n",
            "11050\n",
            "23572\n",
            "15111\n",
            "31732\n",
            "7279\n",
            "23070\n",
            "8812\n",
            "13341\n",
            "11412\n",
            "18701\n",
            "9483\n",
            "41583\n",
            "8825\n",
            "715\n",
            "35158\n",
            "43844\n",
            "11730\n",
            "35415\n",
            "23985\n",
            "10630\n",
            "2860\n",
            "21286\n",
            "14157\n",
            "42572\n",
            "46666\n",
            "18015\n",
            "24336\n",
            "2785\n",
            "23572\n",
            "19621\n",
            "27036\n",
            "5372\n",
            "33169\n",
            "4677\n",
            "46553\n",
            "41721\n",
            "41907\n",
            "39502\n",
            "47104\n",
            "17514\n",
            "28494\n",
            "48882\n",
            "11811\n",
            "41907\n",
            "17163\n",
            "14637\n",
            "2155\n",
            "29674\n",
            "27036\n",
            "27183\n",
            "2798\n",
            "18324\n",
            "47314\n",
            "4226\n",
            "16592\n",
            "15953\n",
            "12241\n",
            "13704\n",
            "38172\n",
            "19722\n",
            "45719\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [12575, 18540, 9996, 13704, 17070, 33761, 8812, 46574, 23070, 43250, 44686, 22239, 26910, 43844, 9702, 6666, 10946, 2860, 8206, 13704, 5543, 39355, 27043, 42180, 44686, 32943, 14637, 5399, 18701, 40605, 17422, 43164, 12575, 44686, 9853, 41643, 29655, 965, 6815, 30015, 18540, 47057, 41721, 46553, 37255, 18324, 9092, 12995, 13704, 23105, 2798, 40465, 20057, 33036, 27183, 19900, 18981, 40605, 37145, 7164, 6919, 33169, 8345, 32541, 3017, 9653, 48574, 27071, 17422, 9853, 43164, 2546, 14999, 33169, 41721, 23070, 23608, 43880, 35199, 8206, 5807, 38172, 42180, 38166, 40465, 27071, 13704, 27612, 14999, 33761, 49244, 43042, 18540, 12995, 23728, 48250, 21582, 13208, 18324, 17422, 39682, 42180, 11131, 17070, 4076, 4609, 48574, 1973, 12437, 46644, 15324, 49783, 33169, 15332, 42399, 42295, 39682, 8206, 2860, 38052, 12995, 48523, 19900, 13704, 25966, 24148, 1247, 42180, 12575, 14515, 26671, 3847, 12908, 17305, 46553, 16346, 6978, 39089, 426, 11050, 23572, 15111, 31732, 7279, 23070, 8812, 13341, 11412, 18701, 9483, 41583, 8825, 715, 35158, 43844, 11730, 35415, 23985, 10630, 2860, 21286, 14157, 42572, 46666, 18015, 24336, 2785, 23572, 19621, 27036, 5372, 33169, 4677, 46553, 41721, 41907, 39502, 47104, 17514, 28494, 48882, 11811, 41907, 17163, 14637, 2155, 29674, 27036, 27183, 2798, 18324, 47314, 4226, 16592, 15953, 12241, 13704, 38172, 19722, 45719]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f7765095b10>\n",
            "Constructing exemplars of class 59\n",
            "46599\n",
            "31150\n",
            "35190\n",
            "982\n",
            "6045\n",
            "43884\n",
            "18757\n",
            "5504\n",
            "34562\n",
            "28258\n",
            "7390\n",
            "18356\n",
            "19261\n",
            "5774\n",
            "32372\n",
            "20491\n",
            "49979\n",
            "34852\n",
            "27956\n",
            "29164\n",
            "17404\n",
            "17090\n",
            "5774\n",
            "34562\n",
            "13005\n",
            "26727\n",
            "41316\n",
            "44095\n",
            "11880\n",
            "41678\n",
            "14376\n",
            "25891\n",
            "3995\n",
            "46599\n",
            "20670\n",
            "13465\n",
            "30553\n",
            "44095\n",
            "4936\n",
            "42336\n",
            "40320\n",
            "11710\n",
            "43884\n",
            "25930\n",
            "30915\n",
            "29164\n",
            "567\n",
            "34562\n",
            "20216\n",
            "17510\n",
            "13873\n",
            "24\n",
            "7093\n",
            "2000\n",
            "7736\n",
            "19136\n",
            "17943\n",
            "10664\n",
            "1562\n",
            "27541\n",
            "4560\n",
            "34852\n",
            "9981\n",
            "42060\n",
            "1341\n",
            "20481\n",
            "18179\n",
            "34562\n",
            "43884\n",
            "13338\n",
            "31501\n",
            "17090\n",
            "27167\n",
            "49979\n",
            "23289\n",
            "16080\n",
            "25654\n",
            "10073\n",
            "47967\n",
            "7093\n",
            "19261\n",
            "21671\n",
            "6045\n",
            "5774\n",
            "39867\n",
            "18865\n",
            "26200\n",
            "41678\n",
            "406\n",
            "14376\n",
            "24\n",
            "30471\n",
            "37228\n",
            "38061\n",
            "42336\n",
            "39867\n",
            "36223\n",
            "16080\n",
            "31150\n",
            "14052\n",
            "37976\n",
            "48169\n",
            "32614\n",
            "11880\n",
            "26194\n",
            "24021\n",
            "5672\n",
            "3488\n",
            "27956\n",
            "11880\n",
            "1237\n",
            "43534\n",
            "20670\n",
            "26727\n",
            "13005\n",
            "34852\n",
            "17090\n",
            "28325\n",
            "19677\n",
            "37987\n",
            "20491\n",
            "23210\n",
            "23289\n",
            "29968\n",
            "7736\n",
            "26213\n",
            "19261\n",
            "40723\n",
            "44865\n",
            "13541\n",
            "7093\n",
            "2000\n",
            "10103\n",
            "37987\n",
            "32648\n",
            "36945\n",
            "28724\n",
            "9679\n",
            "47967\n",
            "43534\n",
            "34852\n",
            "10087\n",
            "3995\n",
            "2000\n",
            "26200\n",
            "36584\n",
            "32372\n",
            "13105\n",
            "43463\n",
            "37719\n",
            "43884\n",
            "15989\n",
            "24658\n",
            "38064\n",
            "16080\n",
            "38962\n",
            "48169\n",
            "19067\n",
            "18250\n",
            "29367\n",
            "41861\n",
            "18865\n",
            "567\n",
            "7831\n",
            "34562\n",
            "28258\n",
            "21144\n",
            "34163\n",
            "47614\n",
            "2530\n",
            "20670\n",
            "6571\n",
            "15989\n",
            "8190\n",
            "2688\n",
            "731\n",
            "1844\n",
            "36945\n",
            "30828\n",
            "27259\n",
            "687\n",
            "48988\n",
            "41926\n",
            "17119\n",
            "7980\n",
            "517\n",
            "26727\n",
            "11555\n",
            "19709\n",
            "47268\n",
            "4708\n",
            "31388\n",
            "18865\n",
            "42060\n",
            "40723\n",
            "25654\n",
            "14758\n",
            "40985\n",
            "4552\n",
            "28258\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [46599, 31150, 35190, 982, 6045, 43884, 18757, 5504, 34562, 28258, 7390, 18356, 19261, 5774, 32372, 20491, 49979, 34852, 27956, 29164, 17404, 17090, 5774, 34562, 13005, 26727, 41316, 44095, 11880, 41678, 14376, 25891, 3995, 46599, 20670, 13465, 30553, 44095, 4936, 42336, 40320, 11710, 43884, 25930, 30915, 29164, 567, 34562, 20216, 17510, 13873, 24, 7093, 2000, 7736, 19136, 17943, 10664, 1562, 27541, 4560, 34852, 9981, 42060, 1341, 20481, 18179, 34562, 43884, 13338, 31501, 17090, 27167, 49979, 23289, 16080, 25654, 10073, 47967, 7093, 19261, 21671, 6045, 5774, 39867, 18865, 26200, 41678, 406, 14376, 24, 30471, 37228, 38061, 42336, 39867, 36223, 16080, 31150, 14052, 37976, 48169, 32614, 11880, 26194, 24021, 5672, 3488, 27956, 11880, 1237, 43534, 20670, 26727, 13005, 34852, 17090, 28325, 19677, 37987, 20491, 23210, 23289, 29968, 7736, 26213, 19261, 40723, 44865, 13541, 7093, 2000, 10103, 37987, 32648, 36945, 28724, 9679, 47967, 43534, 34852, 10087, 3995, 2000, 26200, 36584, 32372, 13105, 43463, 37719, 43884, 15989, 24658, 38064, 16080, 38962, 48169, 19067, 18250, 29367, 41861, 18865, 567, 7831, 34562, 28258, 21144, 34163, 47614, 2530, 20670, 6571, 15989, 8190, 2688, 731, 1844, 36945, 30828, 27259, 687, 48988, 41926, 17119, 7980, 517, 26727, 11555, 19709, 47268, 4708, 31388, 18865, 42060, 40723, 25654, 14758, 40985, 4552, 28258]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f77650f4890>\n",
            "Constructing exemplars of class 39\n",
            "17947\n",
            "47340\n",
            "26139\n",
            "551\n",
            "42909\n",
            "41383\n",
            "28649\n",
            "44644\n",
            "32848\n",
            "32758\n",
            "4912\n",
            "14374\n",
            "15044\n",
            "2830\n",
            "42909\n",
            "20430\n",
            "7060\n",
            "32852\n",
            "4912\n",
            "551\n",
            "49378\n",
            "42224\n",
            "5340\n",
            "46350\n",
            "41383\n",
            "10384\n",
            "11046\n",
            "22654\n",
            "38905\n",
            "28004\n",
            "11046\n",
            "5673\n",
            "5320\n",
            "21997\n",
            "14172\n",
            "30930\n",
            "8415\n",
            "11011\n",
            "16211\n",
            "24768\n",
            "551\n",
            "28001\n",
            "20644\n",
            "20919\n",
            "5414\n",
            "34352\n",
            "28649\n",
            "20453\n",
            "39804\n",
            "20687\n",
            "21774\n",
            "42815\n",
            "28524\n",
            "36185\n",
            "34103\n",
            "35268\n",
            "28649\n",
            "13996\n",
            "30158\n",
            "5320\n",
            "13219\n",
            "39280\n",
            "30725\n",
            "39348\n",
            "31403\n",
            "16211\n",
            "2830\n",
            "5405\n",
            "29734\n",
            "18253\n",
            "29503\n",
            "42909\n",
            "32307\n",
            "18206\n",
            "9764\n",
            "26504\n",
            "12025\n",
            "39348\n",
            "36418\n",
            "41455\n",
            "28986\n",
            "551\n",
            "32848\n",
            "15148\n",
            "47340\n",
            "39280\n",
            "13387\n",
            "17947\n",
            "19357\n",
            "32852\n",
            "42637\n",
            "551\n",
            "24641\n",
            "8415\n",
            "32758\n",
            "49074\n",
            "218\n",
            "10386\n",
            "17467\n",
            "14272\n",
            "15148\n",
            "4457\n",
            "33238\n",
            "14383\n",
            "44596\n",
            "40488\n",
            "2830\n",
            "11046\n",
            "14096\n",
            "20120\n",
            "37727\n",
            "14172\n",
            "31970\n",
            "19522\n",
            "29503\n",
            "19637\n",
            "21397\n",
            "38094\n",
            "28010\n",
            "30647\n",
            "22776\n",
            "45146\n",
            "19637\n",
            "25177\n",
            "47340\n",
            "16303\n",
            "5320\n",
            "20897\n",
            "30725\n",
            "48932\n",
            "41734\n",
            "13996\n",
            "49693\n",
            "9660\n",
            "45131\n",
            "19357\n",
            "551\n",
            "6407\n",
            "23668\n",
            "35009\n",
            "14374\n",
            "2213\n",
            "24768\n",
            "11822\n",
            "44085\n",
            "22654\n",
            "46962\n",
            "49378\n",
            "27986\n",
            "36766\n",
            "17018\n",
            "31964\n",
            "21774\n",
            "16682\n",
            "15263\n",
            "5621\n",
            "13825\n",
            "33238\n",
            "14383\n",
            "45131\n",
            "49920\n",
            "28163\n",
            "30000\n",
            "23828\n",
            "39712\n",
            "45072\n",
            "11775\n",
            "12168\n",
            "20897\n",
            "24768\n",
            "33241\n",
            "18637\n",
            "42159\n",
            "13387\n",
            "248\n",
            "22776\n",
            "18357\n",
            "38386\n",
            "46350\n",
            "49378\n",
            "5711\n",
            "15730\n",
            "34395\n",
            "6696\n",
            "39069\n",
            "1494\n",
            "18172\n",
            "20430\n",
            "49693\n",
            "45674\n",
            "36161\n",
            "49887\n",
            "5673\n",
            "38905\n",
            "18253\n",
            "35772\n",
            "41196\n",
            "21343\n",
            "21774\n",
            "45674\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [17947, 47340, 26139, 551, 42909, 41383, 28649, 44644, 32848, 32758, 4912, 14374, 15044, 2830, 42909, 20430, 7060, 32852, 4912, 551, 49378, 42224, 5340, 46350, 41383, 10384, 11046, 22654, 38905, 28004, 11046, 5673, 5320, 21997, 14172, 30930, 8415, 11011, 16211, 24768, 551, 28001, 20644, 20919, 5414, 34352, 28649, 20453, 39804, 20687, 21774, 42815, 28524, 36185, 34103, 35268, 28649, 13996, 30158, 5320, 13219, 39280, 30725, 39348, 31403, 16211, 2830, 5405, 29734, 18253, 29503, 42909, 32307, 18206, 9764, 26504, 12025, 39348, 36418, 41455, 28986, 551, 32848, 15148, 47340, 39280, 13387, 17947, 19357, 32852, 42637, 551, 24641, 8415, 32758, 49074, 218, 10386, 17467, 14272, 15148, 4457, 33238, 14383, 44596, 40488, 2830, 11046, 14096, 20120, 37727, 14172, 31970, 19522, 29503, 19637, 21397, 38094, 28010, 30647, 22776, 45146, 19637, 25177, 47340, 16303, 5320, 20897, 30725, 48932, 41734, 13996, 49693, 9660, 45131, 19357, 551, 6407, 23668, 35009, 14374, 2213, 24768, 11822, 44085, 22654, 46962, 49378, 27986, 36766, 17018, 31964, 21774, 16682, 15263, 5621, 13825, 33238, 14383, 45131, 49920, 28163, 30000, 23828, 39712, 45072, 11775, 12168, 20897, 24768, 33241, 18637, 42159, 13387, 248, 22776, 18357, 38386, 46350, 49378, 5711, 15730, 34395, 6696, 39069, 1494, 18172, 20430, 49693, 45674, 36161, 49887, 5673, 38905, 18253, 35772, 41196, 21343, 21774, 45674]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f77650e2bd0>\n",
            "Constructing exemplars of class 22\n",
            "2752\n",
            "5102\n",
            "46837\n",
            "4961\n",
            "18894\n",
            "19020\n",
            "12675\n",
            "49069\n",
            "20372\n",
            "46837\n",
            "16753\n",
            "39852\n",
            "44433\n",
            "44918\n",
            "10003\n",
            "23275\n",
            "4647\n",
            "37915\n",
            "779\n",
            "42014\n",
            "16552\n",
            "6369\n",
            "17313\n",
            "44433\n",
            "15318\n",
            "43346\n",
            "26461\n",
            "6968\n",
            "3444\n",
            "37915\n",
            "23275\n",
            "35250\n",
            "41765\n",
            "46426\n",
            "3941\n",
            "40966\n",
            "18894\n",
            "4961\n",
            "46837\n",
            "37478\n",
            "21171\n",
            "29999\n",
            "29131\n",
            "32438\n",
            "37478\n",
            "22367\n",
            "34634\n",
            "38393\n",
            "39852\n",
            "48107\n",
            "6968\n",
            "42062\n",
            "11850\n",
            "33574\n",
            "36684\n",
            "16272\n",
            "17596\n",
            "23275\n",
            "35250\n",
            "13935\n",
            "8920\n",
            "16753\n",
            "3941\n",
            "18894\n",
            "40206\n",
            "29999\n",
            "29366\n",
            "36684\n",
            "19884\n",
            "29380\n",
            "36675\n",
            "17251\n",
            "8942\n",
            "12670\n",
            "45699\n",
            "41185\n",
            "11850\n",
            "8429\n",
            "43947\n",
            "15318\n",
            "10003\n",
            "4807\n",
            "2584\n",
            "35732\n",
            "27847\n",
            "46426\n",
            "10291\n",
            "41765\n",
            "26070\n",
            "24970\n",
            "37219\n",
            "4916\n",
            "3849\n",
            "21200\n",
            "5102\n",
            "48737\n",
            "12429\n",
            "46915\n",
            "45478\n",
            "17611\n",
            "42014\n",
            "49069\n",
            "19401\n",
            "43849\n",
            "22605\n",
            "3293\n",
            "48485\n",
            "40034\n",
            "17251\n",
            "36367\n",
            "4807\n",
            "29930\n",
            "34997\n",
            "10291\n",
            "38469\n",
            "3352\n",
            "40206\n",
            "8638\n",
            "20024\n",
            "48676\n",
            "25186\n",
            "8942\n",
            "19462\n",
            "17596\n",
            "36204\n",
            "4647\n",
            "6300\n",
            "19020\n",
            "40292\n",
            "18894\n",
            "21481\n",
            "5766\n",
            "41570\n",
            "33886\n",
            "12627\n",
            "8016\n",
            "17611\n",
            "21749\n",
            "34817\n",
            "44529\n",
            "41185\n",
            "40431\n",
            "16272\n",
            "19004\n",
            "19617\n",
            "21171\n",
            "3293\n",
            "12675\n",
            "8603\n",
            "14460\n",
            "10422\n",
            "36367\n",
            "24204\n",
            "43471\n",
            "35824\n",
            "48870\n",
            "7761\n",
            "41185\n",
            "26555\n",
            "16272\n",
            "29172\n",
            "25304\n",
            "8117\n",
            "5293\n",
            "7678\n",
            "17313\n",
            "44721\n",
            "32199\n",
            "3444\n",
            "24970\n",
            "19020\n",
            "40292\n",
            "5293\n",
            "6762\n",
            "14594\n",
            "49069\n",
            "36397\n",
            "19020\n",
            "44557\n",
            "8920\n",
            "31976\n",
            "20770\n",
            "35732\n",
            "36122\n",
            "48870\n",
            "6669\n",
            "35616\n",
            "20770\n",
            "49084\n",
            "46997\n",
            "35226\n",
            "41185\n",
            "35892\n",
            "8411\n",
            "32199\n",
            "872\n",
            "15182\n",
            "26441\n",
            "12872\n",
            "39147\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [2752, 5102, 46837, 4961, 18894, 19020, 12675, 49069, 20372, 46837, 16753, 39852, 44433, 44918, 10003, 23275, 4647, 37915, 779, 42014, 16552, 6369, 17313, 44433, 15318, 43346, 26461, 6968, 3444, 37915, 23275, 35250, 41765, 46426, 3941, 40966, 18894, 4961, 46837, 37478, 21171, 29999, 29131, 32438, 37478, 22367, 34634, 38393, 39852, 48107, 6968, 42062, 11850, 33574, 36684, 16272, 17596, 23275, 35250, 13935, 8920, 16753, 3941, 18894, 40206, 29999, 29366, 36684, 19884, 29380, 36675, 17251, 8942, 12670, 45699, 41185, 11850, 8429, 43947, 15318, 10003, 4807, 2584, 35732, 27847, 46426, 10291, 41765, 26070, 24970, 37219, 4916, 3849, 21200, 5102, 48737, 12429, 46915, 45478, 17611, 42014, 49069, 19401, 43849, 22605, 3293, 48485, 40034, 17251, 36367, 4807, 29930, 34997, 10291, 38469, 3352, 40206, 8638, 20024, 48676, 25186, 8942, 19462, 17596, 36204, 4647, 6300, 19020, 40292, 18894, 21481, 5766, 41570, 33886, 12627, 8016, 17611, 21749, 34817, 44529, 41185, 40431, 16272, 19004, 19617, 21171, 3293, 12675, 8603, 14460, 10422, 36367, 24204, 43471, 35824, 48870, 7761, 41185, 26555, 16272, 29172, 25304, 8117, 5293, 7678, 17313, 44721, 32199, 3444, 24970, 19020, 40292, 5293, 6762, 14594, 49069, 36397, 19020, 44557, 8920, 31976, 20770, 35732, 36122, 48870, 6669, 35616, 20770, 49084, 46997, 35226, 41185, 35892, 8411, 32199, 872, 15182, 26441, 12872, 39147]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f7765374e90>\n",
            "Constructing exemplars of class 18\n",
            "47373\n",
            "41532\n",
            "9605\n",
            "48232\n",
            "42736\n",
            "33131\n",
            "14694\n",
            "32258\n",
            "16017\n",
            "27799\n",
            "17061\n",
            "12009\n",
            "17748\n",
            "2435\n",
            "49129\n",
            "37203\n",
            "21552\n",
            "24271\n",
            "20871\n",
            "36687\n",
            "16527\n",
            "22981\n",
            "2854\n",
            "49129\n",
            "31800\n",
            "2811\n",
            "33587\n",
            "6764\n",
            "2854\n",
            "28047\n",
            "49433\n",
            "13477\n",
            "33288\n",
            "31289\n",
            "43649\n",
            "47884\n",
            "36385\n",
            "29811\n",
            "35061\n",
            "27830\n",
            "39440\n",
            "30216\n",
            "2811\n",
            "27943\n",
            "38719\n",
            "32101\n",
            "41094\n",
            "31800\n",
            "49129\n",
            "9605\n",
            "46500\n",
            "9068\n",
            "18932\n",
            "12619\n",
            "3114\n",
            "47098\n",
            "16929\n",
            "20143\n",
            "21552\n",
            "28829\n",
            "28519\n",
            "21348\n",
            "27613\n",
            "33131\n",
            "27943\n",
            "952\n",
            "38498\n",
            "16527\n",
            "36385\n",
            "11808\n",
            "2854\n",
            "24328\n",
            "27830\n",
            "20543\n",
            "4461\n",
            "6764\n",
            "37638\n",
            "8218\n",
            "17055\n",
            "41532\n",
            "47373\n",
            "28047\n",
            "30654\n",
            "35061\n",
            "11159\n",
            "31800\n",
            "49129\n",
            "17561\n",
            "33587\n",
            "35917\n",
            "31962\n",
            "23555\n",
            "38716\n",
            "47090\n",
            "48232\n",
            "32101\n",
            "28047\n",
            "19197\n",
            "2811\n",
            "27943\n",
            "21348\n",
            "46854\n",
            "37016\n",
            "35651\n",
            "41576\n",
            "49129\n",
            "32963\n",
            "14776\n",
            "36385\n",
            "10462\n",
            "21190\n",
            "35004\n",
            "7649\n",
            "13913\n",
            "3556\n",
            "31289\n",
            "27343\n",
            "2003\n",
            "43059\n",
            "41344\n",
            "33587\n",
            "32963\n",
            "21732\n",
            "6431\n",
            "38448\n",
            "31512\n",
            "26947\n",
            "35651\n",
            "28528\n",
            "14512\n",
            "26101\n",
            "4605\n",
            "43586\n",
            "2557\n",
            "18683\n",
            "42153\n",
            "38498\n",
            "32258\n",
            "40907\n",
            "37225\n",
            "24252\n",
            "41344\n",
            "11385\n",
            "13732\n",
            "47373\n",
            "23555\n",
            "41302\n",
            "2435\n",
            "1242\n",
            "7104\n",
            "14102\n",
            "41094\n",
            "14694\n",
            "14776\n",
            "49145\n",
            "4107\n",
            "9670\n",
            "40946\n",
            "16929\n",
            "45162\n",
            "5809\n",
            "41949\n",
            "31962\n",
            "30580\n",
            "32827\n",
            "4605\n",
            "8639\n",
            "1859\n",
            "30654\n",
            "18500\n",
            "14102\n",
            "30216\n",
            "48232\n",
            "45598\n",
            "41302\n",
            "20948\n",
            "33587\n",
            "13477\n",
            "10993\n",
            "38535\n",
            "32775\n",
            "36791\n",
            "41405\n",
            "28519\n",
            "17232\n",
            "16017\n",
            "39706\n",
            "31800\n",
            "47750\n",
            "12009\n",
            "35395\n",
            "46753\n",
            "48232\n",
            "32775\n",
            "16017\n",
            "32827\n",
            "2003\n",
            "20871\n",
            "42406\n",
            "14776\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [47373, 41532, 9605, 48232, 42736, 33131, 14694, 32258, 16017, 27799, 17061, 12009, 17748, 2435, 49129, 37203, 21552, 24271, 20871, 36687, 16527, 22981, 2854, 49129, 31800, 2811, 33587, 6764, 2854, 28047, 49433, 13477, 33288, 31289, 43649, 47884, 36385, 29811, 35061, 27830, 39440, 30216, 2811, 27943, 38719, 32101, 41094, 31800, 49129, 9605, 46500, 9068, 18932, 12619, 3114, 47098, 16929, 20143, 21552, 28829, 28519, 21348, 27613, 33131, 27943, 952, 38498, 16527, 36385, 11808, 2854, 24328, 27830, 20543, 4461, 6764, 37638, 8218, 17055, 41532, 47373, 28047, 30654, 35061, 11159, 31800, 49129, 17561, 33587, 35917, 31962, 23555, 38716, 47090, 48232, 32101, 28047, 19197, 2811, 27943, 21348, 46854, 37016, 35651, 41576, 49129, 32963, 14776, 36385, 10462, 21190, 35004, 7649, 13913, 3556, 31289, 27343, 2003, 43059, 41344, 33587, 32963, 21732, 6431, 38448, 31512, 26947, 35651, 28528, 14512, 26101, 4605, 43586, 2557, 18683, 42153, 38498, 32258, 40907, 37225, 24252, 41344, 11385, 13732, 47373, 23555, 41302, 2435, 1242, 7104, 14102, 41094, 14694, 14776, 49145, 4107, 9670, 40946, 16929, 45162, 5809, 41949, 31962, 30580, 32827, 4605, 8639, 1859, 30654, 18500, 14102, 30216, 48232, 45598, 41302, 20948, 33587, 13477, 10993, 38535, 32775, 36791, 41405, 28519, 17232, 16017, 39706, 31800, 47750, 12009, 35395, 46753, 48232, 32775, 16017, 32827, 2003, 20871, 42406, 14776]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f7765374050>\n",
            "Constructing exemplars of class 65\n",
            "43618\n",
            "20016\n",
            "7789\n",
            "47616\n",
            "47714\n",
            "42353\n",
            "24955\n",
            "36776\n",
            "20934\n",
            "6708\n",
            "17388\n",
            "48126\n",
            "39907\n",
            "432\n",
            "22746\n",
            "26182\n",
            "47616\n",
            "48692\n",
            "28750\n",
            "34206\n",
            "7789\n",
            "49845\n",
            "5103\n",
            "20016\n",
            "29952\n",
            "27378\n",
            "33835\n",
            "24997\n",
            "7789\n",
            "47616\n",
            "32767\n",
            "25843\n",
            "20934\n",
            "41727\n",
            "47714\n",
            "43296\n",
            "47616\n",
            "37992\n",
            "26627\n",
            "7973\n",
            "7323\n",
            "21902\n",
            "2972\n",
            "25214\n",
            "19895\n",
            "22341\n",
            "48829\n",
            "47714\n",
            "38560\n",
            "36020\n",
            "18977\n",
            "42353\n",
            "41417\n",
            "47714\n",
            "6157\n",
            "34105\n",
            "33472\n",
            "37992\n",
            "47616\n",
            "8255\n",
            "17685\n",
            "47720\n",
            "18570\n",
            "41417\n",
            "22123\n",
            "24050\n",
            "432\n",
            "43296\n",
            "11569\n",
            "20321\n",
            "17685\n",
            "47616\n",
            "8225\n",
            "20934\n",
            "43176\n",
            "16875\n",
            "10962\n",
            "47714\n",
            "1175\n",
            "37153\n",
            "33472\n",
            "27493\n",
            "2175\n",
            "9979\n",
            "25214\n",
            "46148\n",
            "31693\n",
            "24775\n",
            "5505\n",
            "20579\n",
            "14308\n",
            "1314\n",
            "46170\n",
            "27378\n",
            "1982\n",
            "28375\n",
            "11925\n",
            "49210\n",
            "4977\n",
            "21639\n",
            "47872\n",
            "25843\n",
            "7563\n",
            "41727\n",
            "11415\n",
            "24997\n",
            "28750\n",
            "35733\n",
            "24597\n",
            "48829\n",
            "26205\n",
            "20934\n",
            "18570\n",
            "39907\n",
            "32767\n",
            "16875\n",
            "210\n",
            "47616\n",
            "24997\n",
            "43135\n",
            "36787\n",
            "20321\n",
            "27358\n",
            "23670\n",
            "11415\n",
            "13805\n",
            "31693\n",
            "44923\n",
            "48692\n",
            "12304\n",
            "1175\n",
            "47616\n",
            "365\n",
            "23825\n",
            "45463\n",
            "6157\n",
            "15296\n",
            "41212\n",
            "6398\n",
            "365\n",
            "9460\n",
            "27378\n",
            "22220\n",
            "14887\n",
            "21135\n",
            "7973\n",
            "41212\n",
            "615\n",
            "20321\n",
            "16779\n",
            "28543\n",
            "22801\n",
            "38115\n",
            "37153\n",
            "35969\n",
            "49170\n",
            "16779\n",
            "20321\n",
            "5103\n",
            "33364\n",
            "46043\n",
            "12304\n",
            "38262\n",
            "6157\n",
            "16099\n",
            "34962\n",
            "19956\n",
            "8808\n",
            "24597\n",
            "32767\n",
            "27378\n",
            "44836\n",
            "1917\n",
            "25106\n",
            "6255\n",
            "1738\n",
            "21347\n",
            "19161\n",
            "33366\n",
            "28855\n",
            "19220\n",
            "19564\n",
            "21135\n",
            "49572\n",
            "13187\n",
            "2424\n",
            "49845\n",
            "432\n",
            "8666\n",
            "17905\n",
            "1738\n",
            "24050\n",
            "30661\n",
            "16679\n",
            "48174\n",
            "10579\n",
            "1917\n",
            "44836\n",
            "28750\n",
            "20321\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [43618, 20016, 7789, 47616, 47714, 42353, 24955, 36776, 20934, 6708, 17388, 48126, 39907, 432, 22746, 26182, 47616, 48692, 28750, 34206, 7789, 49845, 5103, 20016, 29952, 27378, 33835, 24997, 7789, 47616, 32767, 25843, 20934, 41727, 47714, 43296, 47616, 37992, 26627, 7973, 7323, 21902, 2972, 25214, 19895, 22341, 48829, 47714, 38560, 36020, 18977, 42353, 41417, 47714, 6157, 34105, 33472, 37992, 47616, 8255, 17685, 47720, 18570, 41417, 22123, 24050, 432, 43296, 11569, 20321, 17685, 47616, 8225, 20934, 43176, 16875, 10962, 47714, 1175, 37153, 33472, 27493, 2175, 9979, 25214, 46148, 31693, 24775, 5505, 20579, 14308, 1314, 46170, 27378, 1982, 28375, 11925, 49210, 4977, 21639, 47872, 25843, 7563, 41727, 11415, 24997, 28750, 35733, 24597, 48829, 26205, 20934, 18570, 39907, 32767, 16875, 210, 47616, 24997, 43135, 36787, 20321, 27358, 23670, 11415, 13805, 31693, 44923, 48692, 12304, 1175, 47616, 365, 23825, 45463, 6157, 15296, 41212, 6398, 365, 9460, 27378, 22220, 14887, 21135, 7973, 41212, 615, 20321, 16779, 28543, 22801, 38115, 37153, 35969, 49170, 16779, 20321, 5103, 33364, 46043, 12304, 38262, 6157, 16099, 34962, 19956, 8808, 24597, 32767, 27378, 44836, 1917, 25106, 6255, 1738, 21347, 19161, 33366, 28855, 19220, 19564, 21135, 49572, 13187, 2424, 49845, 432, 8666, 17905, 1738, 24050, 30661, 16679, 48174, 10579, 1917, 44836, 28750, 20321]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f7767afb3d0>\n",
            "Constructing exemplars of class 49\n",
            "28054\n",
            "22164\n",
            "28201\n",
            "15401\n",
            "15215\n",
            "2737\n",
            "43883\n",
            "23742\n",
            "6245\n",
            "3029\n",
            "35767\n",
            "22292\n",
            "38938\n",
            "4974\n",
            "47556\n",
            "11652\n",
            "40747\n",
            "12345\n",
            "38562\n",
            "13215\n",
            "49274\n",
            "43713\n",
            "37237\n",
            "24785\n",
            "45848\n",
            "8987\n",
            "14533\n",
            "22278\n",
            "29051\n",
            "33106\n",
            "1140\n",
            "5662\n",
            "19257\n",
            "3638\n",
            "28691\n",
            "17737\n",
            "18001\n",
            "19394\n",
            "46075\n",
            "41037\n",
            "2219\n",
            "10609\n",
            "47041\n",
            "45760\n",
            "33562\n",
            "18001\n",
            "44670\n",
            "47556\n",
            "36523\n",
            "28201\n",
            "41037\n",
            "23219\n",
            "48754\n",
            "28054\n",
            "46075\n",
            "11397\n",
            "18189\n",
            "31578\n",
            "6747\n",
            "34801\n",
            "13663\n",
            "42339\n",
            "41037\n",
            "35731\n",
            "42803\n",
            "7441\n",
            "15215\n",
            "27888\n",
            "27170\n",
            "8181\n",
            "45848\n",
            "35731\n",
            "25633\n",
            "30220\n",
            "7773\n",
            "12631\n",
            "8987\n",
            "11731\n",
            "28054\n",
            "32135\n",
            "5529\n",
            "23966\n",
            "11442\n",
            "49274\n",
            "2219\n",
            "39644\n",
            "46767\n",
            "5908\n",
            "39978\n",
            "5662\n",
            "17229\n",
            "28054\n",
            "31578\n",
            "14246\n",
            "45760\n",
            "13062\n",
            "32386\n",
            "46075\n",
            "30596\n",
            "46595\n",
            "41228\n",
            "47461\n",
            "4264\n",
            "40890\n",
            "33744\n",
            "25127\n",
            "11652\n",
            "23516\n",
            "36778\n",
            "23438\n",
            "5908\n",
            "46815\n",
            "39798\n",
            "4791\n",
            "9911\n",
            "10609\n",
            "42339\n",
            "4264\n",
            "2612\n",
            "32735\n",
            "47582\n",
            "37237\n",
            "29567\n",
            "25127\n",
            "7441\n",
            "12345\n",
            "17737\n",
            "8856\n",
            "23219\n",
            "34224\n",
            "2262\n",
            "2920\n",
            "36778\n",
            "9824\n",
            "12555\n",
            "31358\n",
            "38402\n",
            "46815\n",
            "28691\n",
            "36009\n",
            "46639\n",
            "45650\n",
            "49274\n",
            "47582\n",
            "19048\n",
            "2844\n",
            "6245\n",
            "49043\n",
            "38402\n",
            "11308\n",
            "17229\n",
            "5662\n",
            "49966\n",
            "32260\n",
            "47186\n",
            "37237\n",
            "7209\n",
            "38402\n",
            "22247\n",
            "37168\n",
            "23821\n",
            "27276\n",
            "14406\n",
            "3127\n",
            "31439\n",
            "43738\n",
            "6117\n",
            "40747\n",
            "18189\n",
            "41037\n",
            "15100\n",
            "31401\n",
            "35915\n",
            "23528\n",
            "32260\n",
            "45369\n",
            "12860\n",
            "25127\n",
            "7441\n",
            "42803\n",
            "23966\n",
            "4264\n",
            "6111\n",
            "2236\n",
            "33106\n",
            "29051\n",
            "2612\n",
            "36523\n",
            "36778\n",
            "41037\n",
            "19257\n",
            "30913\n",
            "28201\n",
            "7188\n",
            "43017\n",
            "11694\n",
            "19048\n",
            "2181\n",
            "34303\n",
            "10680\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [28054, 22164, 28201, 15401, 15215, 2737, 43883, 23742, 6245, 3029, 35767, 22292, 38938, 4974, 47556, 11652, 40747, 12345, 38562, 13215, 49274, 43713, 37237, 24785, 45848, 8987, 14533, 22278, 29051, 33106, 1140, 5662, 19257, 3638, 28691, 17737, 18001, 19394, 46075, 41037, 2219, 10609, 47041, 45760, 33562, 18001, 44670, 47556, 36523, 28201, 41037, 23219, 48754, 28054, 46075, 11397, 18189, 31578, 6747, 34801, 13663, 42339, 41037, 35731, 42803, 7441, 15215, 27888, 27170, 8181, 45848, 35731, 25633, 30220, 7773, 12631, 8987, 11731, 28054, 32135, 5529, 23966, 11442, 49274, 2219, 39644, 46767, 5908, 39978, 5662, 17229, 28054, 31578, 14246, 45760, 13062, 32386, 46075, 30596, 46595, 41228, 47461, 4264, 40890, 33744, 25127, 11652, 23516, 36778, 23438, 5908, 46815, 39798, 4791, 9911, 10609, 42339, 4264, 2612, 32735, 47582, 37237, 29567, 25127, 7441, 12345, 17737, 8856, 23219, 34224, 2262, 2920, 36778, 9824, 12555, 31358, 38402, 46815, 28691, 36009, 46639, 45650, 49274, 47582, 19048, 2844, 6245, 49043, 38402, 11308, 17229, 5662, 49966, 32260, 47186, 37237, 7209, 38402, 22247, 37168, 23821, 27276, 14406, 3127, 31439, 43738, 6117, 40747, 18189, 41037, 15100, 31401, 35915, 23528, 32260, 45369, 12860, 25127, 7441, 42803, 23966, 4264, 6111, 2236, 33106, 29051, 2612, 36523, 36778, 41037, 19257, 30913, 28201, 7188, 43017, 11694, 19048, 2181, 34303, 10680]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f7765106190>\n",
            "Constructing exemplars of class 56\n",
            "39054\n",
            "32509\n",
            "19886\n",
            "17692\n",
            "29286\n",
            "21594\n",
            "5361\n",
            "23382\n",
            "45627\n",
            "31644\n",
            "21127\n",
            "22971\n",
            "19240\n",
            "30453\n",
            "3044\n",
            "20393\n",
            "1468\n",
            "33536\n",
            "30453\n",
            "4868\n",
            "29375\n",
            "11376\n",
            "14334\n",
            "18867\n",
            "19946\n",
            "26825\n",
            "16233\n",
            "36623\n",
            "26492\n",
            "39054\n",
            "2918\n",
            "277\n",
            "19246\n",
            "40710\n",
            "26150\n",
            "41899\n",
            "5233\n",
            "36137\n",
            "38176\n",
            "28854\n",
            "8628\n",
            "29456\n",
            "41663\n",
            "21594\n",
            "29639\n",
            "13406\n",
            "17692\n",
            "19240\n",
            "12244\n",
            "21240\n",
            "34304\n",
            "19681\n",
            "787\n",
            "44147\n",
            "18867\n",
            "14334\n",
            "15000\n",
            "47761\n",
            "14825\n",
            "7507\n",
            "12603\n",
            "4523\n",
            "38176\n",
            "45966\n",
            "2001\n",
            "4845\n",
            "37089\n",
            "21020\n",
            "689\n",
            "16963\n",
            "39531\n",
            "5929\n",
            "28131\n",
            "43001\n",
            "2196\n",
            "44674\n",
            "2001\n",
            "18192\n",
            "44648\n",
            "6310\n",
            "43671\n",
            "5429\n",
            "12603\n",
            "28828\n",
            "14334\n",
            "6829\n",
            "47336\n",
            "15023\n",
            "8378\n",
            "45218\n",
            "6486\n",
            "1234\n",
            "41663\n",
            "21127\n",
            "8663\n",
            "33738\n",
            "19084\n",
            "48429\n",
            "31671\n",
            "10274\n",
            "21240\n",
            "44674\n",
            "29375\n",
            "22627\n",
            "13908\n",
            "2327\n",
            "4868\n",
            "27997\n",
            "43869\n",
            "39350\n",
            "7507\n",
            "3044\n",
            "32509\n",
            "37089\n",
            "2327\n",
            "28828\n",
            "29110\n",
            "42473\n",
            "14423\n",
            "32509\n",
            "19886\n",
            "13406\n",
            "44068\n",
            "6290\n",
            "9491\n",
            "43869\n",
            "22829\n",
            "43001\n",
            "277\n",
            "39350\n",
            "43308\n",
            "39940\n",
            "36137\n",
            "21799\n",
            "36578\n",
            "37352\n",
            "47761\n",
            "46755\n",
            "35305\n",
            "20393\n",
            "19246\n",
            "9892\n",
            "41264\n",
            "36578\n",
            "31644\n",
            "49875\n",
            "45501\n",
            "19084\n",
            "19886\n",
            "47761\n",
            "39054\n",
            "8628\n",
            "39721\n",
            "6486\n",
            "38176\n",
            "29456\n",
            "25826\n",
            "27075\n",
            "23461\n",
            "47455\n",
            "35585\n",
            "41899\n",
            "31644\n",
            "24037\n",
            "4868\n",
            "36442\n",
            "45911\n",
            "22401\n",
            "1862\n",
            "4493\n",
            "38889\n",
            "20482\n",
            "47336\n",
            "37352\n",
            "33799\n",
            "24629\n",
            "43374\n",
            "27069\n",
            "5316\n",
            "18984\n",
            "37259\n",
            "17692\n",
            "10274\n",
            "23930\n",
            "42907\n",
            "19946\n",
            "26492\n",
            "28131\n",
            "4788\n",
            "39531\n",
            "43633\n",
            "31185\n",
            "43308\n",
            "5429\n",
            "23486\n",
            "14825\n",
            "36623\n",
            "19681\n",
            "33536\n",
            "16924\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [39054, 32509, 19886, 17692, 29286, 21594, 5361, 23382, 45627, 31644, 21127, 22971, 19240, 30453, 3044, 20393, 1468, 33536, 30453, 4868, 29375, 11376, 14334, 18867, 19946, 26825, 16233, 36623, 26492, 39054, 2918, 277, 19246, 40710, 26150, 41899, 5233, 36137, 38176, 28854, 8628, 29456, 41663, 21594, 29639, 13406, 17692, 19240, 12244, 21240, 34304, 19681, 787, 44147, 18867, 14334, 15000, 47761, 14825, 7507, 12603, 4523, 38176, 45966, 2001, 4845, 37089, 21020, 689, 16963, 39531, 5929, 28131, 43001, 2196, 44674, 2001, 18192, 44648, 6310, 43671, 5429, 12603, 28828, 14334, 6829, 47336, 15023, 8378, 45218, 6486, 1234, 41663, 21127, 8663, 33738, 19084, 48429, 31671, 10274, 21240, 44674, 29375, 22627, 13908, 2327, 4868, 27997, 43869, 39350, 7507, 3044, 32509, 37089, 2327, 28828, 29110, 42473, 14423, 32509, 19886, 13406, 44068, 6290, 9491, 43869, 22829, 43001, 277, 39350, 43308, 39940, 36137, 21799, 36578, 37352, 47761, 46755, 35305, 20393, 19246, 9892, 41264, 36578, 31644, 49875, 45501, 19084, 19886, 47761, 39054, 8628, 39721, 6486, 38176, 29456, 25826, 27075, 23461, 47455, 35585, 41899, 31644, 24037, 4868, 36442, 45911, 22401, 1862, 4493, 38889, 20482, 47336, 37352, 33799, 24629, 43374, 27069, 5316, 18984, 37259, 17692, 10274, 23930, 42907, 19946, 26492, 28131, 4788, 39531, 43633, 31185, 43308, 5429, 23486, 14825, 36623, 19681, 33536, 16924]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f7765095bd0>\n",
            "Constructing exemplars of class 20\n",
            "4731\n",
            "43393\n",
            "18911\n",
            "15716\n",
            "44379\n",
            "20631\n",
            "32672\n",
            "28825\n",
            "19444\n",
            "17936\n",
            "43393\n",
            "18012\n",
            "40053\n",
            "16462\n",
            "15716\n",
            "38237\n",
            "18320\n",
            "31770\n",
            "2604\n",
            "41924\n",
            "19444\n",
            "47208\n",
            "10076\n",
            "2271\n",
            "45573\n",
            "14029\n",
            "5684\n",
            "29519\n",
            "43408\n",
            "19622\n",
            "25507\n",
            "44914\n",
            "42039\n",
            "3033\n",
            "5643\n",
            "25511\n",
            "48340\n",
            "16301\n",
            "26094\n",
            "21426\n",
            "39622\n",
            "14329\n",
            "10234\n",
            "19354\n",
            "42221\n",
            "23045\n",
            "41272\n",
            "18911\n",
            "44379\n",
            "14139\n",
            "37831\n",
            "38996\n",
            "47921\n",
            "42039\n",
            "21615\n",
            "15330\n",
            "30383\n",
            "31785\n",
            "40497\n",
            "36039\n",
            "4648\n",
            "17169\n",
            "33153\n",
            "23838\n",
            "38425\n",
            "2510\n",
            "19853\n",
            "46547\n",
            "31968\n",
            "39206\n",
            "43565\n",
            "10234\n",
            "17415\n",
            "19634\n",
            "46212\n",
            "39022\n",
            "48548\n",
            "41924\n",
            "20098\n",
            "15330\n",
            "33677\n",
            "4774\n",
            "27004\n",
            "14207\n",
            "44986\n",
            "29519\n",
            "509\n",
            "2604\n",
            "16462\n",
            "21426\n",
            "39022\n",
            "35215\n",
            "23045\n",
            "10563\n",
            "35391\n",
            "4774\n",
            "2604\n",
            "20098\n",
            "2395\n",
            "10985\n",
            "5643\n",
            "25507\n",
            "8882\n",
            "20199\n",
            "37473\n",
            "21426\n",
            "39622\n",
            "44914\n",
            "17299\n",
            "44839\n",
            "30460\n",
            "45573\n",
            "26094\n",
            "33242\n",
            "47208\n",
            "5727\n",
            "18911\n",
            "34420\n",
            "37831\n",
            "36588\n",
            "18545\n",
            "5643\n",
            "43710\n",
            "23838\n",
            "44914\n",
            "38996\n",
            "12010\n",
            "12903\n",
            "25504\n",
            "19354\n",
            "10234\n",
            "5643\n",
            "14207\n",
            "1505\n",
            "14329\n",
            "32395\n",
            "2627\n",
            "2020\n",
            "25513\n",
            "33619\n",
            "14089\n",
            "17936\n",
            "38234\n",
            "57\n",
            "49657\n",
            "31770\n",
            "26768\n",
            "3158\n",
            "1505\n",
            "14207\n",
            "1768\n",
            "48239\n",
            "29259\n",
            "36514\n",
            "42719\n",
            "26613\n",
            "15654\n",
            "37657\n",
            "11785\n",
            "30425\n",
            "35391\n",
            "17483\n",
            "22230\n",
            "1383\n",
            "46677\n",
            "2655\n",
            "26058\n",
            "12831\n",
            "26120\n",
            "23602\n",
            "10269\n",
            "47138\n",
            "57\n",
            "34122\n",
            "38234\n",
            "5284\n",
            "34855\n",
            "14329\n",
            "36039\n",
            "7800\n",
            "7813\n",
            "16775\n",
            "19634\n",
            "45453\n",
            "9507\n",
            "10397\n",
            "2655\n",
            "31197\n",
            "39151\n",
            "1768\n",
            "44111\n",
            "37537\n",
            "1377\n",
            "5846\n",
            "37473\n",
            "27880\n",
            "3197\n",
            "48548\n",
            "7205\n",
            "12010\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [4731, 43393, 18911, 15716, 44379, 20631, 32672, 28825, 19444, 17936, 43393, 18012, 40053, 16462, 15716, 38237, 18320, 31770, 2604, 41924, 19444, 47208, 10076, 2271, 45573, 14029, 5684, 29519, 43408, 19622, 25507, 44914, 42039, 3033, 5643, 25511, 48340, 16301, 26094, 21426, 39622, 14329, 10234, 19354, 42221, 23045, 41272, 18911, 44379, 14139, 37831, 38996, 47921, 42039, 21615, 15330, 30383, 31785, 40497, 36039, 4648, 17169, 33153, 23838, 38425, 2510, 19853, 46547, 31968, 39206, 43565, 10234, 17415, 19634, 46212, 39022, 48548, 41924, 20098, 15330, 33677, 4774, 27004, 14207, 44986, 29519, 509, 2604, 16462, 21426, 39022, 35215, 23045, 10563, 35391, 4774, 2604, 20098, 2395, 10985, 5643, 25507, 8882, 20199, 37473, 21426, 39622, 44914, 17299, 44839, 30460, 45573, 26094, 33242, 47208, 5727, 18911, 34420, 37831, 36588, 18545, 5643, 43710, 23838, 44914, 38996, 12010, 12903, 25504, 19354, 10234, 5643, 14207, 1505, 14329, 32395, 2627, 2020, 25513, 33619, 14089, 17936, 38234, 57, 49657, 31770, 26768, 3158, 1505, 14207, 1768, 48239, 29259, 36514, 42719, 26613, 15654, 37657, 11785, 30425, 35391, 17483, 22230, 1383, 46677, 2655, 26058, 12831, 26120, 23602, 10269, 47138, 57, 34122, 38234, 5284, 34855, 14329, 36039, 7800, 7813, 16775, 19634, 45453, 9507, 10397, 2655, 31197, 39151, 1768, 44111, 37537, 1377, 5846, 37473, 27880, 3197, 48548, 7205, 12010]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f7765376790>\n",
            "Constructing exemplars of class 4\n",
            "5841\n",
            "36339\n",
            "5493\n",
            "37609\n",
            "42842\n",
            "5034\n",
            "12963\n",
            "35028\n",
            "36339\n",
            "651\n",
            "3067\n",
            "15063\n",
            "40204\n",
            "36030\n",
            "43974\n",
            "14691\n",
            "15423\n",
            "49483\n",
            "11182\n",
            "33791\n",
            "46335\n",
            "16148\n",
            "10243\n",
            "6854\n",
            "10882\n",
            "14058\n",
            "27514\n",
            "39400\n",
            "3310\n",
            "7819\n",
            "35028\n",
            "35576\n",
            "993\n",
            "1892\n",
            "15063\n",
            "13029\n",
            "32066\n",
            "38364\n",
            "26476\n",
            "1308\n",
            "38364\n",
            "10066\n",
            "12928\n",
            "12078\n",
            "47410\n",
            "16974\n",
            "17903\n",
            "38364\n",
            "36339\n",
            "1205\n",
            "35028\n",
            "37075\n",
            "3310\n",
            "40917\n",
            "37227\n",
            "17509\n",
            "2601\n",
            "43420\n",
            "45545\n",
            "39748\n",
            "37547\n",
            "33177\n",
            "40048\n",
            "27514\n",
            "43098\n",
            "35028\n",
            "12963\n",
            "10099\n",
            "10882\n",
            "22782\n",
            "32066\n",
            "20970\n",
            "5034\n",
            "12963\n",
            "15442\n",
            "13029\n",
            "43098\n",
            "33177\n",
            "49483\n",
            "10074\n",
            "16148\n",
            "46335\n",
            "13029\n",
            "27551\n",
            "22736\n",
            "42842\n",
            "22450\n",
            "41665\n",
            "10882\n",
            "8616\n",
            "3423\n",
            "12078\n",
            "36241\n",
            "25413\n",
            "39400\n",
            "21342\n",
            "34784\n",
            "41557\n",
            "7290\n",
            "46589\n",
            "27761\n",
            "20078\n",
            "7015\n",
            "47355\n",
            "37339\n",
            "4813\n",
            "49483\n",
            "16974\n",
            "36339\n",
            "47109\n",
            "41995\n",
            "26714\n",
            "43260\n",
            "5273\n",
            "41304\n",
            "46155\n",
            "27514\n",
            "17903\n",
            "41904\n",
            "16148\n",
            "10074\n",
            "14691\n",
            "33791\n",
            "36030\n",
            "27514\n",
            "40558\n",
            "39748\n",
            "2711\n",
            "10074\n",
            "27551\n",
            "16148\n",
            "21109\n",
            "42937\n",
            "20039\n",
            "6834\n",
            "33272\n",
            "34784\n",
            "10074\n",
            "13029\n",
            "42467\n",
            "25134\n",
            "35028\n",
            "40048\n",
            "27514\n",
            "7819\n",
            "10104\n",
            "48971\n",
            "32876\n",
            "49929\n",
            "10650\n",
            "17509\n",
            "44162\n",
            "38039\n",
            "21579\n",
            "27019\n",
            "13029\n",
            "8616\n",
            "14691\n",
            "3423\n",
            "32117\n",
            "10243\n",
            "46138\n",
            "42844\n",
            "14549\n",
            "27551\n",
            "44237\n",
            "19620\n",
            "32837\n",
            "38272\n",
            "18460\n",
            "15442\n",
            "37075\n",
            "44104\n",
            "14436\n",
            "48926\n",
            "10104\n",
            "48971\n",
            "17802\n",
            "19094\n",
            "14727\n",
            "27551\n",
            "3230\n",
            "36030\n",
            "36753\n",
            "13442\n",
            "1205\n",
            "12928\n",
            "29036\n",
            "36124\n",
            "2582\n",
            "40753\n",
            "41403\n",
            "27678\n",
            "1729\n",
            "6514\n",
            "16148\n",
            "10074\n",
            "33791\n",
            "43943\n",
            "4304\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [5841, 36339, 5493, 37609, 42842, 5034, 12963, 35028, 36339, 651, 3067, 15063, 40204, 36030, 43974, 14691, 15423, 49483, 11182, 33791, 46335, 16148, 10243, 6854, 10882, 14058, 27514, 39400, 3310, 7819, 35028, 35576, 993, 1892, 15063, 13029, 32066, 38364, 26476, 1308, 38364, 10066, 12928, 12078, 47410, 16974, 17903, 38364, 36339, 1205, 35028, 37075, 3310, 40917, 37227, 17509, 2601, 43420, 45545, 39748, 37547, 33177, 40048, 27514, 43098, 35028, 12963, 10099, 10882, 22782, 32066, 20970, 5034, 12963, 15442, 13029, 43098, 33177, 49483, 10074, 16148, 46335, 13029, 27551, 22736, 42842, 22450, 41665, 10882, 8616, 3423, 12078, 36241, 25413, 39400, 21342, 34784, 41557, 7290, 46589, 27761, 20078, 7015, 47355, 37339, 4813, 49483, 16974, 36339, 47109, 41995, 26714, 43260, 5273, 41304, 46155, 27514, 17903, 41904, 16148, 10074, 14691, 33791, 36030, 27514, 40558, 39748, 2711, 10074, 27551, 16148, 21109, 42937, 20039, 6834, 33272, 34784, 10074, 13029, 42467, 25134, 35028, 40048, 27514, 7819, 10104, 48971, 32876, 49929, 10650, 17509, 44162, 38039, 21579, 27019, 13029, 8616, 14691, 3423, 32117, 10243, 46138, 42844, 14549, 27551, 44237, 19620, 32837, 38272, 18460, 15442, 37075, 44104, 14436, 48926, 10104, 48971, 17802, 19094, 14727, 27551, 3230, 36030, 36753, 13442, 1205, 12928, 29036, 36124, 2582, 40753, 41403, 27678, 1729, 6514, 16148, 10074, 33791, 43943, 4304]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.64 0.7235572934150696\n",
            "TEST GROUP:  0.674\n",
            "TEST ALL:  0.674\n",
            "TRAIN:  4950\n",
            "GROUP:  2\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  20\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.45557519793510437\n",
            "Train step - Step 10, Loss 0.1726565808057785\n",
            "Train step - Step 20, Loss 0.15242813527584076\n",
            "Train step - Step 30, Loss 0.11602232605218887\n",
            "Train step - Step 40, Loss 0.10313083231449127\n",
            "Train step - Step 50, Loss 0.0963713750243187\n",
            "Train epoch - Accuracy: 0.3631654676258993 Loss: 0.1490527396021987 Corrects: 2524\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.09568889439105988\n",
            "Train step - Step 70, Loss 0.09347295016050339\n",
            "Train step - Step 80, Loss 0.10698993504047394\n",
            "Train step - Step 90, Loss 0.09863292425870895\n",
            "Train step - Step 100, Loss 0.09792041778564453\n",
            "Train epoch - Accuracy: 0.5210071942446043 Loss: 0.09608016249301622 Corrects: 3621\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.08940746635198593\n",
            "Train step - Step 120, Loss 0.07614759355783463\n",
            "Train step - Step 130, Loss 0.08495568484067917\n",
            "Train step - Step 140, Loss 0.08451243489980698\n",
            "Train step - Step 150, Loss 0.08402632921934128\n",
            "Train step - Step 160, Loss 0.08188223838806152\n",
            "Train epoch - Accuracy: 0.5646043165467626 Loss: 0.08691266136203739 Corrects: 3924\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.08542726188898087\n",
            "Train step - Step 180, Loss 0.09449271857738495\n",
            "Train step - Step 190, Loss 0.0711749941110611\n",
            "Train step - Step 200, Loss 0.0757090300321579\n",
            "Train step - Step 210, Loss 0.07719923555850983\n",
            "Train epoch - Accuracy: 0.58 Loss: 0.0821588155434286 Corrects: 4031\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.07539545744657516\n",
            "Train step - Step 230, Loss 0.0812288150191307\n",
            "Train step - Step 240, Loss 0.07748778909444809\n",
            "Train step - Step 250, Loss 0.07468803972005844\n",
            "Train step - Step 260, Loss 0.09378980845212936\n",
            "Train step - Step 270, Loss 0.08850035816431046\n",
            "Train epoch - Accuracy: 0.6044604316546762 Loss: 0.07845580761595596 Corrects: 4201\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.0809483453631401\n",
            "Train step - Step 290, Loss 0.07395654916763306\n",
            "Train step - Step 300, Loss 0.07496380060911179\n",
            "Train step - Step 310, Loss 0.07076459378004074\n",
            "Train step - Step 320, Loss 0.08065231889486313\n",
            "Train epoch - Accuracy: 0.6240287769784173 Loss: 0.07568532707451059 Corrects: 4337\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.07811956107616425\n",
            "Train step - Step 340, Loss 0.07782164961099625\n",
            "Train step - Step 350, Loss 0.07825392484664917\n",
            "Train step - Step 360, Loss 0.06834884732961655\n",
            "Train step - Step 370, Loss 0.0674164891242981\n",
            "Train step - Step 380, Loss 0.08009760081768036\n",
            "Train epoch - Accuracy: 0.6317985611510791 Loss: 0.07273562591924942 Corrects: 4391\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.08330186456441879\n",
            "Train step - Step 400, Loss 0.06961464136838913\n",
            "Train step - Step 410, Loss 0.06545151025056839\n",
            "Train step - Step 420, Loss 0.0701669231057167\n",
            "Train step - Step 430, Loss 0.0746433213353157\n",
            "Train epoch - Accuracy: 0.6458992805755396 Loss: 0.07099918428299239 Corrects: 4489\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.06747505813837051\n",
            "Train step - Step 450, Loss 0.0783541351556778\n",
            "Train step - Step 460, Loss 0.06880780309438705\n",
            "Train step - Step 470, Loss 0.06359027326107025\n",
            "Train step - Step 480, Loss 0.05672171711921692\n",
            "Train step - Step 490, Loss 0.0749838575720787\n",
            "Train epoch - Accuracy: 0.6683453237410072 Loss: 0.0677943182238143 Corrects: 4645\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.06922011822462082\n",
            "Train step - Step 510, Loss 0.0671117827296257\n",
            "Train step - Step 520, Loss 0.0645856186747551\n",
            "Train step - Step 530, Loss 0.06818769127130508\n",
            "Train step - Step 540, Loss 0.06399587541818619\n",
            "Train epoch - Accuracy: 0.6728057553956834 Loss: 0.06697223085936882 Corrects: 4676\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.06916066259145737\n",
            "Train step - Step 560, Loss 0.06317796558141708\n",
            "Train step - Step 570, Loss 0.06102646142244339\n",
            "Train step - Step 580, Loss 0.06494811922311783\n",
            "Train step - Step 590, Loss 0.06949399411678314\n",
            "Train step - Step 600, Loss 0.06471562385559082\n",
            "Train epoch - Accuracy: 0.6792805755395683 Loss: 0.06490973989097334 Corrects: 4721\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.06264472007751465\n",
            "Train step - Step 620, Loss 0.07373915612697601\n",
            "Train step - Step 630, Loss 0.0604812391102314\n",
            "Train step - Step 640, Loss 0.059888195246458054\n",
            "Train step - Step 650, Loss 0.06173209100961685\n",
            "Train epoch - Accuracy: 0.683021582733813 Loss: 0.06458058392830032 Corrects: 4747\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.055373501032590866\n",
            "Train step - Step 670, Loss 0.06472428143024445\n",
            "Train step - Step 680, Loss 0.060257602483034134\n",
            "Train step - Step 690, Loss 0.05790267139673233\n",
            "Train step - Step 700, Loss 0.07725442945957184\n",
            "Train step - Step 710, Loss 0.06278526037931442\n",
            "Train epoch - Accuracy: 0.6959712230215828 Loss: 0.06219463811075087 Corrects: 4837\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.06428474187850952\n",
            "Train step - Step 730, Loss 0.0670597180724144\n",
            "Train step - Step 740, Loss 0.05769491195678711\n",
            "Train step - Step 750, Loss 0.05383230000734329\n",
            "Train step - Step 760, Loss 0.06689554452896118\n",
            "Train epoch - Accuracy: 0.6979856115107914 Loss: 0.06124361244251402 Corrects: 4851\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.06106729432940483\n",
            "Train step - Step 780, Loss 0.06162933260202408\n",
            "Train step - Step 790, Loss 0.0631859228014946\n",
            "Train step - Step 800, Loss 0.05527184531092644\n",
            "Train step - Step 810, Loss 0.057262301445007324\n",
            "Train step - Step 820, Loss 0.07594757527112961\n",
            "Train epoch - Accuracy: 0.7034532374100719 Loss: 0.06043678235879047 Corrects: 4889\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.04961349815130234\n",
            "Train step - Step 840, Loss 0.05875897407531738\n",
            "Train step - Step 850, Loss 0.0695512443780899\n",
            "Train step - Step 860, Loss 0.05828620120882988\n",
            "Train step - Step 870, Loss 0.056852925568819046\n",
            "Train epoch - Accuracy: 0.7093525179856115 Loss: 0.059086146060940176 Corrects: 4930\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.061854757368564606\n",
            "Train step - Step 890, Loss 0.06744755059480667\n",
            "Train step - Step 900, Loss 0.05651189759373665\n",
            "Train step - Step 910, Loss 0.057151924818754196\n",
            "Train step - Step 920, Loss 0.05383417755365372\n",
            "Train step - Step 930, Loss 0.05740555748343468\n",
            "Train epoch - Accuracy: 0.7220143884892086 Loss: 0.0585303289298531 Corrects: 5018\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.05559270456433296\n",
            "Train step - Step 950, Loss 0.05949797108769417\n",
            "Train step - Step 960, Loss 0.05537449195981026\n",
            "Train step - Step 970, Loss 0.058301445096731186\n",
            "Train step - Step 980, Loss 0.06276652961969376\n",
            "Train epoch - Accuracy: 0.722158273381295 Loss: 0.05745934764472701 Corrects: 5019\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.05661342293024063\n",
            "Train step - Step 1000, Loss 0.05472058057785034\n",
            "Train step - Step 1010, Loss 0.05525153875350952\n",
            "Train step - Step 1020, Loss 0.05674537643790245\n",
            "Train step - Step 1030, Loss 0.06009797379374504\n",
            "Train step - Step 1040, Loss 0.04628105089068413\n",
            "Train epoch - Accuracy: 0.7253237410071942 Loss: 0.05626823317339952 Corrects: 5041\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.05415533855557442\n",
            "Train step - Step 1060, Loss 0.058873701840639114\n",
            "Train step - Step 1070, Loss 0.06429418176412582\n",
            "Train step - Step 1080, Loss 0.04976396635174751\n",
            "Train step - Step 1090, Loss 0.04988210275769234\n",
            "Train epoch - Accuracy: 0.7322302158273382 Loss: 0.054814142711299786 Corrects: 5089\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.051866352558135986\n",
            "Train step - Step 1110, Loss 0.05106217414140701\n",
            "Train step - Step 1120, Loss 0.04983757436275482\n",
            "Train step - Step 1130, Loss 0.0684247612953186\n",
            "Train step - Step 1140, Loss 0.057454705238342285\n",
            "Train step - Step 1150, Loss 0.05218682438135147\n",
            "Train epoch - Accuracy: 0.7332374100719424 Loss: 0.05460143083934304 Corrects: 5096\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.05580432340502739\n",
            "Train step - Step 1170, Loss 0.055938221514225006\n",
            "Train step - Step 1180, Loss 0.0550493486225605\n",
            "Train step - Step 1190, Loss 0.055494535714387894\n",
            "Train step - Step 1200, Loss 0.05736343935132027\n",
            "Train epoch - Accuracy: 0.74 Loss: 0.05390271813106194 Corrects: 5143\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.045590806752443314\n",
            "Train step - Step 1220, Loss 0.05020821839570999\n",
            "Train step - Step 1230, Loss 0.05654053017497063\n",
            "Train step - Step 1240, Loss 0.055154383182525635\n",
            "Train step - Step 1250, Loss 0.05305163934826851\n",
            "Train step - Step 1260, Loss 0.061750032007694244\n",
            "Train epoch - Accuracy: 0.7435971223021582 Loss: 0.0524083786827626 Corrects: 5168\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.04570404812693596\n",
            "Train step - Step 1280, Loss 0.04590066149830818\n",
            "Train step - Step 1290, Loss 0.05446058511734009\n",
            "Train step - Step 1300, Loss 0.047960638999938965\n",
            "Train step - Step 1310, Loss 0.051065899431705475\n",
            "Train epoch - Accuracy: 0.7448920863309353 Loss: 0.0520676313715873 Corrects: 5177\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.05766329914331436\n",
            "Train step - Step 1330, Loss 0.05298826843500137\n",
            "Train step - Step 1340, Loss 0.04750221222639084\n",
            "Train step - Step 1350, Loss 0.04557911679148674\n",
            "Train step - Step 1360, Loss 0.05621127039194107\n",
            "Train step - Step 1370, Loss 0.05318170785903931\n",
            "Train epoch - Accuracy: 0.7517985611510791 Loss: 0.0516955711708652 Corrects: 5225\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.0429237000644207\n",
            "Train step - Step 1390, Loss 0.05361580476164818\n",
            "Train step - Step 1400, Loss 0.04598640277981758\n",
            "Train step - Step 1410, Loss 0.04699079319834709\n",
            "Train step - Step 1420, Loss 0.04913041368126869\n",
            "Train epoch - Accuracy: 0.7559712230215827 Loss: 0.051069715441130904 Corrects: 5254\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.04516365006566048\n",
            "Train step - Step 1440, Loss 0.05002933740615845\n",
            "Train step - Step 1450, Loss 0.042401354759931564\n",
            "Train step - Step 1460, Loss 0.04522266983985901\n",
            "Train step - Step 1470, Loss 0.05903029441833496\n",
            "Train step - Step 1480, Loss 0.04870588332414627\n",
            "Train epoch - Accuracy: 0.7543884892086331 Loss: 0.05150030511103088 Corrects: 5243\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.0499013252556324\n",
            "Train step - Step 1500, Loss 0.055931176990270615\n",
            "Train step - Step 1510, Loss 0.05428152158856392\n",
            "Train step - Step 1520, Loss 0.05108986049890518\n",
            "Train step - Step 1530, Loss 0.044878799468278885\n",
            "Train epoch - Accuracy: 0.7641726618705036 Loss: 0.04854964014223154 Corrects: 5311\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.04737138748168945\n",
            "Train step - Step 1550, Loss 0.052340395748615265\n",
            "Train step - Step 1560, Loss 0.05071974918246269\n",
            "Train step - Step 1570, Loss 0.05713118240237236\n",
            "Train step - Step 1580, Loss 0.060265399515628815\n",
            "Train step - Step 1590, Loss 0.042809948325157166\n",
            "Train epoch - Accuracy: 0.7612949640287769 Loss: 0.049503622040045346 Corrects: 5291\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.055158983916044235\n",
            "Train step - Step 1610, Loss 0.04894817993044853\n",
            "Train step - Step 1620, Loss 0.04967944324016571\n",
            "Train step - Step 1630, Loss 0.04723717272281647\n",
            "Train step - Step 1640, Loss 0.050937145948410034\n",
            "Train epoch - Accuracy: 0.7624460431654676 Loss: 0.04900462461461266 Corrects: 5299\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.04972963407635689\n",
            "Train step - Step 1660, Loss 0.05126292258501053\n",
            "Train step - Step 1670, Loss 0.05252757668495178\n",
            "Train step - Step 1680, Loss 0.05118752643465996\n",
            "Train step - Step 1690, Loss 0.056469596922397614\n",
            "Train step - Step 1700, Loss 0.04081618785858154\n",
            "Train epoch - Accuracy: 0.7693525179856116 Loss: 0.047872416158374266 Corrects: 5347\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.04123701527714729\n",
            "Train step - Step 1720, Loss 0.05126313120126724\n",
            "Train step - Step 1730, Loss 0.048947688192129135\n",
            "Train step - Step 1740, Loss 0.04549640044569969\n",
            "Train step - Step 1750, Loss 0.0489189438521862\n",
            "Train epoch - Accuracy: 0.7689208633093525 Loss: 0.04741950822819909 Corrects: 5344\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.04824231192469597\n",
            "Train step - Step 1770, Loss 0.04910900816321373\n",
            "Train step - Step 1780, Loss 0.05228276178240776\n",
            "Train step - Step 1790, Loss 0.04734523221850395\n",
            "Train step - Step 1800, Loss 0.049696046859025955\n",
            "Train step - Step 1810, Loss 0.039962537586688995\n",
            "Train epoch - Accuracy: 0.7741007194244605 Loss: 0.04643305649753097 Corrects: 5380\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.04135188087821007\n",
            "Train step - Step 1830, Loss 0.052044641226530075\n",
            "Train step - Step 1840, Loss 0.05175979062914848\n",
            "Train step - Step 1850, Loss 0.054081737995147705\n",
            "Train step - Step 1860, Loss 0.04399966821074486\n",
            "Train epoch - Accuracy: 0.7784172661870503 Loss: 0.045275454763457075 Corrects: 5410\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.0384141206741333\n",
            "Train step - Step 1880, Loss 0.044049471616744995\n",
            "Train step - Step 1890, Loss 0.04896789789199829\n",
            "Train step - Step 1900, Loss 0.047562941908836365\n",
            "Train step - Step 1910, Loss 0.0465858094394207\n",
            "Train step - Step 1920, Loss 0.040455836802721024\n",
            "Train epoch - Accuracy: 0.778273381294964 Loss: 0.04422334522866517 Corrects: 5409\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.04554113745689392\n",
            "Train step - Step 1940, Loss 0.04191511496901512\n",
            "Train step - Step 1950, Loss 0.04992504045367241\n",
            "Train step - Step 1960, Loss 0.03673266991972923\n",
            "Train step - Step 1970, Loss 0.036062587052583694\n",
            "Train epoch - Accuracy: 0.7856115107913669 Loss: 0.04375236574051192 Corrects: 5460\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.04133415222167969\n",
            "Train step - Step 1990, Loss 0.0436847098171711\n",
            "Train step - Step 2000, Loss 0.050853438675403595\n",
            "Train step - Step 2010, Loss 0.04183870926499367\n",
            "Train step - Step 2020, Loss 0.04316435009241104\n",
            "Train step - Step 2030, Loss 0.04441378638148308\n",
            "Train epoch - Accuracy: 0.7877697841726619 Loss: 0.04385529842093694 Corrects: 5475\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.04248690605163574\n",
            "Train step - Step 2050, Loss 0.04650045558810234\n",
            "Train step - Step 2060, Loss 0.03947607800364494\n",
            "Train step - Step 2070, Loss 0.04119792953133583\n",
            "Train step - Step 2080, Loss 0.04180533438920975\n",
            "Train epoch - Accuracy: 0.7907913669064748 Loss: 0.04334411149831127 Corrects: 5496\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.03667742386460304\n",
            "Train step - Step 2100, Loss 0.041005607694387436\n",
            "Train step - Step 2110, Loss 0.04640373960137367\n",
            "Train step - Step 2120, Loss 0.04682229086756706\n",
            "Train step - Step 2130, Loss 0.04716692492365837\n",
            "Train step - Step 2140, Loss 0.036355920135974884\n",
            "Train epoch - Accuracy: 0.7913669064748201 Loss: 0.04251017457075256 Corrects: 5500\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.036793433129787445\n",
            "Train step - Step 2160, Loss 0.03869069740176201\n",
            "Train step - Step 2170, Loss 0.03803384676575661\n",
            "Train step - Step 2180, Loss 0.04052959010004997\n",
            "Train step - Step 2190, Loss 0.04471418261528015\n",
            "Train epoch - Accuracy: 0.7853237410071943 Loss: 0.04281460281327474 Corrects: 5458\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.04029972106218338\n",
            "Train step - Step 2210, Loss 0.034336887300014496\n",
            "Train step - Step 2220, Loss 0.04102104529738426\n",
            "Train step - Step 2230, Loss 0.042273927479982376\n",
            "Train step - Step 2240, Loss 0.04560023546218872\n",
            "Train step - Step 2250, Loss 0.0403602235019207\n",
            "Train epoch - Accuracy: 0.7962589928057554 Loss: 0.04161339699257192 Corrects: 5534\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.037106357514858246\n",
            "Train step - Step 2270, Loss 0.038429565727710724\n",
            "Train step - Step 2280, Loss 0.033480074256658554\n",
            "Train step - Step 2290, Loss 0.03762660548090935\n",
            "Train step - Step 2300, Loss 0.042817652225494385\n",
            "Train epoch - Accuracy: 0.7971223021582734 Loss: 0.041050695687961235 Corrects: 5540\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.03593466430902481\n",
            "Train step - Step 2320, Loss 0.04223963990807533\n",
            "Train step - Step 2330, Loss 0.03980064019560814\n",
            "Train step - Step 2340, Loss 0.03805815801024437\n",
            "Train step - Step 2350, Loss 0.04834146052598953\n",
            "Train step - Step 2360, Loss 0.03562149405479431\n",
            "Train epoch - Accuracy: 0.7992805755395683 Loss: 0.04091403431600804 Corrects: 5555\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.04081125184893608\n",
            "Train step - Step 2380, Loss 0.04152403026819229\n",
            "Train step - Step 2390, Loss 0.034191716462373734\n",
            "Train step - Step 2400, Loss 0.03982660174369812\n",
            "Train step - Step 2410, Loss 0.034190669655799866\n",
            "Train epoch - Accuracy: 0.8034532374100719 Loss: 0.04013753864083359 Corrects: 5584\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.03537542000412941\n",
            "Train step - Step 2430, Loss 0.03515351191163063\n",
            "Train step - Step 2440, Loss 0.03854807838797569\n",
            "Train step - Step 2450, Loss 0.036396484822034836\n",
            "Train step - Step 2460, Loss 0.034384872764348984\n",
            "Train step - Step 2470, Loss 0.04292522743344307\n",
            "Train epoch - Accuracy: 0.8076258992805755 Loss: 0.039415068037861545 Corrects: 5613\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.03689577430486679\n",
            "Train step - Step 2490, Loss 0.041051819920539856\n",
            "Train step - Step 2500, Loss 0.039951957762241364\n",
            "Train step - Step 2510, Loss 0.04396095499396324\n",
            "Train step - Step 2520, Loss 0.04255502298474312\n",
            "Train epoch - Accuracy: 0.8076258992805755 Loss: 0.03990699940984198 Corrects: 5613\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.04367116838693619\n",
            "Train step - Step 2540, Loss 0.036602236330509186\n",
            "Train step - Step 2550, Loss 0.03565463051199913\n",
            "Train step - Step 2560, Loss 0.03534926101565361\n",
            "Train step - Step 2570, Loss 0.037076301872730255\n",
            "Train step - Step 2580, Loss 0.038875870406627655\n",
            "Train epoch - Accuracy: 0.8125179856115108 Loss: 0.03823901811842438 Corrects: 5647\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.04501832649111748\n",
            "Train step - Step 2600, Loss 0.04242256283760071\n",
            "Train step - Step 2610, Loss 0.04336841031908989\n",
            "Train step - Step 2620, Loss 0.036040231585502625\n",
            "Train step - Step 2630, Loss 0.03628559410572052\n",
            "Train epoch - Accuracy: 0.8080575539568345 Loss: 0.03889021113943711 Corrects: 5616\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.03858521580696106\n",
            "Train step - Step 2650, Loss 0.03362731263041496\n",
            "Train step - Step 2660, Loss 0.045284952968358994\n",
            "Train step - Step 2670, Loss 0.038949158042669296\n",
            "Train step - Step 2680, Loss 0.04930984601378441\n",
            "Train step - Step 2690, Loss 0.038233015686273575\n",
            "Train epoch - Accuracy: 0.8129496402877698 Loss: 0.038742255101101006 Corrects: 5650\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.0389353483915329\n",
            "Train step - Step 2710, Loss 0.03959670662879944\n",
            "Train step - Step 2720, Loss 0.030152974650263786\n",
            "Train step - Step 2730, Loss 0.036223798990249634\n",
            "Train step - Step 2740, Loss 0.030435774475336075\n",
            "Train epoch - Accuracy: 0.8302158273381295 Loss: 0.03416759208273545 Corrects: 5770\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.02598227560520172\n",
            "Train step - Step 2760, Loss 0.03251754119992256\n",
            "Train step - Step 2770, Loss 0.03545473515987396\n",
            "Train step - Step 2780, Loss 0.029024137184023857\n",
            "Train step - Step 2790, Loss 0.03635653853416443\n",
            "Train step - Step 2800, Loss 0.041732218116521835\n",
            "Train epoch - Accuracy: 0.8336690647482015 Loss: 0.032267558334542694 Corrects: 5794\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.027276022359728813\n",
            "Train step - Step 2820, Loss 0.02903260663151741\n",
            "Train step - Step 2830, Loss 0.03354182466864586\n",
            "Train step - Step 2840, Loss 0.033193569630384445\n",
            "Train step - Step 2850, Loss 0.029367273673415184\n",
            "Train epoch - Accuracy: 0.840431654676259 Loss: 0.03187997572606416 Corrects: 5841\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.035951387137174606\n",
            "Train step - Step 2870, Loss 0.03431401401758194\n",
            "Train step - Step 2880, Loss 0.03408801928162575\n",
            "Train step - Step 2890, Loss 0.03529922291636467\n",
            "Train step - Step 2900, Loss 0.027849627658724785\n",
            "Train step - Step 2910, Loss 0.026562271639704704\n",
            "Train epoch - Accuracy: 0.8389928057553957 Loss: 0.031958264883259216 Corrects: 5831\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.03015897236764431\n",
            "Train step - Step 2930, Loss 0.027961531654000282\n",
            "Train step - Step 2940, Loss 0.032890111207962036\n",
            "Train step - Step 2950, Loss 0.03598254173994064\n",
            "Train step - Step 2960, Loss 0.03027709759771824\n",
            "Train epoch - Accuracy: 0.8467625899280575 Loss: 0.03051415461644852 Corrects: 5885\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.02554214559495449\n",
            "Train step - Step 2980, Loss 0.036009084433317184\n",
            "Train step - Step 2990, Loss 0.033558644354343414\n",
            "Train step - Step 3000, Loss 0.03214390575885773\n",
            "Train step - Step 3010, Loss 0.029977500438690186\n",
            "Train step - Step 3020, Loss 0.03719145059585571\n",
            "Train epoch - Accuracy: 0.8431654676258993 Loss: 0.031085941997363413 Corrects: 5860\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.03027833066880703\n",
            "Train step - Step 3040, Loss 0.02713766135275364\n",
            "Train step - Step 3050, Loss 0.03035067953169346\n",
            "Train step - Step 3060, Loss 0.034940313547849655\n",
            "Train step - Step 3070, Loss 0.029005859047174454\n",
            "Train epoch - Accuracy: 0.8443165467625899 Loss: 0.030623233076050985 Corrects: 5868\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.027087915688753128\n",
            "Train step - Step 3090, Loss 0.029288023710250854\n",
            "Train step - Step 3100, Loss 0.027387380599975586\n",
            "Train step - Step 3110, Loss 0.03234405443072319\n",
            "Train step - Step 3120, Loss 0.02597779594361782\n",
            "Train step - Step 3130, Loss 0.027094781398773193\n",
            "Train epoch - Accuracy: 0.8451798561151079 Loss: 0.030754147288825015 Corrects: 5874\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.029157206416130066\n",
            "Train step - Step 3150, Loss 0.030172983184456825\n",
            "Train step - Step 3160, Loss 0.025990474969148636\n",
            "Train step - Step 3170, Loss 0.02465463988482952\n",
            "Train step - Step 3180, Loss 0.027606314048171043\n",
            "Train epoch - Accuracy: 0.8437410071942446 Loss: 0.03040014253406645 Corrects: 5864\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.0291591826826334\n",
            "Train step - Step 3200, Loss 0.028883948922157288\n",
            "Train step - Step 3210, Loss 0.028063928708434105\n",
            "Train step - Step 3220, Loss 0.027369512245059013\n",
            "Train step - Step 3230, Loss 0.034024160355329514\n",
            "Train step - Step 3240, Loss 0.032259222120046616\n",
            "Train epoch - Accuracy: 0.8471942446043166 Loss: 0.029697481130631708 Corrects: 5888\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.028147155418992043\n",
            "Train step - Step 3260, Loss 0.029239142313599586\n",
            "Train step - Step 3270, Loss 0.031541209667921066\n",
            "Train step - Step 3280, Loss 0.02799326740205288\n",
            "Train step - Step 3290, Loss 0.027488404884934425\n",
            "Train epoch - Accuracy: 0.8515107913669064 Loss: 0.029636750690799823 Corrects: 5918\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.027970826253294945\n",
            "Train step - Step 3310, Loss 0.028433704748749733\n",
            "Train step - Step 3320, Loss 0.032925795763731\n",
            "Train step - Step 3330, Loss 0.026593396440148354\n",
            "Train step - Step 3340, Loss 0.0249013751745224\n",
            "Train step - Step 3350, Loss 0.0349966399371624\n",
            "Train epoch - Accuracy: 0.8483453237410072 Loss: 0.02999686598777771 Corrects: 5896\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.025070274248719215\n",
            "Train step - Step 3370, Loss 0.033284951001405716\n",
            "Train step - Step 3380, Loss 0.026805182918906212\n",
            "Train step - Step 3390, Loss 0.028184814378619194\n",
            "Train step - Step 3400, Loss 0.02922062948346138\n",
            "Train epoch - Accuracy: 0.8492086330935252 Loss: 0.03005560704272428 Corrects: 5902\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.0322481207549572\n",
            "Train step - Step 3420, Loss 0.02616349793970585\n",
            "Train step - Step 3430, Loss 0.03221748024225235\n",
            "Train step - Step 3440, Loss 0.026800721883773804\n",
            "Train step - Step 3450, Loss 0.026476789265871048\n",
            "Train step - Step 3460, Loss 0.030959684401750565\n",
            "Train epoch - Accuracy: 0.8515107913669064 Loss: 0.030387125499814534 Corrects: 5918\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.02654942311346531\n",
            "Train step - Step 3480, Loss 0.030690524727106094\n",
            "Train step - Step 3490, Loss 0.03045305423438549\n",
            "Train step - Step 3500, Loss 0.02864476852118969\n",
            "Train step - Step 3510, Loss 0.03209839388728142\n",
            "Train epoch - Accuracy: 0.8493525179856115 Loss: 0.02893515695663665 Corrects: 5903\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.028151053935289383\n",
            "Train step - Step 3530, Loss 0.02861682139337063\n",
            "Train step - Step 3540, Loss 0.026101816445589066\n",
            "Train step - Step 3550, Loss 0.027168452739715576\n",
            "Train step - Step 3560, Loss 0.029207630082964897\n",
            "Train step - Step 3570, Loss 0.027058949694037437\n",
            "Train epoch - Accuracy: 0.8525179856115108 Loss: 0.02896503912887985 Corrects: 5925\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.025723133236169815\n",
            "Train step - Step 3590, Loss 0.02995750866830349\n",
            "Train step - Step 3600, Loss 0.02910599112510681\n",
            "Train step - Step 3610, Loss 0.029378658160567284\n",
            "Train step - Step 3620, Loss 0.02594071999192238\n",
            "Train epoch - Accuracy: 0.8561151079136691 Loss: 0.028348231514473615 Corrects: 5950\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.03096984140574932\n",
            "Train step - Step 3640, Loss 0.024692131206393242\n",
            "Train step - Step 3650, Loss 0.029377315193414688\n",
            "Train step - Step 3660, Loss 0.02705465629696846\n",
            "Train step - Step 3670, Loss 0.029976803809404373\n",
            "Train step - Step 3680, Loss 0.02210366539657116\n",
            "Train epoch - Accuracy: 0.8561151079136691 Loss: 0.028102395713007707 Corrects: 5950\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.028569335117936134\n",
            "Train step - Step 3700, Loss 0.027610307559370995\n",
            "Train step - Step 3710, Loss 0.025208139792084694\n",
            "Train step - Step 3720, Loss 0.03129865601658821\n",
            "Train step - Step 3730, Loss 0.026553494855761528\n",
            "Train epoch - Accuracy: 0.8522302158273382 Loss: 0.028445963991953316 Corrects: 5923\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.025557411834597588\n",
            "Train step - Step 3750, Loss 0.026682600378990173\n",
            "Train step - Step 3760, Loss 0.03474465757608414\n",
            "Train step - Step 3770, Loss 0.025360524654388428\n",
            "Train step - Step 3780, Loss 0.02548098936676979\n",
            "Train step - Step 3790, Loss 0.030834997072815895\n",
            "Train epoch - Accuracy: 0.8505035971223022 Loss: 0.028333639142324597 Corrects: 5911\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.02955758012831211\n",
            "Train step - Step 3810, Loss 0.029880527406930923\n",
            "Train step - Step 3820, Loss 0.029650544747710228\n",
            "Train step - Step 3830, Loss 0.027607006952166557\n",
            "Train step - Step 3840, Loss 0.03268864005804062\n",
            "Train epoch - Accuracy: 0.8543884892086331 Loss: 0.0281854169949782 Corrects: 5938\n",
            "Training finished in 435.1786689758301 seconds\n",
            "reducing exemplars for each class\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f77650ddd50>\n",
            "Constructing exemplars of class 79\n",
            "1977\n",
            "38273\n",
            "3302\n",
            "46969\n",
            "7940\n",
            "13207\n",
            "40590\n",
            "1449\n",
            "24240\n",
            "37142\n",
            "12495\n",
            "32435\n",
            "47658\n",
            "19630\n",
            "10315\n",
            "46033\n",
            "45237\n",
            "29504\n",
            "20012\n",
            "1317\n",
            "45237\n",
            "11123\n",
            "39494\n",
            "12765\n",
            "32281\n",
            "25875\n",
            "23184\n",
            "31489\n",
            "45521\n",
            "23707\n",
            "29243\n",
            "1977\n",
            "45237\n",
            "20345\n",
            "4827\n",
            "20202\n",
            "8586\n",
            "13555\n",
            "11310\n",
            "45237\n",
            "21378\n",
            "941\n",
            "21801\n",
            "3302\n",
            "21744\n",
            "13365\n",
            "23184\n",
            "25832\n",
            "43811\n",
            "29775\n",
            "17088\n",
            "44728\n",
            "6842\n",
            "8336\n",
            "37582\n",
            "26638\n",
            "13846\n",
            "27841\n",
            "25172\n",
            "11310\n",
            "31813\n",
            "20345\n",
            "37891\n",
            "38218\n",
            "20202\n",
            "1710\n",
            "7940\n",
            "13207\n",
            "48101\n",
            "4237\n",
            "35060\n",
            "44802\n",
            "34943\n",
            "11123\n",
            "26936\n",
            "36130\n",
            "157\n",
            "29775\n",
            "18647\n",
            "13555\n",
            "29605\n",
            "6842\n",
            "44728\n",
            "26733\n",
            "30381\n",
            "1710\n",
            "36865\n",
            "34350\n",
            "47658\n",
            "5507\n",
            "28216\n",
            "25832\n",
            "43811\n",
            "16868\n",
            "6494\n",
            "34009\n",
            "17492\n",
            "25004\n",
            "44728\n",
            "20202\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [1977, 38273, 3302, 46969, 7940, 13207, 40590, 1449, 24240, 37142, 12495, 32435, 47658, 19630, 10315, 46033, 45237, 29504, 20012, 1317, 45237, 11123, 39494, 12765, 32281, 25875, 23184, 31489, 45521, 23707, 29243, 1977, 45237, 20345, 4827, 20202, 8586, 13555, 11310, 45237, 21378, 941, 21801, 3302, 21744, 13365, 23184, 25832, 43811, 29775, 17088, 44728, 6842, 8336, 37582, 26638, 13846, 27841, 25172, 11310, 31813, 20345, 37891, 38218, 20202, 1710, 7940, 13207, 48101, 4237, 35060, 44802, 34943, 11123, 26936, 36130, 157, 29775, 18647, 13555, 29605, 6842, 44728, 26733, 30381, 1710, 36865, 34350, 47658, 5507, 28216, 25832, 43811, 16868, 6494, 34009, 17492, 25004, 44728, 20202]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f77651060d0>\n",
            "Constructing exemplars of class 47\n",
            "21281\n",
            "13857\n",
            "36872\n",
            "47283\n",
            "17953\n",
            "45365\n",
            "14026\n",
            "32828\n",
            "22067\n",
            "16867\n",
            "14043\n",
            "5345\n",
            "20152\n",
            "40647\n",
            "17064\n",
            "9817\n",
            "6659\n",
            "7519\n",
            "45783\n",
            "33014\n",
            "13857\n",
            "48084\n",
            "33768\n",
            "8653\n",
            "16066\n",
            "21281\n",
            "8844\n",
            "49096\n",
            "41117\n",
            "10124\n",
            "16937\n",
            "10894\n",
            "49650\n",
            "44256\n",
            "9555\n",
            "15845\n",
            "44251\n",
            "17966\n",
            "44246\n",
            "13857\n",
            "47992\n",
            "30233\n",
            "1196\n",
            "11141\n",
            "25313\n",
            "38946\n",
            "9555\n",
            "21281\n",
            "43237\n",
            "40647\n",
            "13857\n",
            "45403\n",
            "33768\n",
            "8653\n",
            "56\n",
            "42078\n",
            "23074\n",
            "33168\n",
            "6659\n",
            "13291\n",
            "42412\n",
            "1628\n",
            "18966\n",
            "25024\n",
            "21281\n",
            "21027\n",
            "8037\n",
            "11960\n",
            "49096\n",
            "16873\n",
            "26437\n",
            "38231\n",
            "13291\n",
            "42412\n",
            "5911\n",
            "47283\n",
            "13857\n",
            "36872\n",
            "15845\n",
            "33768\n",
            "40637\n",
            "7519\n",
            "44341\n",
            "45403\n",
            "33733\n",
            "1121\n",
            "8653\n",
            "16066\n",
            "10323\n",
            "47598\n",
            "6773\n",
            "27083\n",
            "8185\n",
            "9817\n",
            "1519\n",
            "13857\n",
            "16873\n",
            "17926\n",
            "25421\n",
            "16867\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [21281, 13857, 36872, 47283, 17953, 45365, 14026, 32828, 22067, 16867, 14043, 5345, 20152, 40647, 17064, 9817, 6659, 7519, 45783, 33014, 13857, 48084, 33768, 8653, 16066, 21281, 8844, 49096, 41117, 10124, 16937, 10894, 49650, 44256, 9555, 15845, 44251, 17966, 44246, 13857, 47992, 30233, 1196, 11141, 25313, 38946, 9555, 21281, 43237, 40647, 13857, 45403, 33768, 8653, 56, 42078, 23074, 33168, 6659, 13291, 42412, 1628, 18966, 25024, 21281, 21027, 8037, 11960, 49096, 16873, 26437, 38231, 13291, 42412, 5911, 47283, 13857, 36872, 15845, 33768, 40637, 7519, 44341, 45403, 33733, 1121, 8653, 16066, 10323, 47598, 6773, 27083, 8185, 9817, 1519, 13857, 16873, 17926, 25421, 16867]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f77650fc510>\n",
            "Constructing exemplars of class 7\n",
            "31134\n",
            "38690\n",
            "11548\n",
            "10929\n",
            "49317\n",
            "34406\n",
            "46051\n",
            "10272\n",
            "15591\n",
            "15243\n",
            "30554\n",
            "29293\n",
            "43668\n",
            "13986\n",
            "36934\n",
            "20596\n",
            "19348\n",
            "13962\n",
            "2164\n",
            "13162\n",
            "41213\n",
            "36420\n",
            "27113\n",
            "19348\n",
            "10590\n",
            "9862\n",
            "34406\n",
            "46051\n",
            "30165\n",
            "30554\n",
            "4000\n",
            "44445\n",
            "43668\n",
            "17607\n",
            "21893\n",
            "45221\n",
            "36892\n",
            "27712\n",
            "2149\n",
            "43668\n",
            "49939\n",
            "43201\n",
            "28363\n",
            "13162\n",
            "33239\n",
            "41616\n",
            "36420\n",
            "17607\n",
            "3723\n",
            "25231\n",
            "1981\n",
            "41650\n",
            "35626\n",
            "44210\n",
            "37071\n",
            "38690\n",
            "11548\n",
            "10929\n",
            "33204\n",
            "31134\n",
            "15243\n",
            "15591\n",
            "41213\n",
            "23862\n",
            "19348\n",
            "38690\n",
            "44080\n",
            "21212\n",
            "37071\n",
            "46051\n",
            "13162\n",
            "38438\n",
            "3431\n",
            "49627\n",
            "31840\n",
            "15591\n",
            "49939\n",
            "16968\n",
            "2149\n",
            "41650\n",
            "18513\n",
            "42898\n",
            "31769\n",
            "4000\n",
            "18009\n",
            "46051\n",
            "47435\n",
            "37623\n",
            "21694\n",
            "21893\n",
            "49119\n",
            "29018\n",
            "46653\n",
            "18009\n",
            "46769\n",
            "37623\n",
            "20596\n",
            "21893\n",
            "35079\n",
            "37071\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [31134, 38690, 11548, 10929, 49317, 34406, 46051, 10272, 15591, 15243, 30554, 29293, 43668, 13986, 36934, 20596, 19348, 13962, 2164, 13162, 41213, 36420, 27113, 19348, 10590, 9862, 34406, 46051, 30165, 30554, 4000, 44445, 43668, 17607, 21893, 45221, 36892, 27712, 2149, 43668, 49939, 43201, 28363, 13162, 33239, 41616, 36420, 17607, 3723, 25231, 1981, 41650, 35626, 44210, 37071, 38690, 11548, 10929, 33204, 31134, 15243, 15591, 41213, 23862, 19348, 38690, 44080, 21212, 37071, 46051, 13162, 38438, 3431, 49627, 31840, 15591, 49939, 16968, 2149, 41650, 18513, 42898, 31769, 4000, 18009, 46051, 47435, 37623, 21694, 21893, 49119, 29018, 46653, 18009, 46769, 37623, 20596, 21893, 35079, 37071]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f7765295d10>\n",
            "Constructing exemplars of class 82\n",
            "20686\n",
            "31194\n",
            "43606\n",
            "32277\n",
            "12408\n",
            "3507\n",
            "4019\n",
            "37064\n",
            "41778\n",
            "10283\n",
            "34911\n",
            "20686\n",
            "32277\n",
            "16784\n",
            "5073\n",
            "47862\n",
            "23402\n",
            "46394\n",
            "41778\n",
            "38171\n",
            "32277\n",
            "46768\n",
            "28164\n",
            "37064\n",
            "10519\n",
            "14557\n",
            "31194\n",
            "20686\n",
            "32725\n",
            "3522\n",
            "31759\n",
            "1029\n",
            "6277\n",
            "34608\n",
            "22457\n",
            "466\n",
            "20686\n",
            "40409\n",
            "40408\n",
            "19657\n",
            "47957\n",
            "7471\n",
            "27297\n",
            "18646\n",
            "28164\n",
            "31815\n",
            "41181\n",
            "36688\n",
            "49832\n",
            "37064\n",
            "44112\n",
            "34088\n",
            "26921\n",
            "31249\n",
            "34911\n",
            "34608\n",
            "13318\n",
            "47862\n",
            "20686\n",
            "466\n",
            "22457\n",
            "10283\n",
            "9872\n",
            "39514\n",
            "45076\n",
            "40952\n",
            "36272\n",
            "31074\n",
            "16467\n",
            "31443\n",
            "17455\n",
            "28164\n",
            "32349\n",
            "47572\n",
            "44523\n",
            "47957\n",
            "8170\n",
            "37189\n",
            "41719\n",
            "28164\n",
            "32175\n",
            "27297\n",
            "17455\n",
            "19830\n",
            "28164\n",
            "38171\n",
            "34471\n",
            "48590\n",
            "35967\n",
            "8573\n",
            "7027\n",
            "3522\n",
            "28101\n",
            "19657\n",
            "20686\n",
            "7471\n",
            "4019\n",
            "26399\n",
            "32449\n",
            "43442\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [20686, 31194, 43606, 32277, 12408, 3507, 4019, 37064, 41778, 10283, 34911, 20686, 32277, 16784, 5073, 47862, 23402, 46394, 41778, 38171, 32277, 46768, 28164, 37064, 10519, 14557, 31194, 20686, 32725, 3522, 31759, 1029, 6277, 34608, 22457, 466, 20686, 40409, 40408, 19657, 47957, 7471, 27297, 18646, 28164, 31815, 41181, 36688, 49832, 37064, 44112, 34088, 26921, 31249, 34911, 34608, 13318, 47862, 20686, 466, 22457, 10283, 9872, 39514, 45076, 40952, 36272, 31074, 16467, 31443, 17455, 28164, 32349, 47572, 44523, 47957, 8170, 37189, 41719, 28164, 32175, 27297, 17455, 19830, 28164, 38171, 34471, 48590, 35967, 8573, 7027, 3522, 28101, 19657, 20686, 7471, 4019, 26399, 32449, 43442]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f7765109d50>\n",
            "Constructing exemplars of class 34\n",
            "29659\n",
            "6782\n",
            "25221\n",
            "3680\n",
            "27238\n",
            "29659\n",
            "43022\n",
            "25363\n",
            "15614\n",
            "2726\n",
            "18257\n",
            "43297\n",
            "26347\n",
            "32901\n",
            "5064\n",
            "19009\n",
            "10016\n",
            "39428\n",
            "29284\n",
            "13748\n",
            "38631\n",
            "21349\n",
            "48834\n",
            "16299\n",
            "31864\n",
            "31783\n",
            "5601\n",
            "1472\n",
            "29659\n",
            "6782\n",
            "1969\n",
            "21349\n",
            "43149\n",
            "29284\n",
            "7194\n",
            "31750\n",
            "29129\n",
            "46555\n",
            "15820\n",
            "27630\n",
            "30789\n",
            "30393\n",
            "15310\n",
            "30618\n",
            "29129\n",
            "7206\n",
            "48286\n",
            "35024\n",
            "6782\n",
            "32901\n",
            "48468\n",
            "32916\n",
            "42407\n",
            "31560\n",
            "21072\n",
            "18257\n",
            "2726\n",
            "15614\n",
            "49796\n",
            "3512\n",
            "21349\n",
            "42407\n",
            "34877\n",
            "40033\n",
            "4681\n",
            "15614\n",
            "20767\n",
            "21142\n",
            "31323\n",
            "27630\n",
            "28439\n",
            "46887\n",
            "32094\n",
            "15359\n",
            "32901\n",
            "3077\n",
            "26172\n",
            "29129\n",
            "16299\n",
            "264\n",
            "35524\n",
            "47241\n",
            "35334\n",
            "18893\n",
            "42407\n",
            "49796\n",
            "8949\n",
            "22595\n",
            "24376\n",
            "25221\n",
            "19814\n",
            "16299\n",
            "29284\n",
            "3680\n",
            "48048\n",
            "23992\n",
            "21662\n",
            "18275\n",
            "38200\n",
            "24224\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [29659, 6782, 25221, 3680, 27238, 29659, 43022, 25363, 15614, 2726, 18257, 43297, 26347, 32901, 5064, 19009, 10016, 39428, 29284, 13748, 38631, 21349, 48834, 16299, 31864, 31783, 5601, 1472, 29659, 6782, 1969, 21349, 43149, 29284, 7194, 31750, 29129, 46555, 15820, 27630, 30789, 30393, 15310, 30618, 29129, 7206, 48286, 35024, 6782, 32901, 48468, 32916, 42407, 31560, 21072, 18257, 2726, 15614, 49796, 3512, 21349, 42407, 34877, 40033, 4681, 15614, 20767, 21142, 31323, 27630, 28439, 46887, 32094, 15359, 32901, 3077, 26172, 29129, 16299, 264, 35524, 47241, 35334, 18893, 42407, 49796, 8949, 22595, 24376, 25221, 19814, 16299, 29284, 3680, 48048, 23992, 21662, 18275, 38200, 24224]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f776508ef10>\n",
            "Constructing exemplars of class 81\n",
            "25857\n",
            "30409\n",
            "9956\n",
            "36296\n",
            "36497\n",
            "21157\n",
            "24275\n",
            "29252\n",
            "18325\n",
            "38528\n",
            "975\n",
            "25909\n",
            "35221\n",
            "38668\n",
            "10208\n",
            "47627\n",
            "3335\n",
            "11575\n",
            "11675\n",
            "35026\n",
            "19400\n",
            "9535\n",
            "49309\n",
            "47035\n",
            "28774\n",
            "33016\n",
            "11575\n",
            "10584\n",
            "17474\n",
            "27099\n",
            "21157\n",
            "35102\n",
            "21021\n",
            "29516\n",
            "36497\n",
            "40965\n",
            "675\n",
            "48255\n",
            "27367\n",
            "44247\n",
            "37849\n",
            "1943\n",
            "49309\n",
            "6209\n",
            "17474\n",
            "20081\n",
            "9780\n",
            "18330\n",
            "1429\n",
            "21025\n",
            "31782\n",
            "36359\n",
            "14235\n",
            "29516\n",
            "29499\n",
            "21021\n",
            "13592\n",
            "11347\n",
            "49309\n",
            "19400\n",
            "27896\n",
            "22308\n",
            "503\n",
            "18708\n",
            "31655\n",
            "39292\n",
            "19400\n",
            "14809\n",
            "35221\n",
            "12077\n",
            "49473\n",
            "32660\n",
            "12661\n",
            "34026\n",
            "18037\n",
            "234\n",
            "10180\n",
            "40255\n",
            "37849\n",
            "27099\n",
            "19993\n",
            "36826\n",
            "503\n",
            "16857\n",
            "14006\n",
            "34261\n",
            "6258\n",
            "34514\n",
            "39219\n",
            "4599\n",
            "16325\n",
            "32109\n",
            "234\n",
            "23286\n",
            "13592\n",
            "3335\n",
            "24801\n",
            "18708\n",
            "26045\n",
            "49309\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [25857, 30409, 9956, 36296, 36497, 21157, 24275, 29252, 18325, 38528, 975, 25909, 35221, 38668, 10208, 47627, 3335, 11575, 11675, 35026, 19400, 9535, 49309, 47035, 28774, 33016, 11575, 10584, 17474, 27099, 21157, 35102, 21021, 29516, 36497, 40965, 675, 48255, 27367, 44247, 37849, 1943, 49309, 6209, 17474, 20081, 9780, 18330, 1429, 21025, 31782, 36359, 14235, 29516, 29499, 21021, 13592, 11347, 49309, 19400, 27896, 22308, 503, 18708, 31655, 39292, 19400, 14809, 35221, 12077, 49473, 32660, 12661, 34026, 18037, 234, 10180, 40255, 37849, 27099, 19993, 36826, 503, 16857, 14006, 34261, 6258, 34514, 39219, 4599, 16325, 32109, 234, 23286, 13592, 3335, 24801, 18708, 26045, 49309]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f776508e190>\n",
            "Constructing exemplars of class 21\n",
            "37883\n",
            "9588\n",
            "20131\n",
            "37128\n",
            "37468\n",
            "19859\n",
            "31866\n",
            "33057\n",
            "9588\n",
            "8818\n",
            "18251\n",
            "10171\n",
            "21043\n",
            "13260\n",
            "44754\n",
            "3722\n",
            "21686\n",
            "32647\n",
            "10202\n",
            "37883\n",
            "33057\n",
            "9544\n",
            "23416\n",
            "42053\n",
            "13788\n",
            "10284\n",
            "1702\n",
            "8784\n",
            "40052\n",
            "48052\n",
            "30707\n",
            "39211\n",
            "13362\n",
            "2193\n",
            "18251\n",
            "8784\n",
            "44887\n",
            "48598\n",
            "39515\n",
            "12355\n",
            "12294\n",
            "31866\n",
            "8818\n",
            "10171\n",
            "45167\n",
            "11888\n",
            "7991\n",
            "15320\n",
            "32647\n",
            "10676\n",
            "39144\n",
            "1702\n",
            "8784\n",
            "20131\n",
            "21611\n",
            "354\n",
            "42653\n",
            "34319\n",
            "19569\n",
            "29578\n",
            "25279\n",
            "17807\n",
            "39515\n",
            "354\n",
            "15877\n",
            "22919\n",
            "41054\n",
            "33578\n",
            "37994\n",
            "37883\n",
            "7680\n",
            "3722\n",
            "39743\n",
            "28564\n",
            "44316\n",
            "19041\n",
            "18137\n",
            "13788\n",
            "7702\n",
            "3900\n",
            "7991\n",
            "19278\n",
            "24902\n",
            "11031\n",
            "2555\n",
            "48058\n",
            "41589\n",
            "27406\n",
            "26684\n",
            "354\n",
            "15877\n",
            "772\n",
            "22919\n",
            "15320\n",
            "19569\n",
            "37128\n",
            "41589\n",
            "7991\n",
            "32647\n",
            "49504\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [37883, 9588, 20131, 37128, 37468, 19859, 31866, 33057, 9588, 8818, 18251, 10171, 21043, 13260, 44754, 3722, 21686, 32647, 10202, 37883, 33057, 9544, 23416, 42053, 13788, 10284, 1702, 8784, 40052, 48052, 30707, 39211, 13362, 2193, 18251, 8784, 44887, 48598, 39515, 12355, 12294, 31866, 8818, 10171, 45167, 11888, 7991, 15320, 32647, 10676, 39144, 1702, 8784, 20131, 21611, 354, 42653, 34319, 19569, 29578, 25279, 17807, 39515, 354, 15877, 22919, 41054, 33578, 37994, 37883, 7680, 3722, 39743, 28564, 44316, 19041, 18137, 13788, 7702, 3900, 7991, 19278, 24902, 11031, 2555, 48058, 41589, 27406, 26684, 354, 15877, 772, 22919, 15320, 19569, 37128, 41589, 7991, 32647, 49504]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f77651060d0>\n",
            "Constructing exemplars of class 80\n",
            "12901\n",
            "3179\n",
            "48381\n",
            "24698\n",
            "37506\n",
            "49446\n",
            "10575\n",
            "36652\n",
            "4561\n",
            "15402\n",
            "7929\n",
            "3179\n",
            "36436\n",
            "22486\n",
            "9446\n",
            "41354\n",
            "44834\n",
            "7599\n",
            "11160\n",
            "19973\n",
            "12901\n",
            "22392\n",
            "11160\n",
            "11791\n",
            "43167\n",
            "15974\n",
            "42448\n",
            "49446\n",
            "25986\n",
            "16948\n",
            "5728\n",
            "48095\n",
            "32319\n",
            "24126\n",
            "28709\n",
            "31533\n",
            "14495\n",
            "49446\n",
            "39496\n",
            "10149\n",
            "20882\n",
            "11160\n",
            "12901\n",
            "19973\n",
            "7929\n",
            "20037\n",
            "16948\n",
            "37506\n",
            "42828\n",
            "28406\n",
            "16591\n",
            "44869\n",
            "17156\n",
            "24968\n",
            "46917\n",
            "10725\n",
            "16953\n",
            "15402\n",
            "40176\n",
            "40322\n",
            "10725\n",
            "32698\n",
            "32319\n",
            "24111\n",
            "35914\n",
            "17156\n",
            "6320\n",
            "13578\n",
            "49446\n",
            "10575\n",
            "7886\n",
            "14954\n",
            "48381\n",
            "9844\n",
            "24698\n",
            "48095\n",
            "32319\n",
            "33894\n",
            "28987\n",
            "44384\n",
            "7032\n",
            "25422\n",
            "43167\n",
            "11160\n",
            "5676\n",
            "15005\n",
            "9215\n",
            "42448\n",
            "8240\n",
            "37907\n",
            "36436\n",
            "11160\n",
            "33894\n",
            "4501\n",
            "28987\n",
            "3026\n",
            "31929\n",
            "1253\n",
            "29481\n",
            "2045\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [12901, 3179, 48381, 24698, 37506, 49446, 10575, 36652, 4561, 15402, 7929, 3179, 36436, 22486, 9446, 41354, 44834, 7599, 11160, 19973, 12901, 22392, 11160, 11791, 43167, 15974, 42448, 49446, 25986, 16948, 5728, 48095, 32319, 24126, 28709, 31533, 14495, 49446, 39496, 10149, 20882, 11160, 12901, 19973, 7929, 20037, 16948, 37506, 42828, 28406, 16591, 44869, 17156, 24968, 46917, 10725, 16953, 15402, 40176, 40322, 10725, 32698, 32319, 24111, 35914, 17156, 6320, 13578, 49446, 10575, 7886, 14954, 48381, 9844, 24698, 48095, 32319, 33894, 28987, 44384, 7032, 25422, 43167, 11160, 5676, 15005, 9215, 42448, 8240, 37907, 36436, 11160, 33894, 4501, 28987, 3026, 31929, 1253, 29481, 2045]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f77650f4c50>\n",
            "Constructing exemplars of class 68\n",
            "5682\n",
            "48206\n",
            "9030\n",
            "44121\n",
            "8412\n",
            "40674\n",
            "22696\n",
            "25899\n",
            "19543\n",
            "8137\n",
            "15864\n",
            "1185\n",
            "10351\n",
            "415\n",
            "21731\n",
            "33721\n",
            "18331\n",
            "5934\n",
            "45019\n",
            "29548\n",
            "25484\n",
            "26844\n",
            "26732\n",
            "5682\n",
            "40674\n",
            "31450\n",
            "12103\n",
            "45044\n",
            "30363\n",
            "43417\n",
            "32740\n",
            "9030\n",
            "48206\n",
            "5486\n",
            "36703\n",
            "1185\n",
            "8412\n",
            "3836\n",
            "48206\n",
            "18947\n",
            "35295\n",
            "8710\n",
            "20659\n",
            "25484\n",
            "7944\n",
            "9030\n",
            "25899\n",
            "9723\n",
            "37548\n",
            "38238\n",
            "38288\n",
            "19905\n",
            "48206\n",
            "17531\n",
            "8718\n",
            "5682\n",
            "39628\n",
            "25899\n",
            "8399\n",
            "47799\n",
            "6929\n",
            "33864\n",
            "28636\n",
            "11913\n",
            "14817\n",
            "45778\n",
            "4063\n",
            "26939\n",
            "32918\n",
            "24222\n",
            "40674\n",
            "31953\n",
            "39734\n",
            "18381\n",
            "20739\n",
            "10351\n",
            "17077\n",
            "25987\n",
            "5486\n",
            "17531\n",
            "36461\n",
            "42651\n",
            "18331\n",
            "30316\n",
            "1185\n",
            "19950\n",
            "37548\n",
            "48206\n",
            "3836\n",
            "8412\n",
            "15345\n",
            "41640\n",
            "26939\n",
            "5462\n",
            "25484\n",
            "20659\n",
            "42651\n",
            "39862\n",
            "32945\n",
            "7412\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [5682, 48206, 9030, 44121, 8412, 40674, 22696, 25899, 19543, 8137, 15864, 1185, 10351, 415, 21731, 33721, 18331, 5934, 45019, 29548, 25484, 26844, 26732, 5682, 40674, 31450, 12103, 45044, 30363, 43417, 32740, 9030, 48206, 5486, 36703, 1185, 8412, 3836, 48206, 18947, 35295, 8710, 20659, 25484, 7944, 9030, 25899, 9723, 37548, 38238, 38288, 19905, 48206, 17531, 8718, 5682, 39628, 25899, 8399, 47799, 6929, 33864, 28636, 11913, 14817, 45778, 4063, 26939, 32918, 24222, 40674, 31953, 39734, 18381, 20739, 10351, 17077, 25987, 5486, 17531, 36461, 42651, 18331, 30316, 1185, 19950, 37548, 48206, 3836, 8412, 15345, 41640, 26939, 5462, 25484, 20659, 42651, 39862, 32945, 7412]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f77652fbfd0>\n",
            "Constructing exemplars of class 16\n",
            "21274\n",
            "9477\n",
            "36719\n",
            "35005\n",
            "35745\n",
            "25632\n",
            "45932\n",
            "21324\n",
            "1833\n",
            "36935\n",
            "1792\n",
            "2868\n",
            "27967\n",
            "23781\n",
            "40625\n",
            "573\n",
            "14309\n",
            "7199\n",
            "22162\n",
            "11166\n",
            "32487\n",
            "5867\n",
            "25137\n",
            "36719\n",
            "6965\n",
            "21324\n",
            "34311\n",
            "21274\n",
            "25632\n",
            "39379\n",
            "17042\n",
            "30114\n",
            "27470\n",
            "8120\n",
            "26789\n",
            "24761\n",
            "44380\n",
            "47233\n",
            "9548\n",
            "14019\n",
            "12709\n",
            "32995\n",
            "7826\n",
            "25135\n",
            "5867\n",
            "8120\n",
            "26789\n",
            "32487\n",
            "19928\n",
            "21609\n",
            "20797\n",
            "32995\n",
            "23953\n",
            "21274\n",
            "22105\n",
            "40773\n",
            "38789\n",
            "12350\n",
            "14785\n",
            "26048\n",
            "22338\n",
            "36719\n",
            "40149\n",
            "30215\n",
            "43404\n",
            "49055\n",
            "5642\n",
            "17042\n",
            "45932\n",
            "5867\n",
            "25137\n",
            "8120\n",
            "27470\n",
            "21718\n",
            "6793\n",
            "13413\n",
            "27570\n",
            "5867\n",
            "22380\n",
            "24761\n",
            "45932\n",
            "573\n",
            "14785\n",
            "22519\n",
            "48929\n",
            "21274\n",
            "35111\n",
            "14785\n",
            "15979\n",
            "21145\n",
            "17042\n",
            "19963\n",
            "14309\n",
            "37757\n",
            "47233\n",
            "32995\n",
            "37605\n",
            "27470\n",
            "20761\n",
            "24744\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [21274, 9477, 36719, 35005, 35745, 25632, 45932, 21324, 1833, 36935, 1792, 2868, 27967, 23781, 40625, 573, 14309, 7199, 22162, 11166, 32487, 5867, 25137, 36719, 6965, 21324, 34311, 21274, 25632, 39379, 17042, 30114, 27470, 8120, 26789, 24761, 44380, 47233, 9548, 14019, 12709, 32995, 7826, 25135, 5867, 8120, 26789, 32487, 19928, 21609, 20797, 32995, 23953, 21274, 22105, 40773, 38789, 12350, 14785, 26048, 22338, 36719, 40149, 30215, 43404, 49055, 5642, 17042, 45932, 5867, 25137, 8120, 27470, 21718, 6793, 13413, 27570, 5867, 22380, 24761, 45932, 573, 14785, 22519, 48929, 21274, 35111, 14785, 15979, 21145, 17042, 19963, 14309, 37757, 47233, 32995, 37605, 27470, 20761, 24744]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.42 0.5277870297431946\n",
            "TEST GROUP:  0.416\n",
            "TEST ALL:  0.4065\n",
            "TRAIN:  4950\n",
            "GROUP:  3\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "Len TOTAL train susbset:  7950\n",
            "training\n",
            "num classes till now:  30\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.3575904965400696\n",
            "Train step - Step 10, Loss 0.09328632801771164\n",
            "Train step - Step 20, Loss 0.0982017070055008\n",
            "Train step - Step 30, Loss 0.07403365522623062\n",
            "Train step - Step 40, Loss 0.08019430190324783\n",
            "Train step - Step 50, Loss 0.05999847128987312\n",
            "Train step - Step 60, Loss 0.0594242699444294\n",
            "Train epoch - Accuracy: 0.3231446540880503 Loss: 0.09601507915265906 Corrects: 2569\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 70, Loss 0.05760251730680466\n",
            "Train step - Step 80, Loss 0.06706584990024567\n",
            "Train step - Step 90, Loss 0.05107118934392929\n",
            "Train step - Step 100, Loss 0.05381156504154205\n",
            "Train step - Step 110, Loss 0.05320727452635765\n",
            "Train step - Step 120, Loss 0.05347816273570061\n",
            "Train epoch - Accuracy: 0.47874213836477986 Loss: 0.055215021786074965 Corrects: 3806\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 130, Loss 0.05213890224695206\n",
            "Train step - Step 140, Loss 0.049681924283504486\n",
            "Train step - Step 150, Loss 0.05714412406086922\n",
            "Train step - Step 160, Loss 0.04534497484564781\n",
            "Train step - Step 170, Loss 0.056053124368190765\n",
            "Train step - Step 180, Loss 0.046317875385284424\n",
            "Train epoch - Accuracy: 0.5077987421383647 Loss: 0.050324170723463756 Corrects: 4037\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 190, Loss 0.04393252730369568\n",
            "Train step - Step 200, Loss 0.051145587116479874\n",
            "Train step - Step 210, Loss 0.04391053691506386\n",
            "Train step - Step 220, Loss 0.045801907777786255\n",
            "Train step - Step 230, Loss 0.040967583656311035\n",
            "Train step - Step 240, Loss 0.04884340241551399\n",
            "Train step - Step 250, Loss 0.04484698921442032\n",
            "Train epoch - Accuracy: 0.5230188679245283 Loss: 0.046912463565105166 Corrects: 4158\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 260, Loss 0.04607342556118965\n",
            "Train step - Step 270, Loss 0.045963823795318604\n",
            "Train step - Step 280, Loss 0.045426927506923676\n",
            "Train step - Step 290, Loss 0.046094879508018494\n",
            "Train step - Step 300, Loss 0.045578163117170334\n",
            "Train step - Step 310, Loss 0.04174496978521347\n",
            "Train epoch - Accuracy: 0.540503144654088 Loss: 0.045041759287786184 Corrects: 4297\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.03828229755163193\n",
            "Train step - Step 330, Loss 0.04272458702325821\n",
            "Train step - Step 340, Loss 0.041969649493694305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxo3V31xh2Ou"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}