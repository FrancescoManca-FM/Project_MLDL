{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di LWF funzionante.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoManca-FM/Project_MLDL/blob/main/Progetto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAfp19qRVa7l"
      },
      "source": [
        "### GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag2iq0PaVWC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d22717-b8ff-4e4e-f746-468d2a8def3a"
      },
      "source": [
        "# Check GPU assigned\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OWtqytyzmMF"
      },
      "source": [
        "## Network, dataset, functions and parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itMjhanGVTUp"
      },
      "source": [
        "### LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6tCA_s2rru"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nHVORYbiZRx"
      },
      "source": [
        "### Resnet32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIdSy6PnibZA"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\"\"\"\n",
        "Credits to @hshustc\n",
        "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from torch.nn import init\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_relu=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.stride = stride\n",
        "        self.use_relu = use_relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "        if self.use_relu:\n",
        "            out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        assert stride == 2\n",
        "        self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat((x, x.mul(0)), 1)\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block=BasicBlock, depth=32, num_classes=0, channels=3):\n",
        "\n",
        "        super(CifarResNet, self).__init__()\n",
        "\n",
        "        # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
        "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
        "        layer_blocks = (depth - 2) // 6\n",
        "        bn = nn.BatchNorm2d\n",
        "        self.inplanes = 16\n",
        "\n",
        "        self.conv_1_3x3 = nn.Conv2d(channels, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn_1 = bn(self.inplanes)\n",
        "\n",
        "        self.stage_1 = self._make_layer(block, 16, layer_blocks, 1)\n",
        "        self.stage_2 = self._make_layer(block, 32, layer_blocks, 2)\n",
        "        self.stage_3 = self._make_layer(block, 64, layer_blocks, 2, last=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.relu = nn.ReLU()\n",
        "        #self.linear = nn.Linear(64, num_classes)\n",
        "        self.fcs = nn.ModuleList([nn.Linear(64, num_classes)])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, math.sqrt(1. / 64.))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, last=False):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = Downsample(self.\n",
        "inplanes, planes * block.expansion, stride)\n",
        "\n",
        "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        if last:\n",
        "            for i in range(1, blocks - 1):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            layers.append(block(self.inplanes, planes, use_relu=False))\n",
        "\n",
        "        else:\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv_1_3x3(x)\n",
        "\n",
        "        x = self.bn_1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.stage_1(x)\n",
        "        x = self.stage_2(x)\n",
        "        x = self.stage_3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        out = []\n",
        "        for fc in self.fcs:\n",
        "            out.append(fc(x))\n",
        "        out = torch.cat(out, dim=1)\n",
        "        return out\n",
        "\n",
        "    def addOutputNodes(self, num_classes):\n",
        "        self.fcs.append(nn.Linear(64, num_classes))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ga-SaCsjkYV"
      },
      "source": [
        "### Cifar100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHAyljuejmkd"
      },
      "source": [
        "# ref:\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py\n",
        "# https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR10\n",
        "# homework2 (caltech)\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# This is an handler class for the Cifar dataset\n",
        "class CIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    `CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "    This is a subclass of the `CIFAR100` Dataset.\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar-100-python'\n",
        "    train_file = 'train'\n",
        "    test_file = 'test'\n",
        "    meta_file = 'meta'\n",
        "\n",
        "    def __init__(self, root, split = 'train', transform = None):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              root (string): Root directory of the dataset where directory\n",
        "                  cifar-100-python exists.\n",
        "              split (string, optional): If 'train', creates dataset from training\n",
        "                  set, otherwise creates from test set.\n",
        "              transform (callable, optional): A function/transform that takes in a\n",
        "                  PIL image and returns a transformed version.\n",
        "        \"\"\"\n",
        "        super(CIFAR100, self).__init__(root, transform=transform)\n",
        "\n",
        "        self.split = split\n",
        "        if split == 'train':\n",
        "            filename = self.train_file\n",
        "        else:\n",
        "            filename = self.test_file\n",
        "\n",
        "        data_path = os.path.join(self.root, self.base_folder, filename)\n",
        "        data = None\n",
        "        labels = None\n",
        "\n",
        "        with open(data_path, 'rb') as f:\n",
        "            entry = pickle.load(f, encoding='latin1')\n",
        "            data = entry['data']\n",
        "            labels = entry['fine_labels']\n",
        "        \n",
        "        data = np.vstack(data).reshape(-1, 3, 32, 32)\n",
        "        data = data.transpose((0, 2, 3, 1))  # Convert to HWC\n",
        "        \n",
        "        labels = np.array(labels)\n",
        "\n",
        "        self.df = pd.DataFrame()\n",
        "        self.df['data'] = pd.Series(list(data))\n",
        "        self.df['labels'] = labels\n",
        "\n",
        "        self.data = self.df['data']\n",
        "        self.labels = self.df['labels']\n",
        "\n",
        "        self._load_meta()\n",
        "\n",
        "    def _load_meta(self):\n",
        "        meta_path = os.path.join(self.root, self.base_folder, self.meta_file)\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f, encoding='latin1')\n",
        "            self.label_names = meta['fine_label_names']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.df.loc[index, 'data'], self.df.loc[index, 'labels']\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img) # Return a PIL image\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return index, img, target\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def getTargets(self):\n",
        "        return set(self.labels)\n",
        "     \n",
        "    # test\n",
        "    def get_indices(self, labels):\n",
        "        return list(self.df[self.df['labels'].isin(labels)].index)\n",
        "\n",
        "    def split_classes(self, n_splits=10, seed=None, dictionary_of='dataframes'):\n",
        "        if dictionary_of not in ['dataframes','indices']:\n",
        "            raise ValueError(\"'dictionary_of' must be equal to 'dataframes' or 'indices'\")\n",
        "\n",
        "        all_classes = list(self.df['labels'].value_counts().index)\n",
        "        dictionary = {}\n",
        "        random.seed(seed)\n",
        "        random.shuffle(all_classes)\n",
        "        split_size = int(len(all_classes)/n_splits)\n",
        "        for j in range(n_splits):\n",
        "            if ((j+1)*split_size < len(all_classes)):\n",
        "                split_end = (j+1)*split_size\n",
        "            else:\n",
        "                split_end = None\n",
        "            subgroup = all_classes[j*split_size:split_end]\n",
        "            if dictionary_of == 'dataframes':\n",
        "                dictionary[j] = self.df[self.df['labels'].isin(subgroup)]\n",
        "            elif dictionary_of == 'indices':\n",
        "                dictionary[j] = list(self.df[self.df['labels'].isin(subgroup)].index)\n",
        "        return dictionary\n",
        "\n",
        "    def split_groups_in_train_validation(self, groups, ratio=0.5, seed=None):\n",
        "        groups_train_val = dict()\n",
        "        for k, subdf in groups.items():\n",
        "            train_indexes = []\n",
        "            val_indexes = []\n",
        "            split_labels = list(subdf['labels'].value_counts().index)\n",
        "            for l in split_labels:\n",
        "                indexes_to_sample = list(subdf[subdf['labels'] == l].index)\n",
        "                random.seed(seed)\n",
        "                train_samples = random.sample(indexes_to_sample, int(len(indexes_to_sample)*ratio))\n",
        "                train_indexes = train_indexes + train_samples\n",
        "                val_indexes = val_indexes + list(set(indexes_to_sample).difference(set(train_samples)))\n",
        "            groups_train_val[k] = {\n",
        "                'train': train_indexes,\n",
        "                'val': val_indexes\n",
        "            }\n",
        "        return groups_train_val\n",
        "    \n",
        "    def split_in_train_val_groups(self, n_splits=10, ratio=0.5, seed=None):\n",
        "        groups = self.split_classes(n_splits=n_splits, seed=seed)\n",
        "        return self.split_groups_in_train_validation(groups, ratio=ratio, seed=seed)\n",
        "\n",
        "    # given a tensors returns an image (used in exemplars)\n",
        "    #def tensorToImg(self, tensor):\n",
        "    #   return Variable(transform(Image.fromarray(img)), volatile=True)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_M1anVYkRnF"
      },
      "source": [
        "### Reverse Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j66mUOXekTlM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "__all__ = ['ReverseIndex']\n",
        "\n",
        "class ReverseIndex():\n",
        "\n",
        "    def __init__(self, dataset, splits, device='cuda'):\n",
        "\n",
        "        self.df = pd.DataFrame(columns=['group', 'labels'])\n",
        "\n",
        "        for k in splits.keys():\n",
        "            labels = list(dataset.df.loc[splits[k]['train'],'labels'].value_counts().index)\n",
        "            group = [k for i in range(len(labels))]\n",
        "            data = pd.DataFrame(np.array([group, labels]).T, columns=['group', 'labels'])\n",
        "            self.df = self.df.append(data, ignore_index=True)\n",
        "\n",
        "        self.df['nodes'] = self.df.index\n",
        "        self.device = device\n",
        "    \n",
        "    def _changeIndex(self, reverse_index, column):\n",
        "        reverse_index = reverse_index.set_index(column)\n",
        "        reverse_index[column] = reverse_index.index\n",
        "        return reverse_index\n",
        "\n",
        "    def getLabels(self, outputs):\n",
        "        outs = outputs.cpu().numpy()\n",
        "        reverse_index = self._changeIndex(self.df, 'nodes')\n",
        "        labels = reverse_index.loc[outs, 'labels']\n",
        "\n",
        "        labels = torch.tensor(list(labels))\n",
        "        return labels.to(self.device)\n",
        "\n",
        "    def getNodes(self, labels):\n",
        "        labels = labels.cpu().numpy()\n",
        "       \n",
        "        reverse_index = self._changeIndex(self.df, 'labels')\n",
        "\n",
        "        nodes = reverse_index.loc[labels, 'nodes']\n",
        "\n",
        "        nodes = torch.tensor(list(nodes))\n",
        "        return nodes.to(self.device)\n",
        "\n",
        "    def getGroups(self, distinct=True):\n",
        "        return self.df['group'].value_counts().index.sort_values()\n",
        "    \n",
        "    def getLabelsOfGroup(self, group):\n",
        "        return self.df.loc[self.df['group'] == group, 'labels']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVfOeOCuknB8"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QpAHzFkoW0"
      },
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Module\n",
        "\n",
        "# These are the default iCaRL hyper-parameters\n",
        "def getHyperparams():\n",
        "\tdictHyperparams = {\n",
        "\t\t\"LR\": 2,\n",
        "\t\t\"MOMENTUM\": 0.9,\n",
        "\t\t\"WEIGHT_DECAY\": 1e-5,\n",
        "\t\t\"NUM_EPOCHS\": 70,\n",
        "\t\t\"MILESTONES\": [49, 63],\n",
        "\t\t\"BATCH_SIZE\": 128,\n",
        "\t\t\"DEVICE\": 'cuda',\n",
        "\t\t\"GAMMA\": 0.2,\n",
        "\t\t\"SEED\": 66, #use 30, 42, 16\n",
        "\t\t\"LOG_FREQUENCY\": 10,\n",
        "\t\t\"NUM_CLASSES\": 100\n",
        "\t}\n",
        "\treturn dictHyperparams\n",
        "\n",
        "def getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize):\n",
        "\toptimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\tscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA, last_epoch=-1) \n",
        "\treturn optimizer, scheduler\n",
        "\n",
        "# the mean and the std have been found on the web as mean and std of cifar100\n",
        "# alternative (realistic): compute mean and std for the dataset\n",
        "def getTransformations():\n",
        "\t# Define transforms for training phase\n",
        "\ttrain_transform = transforms.Compose([transforms.RandomHorizontalFlip(), # Randomly flip the image with probability of 0.5\n",
        "\t                                      transforms.Pad(4), # Add padding\n",
        "\t                                      transforms.RandomCrop(32),# Crops a random squares of the image\n",
        "\t                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\t# Define transforms for the evaluation phase\n",
        "\teval_transform = transforms.Compose([\n",
        "\t                                      transforms.ToTensor(),\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\treturn train_transform, eval_transform\n",
        "\n",
        "# BCEWithLogits = Sigmoid + BCE, is the loss used in iCaRL\n",
        "def getLossCriterion():\n",
        "\tcriterion = nn.BCEWithLogitsLoss(reduction = 'mean') \n",
        "\treturn criterion\n",
        "\n",
        "# CrossEntropyLoss \n",
        "def computeLoss(criterion, outputs, labels):\n",
        "\treturn criterion(outputs, labels)\n",
        " \n",
        "# Loss L2\n",
        "def l2Loss (outputs, labels):\n",
        "  criterion = nn.MSELoss()\n",
        "  return criterion(outputs, labels)\n",
        "\n",
        "# Loss L1\n",
        "def l1Loss(outputs, labels):\n",
        "  criterion = nn.L1Loss()\n",
        "  return criterion(outputs, labels)\n",
        "\n",
        "# support BCE\n",
        "def _one_hot_encode(labels, n_classes, reverse_index, dtype=None, device='cuda'):\n",
        "\tbatch_size = len(labels)\n",
        "\tenconded = torch.zeros(batch_size, n_classes, dtype=dtype, device=device)\n",
        "\tlabels=map_to_outputs(labels, reverse_index)\n",
        "\tfor i, l in enumerate(labels):\n",
        "\t  enconded[i, l] = 1\n",
        "\treturn enconded\n",
        "\n",
        "def map_to_outputs(labels, reverse_index):\n",
        "\tif reverse_index is None:\n",
        "\t  return labels\n",
        "\tif type(labels) == int:\n",
        "\t  return int(reverse_index.getNodes(torch.tensor([labels])))\n",
        "\telif type(labels) == torch.Tensor:\n",
        "\t\treturn reverse_index.getNodes(labels)\n",
        "\n",
        "\n",
        "def plotAccuracyTrend(method, data_plot_line, seed):\n",
        "\tplt.figure(figsize=(20,7))\n",
        "\taccuracyDF=pd.DataFrame(data_plot_line, columns = ['Classes','Accuracy'])\n",
        "\tax = sns.lineplot(x=\"Classes\", y=\"Accuracy\",data=accuracyDF, marker = 'o')\n",
        "\tax.minorticks_on()\n",
        "\tax.set_xticks(np.arange(10,110,10))\n",
        "\tax.set_xlim(xmin=9, xmax=101)\n",
        "\tax.set_ylim(ymin=0, ymax=1)\n",
        "\tplt.legend(['Accuracy {}'.format(method)])\n",
        "\tax.grid(axis='y')\n",
        "\tplt.title(\"Accuracies against seen classes {} - seed: {}\".format(method, seed))\n",
        "\t\n",
        "\tfilename = \"acc_{}_{}.jpg\".format(method, seed) # ex. acc_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "def plotConfusionMatrix(method, confusionMatrixData, seed):\n",
        "\tfig,ax=plt.subplots(figsize=(10,10))\n",
        "\tsns.heatmap(confusionMatrixData,cmap='terrain',ax=ax)\n",
        "\tplt.ylabel('True label')\n",
        "\tplt.xlabel('Predicted label')\n",
        "\tplt.title(\"Confusion Matrix {} - seed: {}\".format(method, seed))\n",
        "\n",
        "\tfilename = \"cm_{}_{}.jpg\".format(method, seed) # ex. cm_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "# Write down the metrics (accuracy trand and confusion matrix)\n",
        "# this method is a shortcut when perfoming multiple tests with different splits (random_seed)\n",
        "# and allow us to plot on the same graph the data from multiple models (accuracy)\n",
        "def writeMetrics(method, seed, accuracies, confusionMatrixData):\n",
        "  data = {}\n",
        "  data['accuracies'] = []\n",
        "  data['cm'] = [] #cm line\n",
        "  i = 0\n",
        "  for classes_seen in range(10, 110, 10): #x axis on the plot\n",
        "    data['accuracies'].append({classes_seen : accuracies[i]}) \n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  for class_num in range(0,len(confusionMatrixData)): #rows of the cm\n",
        "    data['cm'].append({class_num : confusionMatrixData[i].tolist()}) \n",
        "    i += 1\n",
        "  \n",
        "  # dump to file\n",
        "  aus = method + '_' + str(seed)\n",
        "  filename = 'data_{}.json'.format(aus)\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "\n",
        "# Functions\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getResNet32():\n",
        "    net = CifarResNet()\n",
        "    # net.fc = nn.Linear(net.fc.in_features, output_size) # embedded in the class\n",
        "\n",
        "    criterion = getLossCriterion()\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return net, criterion, optimizer, scheduler\n",
        "\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getNet():\n",
        "    return getResNet32()\n",
        "\n",
        "def getSchedulerOptimizer(net):\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return optimizer, scheduler\n",
        "def softmax(x):\n",
        "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
        "    return f_x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfJj1z2LSxp0"
      },
      "source": [
        "### DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysghtAWOPYZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b10ade18-c9ca-4c34-d0a3-318957cc8f1d"
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "if not os.path.isdir('./{}'.format(\"$DATA_DIR/cifar-100-python\")):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-05 15:13:08--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  97.5MB/s    in 1.7s    \n",
            "\n",
            "2021-07-05 15:13:10 (97.5 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY10HfpTS1lq"
      },
      "source": [
        "### HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-pqSNg4_Ris"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100\n",
        "BATCH_SIZE = 128 \n",
        "LR = 0.1    \n",
        "MOMENTUM = 0.9     \n",
        "WEIGHT_DECAY = 1e-05\n",
        "NUM_EPOCHS = 1\n",
        "GAMMA = 0.2\n",
        "LOG_FREQUENCY = 10\n",
        "MILESTONES = [49,63]\n",
        "RANDOM_SEED = 66\n",
        "THRESHOLD = 0.5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW7WxV_QS4re"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_jHycn_1kk"
      },
      "source": [
        "train_transform, eval_transform = getTransformations()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPrwhTcIqUbA"
      },
      "source": [
        "### train & test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJKwvGljJj2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a40ba4-46c4-46e1-f75f-9de25ed12455"
      },
      "source": [
        "# Import dataset and apply transformations \n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# Check datasets length \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnooWwcyS_YN"
      },
      "source": [
        "### SPLIT DATA IN CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckn3H69iJj2X"
      },
      "source": [
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpcJvhxhJOLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a6542c-dc34-4628-c5d7-a846bdd1f825"
      },
      "source": [
        "# TRAIN / VAL split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "print(outputs_labels_mapping.getGroups())\n",
        "\n",
        "# TEST split\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MSItI0QVpn"
      },
      "source": [
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10):\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpWv5ZkhxTPJ"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4NQa-wNxWL1"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pisadUTPxcRP"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            # if (group_id > 1):\n",
        "            #   old_features = old_net.forward(images)\n",
        "            #   old_outputs = old_net.predict(old_features)\n",
        "            #   labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX_61nSuxh3v"
      },
      "source": [
        "### sequential learning fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM0GysHZxk1B"
      },
      "source": [
        "### Fine tuning\n",
        "def sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    old_net = None\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    #confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2X2IVZ-xrVT"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DGpd8lslxtUW",
        "outputId": "2a7a5171-8688-4b1f-fe52-f88f4b532a6a"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.7173749208450317\n",
            "Train step - Step 10, Loss 0.31387636065483093\n",
            "Train step - Step 20, Loss 0.2874338626861572\n",
            "Train step - Step 30, Loss 0.2654590606689453\n",
            "Train epoch - Accuracy: 0.318989898989899 Loss: 0.34560690457772725 Corrects: 1579\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.26789265871047974\n",
            "Train step - Step 50, Loss 0.258944034576416\n",
            "Train step - Step 60, Loss 0.2350231260061264\n",
            "Train step - Step 70, Loss 0.23865528404712677\n",
            "Train epoch - Accuracy: 0.44363636363636366 Loss: 0.24301170718790305 Corrects: 2196\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.23784291744232178\n",
            "Train step - Step 90, Loss 0.22167158126831055\n",
            "Train step - Step 100, Loss 0.22836807370185852\n",
            "Train step - Step 110, Loss 0.21694694459438324\n",
            "Train epoch - Accuracy: 0.48707070707070704 Loss: 0.22610946089330344 Corrects: 2411\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.21196965873241425\n",
            "Train step - Step 130, Loss 0.20575585961341858\n",
            "Train step - Step 140, Loss 0.22211872041225433\n",
            "Train step - Step 150, Loss 0.20619133114814758\n",
            "Train epoch - Accuracy: 0.5016161616161616 Loss: 0.21912972303954037 Corrects: 2483\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.2055768072605133\n",
            "Train step - Step 170, Loss 0.20553207397460938\n",
            "Train step - Step 180, Loss 0.22383907437324524\n",
            "Train step - Step 190, Loss 0.19269728660583496\n",
            "Train epoch - Accuracy: 0.5418181818181819 Loss: 0.20608459960932685 Corrects: 2682\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.19962547719478607\n",
            "Train step - Step 210, Loss 0.1956397444009781\n",
            "Train step - Step 220, Loss 0.23317204415798187\n",
            "Train step - Step 230, Loss 0.1870502084493637\n",
            "Train epoch - Accuracy: 0.5523232323232323 Loss: 0.19966931800047558 Corrects: 2734\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.19728612899780273\n",
            "Train step - Step 250, Loss 0.17967957258224487\n",
            "Train step - Step 260, Loss 0.1929226964712143\n",
            "Train step - Step 270, Loss 0.19188766181468964\n",
            "Train epoch - Accuracy: 0.5834343434343434 Loss: 0.19277908541939476 Corrects: 2888\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1998911201953888\n",
            "Train step - Step 290, Loss 0.18329913914203644\n",
            "Train step - Step 300, Loss 0.16586630046367645\n",
            "Train step - Step 310, Loss 0.17675089836120605\n",
            "Train epoch - Accuracy: 0.598989898989899 Loss: 0.1864176897809963 Corrects: 2965\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.17738604545593262\n",
            "Train step - Step 330, Loss 0.18055176734924316\n",
            "Train step - Step 340, Loss 0.17641305923461914\n",
            "Train step - Step 350, Loss 0.14969894289970398\n",
            "Train epoch - Accuracy: 0.6155555555555555 Loss: 0.17903075204955207 Corrects: 3047\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.1671830266714096\n",
            "Train step - Step 370, Loss 0.1687958985567093\n",
            "Train step - Step 380, Loss 0.159011110663414\n",
            "Train epoch - Accuracy: 0.6309090909090909 Loss: 0.1738420210462628 Corrects: 3123\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.15419088304042816\n",
            "Train step - Step 400, Loss 0.14775967597961426\n",
            "Train step - Step 410, Loss 0.16979841887950897\n",
            "Train step - Step 420, Loss 0.1525963395833969\n",
            "Train epoch - Accuracy: 0.6539393939393939 Loss: 0.1639658623993999 Corrects: 3237\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.19196446239948273\n",
            "Train step - Step 440, Loss 0.1466265171766281\n",
            "Train step - Step 450, Loss 0.17056730389595032\n",
            "Train step - Step 460, Loss 0.18163080513477325\n",
            "Train epoch - Accuracy: 0.6608080808080808 Loss: 0.16202940798167026 Corrects: 3271\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.176211878657341\n",
            "Train step - Step 480, Loss 0.1588747799396515\n",
            "Train step - Step 490, Loss 0.1342204362154007\n",
            "Train step - Step 500, Loss 0.14058950543403625\n",
            "Train epoch - Accuracy: 0.6767676767676768 Loss: 0.15383696049752862 Corrects: 3350\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.1555892676115036\n",
            "Train step - Step 520, Loss 0.13571906089782715\n",
            "Train step - Step 530, Loss 0.1515876054763794\n",
            "Train step - Step 540, Loss 0.1359780877828598\n",
            "Train epoch - Accuracy: 0.702020202020202 Loss: 0.14594818502363532 Corrects: 3475\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.15486030280590057\n",
            "Train step - Step 560, Loss 0.13412602245807648\n",
            "Train step - Step 570, Loss 0.15869688987731934\n",
            "Train step - Step 580, Loss 0.13754160702228546\n",
            "Train epoch - Accuracy: 0.7034343434343434 Loss: 0.14330182038172326 Corrects: 3482\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.13797789812088013\n",
            "Train step - Step 600, Loss 0.14471013844013214\n",
            "Train step - Step 610, Loss 0.1659015417098999\n",
            "Train step - Step 620, Loss 0.13035106658935547\n",
            "Train epoch - Accuracy: 0.712929292929293 Loss: 0.1402043603164981 Corrects: 3529\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.10688119381666183\n",
            "Train step - Step 640, Loss 0.10551120340824127\n",
            "Train step - Step 650, Loss 0.11231940239667892\n",
            "Train step - Step 660, Loss 0.12159333378076553\n",
            "Train epoch - Accuracy: 0.7434343434343434 Loss: 0.1296956703397963 Corrects: 3680\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.13172531127929688\n",
            "Train step - Step 680, Loss 0.142404243350029\n",
            "Train step - Step 690, Loss 0.12234494835138321\n",
            "Train step - Step 700, Loss 0.14414963126182556\n",
            "Train epoch - Accuracy: 0.756969696969697 Loss: 0.12469814313180519 Corrects: 3747\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.1333446353673935\n",
            "Train step - Step 720, Loss 0.15927483141422272\n",
            "Train step - Step 730, Loss 0.11001887172460556\n",
            "Train step - Step 740, Loss 0.12100914865732193\n",
            "Train epoch - Accuracy: 0.7450505050505051 Loss: 0.12714890332836093 Corrects: 3688\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.11506552994251251\n",
            "Train step - Step 760, Loss 0.10524898767471313\n",
            "Train step - Step 770, Loss 0.0984029471874237\n",
            "Train epoch - Accuracy: 0.7668686868686869 Loss: 0.11733168877435453 Corrects: 3796\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.12441835552453995\n",
            "Train step - Step 790, Loss 0.11353017389774323\n",
            "Train step - Step 800, Loss 0.13449843227863312\n",
            "Train step - Step 810, Loss 0.12670955061912537\n",
            "Train epoch - Accuracy: 0.7739393939393939 Loss: 0.11266988289175611 Corrects: 3831\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.10420570522546768\n",
            "Train step - Step 830, Loss 0.10050588846206665\n",
            "Train step - Step 840, Loss 0.09052333980798721\n",
            "Train step - Step 850, Loss 0.07840898633003235\n",
            "Train epoch - Accuracy: 0.7808080808080808 Loss: 0.11037116950509523 Corrects: 3865\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.13141851127147675\n",
            "Train step - Step 870, Loss 0.11617157608270645\n",
            "Train step - Step 880, Loss 0.11001942306756973\n",
            "Train step - Step 890, Loss 0.08741968870162964\n",
            "Train epoch - Accuracy: 0.7844444444444445 Loss: 0.10794938291564132 Corrects: 3883\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.11431687325239182\n",
            "Train step - Step 910, Loss 0.0919078066945076\n",
            "Train step - Step 920, Loss 0.1131000742316246\n",
            "Train step - Step 930, Loss 0.12487182766199112\n",
            "Train epoch - Accuracy: 0.7935353535353535 Loss: 0.10410453104310566 Corrects: 3928\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.0851389467716217\n",
            "Train step - Step 950, Loss 0.10327544063329697\n",
            "Train step - Step 960, Loss 0.09313042461872101\n",
            "Train step - Step 970, Loss 0.12224413454532623\n",
            "Train epoch - Accuracy: 0.8014141414141415 Loss: 0.10208582940125706 Corrects: 3967\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.10216508060693741\n",
            "Train step - Step 990, Loss 0.10625173896551132\n",
            "Train step - Step 1000, Loss 0.12439626455307007\n",
            "Train step - Step 1010, Loss 0.09492490440607071\n",
            "Train epoch - Accuracy: 0.805050505050505 Loss: 0.10121105768764863 Corrects: 3985\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.1075296625494957\n",
            "Train step - Step 1030, Loss 0.08558668196201324\n",
            "Train step - Step 1040, Loss 0.10448060184717178\n",
            "Train step - Step 1050, Loss 0.0987623855471611\n",
            "Train epoch - Accuracy: 0.8115151515151515 Loss: 0.09598487866647316 Corrects: 4017\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.11706822365522385\n",
            "Train step - Step 1070, Loss 0.08957835286855698\n",
            "Train step - Step 1080, Loss 0.1004997119307518\n",
            "Train step - Step 1090, Loss 0.10269360989332199\n",
            "Train epoch - Accuracy: 0.8151515151515152 Loss: 0.093790872918837 Corrects: 4035\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10780717432498932\n",
            "Train step - Step 1110, Loss 0.0829559937119484\n",
            "Train step - Step 1120, Loss 0.06946247071027756\n",
            "Train step - Step 1130, Loss 0.08679612725973129\n",
            "Train epoch - Accuracy: 0.8232323232323232 Loss: 0.09224864899811118 Corrects: 4075\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.0877915769815445\n",
            "Train step - Step 1150, Loss 0.07081689685583115\n",
            "Train step - Step 1160, Loss 0.09684779495000839\n",
            "Train epoch - Accuracy: 0.8305050505050505 Loss: 0.08737428996298048 Corrects: 4111\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.08674424141645432\n",
            "Train step - Step 1180, Loss 0.08422601222991943\n",
            "Train step - Step 1190, Loss 0.09191498905420303\n",
            "Train step - Step 1200, Loss 0.09191594272851944\n",
            "Train epoch - Accuracy: 0.834949494949495 Loss: 0.08547621402174535 Corrects: 4133\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.07304417341947556\n",
            "Train step - Step 1220, Loss 0.09637052565813065\n",
            "Train step - Step 1230, Loss 0.08423800766468048\n",
            "Train step - Step 1240, Loss 0.09754743427038193\n",
            "Train epoch - Accuracy: 0.8418181818181818 Loss: 0.0811596822106477 Corrects: 4167\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.08752617239952087\n",
            "Train step - Step 1260, Loss 0.08612791448831558\n",
            "Train step - Step 1270, Loss 0.09345199167728424\n",
            "Train step - Step 1280, Loss 0.06988094002008438\n",
            "Train epoch - Accuracy: 0.8432323232323232 Loss: 0.08195995536717501 Corrects: 4174\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.063926100730896\n",
            "Train step - Step 1300, Loss 0.09519979357719421\n",
            "Train step - Step 1310, Loss 0.0794561430811882\n",
            "Train step - Step 1320, Loss 0.09330239146947861\n",
            "Train epoch - Accuracy: 0.8434343434343434 Loss: 0.08210123597973525 Corrects: 4175\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.06706669181585312\n",
            "Train step - Step 1340, Loss 0.07936250418424606\n",
            "Train step - Step 1350, Loss 0.08014490455389023\n",
            "Train step - Step 1360, Loss 0.08957503736019135\n",
            "Train epoch - Accuracy: 0.8488888888888889 Loss: 0.07934777613541093 Corrects: 4202\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.08020991086959839\n",
            "Train step - Step 1380, Loss 0.09724964946508408\n",
            "Train step - Step 1390, Loss 0.06381719559431076\n",
            "Train step - Step 1400, Loss 0.06528539955615997\n",
            "Train epoch - Accuracy: 0.8549494949494949 Loss: 0.07585879642855037 Corrects: 4232\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.07746966183185577\n",
            "Train step - Step 1420, Loss 0.08617367595434189\n",
            "Train step - Step 1430, Loss 0.06970410794019699\n",
            "Train step - Step 1440, Loss 0.08160582929849625\n",
            "Train epoch - Accuracy: 0.8527272727272728 Loss: 0.07876499318715298 Corrects: 4221\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.06916437298059464\n",
            "Train step - Step 1460, Loss 0.06720501184463501\n",
            "Train step - Step 1470, Loss 0.06595558673143387\n",
            "Train step - Step 1480, Loss 0.0707656592130661\n",
            "Train epoch - Accuracy: 0.8654545454545455 Loss: 0.07132082007449082 Corrects: 4284\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.05659304931759834\n",
            "Train step - Step 1500, Loss 0.06974741071462631\n",
            "Train step - Step 1510, Loss 0.0784800574183464\n",
            "Train step - Step 1520, Loss 0.06901217252016068\n",
            "Train epoch - Accuracy: 0.8672727272727273 Loss: 0.07315080337753199 Corrects: 4293\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.07148046046495438\n",
            "Train step - Step 1540, Loss 0.08015324175357819\n",
            "Train step - Step 1550, Loss 0.07519340515136719\n",
            "Train epoch - Accuracy: 0.8682828282828283 Loss: 0.06976768631826748 Corrects: 4298\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.061872996389865875\n",
            "Train step - Step 1570, Loss 0.08952672779560089\n",
            "Train step - Step 1580, Loss 0.0770217701792717\n",
            "Train step - Step 1590, Loss 0.06781128793954849\n",
            "Train epoch - Accuracy: 0.8719191919191919 Loss: 0.06902587638659911 Corrects: 4316\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.0734851062297821\n",
            "Train step - Step 1610, Loss 0.055662430822849274\n",
            "Train step - Step 1620, Loss 0.08112241327762604\n",
            "Train step - Step 1630, Loss 0.07834818959236145\n",
            "Train epoch - Accuracy: 0.8739393939393939 Loss: 0.06590250587222551 Corrects: 4326\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.05307411774992943\n",
            "Train step - Step 1650, Loss 0.04475534334778786\n",
            "Train step - Step 1660, Loss 0.06450628489255905\n",
            "Train step - Step 1670, Loss 0.06469791382551193\n",
            "Train epoch - Accuracy: 0.8838383838383839 Loss: 0.06405000432874217 Corrects: 4375\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.055307772010564804\n",
            "Train step - Step 1690, Loss 0.06018352508544922\n",
            "Train step - Step 1700, Loss 0.05738571286201477\n",
            "Train step - Step 1710, Loss 0.07564640045166016\n",
            "Train epoch - Accuracy: 0.883030303030303 Loss: 0.06291912011124871 Corrects: 4371\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.0579797700047493\n",
            "Train step - Step 1730, Loss 0.05599530413746834\n",
            "Train step - Step 1740, Loss 0.07791357487440109\n",
            "Train step - Step 1750, Loss 0.07570113986730576\n",
            "Train epoch - Accuracy: 0.8860606060606061 Loss: 0.06122048602411241 Corrects: 4386\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.046970587223768234\n",
            "Train step - Step 1770, Loss 0.05448814854025841\n",
            "Train step - Step 1780, Loss 0.0570836178958416\n",
            "Train step - Step 1790, Loss 0.052846431732177734\n",
            "Train epoch - Accuracy: 0.8921212121212121 Loss: 0.05948092536342264 Corrects: 4416\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.0464363619685173\n",
            "Train step - Step 1810, Loss 0.056273628026247025\n",
            "Train step - Step 1820, Loss 0.0658160001039505\n",
            "Train step - Step 1830, Loss 0.0690997764468193\n",
            "Train epoch - Accuracy: 0.8955555555555555 Loss: 0.05688923456151076 Corrects: 4433\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.059149112552404404\n",
            "Train step - Step 1850, Loss 0.05253974348306656\n",
            "Train step - Step 1860, Loss 0.07070215791463852\n",
            "Train step - Step 1870, Loss 0.05269395932555199\n",
            "Train epoch - Accuracy: 0.8927272727272727 Loss: 0.05812157820571553 Corrects: 4419\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.04611649736762047\n",
            "Train step - Step 1890, Loss 0.06521129608154297\n",
            "Train step - Step 1900, Loss 0.0597667470574379\n",
            "Train step - Step 1910, Loss 0.07057886570692062\n",
            "Train epoch - Accuracy: 0.9002020202020202 Loss: 0.05608734115506663 Corrects: 4456\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.042010191828012466\n",
            "Train step - Step 1930, Loss 0.043184537440538406\n",
            "Train step - Step 1940, Loss 0.0397011898458004\n",
            "Train epoch - Accuracy: 0.9242424242424242 Loss: 0.04325935698518849 Corrects: 4575\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.039822064340114594\n",
            "Train step - Step 1960, Loss 0.036546554416418076\n",
            "Train step - Step 1970, Loss 0.04635452851653099\n",
            "Train step - Step 1980, Loss 0.033635109663009644\n",
            "Train epoch - Accuracy: 0.9432323232323232 Loss: 0.037418713830035144 Corrects: 4669\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.03440694510936737\n",
            "Train step - Step 2000, Loss 0.04546314477920532\n",
            "Train step - Step 2010, Loss 0.03715860843658447\n",
            "Train step - Step 2020, Loss 0.050867773592472076\n",
            "Train epoch - Accuracy: 0.9412121212121212 Loss: 0.03600090804876703 Corrects: 4659\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.02928098477423191\n",
            "Train step - Step 2040, Loss 0.03901729732751846\n",
            "Train step - Step 2050, Loss 0.037106212228536606\n",
            "Train step - Step 2060, Loss 0.03669671341776848\n",
            "Train epoch - Accuracy: 0.9452525252525252 Loss: 0.03400145272564406 Corrects: 4679\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.0314776748418808\n",
            "Train step - Step 2080, Loss 0.031674575060606\n",
            "Train step - Step 2090, Loss 0.018039535731077194\n",
            "Train step - Step 2100, Loss 0.0348820686340332\n",
            "Train epoch - Accuracy: 0.9468686868686869 Loss: 0.033380321872354755 Corrects: 4687\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.02144073322415352\n",
            "Train step - Step 2120, Loss 0.029142146930098534\n",
            "Train step - Step 2130, Loss 0.03788841888308525\n",
            "Train step - Step 2140, Loss 0.032097745686769485\n",
            "Train epoch - Accuracy: 0.9496969696969697 Loss: 0.03260654991022264 Corrects: 4701\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.027982238680124283\n",
            "Train step - Step 2160, Loss 0.020777106285095215\n",
            "Train step - Step 2170, Loss 0.030742371454834938\n",
            "Train step - Step 2180, Loss 0.0357779823243618\n",
            "Train epoch - Accuracy: 0.9543434343434344 Loss: 0.03025592699345916 Corrects: 4724\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.031895961612463\n",
            "Train step - Step 2200, Loss 0.025660132989287376\n",
            "Train step - Step 2210, Loss 0.038197994232177734\n",
            "Train step - Step 2220, Loss 0.025359375402331352\n",
            "Train epoch - Accuracy: 0.9525252525252526 Loss: 0.03145596713730783 Corrects: 4715\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.03523494675755501\n",
            "Train step - Step 2240, Loss 0.02871292270720005\n",
            "Train step - Step 2250, Loss 0.028518855571746826\n",
            "Train step - Step 2260, Loss 0.028394445776939392\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.031200148419599342 Corrects: 4703\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.026044586673378944\n",
            "Train step - Step 2280, Loss 0.03078872337937355\n",
            "Train step - Step 2290, Loss 0.03193800523877144\n",
            "Train step - Step 2300, Loss 0.05611391365528107\n",
            "Train epoch - Accuracy: 0.9555555555555556 Loss: 0.030103796654277377 Corrects: 4730\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.02573627233505249\n",
            "Train step - Step 2320, Loss 0.026890838518738747\n",
            "Train step - Step 2330, Loss 0.03133704885840416\n",
            "Train epoch - Accuracy: 0.953939393939394 Loss: 0.0305143504032884 Corrects: 4722\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.029058242216706276\n",
            "Train step - Step 2350, Loss 0.02426040545105934\n",
            "Train step - Step 2360, Loss 0.03160322457551956\n",
            "Train step - Step 2370, Loss 0.04511835798621178\n",
            "Train epoch - Accuracy: 0.9563636363636364 Loss: 0.029337084354324774 Corrects: 4734\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.031107379123568535\n",
            "Train step - Step 2390, Loss 0.029924381524324417\n",
            "Train step - Step 2400, Loss 0.024590641260147095\n",
            "Train step - Step 2410, Loss 0.020830800756812096\n",
            "Train epoch - Accuracy: 0.961010101010101 Loss: 0.02768125159346094 Corrects: 4757\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.02306819148361683\n",
            "Train step - Step 2430, Loss 0.05020679160952568\n",
            "Train step - Step 2440, Loss 0.032019324600696564\n",
            "Train step - Step 2450, Loss 0.035698115825653076\n",
            "Train epoch - Accuracy: 0.9587878787878787 Loss: 0.028257839812172784 Corrects: 4746\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.03360297158360481\n",
            "Train step - Step 2470, Loss 0.02808537892997265\n",
            "Train step - Step 2480, Loss 0.01882505789399147\n",
            "Train step - Step 2490, Loss 0.01958288811147213\n",
            "Train epoch - Accuracy: 0.96 Loss: 0.026811572033347504 Corrects: 4752\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.024240998551249504\n",
            "Train step - Step 2510, Loss 0.015550089068710804\n",
            "Train step - Step 2520, Loss 0.01995779201388359\n",
            "Train step - Step 2530, Loss 0.02513287402689457\n",
            "Train epoch - Accuracy: 0.962020202020202 Loss: 0.025361865840174934 Corrects: 4762\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.021166345104575157\n",
            "Train step - Step 2550, Loss 0.018641367554664612\n",
            "Train step - Step 2560, Loss 0.021410910412669182\n",
            "Train step - Step 2570, Loss 0.03057834319770336\n",
            "Train epoch - Accuracy: 0.9646464646464646 Loss: 0.023534335130167127 Corrects: 4775\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.029833734035491943\n",
            "Train step - Step 2590, Loss 0.018363453447818756\n",
            "Train step - Step 2600, Loss 0.026480138301849365\n",
            "Train step - Step 2610, Loss 0.028178563341498375\n",
            "Train epoch - Accuracy: 0.9660606060606061 Loss: 0.024363452725187695 Corrects: 4782\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.0342327281832695\n",
            "Train step - Step 2630, Loss 0.02055048942565918\n",
            "Train step - Step 2640, Loss 0.020426509901881218\n",
            "Train step - Step 2650, Loss 0.021581510081887245\n",
            "Train epoch - Accuracy: 0.9648484848484848 Loss: 0.025252315486320343 Corrects: 4776\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.03008689358830452\n",
            "Train step - Step 2670, Loss 0.0230241846293211\n",
            "Train step - Step 2680, Loss 0.019735518842935562\n",
            "Train step - Step 2690, Loss 0.01372586376965046\n",
            "Train epoch - Accuracy: 0.9644444444444444 Loss: 0.024579011685769966 Corrects: 4774\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.02005911059677601\n",
            "Train step - Step 2710, Loss 0.023675382137298584\n",
            "Train step - Step 2720, Loss 0.02118927799165249\n",
            "Train epoch - Accuracy: 0.9634343434343434 Loss: 0.024567253339772274 Corrects: 4769\n",
            "Training finished in 203.4930591583252 seconds\n",
            "EVALUATION:  0.78 0.12797264754772186\n",
            "TEST GROUP:  0.836\n",
            "TEST ALL:  0.836\n",
            "GROUP:  2\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.5198947787284851\n",
            "Train step - Step 10, Loss 0.18119554221630096\n",
            "Train step - Step 20, Loss 0.1347174048423767\n",
            "Train step - Step 30, Loss 0.10982789844274521\n",
            "Train epoch - Accuracy: 0.36808080808080806 Loss: 0.17154767145111102 Corrects: 1822\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.09604242444038391\n",
            "Train step - Step 50, Loss 0.09236812591552734\n",
            "Train step - Step 60, Loss 0.08287163078784943\n",
            "Train step - Step 70, Loss 0.08627394586801529\n",
            "Train epoch - Accuracy: 0.6482828282828282 Loss: 0.08684955637563359 Corrects: 3209\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.08100271224975586\n",
            "Train step - Step 90, Loss 0.07603686302900314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-754cea9f07ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequentialLearningFineTuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_subsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-31e6b3e7dcf7>\u001b[0m in \u001b[0;36msequentialLearningFineTuning\u001b[0;34m(train_subsets, val_subsets, test_subsets)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSchedulerOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset learning rate and step_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes_seen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# Validate on current group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-20cff53f0192>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Bring images and labels to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aakVopSdyeGr"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ0VbtGQygRH"
      },
      "source": [
        "method = \"finetuning\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ-J8aC_xx-c"
      },
      "source": [
        "## LWF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZBNivOix1FB"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdfIllqBx59v"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkzFiDVDyAm_"
      },
      "source": [
        "### sequential learning LWF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf7REFkIyEiQ"
      },
      "source": [
        "### LWF\n",
        "def sequentialLearningLWF(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZTwTS_4TcES"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkrMQy2TuUAb"
      },
      "source": [
        "# train\n",
        "# net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningLWF(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2qUxVyMTfB4"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_4xLfwcpDz"
      },
      "source": [
        "method = \"Learning Without Forgetting\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGHBWaLNXGeI"
      },
      "source": [
        "\"\"\"num_classes_seen = 100\n",
        "dif_accuracies=printAccuracyDifference(net,old_accuracies, num_classes_seen)\n",
        "dif_accuracies\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjGRIqDtNQP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nX10znUi6qd"
      },
      "source": [
        "## iCaRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKBdrJ8Csy2a"
      },
      "source": [
        "### classify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAPTOU9bszTY"
      },
      "source": [
        "def classify(batch_img, net, exemplar_sets):\n",
        "  \"\"\" classify images by nearest mean-of-exemplars \"\"\" \n",
        "  \"\"\" \n",
        "  class1: list_of_indices for images that represent class1\n",
        "  class2: list_of_indices for images that represent class2\n",
        "  ...\n",
        "  class100: list_of_indices for images that represent class 100\n",
        "  with Subset I retrieve the images from the train_dataset\n",
        "  iterate over it to calculate mean for each class\n",
        "  \"\"\"\n",
        "  net.eval()\n",
        "  classes_mean = []\n",
        "  for k, exemplars_indices in exemplar_sets.items():\n",
        "    features = []\n",
        "    class_images_set = Subset(train_dataset, exemplars_indices)\n",
        "    class_images = DataLoader(class_images_set, batch_size=BATCH_SIZE, num_workers=2)\n",
        "    for _, images, labels in class_images:\n",
        "      # for each class (paper from y=1...t) calculate features and then mean\n",
        "      feature = net.forward(images)\n",
        "      features.append(feature)\n",
        "    features_s = torch.cat(features)\n",
        "    class_mean = features_s.mean(0)\n",
        "    classes_mean.append(class_mean)\n",
        "    means_exemplars = torch.cat(classes_mean, dim=0)\n",
        "    means_exemplars = torch.stack([means_exemplars] * BATCH_SIZE)\n",
        "    means_exemplars = means_exemplars.transpose(1,2)\n",
        "  feature_images_to_classify = net.forward(batch_img)\n",
        "  # sono da normalizzare?\n",
        "  feature_images_to_classify = feature_images_to_classify.unsqueeze(2)\n",
        "  feature_images_to_classify = feature_images_to_classify.expand_as(means_exemplars) # expand_as to get the same dimension\n",
        "  preds = torch.argmin((feature_images_to_classify - means_exemplars).pow(2).sum(1), dim=1)\n",
        "  return preds"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5HRagcujHgM"
      },
      "source": [
        "### construct exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blwBqG6BjNsM"
      },
      "source": [
        "import sys\n",
        "\n",
        "def constructExemplarSet(net, Xclass, m):\n",
        "#Xclass contiene immagini e label della classe X\n",
        "  exemplars_set = []\n",
        "  feature_exemplars = []\n",
        "  indexes = []\n",
        "  features = [] \n",
        "  class_images = []\n",
        "  net.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    XtrainLoader = torch.utils.data.DataLoader(Xclass, shuffle = True, batch_size=1, num_workers=2)\n",
        "    for _, image, labels in XtrainLoader:\n",
        "      image = image.to(DEVICE)\n",
        "      class_images.append(image)\n",
        "      # per ogni immagine della classe x, prendiamo le rispettive feature e le uniamo nel vettore features, che contiene tutte quelle delle immagini della classe x\n",
        "      feature = net.forward(image)\n",
        "      #feature = net.predict(featuretmp)\n",
        "      feature = feature/np.linalg.norm(feature.cpu())\n",
        "      features.append(feature)\n",
        "\n",
        "    features = torch.cat(features, dim=0) #cat solve the problem of inequal size of tensors \n",
        "    current_class_mean = features.mean(0) # mu = media delle features delle immagini della classe\n",
        "\n",
        "    for k in range(1, m+1):\n",
        "      min = 100000\n",
        "      sum = 0\n",
        "      for j in range(k-1):\n",
        "        sum += feature_exemplars[j]\n",
        "      for x in range(len(Xclass)): \n",
        "        if (x not in indexes):\n",
        "          phiX = features[x]\n",
        "          val = current_class_mean - ((phiX + sum)/k)\n",
        "          val = np.linalg.norm(val.cpu().numpy()) ## NORMA \n",
        "          if (val < min):\n",
        "            min = val\n",
        "            feature_min = phiX\n",
        "            index_min = x\n",
        "      feature_exemplars.append(feature_min)\n",
        "      exemplars_set.append(Xclass[index_min][0])\n",
        "\n",
        "    print(\"lunghezza exemplar set: \", len(exemplars_set))\n",
        "    return exemplars_set"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy_HVL-tjZcb"
      },
      "source": [
        "### reduce exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7kuQ5DnjcCP"
      },
      "source": [
        "# DA CHIAMARE PER OGNI CLASSE k: riduce il numero di exemplars all'interno dell'exemplar_set della classe k \n",
        "# EXEMPLAR SET è una lista di indici che rappresentano la posizione dell'immagine selezionata per l'exemplar della classe corrente nel dataset di partenza\n",
        "def reduceExemplarSet(m, exemplars_set):\n",
        "  exemplars_new = []\n",
        "  for i in range(m):\n",
        "    if (exemplars_set != []):\n",
        "      exemplars_new.append(exemplars_set[i])\n",
        "\n",
        "  return exemplars_new"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMMlEpLsjfaj"
      },
      "source": [
        "### update representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRdA9RtDjhak"
      },
      "source": [
        "def updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot, old_net):\n",
        "  #exemplars_set_tot contiene tutti gli exemplars set ottenuti fino ad ora\n",
        "  #train_iter contiene tutti i dati (immagini + labels) delle classi nuove (s, .., t)\n",
        "  exemplars_subset = []\n",
        "  exemplars_indices = []\n",
        "  total_exemplars = []\n",
        "  labels_tot = []\n",
        "\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "    if (exemplar_set_class_k != []):\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "\n",
        "  if group_id > 1:\n",
        "    train_subset_total = torch.utils.data.ConcatDataset([train_subset, total_exemplars])\n",
        "  else:\n",
        "    train_subset_total = train_subset\n",
        "    \n",
        "  print(\"Len TOTAL train susbset: \", len(train_subset_total))\n",
        "  train_loader = torch.utils.data.DataLoader(train_subset_total, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  # train_loader è la concatenazione delle nuove classi con gli exemplar_sets calcolati fino a questo punto \n",
        "  print(\"training\")\n",
        "  train(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)\n",
        "  # trainWithOtherLosses(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)\n",
        "  #trainCEandL1(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac65-6QGjm9O"
      },
      "source": [
        "### incremental train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPU_32ArjpPH"
      },
      "source": [
        "import copy\n",
        "def incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_until_now):\n",
        "  print(\"Starting the update representation\")\n",
        "  exemplar_indices = None\n",
        "  num_classes = 10\n",
        "  new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "  print(\"NEW CLASSES: \", new_classes_examined)\n",
        "\n",
        "  updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net)\n",
        "                \n",
        "  iteration = group_id - 1\n",
        "  t = (num_classes * iteration) + num_classes # num_classes ricevute fino a questo momento \n",
        "  m = int(K/t) #casto ad intero ? per difetto o eccesso?\n",
        "  #s = num_classes * iteration\n",
        "\n",
        "  # REDUCING EXEMPLAR SET FOR EXISTING CLASSES\n",
        "  print(\"reducing exemplars for each class\")\n",
        "  print(total_classes_until_now)\n",
        "  for y in total_classes_until_now: #ci serve un set con tutte le classi fino ad ora viste\n",
        "    exemplar_y_new = reduceExemplarSet(m, exemplars_set_tot[y]) # valore associato alla chiave y che rappresenta il label della classe \n",
        "    print(\"REDUCED EXEMPLAR: \", len(exemplar_y_new))\n",
        "    exemplars_set_tot[y] = exemplar_y_new\n",
        "\n",
        "  \n",
        "  # CONSTRUCTION EXEMPLAR SET FOR NEW CLASSES\n",
        "  for y in new_classes_examined: # nuovi classi in arrivo di cui vogliamo costruire il set rappresentativo\n",
        "    images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data']\n",
        "    imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "    class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "    print(\"class train: \", class_train_subset)\n",
        "    print(\"Constructing exemplars of class\", y)\n",
        "    exemplars_set = constructExemplarSet(net, class_train_subset, m) # exemplar set è un set di indici\n",
        "    #devo recuperare dal dataset iniziale l'indice delle immagini dell'exemplar set creato \n",
        "    #for image in exemplars_set:\n",
        "     # exemplars_set = train_dataset.df.index[train_dataset.df['data'] == image].tolist()\n",
        "    exemplars_set_tot[y] = exemplars_set\n",
        "    print(\"exemplar set: \", exemplars_set)\n",
        "\n",
        "  # IMPLEMENTATION 'END-to-END Incremental Learning' PAPER\n",
        "  balancedFinetune(net, group_id, exemplars_set_tot, NUM_EPOCHS_FINETUNE, old_net)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDxmsQlWjp5D"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dQznGoEju9J"
      },
      "source": [
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes_till_now, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    print(\"num classes till now: \", num_classes_till_now)\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes_till_now, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!feautressssssssssss: \", features.shape)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            #if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcZUqVCjeMG4"
      },
      "source": [
        "### Train con CE + L1Loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl4zFMYGtAEJ"
      },
      "source": [
        "import copy\n",
        "def trainCEandL1(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    distillation_loss = 0\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "            # Classification LOSS\n",
        "            lr = 0.01\n",
        "            classification_loss = nn.CrossEntropyLoss()(outputs, labels) # BCE\n",
        "\n",
        "            # Distillation LOSS \n",
        "            if (group_id > 1):\n",
        "               old_features = old_net.forward(images)\n",
        "               old_outputs = old_net.predict(old_features)\n",
        "               #labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "               new_labels = torch.sigmoid(old_outputs)\n",
        "               new_outputs = outputs[:, 0:num_classes_till_previous_step]\n",
        "               lr = 1e-3\n",
        "               distillation_loss = l1Loss(new_outputs, new_labels) # L2\n",
        "               print(distillation_loss)\n",
        "\n",
        "            loss = classification_loss + distillation_loss\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrbPsdT2eRho"
      },
      "source": [
        "### Train con CE + KLDiv Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzTrBE-Dea1S"
      },
      "source": [
        "import copy\n",
        "def trainWithOtherLosses(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    distillation_loss = 0\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "            # Classification LOSS\n",
        "            classification_loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "            # Distillation LOSS \n",
        "            if (group_id > 1):\n",
        "               old_features = old_net.forward(images)\n",
        "               old_outputs = old_net.predict(old_features)\n",
        "               T = 2\n",
        "               beta = 0.25\n",
        "               distillation_loss = nn.KLDivLoss()(F.log_softmax(outputs[:, 0:num_classes_till_previous_step]/T, dim = 1), F.softmax(old_outputs.detach()/T, dim = 1)) * T * T * beta * num_classes_till_previous_step\n",
        "\n",
        "            loss = classification_loss + distillation_loss\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHpi410ljzWD"
      },
      "source": [
        "### test and validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQYB0-cLj0sP"
      },
      "source": [
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        #_, preds = classify(images, )\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "246M7j-s4Chq"
      },
      "source": [
        "### Balanced Finetune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtFJY_fB5ymS"
      },
      "source": [
        "def data_augmentation_e2e(img, lab):\n",
        "    \"\"\"\n",
        "        Realize the data augmentation in End-to-End paper\n",
        "        Parameters\n",
        "        ----------\n",
        "        img: the original images, size = (n, c, w, h)\n",
        "        lab: the original labels, size = (n)\n",
        "        Returns\n",
        "        ----------\n",
        "        img_aug: the original images, size = (n * 12, c, w, h)\n",
        "        lab_aug: the original labels, size = (n * 12)\n",
        "    \"\"\"\n",
        "    \n",
        "    shape = np.shape(img)\n",
        "    # print(\"IMG is: \",img)\n",
        "    # print(shape[0], 1, shape[1], shape[2], shape[3])\n",
        "    img_aug = np.zeros((shape[0], 6, shape[1], shape[2], shape[3]))\n",
        "    img_aug[:, 0, :, :, :] = img\n",
        "    lab_aug = np.zeros((shape[0], 6))\n",
        "    # print(\"IMG_AUG is: \", img_aug)\n",
        "\n",
        "    for i in range(shape[0]):\n",
        "        # np.random.seed(int(time.time()) % 1000)\n",
        "\n",
        "        # convert image from tensor to numpy\n",
        "        image=img.numpy()\n",
        "        im = image[i]\n",
        "      \n",
        "        # # brightness\n",
        "        brightness = (np.random.rand(1)-0.5)*2*63\n",
        "        im_temp = im + brightness\n",
        "\n",
        "        img_aug[i, 1] = im_temp\n",
        "\n",
        "\n",
        "        # constrast\n",
        "        constrast = (np.random.rand(1)-0.5)*2*0.8+1\n",
        "        m0 = np.mean(im[0])\n",
        "        m1 = np.mean(im[1])\n",
        "        m2 = np.mean(im[2])\n",
        "        im_temp = im\n",
        "        im_temp[0] = (im_temp[0]-m0)*constrast + m0\n",
        "        im_temp[1] = (im_temp[1]-m1)*constrast + m1\n",
        "        im_temp[2] = (im_temp[2]-m2)*constrast + m2\n",
        "        img_aug[i, 2] = im_temp\n",
        "\n",
        "        # crop\n",
        "        im_temp = img_aug[i, :3]\n",
        "        for j in range(3):\n",
        "            x_ = int(np.random.rand(1)*1000)%8\n",
        "            y_ = int(np.random.rand(1)*1000)%8\n",
        "            im_temp = np.zeros(shape=(shape[1], shape[2]+8, shape[3]+8))\n",
        "            im_temp[:, 4:-4, 4:-4] = img_aug[i, j]\n",
        "            img_aug[i, 3+j] = im_temp[:, x_:x_+shape[2], y_:y_+shape[3]]\n",
        "\n",
        "\n",
        "\n",
        "        # mirror\n",
        "        # for j in range(6):\n",
        "        #     im_temp = img_aug[i, j]\n",
        "        #     img_aug[i, 6 + j] = im_temp[:,-1::-1,:]\n",
        "\n",
        "        lab_aug[i, :] = lab[i]\n",
        "\n",
        "    # idx = np.where(img_aug>255)\n",
        "    # img_aug[idx] = 255\n",
        "    # idx = np.where(img_aug<0)\n",
        "    # img_aug[idx] = 0\n",
        "\n",
        "    img_aug = np.reshape(img_aug, newshape=(shape[0]*6, shape[1], shape[2], shape[3]))\n",
        "    img_aug = np.array(img_aug, dtype=np.float64)\n",
        "    lab_aug = np.reshape(lab_aug, newshape=(shape[0]*6))\n",
        "    lab_aug = np.array(lab_aug, dtype=np.float64)\n",
        "    return img_aug, lab_aug"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC_zGBOB4B4J"
      },
      "source": [
        "def balancedFinetune(net, iteration, exemplars_set_tot, NUM_EPOCHS, old_net):\n",
        "  num_classes_till_previous_step = iteration * 10 - 10\n",
        "  num_classes = iteration * 10\n",
        "  total_exemplars = []\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  #if iteration > 1:\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    if (exemplar_set_class_k != []):\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "    \n",
        "  reduced_train_loader = torch.utils.data.DataLoader(total_exemplars, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  # reduced train loader is the sets of all exemplars of new and old classes\n",
        "\n",
        "  # finetune\n",
        "  lrc = LR *0.1 # small learning rate for finetune\n",
        "  print('current lr = %f' % (lrc))\n",
        "  softmax = nn.Softmax(dim=-1).cuda()\n",
        "  current_step = 0\n",
        "  acc_finetune_train = []\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS_FINETUNE):\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for _, images, labels in reduced_train_loader:\n",
        "      images, labels = data_augmentation_e2e(images,labels) \n",
        "      images = torch.from_numpy(images) \n",
        "      labels = torch.from_numpy(labels) \n",
        "    # reduced_train_loader contains the same number of images for both new classes and old classes\n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "      optimizer = torch.optim.SGD(net.parameters(), lr=lrc, momentum=MOMENTUM,\n",
        "                                    weight_decay= WEIGHT_DECAY, nesterov=True)\n",
        "\n",
        "\n",
        "      # print(\"Outside: input size\", img.size(), \"output_size\", lab.size())\n",
        "      features = net.forward(images)\n",
        "      outputs = net.predict(features)\n",
        "\n",
        "      # classification loss\n",
        "      prob_cls = softmax(outputs)\n",
        "      labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "      labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "    # distillation loss for all classes (maybe the author only distillates for novel classes)\n",
        "      if iteration > 1:\n",
        "        old_features = old_net.forward(images)\n",
        "        old_outputs = old_net.predict(old_features)\n",
        "        labels_enc[:, 0:num_classes_till_previous_step] = torch.sigmoid(old_outputs[:, 0:num_classes_till_previous_step])\n",
        "\n",
        "\n",
        "      loss = computeLoss(criterion, outputs, labels_enc)\n",
        "\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "          \n",
        "    # Update Corrects & Loss\n",
        "      running_loss += loss.item() * images.size(0)\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    # Log loss\n",
        "      if current_step % LOG_FREQUENCY == 0:\n",
        "        print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss = running_loss / float(len(reduced_train_loader.dataset))\n",
        "      epoch_acc = running_corrects / float(len(reduced_train_loader.dataset))\n",
        "      \n",
        "    print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTv6zBetj3Cb"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_cnELYdkap4"
      },
      "source": [
        "def sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 10\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      print(\"TEST SET LENGHT: \", len(test_set))\n",
        "      print(\"TEST CURRENT GROUP SET LENGHT: \", len(test_subset))\n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      test_classes = list(test_dataset.df.loc[test_set.indices, 'labels'].value_counts().index)\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TEST_SET CLASSES: \", test_classes)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTV1Gxq6yF5-",
        "outputId": "3be9bc8d-fada-40b8-ea55-b9f16255ed19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9467244d667a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequentialLearningiCaRL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_subsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_subsets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-o1n4skl-77"
      },
      "source": [
        "##CLOSED AND OPEN WORLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lUUjCewmNLo"
      },
      "source": [
        "#Dataset divided into 2 halves, 50 for closed 50 for open (choose five different random division)\n",
        "#1) closed world\n",
        "#  1.1)without rejection -> standard incremental scenario (train and test using selected 10 classes) but with 50 classes\n",
        "#      iter = 0 -> 10 or 20 (he does so in BDOC) classes ? ask Dario\n",
        "#      next iters -> add 10 until 50\n",
        "#      result expected -> equal to incremental \n",
        "#\n",
        "#  1.2)with rejection -> same procedure of above but we implement a rejection technique that \n",
        "#      classify as unknown an object that doesn't belong to the classes seen in the training (for the alg follow BDOC)\n",
        "#      result expected -> idealistic the model should not reject any of the object because we've tested the model with classes seen in the training\n",
        "#\n",
        "#2) open world\n",
        "#    at each step -> test the model only on unknown samples (the second half of dataset)\n",
        "#    \n",
        "#    result expected -> idealist the model should reject all of the test objects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFHiLXyLmaWu"
      },
      "source": [
        "###download and dividing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OJxXhGZmdDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d82359-791d-4939-c2b0-0160164081bb"
      },
      "source": [
        "# Import dataset and apply transformations \n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# Check datasets length \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZFLVy0Tmt3G"
      },
      "source": [
        "#closed and open world\n",
        "splits_of_10 = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "#first 5 splits to closed world\n",
        "closed_data = {k:splits_of_10[k] for k in range(5)}\n",
        "\n",
        "#last 5 to open (removing the train val splits)\n",
        "open = []\n",
        "for k in range(5,10):\n",
        "  for j in[\"train\", \"val\"]:\n",
        "    open += splits_of_10[k][j]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQKnRMfjEaA-"
      },
      "source": [
        "### Modified iCaRL for closed/open"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fIvHXxhzwNI"
      },
      "source": [
        "def updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot, old_net, rejection=False, closed=True):\n",
        "  #exemplars_set_tot contiene tutti gli exemplars set ottenuti fino ad ora\n",
        "  #train_iter contiene tutti i dati (immagini + labels) delle classi nuove (s, .., t)\n",
        "  exemplars_subset = []\n",
        "  exemplars_indices = []\n",
        "  total_exemplars = []\n",
        "  labels_tot = []\n",
        "\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "    if (exemplar_set_class_k != []):\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "\n",
        "  if group_id > 1:\n",
        "    train_subset_total = torch.utils.data.ConcatDataset([train_subset, total_exemplars])\n",
        "  else:\n",
        "    train_subset_total = train_subset\n",
        "    \n",
        "  print(\"Len TOTAL train susbset: \", len(train_subset_total))\n",
        "  train_loader = torch.utils.data.DataLoader(train_subset_total, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  # train_loader è la concatenazione delle nuove classi con gli exemplar_sets calcolati fino a questo punto \n",
        "  print(\"training\")\n",
        "  train(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net, rejection = rejection, closed = closed)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAJtrUjIzqz-"
      },
      "source": [
        "import copy\n",
        "def incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_until_now, rejection=False, closed=True):\n",
        "  print(\"Starting the update representation\")\n",
        "  exemplar_indices = None\n",
        "  num_classes = 10\n",
        "  new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "  print(\"NEW CLASSES: \", new_classes_examined)\n",
        "\n",
        "  updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, rejection=rejection, closed=closed)\n",
        "                \n",
        "  iteration = group_id - 1\n",
        "  t = (num_classes * iteration) + num_classes # num_classes ricevute fino a questo momento \n",
        "  m = int(K/t) #casto ad intero ? per difetto o eccesso?\n",
        "  #s = num_classes * iteration\n",
        "\n",
        "  # REDUCING EXEMPLAR SET FOR EXISTING CLASSES\n",
        "  print(\"reducing exemplars for each class\")\n",
        "  print(total_classes_until_now)\n",
        "  for y in total_classes_until_now: #ci serve un set con tutte le classi fino ad ora viste\n",
        "    exemplar_y_new = reduceExemplarSet(m, exemplars_set_tot[y]) # valore associato alla chiave y che rappresenta il label della classe \n",
        "    print(\"REDUCED EXEMPLAR: \", len(exemplar_y_new))\n",
        "    exemplars_set_tot[y] = exemplar_y_new\n",
        "\n",
        "  \n",
        "  # CONSTRUCTION EXEMPLAR SET FOR NEW CLASSES\n",
        "  for y in new_classes_examined: # nuovi classi in arrivo di cui vogliamo costruire il set rappresentativo\n",
        "    images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data']\n",
        "    imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "    class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "    print(\"class train: \", class_train_subset)\n",
        "    print(\"Constructing exemplars of class\", y)\n",
        "    exemplars_set = constructExemplarSet(net, class_train_subset, m) # exemplar set è un set di indici\n",
        "    #devo recuperare dal dataset iniziale l'indice delle immagini dell'exemplar set creato \n",
        "    #for image in exemplars_set:\n",
        "     # exemplars_set = train_dataset.df.index[train_dataset.df['data'] == image].tolist()\n",
        "    exemplars_set_tot[y] = exemplars_set\n",
        "    print(\"exemplar set: \", exemplars_set)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkpzD3lazndM"
      },
      "source": [
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes_till_now, group_id, old_net, num_epochs=NUM_EPOCHS, rejection=False, closed=True, threshold=THRESHOLD):    \n",
        "\n",
        "      \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    print(\"num classes till now: \", num_classes_till_now)\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    print(num_epochs)\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        if rejection == True:\n",
        "          n_sample_known = 0\n",
        "          n_sample_unknown = 0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes_till_now, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            #if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            if rejection == True:\n",
        "              prediction_batch = outputs.data.cpu().numpy()\n",
        "              for i in range(len(prediction_batch)):\n",
        "                current_softmax = softmax(prediction_batch[i])\n",
        "                if max(current_softmax)>THRESHOLD:\n",
        "                  n_sample_known += 1\n",
        "                else:\n",
        "                  n_sample_unknown += 1\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        if rejection == True:\n",
        "          if closed == True:\n",
        "            epoch_acc = n_sample_known / float(len(train_dataloader.dataset))\n",
        "            numb = n_sample_known\n",
        "          else:\n",
        "            epoch_acc = n_sample_unknown / float(len(train_dataloader.dataset))\n",
        "            numb = n_sample_unknown\n",
        "        else:\n",
        "          epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, numb))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0xrVtFyzj75"
      },
      "source": [
        "def validate(net, val_dataloader, criterion, num_classes, rejection = False, closed = False):\n",
        "    #counter for rejection part, known and unknown\n",
        "    if rejection==True:\n",
        "      n_sample_known = 0\n",
        "      n_sample_unknown = 0\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            \n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        if rejection == True:\n",
        "          prediction_batch = outputs.data.cpu().numpy()\n",
        "          print(len(prediction_batch))\n",
        "          for i in range(len(prediction_batch)):\n",
        "            current_softmax = softmax(prediction_batch[i])\n",
        "            if max(current_softmax)>THRESHOLD:\n",
        "              n_sample_known += 1\n",
        "            else:\n",
        "              n_sample_unknown += 1\n",
        "        else:\n",
        "          #_, preds = classify(images, )\n",
        "          running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "          all_preds_cm.extend(preds.tolist())\n",
        "          all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    if rejection == True:\n",
        "      if closed == True:\n",
        "        acc = n_sample_known / float(len(val_dataloader.dataset))\n",
        "      else:\n",
        "        acc = n_sample_unknown / float(len(val_dataloader.dataset))\n",
        "    else:\n",
        "      acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes, rejection = False, closed = True):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes, rejection, closed)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcN9P3Gr1ICR"
      },
      "source": [
        "def sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets, rejection = False, closed = True):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 5\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      print(\"TEST SET LENGHT: \", len(test_set))\n",
        "      print(\"TEST CURRENT GROUP SET LENGHT: \", len(test_subset))\n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      test_classes = list(test_dataset.df.loc[test_set.indices, 'labels'].value_counts().index)\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TEST_SET CLASSES: \", test_classes)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen, rejection=rejection, closed=closed) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen, rejection=rejection, closed=closed)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen, rejection=rejection, closed=closed)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trLViwIa2_bY"
      },
      "source": [
        "### Closed World"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyhNorNImn9E",
        "outputId": "b6e6534d-f50a-4504-9278-85cf41472d28"
      },
      "source": [
        "# Reverse indexing for closed and open world\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, splits_of_10)\n",
        "print(outputs_labels_mapping.getGroups())\n",
        "\n",
        "# TEST split\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)\n",
        "#above are 10 splits but I want 5 for closed and 5 for open\n",
        "test_splits_closed = {i:test_splits[i] for i in range(5)}\n",
        "test_splits_open = []\n",
        "for i in range(5,10):\n",
        "  test_splits_open += test_splits[i]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akWuYU2cB1j9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2bb062d-1381-4a3c-c5d5-835d87a6b7ab"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "for i in outputs_labels_mapping.getGroups():\n",
        "  print(outputs_labels_mapping.getLabelsOfGroup(i))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    67\n",
            "1    59\n",
            "2    39\n",
            "3    22\n",
            "4    18\n",
            "5    65\n",
            "6    49\n",
            "7    56\n",
            "8    20\n",
            "9     4\n",
            "Name: labels, dtype: object\n",
            "10    79\n",
            "11    47\n",
            "12     7\n",
            "13    82\n",
            "14    34\n",
            "15    81\n",
            "16    21\n",
            "17    80\n",
            "18    68\n",
            "19    16\n",
            "Name: labels, dtype: object\n",
            "20    75\n",
            "21    23\n",
            "22    90\n",
            "23    10\n",
            "24    61\n",
            "25    76\n",
            "26    64\n",
            "27    32\n",
            "28    24\n",
            "29     0\n",
            "Name: labels, dtype: object\n",
            "30    95\n",
            "31    83\n",
            "32    63\n",
            "33    42\n",
            "34    30\n",
            "35     6\n",
            "36     2\n",
            "37    97\n",
            "38    72\n",
            "39    36\n",
            "Name: labels, dtype: object\n",
            "40    55\n",
            "41    31\n",
            "42    19\n",
            "43    98\n",
            "44    94\n",
            "45    54\n",
            "46    93\n",
            "47    85\n",
            "48     9\n",
            "49    96\n",
            "Name: labels, dtype: object\n",
            "50    99\n",
            "51    15\n",
            "52    14\n",
            "53    57\n",
            "54    45\n",
            "55    13\n",
            "56    88\n",
            "57    60\n",
            "58    40\n",
            "59     8\n",
            "Name: labels, dtype: object\n",
            "60    35\n",
            "61    27\n",
            "62    86\n",
            "63    70\n",
            "64    50\n",
            "65    69\n",
            "66    53\n",
            "67    17\n",
            "68    84\n",
            "69    52\n",
            "Name: labels, dtype: object\n",
            "70    71\n",
            "71    51\n",
            "72    43\n",
            "73    78\n",
            "74    74\n",
            "75    38\n",
            "76    37\n",
            "77    29\n",
            "78    48\n",
            "79    44\n",
            "Name: labels, dtype: object\n",
            "80    87\n",
            "81    58\n",
            "82    46\n",
            "83    26\n",
            "84    77\n",
            "85    41\n",
            "86     5\n",
            "87    92\n",
            "88    28\n",
            "89    12\n",
            "Name: labels, dtype: object\n",
            "90    91\n",
            "91    11\n",
            "92     3\n",
            "93    66\n",
            "94    62\n",
            "95    89\n",
            "96    73\n",
            "97    33\n",
            "98    25\n",
            "99     1\n",
            "Name: labels, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W6-U42kmn9F"
      },
      "source": [
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in closed_data.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,5):\n",
        "    v=test_splits_closed[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDPGgRJCtJe4"
      },
      "source": [
        "#for the test of open world\n",
        "open_test = Subset(test_dataset, test_splits_open)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgnJSvjtKYfS"
      },
      "source": [
        "targets_open = set()\n",
        "for i in range(len(open_test.indices)):\n",
        "  targets_open.add(test_dataset.__getitem__(open_test.indices[i])[2])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P76U7RxfLoxo"
      },
      "source": [
        "targets_closed = set()\n",
        "for k in test_splits_closed:\n",
        "  for j in range(len(test_splits_closed[k])):\n",
        "    targets_closed.add(test_dataset.__getitem__(test_splits_closed[k][j])[2])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gp8L6LONeqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9bc5260-1e6e-48a4-fac4-7681bcb61c53"
      },
      "source": [
        "#verifing that there aren't objects of the same class\n",
        "list(targets_closed.intersection(targets_open))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMyXZhVemn8q"
      },
      "source": [
        "### Closed world without rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl8IKzFsocw5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "ba9679d3-86f9-484f-e886-230833cf7a56"
      },
      "source": [
        "#without rejection\n",
        "rejection = False\n",
        "closed = True\n",
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN:  4950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-31167443351d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclosed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequentialLearningiCaRL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_subsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-fc68f2ed3622>\u001b[0m in \u001b[0;36msequentialLearningiCaRL\u001b[0;34m(train_subsets, val_subsets, test_subsets, rejection, closed)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mold_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mold_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0maddOutputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI36G5hjodD8"
      },
      "source": [
        "method = \"Closed world without Rejection\"\n",
        "print(\"metrics ClosedWord for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,5):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBFSAonO0mgQ"
      },
      "source": [
        "### Closed world with rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32Dv40KK1ICR",
        "outputId": "f69af6cc-849f-4ab4-a9e2-1a058929e8ed"
      },
      "source": [
        "# train closed world with rejection\n",
        "rejection = True\n",
        "closed = True\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets, rejection=rejection, closed=closed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  1000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [67, 65, 59, 56, 49, 39, 22, 20, 18, 4]\n",
            "TRAIN_SET CLASSES:  [67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "VALIDATION CLASSES:  [59, 56, 49, 39, 22, 20, 18, 4, 67, 65]\n",
            "GROUP:  1\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "Len TOTAL train susbset:  4950\n",
            "training\n",
            "num classes till now:  10\n",
            "5\n",
            "Starting epoch 1/5, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.6994286775588989\n",
            "Train step - Step 10, Loss 0.30136817693710327\n",
            "Train step - Step 20, Loss 0.2911085784435272\n",
            "Train step - Step 30, Loss 0.26708194613456726\n",
            "Train epoch - Accuracy: 0.14585858585858585 Loss: 0.35155493756135303 Corrects: 1489\n",
            "Starting epoch 2/5, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.27089643478393555\n",
            "Train step - Step 50, Loss 0.2680715024471283\n",
            "Train step - Step 60, Loss 0.2523084878921509\n",
            "Train step - Step 70, Loss 0.2466038018465042\n",
            "Train epoch - Accuracy: 0.31555555555555553 Loss: 0.24956819676389597 Corrects: 2074\n",
            "Starting epoch 3/5, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.2362622767686844\n",
            "Train step - Step 90, Loss 0.24306665360927582\n",
            "Train step - Step 100, Loss 0.23924599587917328\n",
            "Train step - Step 110, Loss 0.1914578527212143\n",
            "Train epoch - Accuracy: 0.4163636363636364 Loss: 0.23055763977946658 Corrects: 2346\n",
            "Starting epoch 4/5, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.21142147481441498\n",
            "Train step - Step 130, Loss 0.2643110156059265\n",
            "Train step - Step 140, Loss 0.21732209622859955\n",
            "Train step - Step 150, Loss 0.20671935379505157\n",
            "Train epoch - Accuracy: 0.4624242424242424 Loss: 0.22319907110748868 Corrects: 2468\n",
            "Starting epoch 5/5, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.1971411406993866\n",
            "Train step - Step 170, Loss 0.2160784751176834\n",
            "Train step - Step 180, Loss 0.20748233795166016\n",
            "Train step - Step 190, Loss 0.1985824704170227\n",
            "Train epoch - Accuracy: 0.5115151515151515 Loss: 0.2116549697548452 Corrects: 2601\n",
            "Training finished in 13.8569917678833 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3d7f90>\n",
            "Constructing exemplars of class 67\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [2493, 27223, 4863, 13909, 3847, 46588, 49104, 28814, 29434, 38330, 49439, 20414, 9853, 36337, 5730, 12995, 37065, 26856, 17396, 45668, 41907, 42180, 9791, 25966, 16820, 1027, 37188, 42128, 22892, 9034, 33991, 37012, 11412, 37583, 5825, 37145, 4362, 9034, 8825, 37065, 40581, 6455, 4076, 18015, 6815, 16047, 33264, 6373, 8176, 1392, 965, 28814, 41706, 20414, 41648, 43635, 9174, 20057, 15332, 13905, 18423, 46553, 5825, 38422, 10194, 23836, 13704, 12575, 40782, 30805, 14515, 40906, 38671, 6928, 20452, 6036, 49244, 13704, 6036, 49104, 39931, 27380, 41648, 1088, 49407, 24092, 965, 30890, 41643, 9653, 37700, 27071, 23836, 20995, 19900, 46574, 35757, 36337, 46644, 2816, 30890, 41643, 34036, 39844, 26266, 11412, 19621, 24720, 43781, 11435, 32687, 11435, 2860, 39931, 9174, 10630, 46608, 1392, 5825, 11761, 36337, 35749, 44900, 16047, 7768, 22218, 47514, 4516, 2785, 30805, 25145, 27223, 1973, 27612, 12575, 10948, 22714, 19621, 14604, 15270, 25356, 18901, 11571, 27183, 40521, 30830, 24336, 12995, 4609, 9791, 6413, 10988, 9653, 40581, 36337, 6413, 14843, 40521, 15268, 37012, 16111, 38671, 27157, 41392, 34679, 36003, 35199, 49458, 37583, 13208, 25011, 35749, 12149, 46608, 1027, 49340, 38366, 49153, 284, 17163, 38671, 20422, 10194, 36938, 39243, 49407, 34679, 47229, 32224, 10630, 47386, 37145, 12908, 26351, 25463, 15465, 13704, 32655, 24336, 42572]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3d66d0>\n",
            "Constructing exemplars of class 59\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [24261, 37987, 15381, 9026, 10920, 42912, 12832, 26977, 19092, 2834, 1270, 12082, 15810, 16001, 14004, 13465, 1571, 11260, 24255, 6973, 34779, 48359, 1160, 11254, 38279, 32614, 35676, 35501, 16738, 24236, 32372, 10073, 17507, 10111, 12254, 47967, 3711, 42912, 47226, 19949, 20713, 25553, 3165, 12054, 3500, 33020, 16001, 15810, 26849, 3165, 30109, 16650, 10087, 13249, 32077, 6973, 37987, 7065, 10919, 27587, 10064, 2987, 16650, 17427, 28218, 27167, 16001, 16777, 36223, 34455, 11004, 10103, 47372, 46093, 15381, 9981, 19949, 33062, 38279, 32264, 30471, 34455, 41109, 1562, 5504, 25930, 30328, 35811, 10111, 4708, 30883, 37676, 37719, 27167, 10920, 35811, 27489, 1048, 22988, 47809, 7093, 35676, 30055, 32264, 5277, 42060, 19474, 41678, 35136, 30109, 38195, 47618, 29305, 18985, 28218, 29872, 40007, 17567, 15810, 6571, 20207, 28860, 25566, 21649, 32192, 973, 20713, 30828, 22130, 36945, 14671, 14376, 13873, 30109, 16738, 26977, 37533, 23210, 38064, 16001, 13178, 13420, 35920, 11260, 16709, 14004, 36637, 6283, 16189, 31388, 12082, 35811, 16250, 20491, 23044, 25142, 48441, 567, 11637, 2688, 34455, 29873, 23044, 26200, 35811, 3488, 30787, 10919, 4560, 40007, 18467, 43463, 16650, 24308, 34799, 28258, 13249, 12082, 7980, 24627, 34779, 8081, 7037, 34799, 25012, 26213, 3995, 18938, 10073, 30817, 2290, 48359, 1160, 30828, 11587, 34779, 33418, 20216, 10064, 34163]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c676a10>\n",
            "Constructing exemplars of class 39\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [49423, 32037, 38905, 25386, 18143, 23943, 218, 24067, 20430, 6574, 41688, 42644, 7493, 13811, 12184, 22599, 42224, 45131, 7060, 17307, 25467, 24442, 6795, 13489, 14383, 44582, 18624, 25177, 1446, 28001, 35244, 47632, 46877, 5752, 44790, 31843, 7060, 3495, 18637, 32758, 32620, 45674, 19268, 29457, 44596, 20453, 25252, 49897, 5752, 13219, 8415, 15132, 34011, 38041, 37857, 15156, 18147, 15148, 30788, 11597, 5021, 32802, 36418, 32758, 41736, 41565, 29450, 17018, 6776, 11597, 5344, 37881, 47366, 200, 49899, 18206, 6696, 32305, 24205, 22776, 20919, 6776, 18147, 15156, 13825, 11169, 21397, 46764, 23492, 11179, 16816, 1062, 48671, 15148, 37727, 11427, 34507, 16816, 2214, 45886, 35280, 5930, 13489, 6124, 26365, 46877, 9660, 46089, 19637, 10125, 5405, 22175, 25252, 36766, 45884, 15098, 13489, 19080, 1446, 14670, 27895, 25704, 27868, 33518, 17472, 32835, 35777, 11179, 20763, 49920, 33238, 30686, 12025, 10953, 11822, 15580, 48448, 49064, 37727, 5930, 43077, 49155, 40098, 11427, 14172, 31970, 46368, 47014, 45385, 7493, 6542, 19725, 23668, 36418, 5688, 35268, 5414, 25537, 42628, 37062, 45674, 19268, 36912, 23011, 28986, 44596, 17437, 5752, 28163, 41383, 37881, 16475, 41688, 3965, 48544, 24369, 47593, 2213, 39279, 7639, 38449, 21409, 101, 29050, 22175, 5021, 37919, 19725, 45146, 19637, 3758, 1446, 38905, 31502, 32439, 12263, 25126, 10340, 13745, 6542]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c23e990>\n",
            "Constructing exemplars of class 22\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [48379, 25457, 22774, 30469, 17596, 30764, 632, 33877, 14862, 15329, 15804, 8411, 38235, 44433, 44520, 27911, 17623, 4915, 14507, 31767, 38016, 45986, 36375, 27012, 37478, 34873, 6544, 26696, 25300, 13977, 8656, 36122, 9305, 28257, 42014, 34912, 4522, 6526, 6376, 11635, 17251, 7341, 35337, 33853, 2053, 38451, 11429, 8088, 44433, 21510, 45561, 20180, 39111, 36675, 31053, 46837, 37861, 40034, 5766, 7341, 16612, 22156, 46552, 13630, 30411, 7761, 8389, 9681, 42949, 12726, 8126, 26982, 21790, 19004, 36397, 2503, 8706, 19813, 41161, 23454, 24970, 44529, 22156, 7678, 28109, 1442, 42949, 5402, 40972, 5293, 4688, 5725, 12872, 8638, 24262, 26696, 11693, 40431, 36844, 8656, 6300, 17599, 19765, 40431, 39621, 44557, 24165, 15667, 40349, 18163, 48818, 19462, 6369, 16472, 46230, 11002, 4647, 35638, 5293, 632, 22036, 28257, 20024, 8389, 24415, 6300, 7761, 25186, 35194, 6090, 26778, 48995, 35732, 31767, 2584, 7761, 30411, 37554, 21333, 40292, 747, 36977, 34912, 29999, 19765, 30472, 5102, 36314, 10826, 13150, 7888, 1442, 5885, 38901, 33527, 6762, 12675, 30505, 17326, 43175, 19004, 19020, 33886, 8638, 37382, 35337, 424, 36204, 3004, 31767, 7784, 14862, 18614, 42949, 27820, 32505, 12670, 9828, 7784, 945, 48818, 11443, 10936, 27911, 31223, 22156, 47193, 9681, 3293, 11203, 9104, 46552, 9038, 46426, 20024, 17646, 36093, 44704, 10656, 48995]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c676fd0>\n",
            "Constructing exemplars of class 18\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [25395, 49145, 17430, 38092, 27799, 8405, 31736, 13477, 43832, 10419, 20720, 32258, 9782, 29034, 31962, 29210, 12968, 15174, 13417, 7557, 29811, 13606, 33465, 11999, 46367, 35442, 15969, 36385, 5060, 43832, 16017, 10829, 12008, 35061, 13030, 1273, 21078, 44655, 30043, 2778, 18176, 22390, 2434, 24328, 27613, 34561, 4107, 440, 2811, 49759, 10144, 4461, 22438, 9144, 5477, 20174, 16929, 23573, 10693, 39132, 7149, 3343, 19596, 29926, 36385, 38118, 2003, 43749, 7908, 22976, 6431, 28528, 46925, 20261, 32963, 43156, 18683, 22115, 47407, 9605, 2435, 40615, 38379, 30216, 4461, 14811, 38976, 41532, 30092, 40085, 35968, 31961, 10693, 13269, 24271, 42724, 30043, 39149, 27698, 46367, 35442, 2554, 30092, 13785, 33465, 14776, 35917, 14102, 45162, 2692, 10693, 27981, 38719, 16819, 13269, 17232, 27981, 46622, 43870, 38974, 32122, 13417, 20584, 21650, 14283, 36802, 41949, 49433, 22390, 47407, 42420, 7649, 31530, 31962, 21960, 35004, 1597, 13172, 18541, 45817, 38776, 2434, 4461, 19077, 30092, 18932, 38379, 49129, 26439, 21078, 41005, 35442, 22397, 9774, 27981, 32431, 25531, 27221, 5189, 27169, 17134, 4646, 24795, 14102, 18503, 19596, 37924, 25982, 24601, 27981, 24252, 33606, 22322, 3570, 36761, 2778, 3114, 39521, 1005, 14279, 37016, 1273, 41094, 21442, 5060, 17561, 21078, 4786, 35637, 21348, 38170, 20386, 27343, 33465, 11808, 10419, 25395, 19921, 3114, 37638]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c966790>\n",
            "Constructing exemplars of class 65\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [47299, 23560, 32485, 39587, 3602, 36020, 14986, 37153, 16237, 47616, 20321, 33367, 23464, 432, 22801, 6254, 26205, 21279, 26586, 16041, 35284, 15499, 28135, 28375, 17685, 8531, 8338, 6708, 45893, 22195, 39396, 7322, 46170, 17480, 18570, 10962, 10704, 19734, 35631, 35284, 2972, 28962, 983, 18720, 7973, 8225, 34105, 22729, 22801, 3206, 18709, 44985, 24955, 4158, 18691, 463, 31718, 33835, 17044, 3576, 22123, 9040, 29000, 16779, 45287, 27400, 12674, 14941, 22498, 7010, 42091, 13805, 3807, 21354, 49493, 21353, 432, 29924, 29675, 44923, 8009, 24955, 4158, 25387, 15499, 18252, 35425, 27376, 42091, 18498, 34695, 11441, 645, 44002, 13399, 22720, 21975, 42353, 38680, 14712, 48711, 49572, 22720, 17268, 29952, 6255, 33781, 43135, 14200, 21639, 365, 39308, 18709, 44985, 13533, 22720, 19895, 48164, 39714, 36776, 3206, 3602, 2424, 31883, 3799, 23825, 16679, 41727, 7762, 6157, 33366, 10177, 35407, 1814, 16039, 30950, 32583, 17044, 36547, 44836, 7354, 41417, 33043, 28962, 24847, 36076, 19220, 10704, 44459, 34695, 7777, 16986, 21279, 37662, 28962, 32987, 45463, 29364, 47616, 33597, 19199, 7323, 34559, 39188, 39587, 18315, 34695, 49493, 23819, 12674, 42643, 36076, 4158, 15789, 11836, 47845, 46475, 39714, 13152, 21347, 15296, 3576, 32585, 26757, 28855, 4603, 49003, 21639, 8531, 8338, 49752, 594, 4158, 22729, 18720, 21366, 8526, 44985, 16505, 43574]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c226510>\n",
            "Constructing exemplars of class 49\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [38251, 30220, 25633, 30522, 23821, 30826, 1542, 16668, 34270, 39979, 38402, 21727, 17272, 46044, 14890, 37237, 8515, 10421, 15401, 8856, 6758, 23625, 6117, 16363, 49043, 46569, 19075, 35671, 19911, 29350, 28201, 38236, 34270, 4264, 30826, 14536, 43512, 31401, 39644, 23169, 30549, 46931, 6144, 30821, 36373, 11308, 15491, 39780, 38404, 18456, 45722, 30522, 6622, 17524, 34224, 23625, 29777, 29526, 43512, 515, 13859, 35949, 34303, 1147, 19676, 25127, 46767, 40890, 28201, 14536, 8301, 27143, 14929, 34951, 6117, 42740, 28054, 2265, 46044, 617, 43738, 1147, 23913, 17099, 31146, 31350, 28691, 31975, 2265, 39780, 30549, 36009, 36373, 1434, 18112, 5061, 45718, 18001, 45095, 24785, 42325, 19812, 40890, 13859, 2920, 43337, 42904, 4047, 41037, 34224, 49659, 11736, 16421, 27170, 9824, 31721, 43512, 17896, 17790, 45945, 19812, 47186, 6809, 11919, 20211, 46044, 49412, 48754, 33433, 40747, 1140, 30522, 12722, 32735, 31003, 46075, 31578, 45633, 48987, 45650, 2236, 2219, 37546, 423, 35911, 18136, 41320, 21923, 5662, 43049, 22725, 43017, 15100, 34801, 20532, 33744, 15544, 11517, 29706, 15958, 21727, 1147, 34270, 49080, 23169, 31975, 1298, 9020, 16251, 45762, 43738, 34951, 17272, 17576, 44670, 27143, 8301, 6351, 11652, 19911, 467, 2262, 48987, 17524, 5908, 38209, 14929, 37546, 14804, 34270, 30821, 34111, 25633, 4351, 37546, 17229, 40191, 2444, 36373, 45762]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c2267d0>\n",
            "Constructing exemplars of class 56\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [34146, 1822, 21594, 26410, 6509, 26150, 22623, 29297, 30691, 9632, 39945, 8914, 12499, 40994, 22971, 47438, 10856, 41541, 48466, 4387, 21613, 7289, 13025, 14392, 11843, 108, 10239, 27809, 25826, 43671, 6766, 43311, 47662, 18867, 39221, 25059, 29639, 30715, 17919, 42473, 40733, 37352, 25780, 47805, 27676, 8628, 49001, 24037, 31891, 46451, 21522, 2762, 629, 26393, 39054, 20305, 48478, 27001, 9102, 31644, 49875, 9542, 39591, 38889, 49633, 14841, 1468, 43671, 5954, 25519, 48484, 48820, 33660, 14392, 33586, 46633, 19681, 16238, 27200, 45142, 43308, 41946, 39314, 31671, 15432, 37299, 34304, 39436, 45836, 6486, 17754, 269, 7751, 6595, 3044, 36898, 19240, 9491, 4493, 6290, 43633, 26544, 16420, 8063, 12564, 11376, 2764, 34304, 34183, 12571, 13189, 3044, 31427, 27876, 29454, 1529, 6486, 42019, 25059, 31574, 48478, 39500, 7125, 14906, 40710, 31644, 26153, 45142, 689, 12499, 32266, 40003, 48788, 21613, 5316, 15000, 42786, 7619, 9452, 7289, 23839, 36137, 6829, 44068, 42473, 7492, 13045, 1701, 35579, 30758, 48440, 44808, 4387, 20305, 43308, 47956, 1468, 26765, 24599, 40710, 49638, 31465, 41268, 34378, 26289, 25059, 26150, 31891, 26765, 1529, 40710, 14752, 19144, 14392, 18192, 5233, 37809, 17822, 47423, 8266, 4657, 28823, 19718, 47662, 29524, 35565, 41663, 43212, 45845, 1918, 39500, 24037, 24599, 26765, 44416, 43011, 7751, 43308, 37281, 48440]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c226150>\n",
            "Constructing exemplars of class 20\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [15166, 34159, 46547, 11785, 10219, 16438, 32921, 42719, 25513, 6728, 14139, 11001, 2747, 26111, 2020, 48170, 25513, 26973, 27424, 4586, 23321, 3158, 48567, 9289, 46677, 6072, 24084, 41466, 11958, 18131, 11488, 5401, 3033, 41209, 4586, 45453, 23976, 57, 23000, 134, 44379, 1827, 34122, 30460, 13794, 49986, 46897, 3033, 28851, 30824, 18453, 118, 5284, 18320, 49354, 29519, 39758, 19539, 15654, 41924, 44861, 36039, 3117, 26407, 5231, 9289, 29259, 44861, 14029, 47829, 25136, 3197, 46108, 30827, 5284, 9507, 14207, 16301, 16649, 30383, 27548, 26613, 41209, 4768, 49657, 38013, 32955, 34493, 16302, 18453, 47829, 44177, 22230, 48340, 26405, 10985, 29591, 5284, 23045, 19132, 20210, 7285, 37657, 37732, 13760, 39849, 2020, 28391, 25513, 9289, 5940, 29591, 27548, 42123, 7038, 12253, 49855, 43736, 40494, 45704, 24084, 7205, 908, 13794, 31785, 46001, 44111, 25504, 32672, 45453, 47798, 9374, 17318, 6279, 14207, 34122, 32955, 12314, 17446, 1819, 32262, 19763, 37898, 41924, 39151, 42719, 31197, 32262, 44914, 47829, 134, 46898, 36514, 10335, 16774, 25015, 32705, 46015, 47108, 49630, 46108, 30827, 32955, 22320, 3158, 30383, 26830, 40989, 4623, 24829, 33829, 5846, 34663, 20631, 49657, 27649, 15143, 26572, 1768, 48324, 1819, 47921, 5643, 37332, 6307, 48767, 46677, 49855, 39401, 1827, 46008, 19855, 33153, 1383, 44839, 636, 39022, 21572, 26768, 23654]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c980210>\n",
            "Constructing exemplars of class 4\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [8616, 19191, 36030, 11074, 13211, 40009, 4813, 36339, 21645, 28584, 40386, 7015, 33312, 44200, 9380, 33880, 4021, 37055, 42042, 24046, 26580, 32837, 28628, 25099, 46992, 49250, 13442, 17304, 48414, 33850, 27056, 46937, 43490, 35690, 6591, 12963, 13422, 43490, 38272, 6834, 30288, 2769, 12526, 46335, 37645, 33791, 21103, 20735, 23505, 42042, 24046, 33174, 10243, 38272, 31321, 37030, 18449, 20446, 39400, 28584, 42727, 20583, 3230, 45367, 21645, 20078, 40753, 2711, 32866, 6993, 21069, 12963, 30683, 5542, 24154, 14436, 33329, 46589, 44162, 12802, 41403, 36124, 24046, 33793, 27902, 25134, 6993, 4851, 4865, 3470, 16148, 12526, 20804, 47355, 32185, 43786, 9833, 825, 36241, 37057, 1308, 6051, 24619, 7083, 20970, 34433, 26476, 7015, 27514, 12122, 46589, 7819, 18449, 5677, 30185, 44872, 12526, 30683, 27884, 18449, 33791, 145, 10417, 3423, 3230, 41403, 11798, 32771, 4449, 18449, 12897, 5139, 45465, 8946, 3423, 12526, 6591, 35390, 21109, 10983, 20446, 24619, 4899, 43786, 8292, 42050, 40230, 44629, 1636, 47109, 3865, 49483, 389, 35649, 29495, 12122, 31043, 44104, 4449, 19181, 12963, 39748, 15063, 3310, 4449, 113, 25099, 42842, 8179, 14826, 23270, 13454, 20699, 12802, 20970, 3801, 1302, 8008, 42842, 7682, 19118, 42349, 3470, 2139, 2866, 3865, 14865, 3779, 15812, 43490, 25090, 8946, 49483, 44237, 2866, 30853, 18400, 32771, 33880, 24046]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.56 0.2094002068042755\n",
            "TEST GROUP:  0.612\n",
            "TEST ALL:  0.508\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  2000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [82, 81, 7, 16, 18, 20, 21, 22, 34, 39, 47, 49, 56, 59, 65, 67, 68, 79, 80, 4]\n",
            "TRAIN_SET CLASSES:  [79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "VALIDATION CLASSES:  [47, 34, 21, 16, 82, 81, 80, 79, 7, 68]\n",
            "GROUP:  2\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  20\n",
            "5\n",
            "Starting epoch 1/5, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.4786575734615326\n",
            "Train step - Step 10, Loss 0.2408965677022934\n",
            "Train step - Step 20, Loss 0.2520991265773773\n",
            "Train step - Step 30, Loss 0.22044594585895538\n",
            "Train step - Step 40, Loss 0.21477492153644562\n",
            "Train step - Step 50, Loss 0.21496336162090302\n",
            "Train epoch - Accuracy: 0.25625899280575537 Loss: 0.24712913065076733 Corrects: 1403\n",
            "Starting epoch 2/5, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.21163992583751678\n",
            "Train step - Step 70, Loss 0.2167361080646515\n",
            "Train step - Step 80, Loss 0.21134009957313538\n",
            "Train step - Step 90, Loss 0.20694971084594727\n",
            "Train step - Step 100, Loss 0.21117554605007172\n",
            "Train epoch - Accuracy: 0.28647482014388487 Loss: 0.20996199115145978 Corrects: 1962\n",
            "Starting epoch 3/5, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.20255409181118011\n",
            "Train step - Step 120, Loss 0.21074318885803223\n",
            "Train step - Step 130, Loss 0.2090960592031479\n",
            "Train step - Step 140, Loss 0.2045198529958725\n",
            "Train step - Step 150, Loss 0.22030936181545258\n",
            "Train step - Step 160, Loss 0.19820432364940643\n",
            "Train epoch - Accuracy: 0.33323741007194246 Loss: 0.2043877715729981 Corrects: 2244\n",
            "Starting epoch 4/5, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.19694355130195618\n",
            "Train step - Step 180, Loss 0.20370490849018097\n",
            "Train step - Step 190, Loss 0.20076122879981995\n",
            "Train step - Step 200, Loss 0.19872407615184784\n",
            "Train step - Step 210, Loss 0.19086265563964844\n",
            "Train epoch - Accuracy: 0.3601438848920863 Loss: 0.19970232619227266 Corrects: 2456\n",
            "Starting epoch 5/5, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.2055506706237793\n",
            "Train step - Step 230, Loss 0.18172195553779602\n",
            "Train step - Step 240, Loss 0.19336792826652527\n",
            "Train step - Step 250, Loss 0.1985117644071579\n",
            "Train step - Step 260, Loss 0.20734453201293945\n",
            "Train step - Step 270, Loss 0.2019658088684082\n",
            "Train epoch - Accuracy: 0.378705035971223 Loss: 0.19654389035358705 Corrects: 2578\n",
            "Training finished in 32.25327444076538 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c366650>\n",
            "Constructing exemplars of class 79\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [32829, 27602, 1977, 23535, 46033, 16341, 29605, 8586, 24098, 7140, 47938, 5404, 49095, 36438, 37142, 38156, 35165, 9793, 4744, 23535, 16517, 47158, 29031, 24260, 25718, 38926, 49072, 36145, 14441, 26446, 40781, 41564, 11310, 8223, 25004, 6211, 5485, 6513, 13555, 33252, 3139, 46745, 22362, 21378, 34943, 28188, 20711, 31813, 24493, 7775, 11310, 9378, 29504, 39947, 41078, 28063, 6211, 35258, 30418, 16514, 13846, 28435, 23381, 363, 42823, 34273, 5804, 29059, 7016, 10556, 30124, 29504, 6494, 12495, 29243, 45237, 11123, 47739, 38175, 24547, 38515, 8336, 13555, 6513, 23272, 48012, 37245, 6842, 7133, 40565, 42723, 26188, 7016, 640, 38725, 38926, 18378, 12646, 40529, 10996]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c369c90>\n",
            "Constructing exemplars of class 47\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [17416, 23245, 33168, 33014, 13857, 39354, 38323, 33273, 18652, 44843, 25483, 22234, 36842, 24080, 38500, 3281, 5134, 19937, 43570, 14964, 1786, 26618, 4240, 49031, 34456, 9555, 2667, 38536, 17620, 38536, 14038, 27133, 23407, 19927, 8878, 13301, 24422, 5345, 3327, 59, 39661, 10799, 1670, 16654, 9644, 59, 49764, 7059, 36230, 16873, 41073, 25576, 6429, 22234, 19824, 7121, 25313, 19157, 5020, 15845, 6704, 43269, 24646, 45364, 44251, 31892, 47802, 44779, 45783, 38536, 49526, 26642, 56, 45979, 31553, 8653, 26632, 42412, 1786, 26618, 45800, 22234, 16937, 14666, 34621, 33168, 31025, 29029, 30680, 8193, 44779, 10245, 21027, 43570, 1786, 8185, 9704, 28601, 34532, 1864]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c7d2810>\n",
            "Constructing exemplars of class 7\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [18920, 9862, 34285, 24978, 24869, 44096, 13986, 23354, 5279, 19980, 15748, 18207, 21893, 20596, 32730, 16035, 31764, 47924, 8110, 38438, 36670, 26002, 11767, 15496, 37815, 42898, 27018, 3810, 43273, 39511, 32909, 22759, 15748, 6367, 9118, 23354, 40476, 41605, 25806, 29293, 13962, 38690, 28243, 29732, 482, 46051, 42288, 377, 40128, 40805, 36892, 26640, 49282, 47435, 28363, 8444, 40805, 26593, 14762, 20596, 8716, 12202, 44860, 38876, 4978, 12293, 12134, 15042, 24869, 23752, 45508, 6367, 6679, 15243, 7910, 4275, 25806, 40734, 35841, 8110, 20569, 44096, 4884, 7110, 2161, 36222, 35960, 11540, 13286, 14701, 37565, 7002, 377, 29697, 45809, 36934, 13986, 30814, 31942, 39511]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f340b95fad0>\n",
            "Constructing exemplars of class 82\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [36301, 36327, 7586, 21064, 40409, 4037, 32449, 26501, 37521, 37189, 37510, 32741, 28265, 12210, 12252, 20686, 35369, 18202, 18702, 26852, 41977, 44523, 30528, 40655, 274, 23719, 5904, 31729, 28164, 40082, 18937, 871, 36690, 36727, 34612, 5454, 15304, 42693, 47534, 23130, 16467, 6277, 6227, 40367, 9111, 41900, 37521, 6141, 32106, 324, 44213, 10381, 30911, 28164, 29650, 4037, 38355, 34412, 27709, 27053, 32644, 6761, 16228, 12518, 15113, 13309, 49631, 19060, 29229, 16897, 18644, 34673, 44928, 25265, 11901, 9637, 45373, 42250, 19839, 7471, 32602, 8061, 43699, 46768, 26562, 5578, 33931, 18937, 6057, 10519, 3842, 3340, 16791, 16838, 33522, 31815, 14807, 11714, 23130, 45482]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c35e150>\n",
            "Constructing exemplars of class 34\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [41684, 49918, 28754, 36335, 48048, 28310, 31864, 42407, 8555, 15671, 5308, 21337, 3773, 15614, 30618, 47152, 5552, 39095, 10216, 35278, 43775, 2382, 13272, 8396, 40019, 31864, 29853, 5379, 35762, 120, 42093, 2324, 16611, 1472, 9108, 4091, 48286, 32312, 4556, 44854, 47241, 24571, 5308, 27664, 27630, 29129, 12329, 29468, 6096, 14310, 14859, 36177, 48940, 45766, 35325, 36330, 45900, 7853, 49605, 7368, 40597, 11646, 48783, 17608, 10940, 27028, 41972, 2648, 21197, 31833, 49111, 24459, 45766, 39095, 37038, 8406, 23411, 46887, 28868, 25510, 40438, 29468, 1125, 6849, 4681, 13335, 37038, 8824, 33386, 47241, 48286, 46124, 13038, 12459, 20892, 16193, 15179, 1076, 25110, 16944]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c35b990>\n",
            "Constructing exemplars of class 81\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [12751, 186, 45181, 23159, 20226, 2097, 30409, 503, 39819, 1943, 43532, 40790, 19383, 4925, 8321, 25857, 47627, 49830, 14183, 34026, 9696, 186, 13014, 38079, 9789, 41277, 8668, 5539, 35648, 40177, 22076, 14183, 20504, 43892, 10584, 16503, 6997, 11223, 47237, 42338, 37290, 49232, 22308, 2841, 20278, 31655, 23159, 10180, 18708, 41277, 45736, 21021, 42418, 35097, 36296, 32821, 37894, 9281, 27129, 18389, 41549, 49742, 24532, 20504, 45736, 7554, 23171, 49102, 19095, 49830, 45157, 31655, 22260, 5539, 8276, 6172, 40631, 32141, 35119, 40965, 39790, 43447, 11223, 48066, 34460, 33201, 12746, 40369, 18325, 14648, 28000, 38528, 49309, 25500, 18037, 42926, 36296, 15149, 15119, 23286]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c23aa10>\n",
            "Constructing exemplars of class 21\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [6133, 40466, 12355, 32641, 10676, 35819, 2495, 12998, 7991, 7314, 21562, 16105, 46760, 49609, 30804, 42398, 12998, 3277, 14427, 48058, 16826, 19339, 27524, 41054, 44316, 45390, 13912, 40270, 15874, 42236, 8700, 30132, 41092, 1902, 38040, 21758, 13788, 17657, 46251, 11367, 13806, 1672, 15256, 35619, 3737, 17263, 23559, 49609, 37353, 30904, 12784, 42398, 5713, 2406, 23522, 3900, 3043, 4610, 22795, 35176, 42679, 172, 11712, 4455, 3900, 36549, 49390, 14101, 25784, 36592, 35619, 18764, 11210, 9433, 6357, 21043, 39688, 43491, 2193, 17386, 8725, 24099, 38201, 1794, 37128, 9177, 39688, 37046, 10284, 2695, 49014, 23328, 37120, 13806, 26004, 48668, 40995, 41431, 14419, 21043]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3ecdd0>\n",
            "Constructing exemplars of class 80\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [8851, 16681, 15026, 684, 361, 30281, 41023, 41504, 42914, 7599, 3095, 31568, 18078, 2586, 7191, 37506, 28931, 30641, 40423, 33526, 6320, 43266, 40896, 17949, 18, 40095, 15106, 21740, 192, 13998, 33526, 19750, 43009, 25878, 7771, 18375, 43204, 28928, 34262, 1404, 37298, 24057, 3095, 28406, 36788, 20875, 17441, 7523, 41504, 45254, 28406, 42529, 37374, 25878, 30112, 39656, 43266, 48245, 46302, 45959, 44870, 4924, 19563, 1212, 25749, 39537, 42210, 28299, 3514, 33392, 42206, 42529, 37298, 18021, 17990, 39496, 9302, 40844, 49357, 19346, 28931, 2045, 35914, 33348, 4380, 1124, 38931, 47146, 340, 37506, 1090, 29240, 22392, 32587, 11142, 15974, 34060, 16519, 697, 35914]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3d7e50>\n",
            "Constructing exemplars of class 68\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [27599, 31919, 31044, 46739, 36879, 36099, 49932, 26793, 44804, 10921, 14817, 37785, 45021, 11225, 41493, 10028, 38288, 22453, 49464, 28443, 35328, 9294, 932, 1245, 17077, 30341, 9441, 18331, 46517, 37234, 47074, 24727, 45714, 35058, 29473, 24338, 39701, 18690, 30834, 36856, 4080, 22345, 5462, 320, 46401, 19562, 44458, 35625, 20816, 4342, 40435, 33990, 2815, 46591, 29117, 22905, 37246, 37785, 3836, 17859, 32979, 44652, 47696, 2116, 13212, 8399, 43263, 22453, 34184, 38822, 15046, 15022, 4342, 9591, 49013, 3918, 47868, 7915, 6378, 9723, 25899, 31434, 20014, 22591, 36099, 26793, 9196, 15573, 24524, 18740, 1831, 20325, 28450, 40017, 41928, 32181, 46739, 21729, 2754, 36856]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c22ab90>\n",
            "Constructing exemplars of class 16\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [13543, 22380, 18738, 6305, 34239, 1414, 40901, 34895, 24199, 20465, 15396, 41484, 32487, 22297, 26710, 7513, 7930, 14309, 36935, 15439, 23953, 11817, 12859, 5810, 19097, 2868, 22451, 4542, 23692, 37111, 10619, 35546, 27967, 46976, 11166, 44380, 38554, 12643, 29271, 37324, 13725, 35745, 16965, 1898, 13689, 47601, 38519, 25632, 29373, 10616, 28550, 49991, 26400, 43035, 7930, 20019, 30494, 22825, 48907, 45065, 18348, 24721, 35320, 39606, 20240, 4567, 24132, 35005, 596, 41771, 699, 13792, 7826, 36421, 40527, 8702, 41823, 26484, 29271, 16534, 30251, 4551, 40313, 13689, 30331, 38743, 49055, 3350, 5867, 25137, 36935, 14182, 45983, 39201, 3869, 22549, 41865, 9864, 29901, 5511]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.2 0.1707935631275177\n",
            "TEST GROUP:  0.391\n",
            "TEST ALL:  0.406\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  3000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [81, 79, 4, 10, 16, 18, 20, 22, 24, 32, 34, 56, 64, 68, 76, 80, 82, 90, 7, 21, 23, 39, 47, 49, 59, 61, 65, 67, 75, 0]\n",
            "TRAIN_SET CLASSES:  [75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "VALIDATION CLASSES:  [61, 32, 90, 24, 23, 76, 75, 10, 0, 64]\n",
            "GROUP:  3\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  30\n",
            "5\n",
            "Starting epoch 1/5, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.4100273847579956\n",
            "Train step - Step 10, Loss 0.22518660128116608\n",
            "Train step - Step 20, Loss 0.2127355933189392\n",
            "Train step - Step 30, Loss 0.204080268740654\n",
            "Train step - Step 40, Loss 0.20063260197639465\n",
            "Train step - Step 50, Loss 0.19907277822494507\n",
            "Train epoch - Accuracy: 0.1776978417266187 Loss: 0.22165248812531396 Corrects: 1164\n",
            "Starting epoch 2/5, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.19283218681812286\n",
            "Train step - Step 70, Loss 0.19041304290294647\n",
            "Train step - Step 80, Loss 0.1957685351371765\n",
            "Train step - Step 90, Loss 0.19302868843078613\n",
            "Train step - Step 100, Loss 0.19334523379802704\n",
            "Train epoch - Accuracy: 0.20014388489208634 Loss: 0.19164166227090274 Corrects: 1844\n",
            "Starting epoch 3/5, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.1907123327255249\n",
            "Train step - Step 120, Loss 0.18456396460533142\n",
            "Train step - Step 130, Loss 0.1883639097213745\n",
            "Train step - Step 140, Loss 0.193313866853714\n",
            "Train step - Step 150, Loss 0.18746034801006317\n",
            "Train step - Step 160, Loss 0.18368922173976898\n",
            "Train epoch - Accuracy: 0.23568345323741008 Loss: 0.18805720366162362 Corrects: 2148\n",
            "Starting epoch 4/5, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.18554319441318512\n",
            "Train step - Step 180, Loss 0.1876971274614334\n",
            "Train step - Step 190, Loss 0.18833711743354797\n",
            "Train step - Step 200, Loss 0.18500302731990814\n",
            "Train step - Step 210, Loss 0.1818116307258606\n",
            "Train epoch - Accuracy: 0.26402877697841726 Loss: 0.1862017927752982 Corrects: 2327\n",
            "Starting epoch 5/5, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.18272311985492706\n",
            "Train step - Step 230, Loss 0.18316835165023804\n",
            "Train step - Step 240, Loss 0.1806299090385437\n",
            "Train step - Step 250, Loss 0.18698008358478546\n",
            "Train step - Step 260, Loss 0.18355143070220947\n",
            "Train step - Step 270, Loss 0.18932068347930908\n",
            "Train epoch - Accuracy: 0.28949640287769784 Loss: 0.1841672743696103 Corrects: 2517\n",
            "Training finished in 32.54950284957886 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3b6250>\n",
            "Constructing exemplars of class 75\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [36676, 19729, 4358, 49346, 43454, 19926, 25325, 15616, 26802, 36510, 48685, 42320, 3022, 30564, 30295, 49346, 48685, 46180, 18908, 2672, 37294, 44017, 11393, 16112, 32982, 33100, 8446, 15984, 2571, 31392, 49868, 42096, 46180, 38077, 30895, 9825, 14355, 9778, 12573, 35128, 14259, 40843, 27180, 25608, 14236, 1625, 39461, 9869, 96, 38527, 1221, 11235, 34141, 33079, 2968, 23138, 10321, 35682, 8446, 15984, 17148, 39246, 46843, 36115, 16990, 37702]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3993d0>\n",
            "Constructing exemplars of class 23\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [901, 26570, 32865, 29128, 4436, 23635, 47380, 42614, 16680, 12494, 44638, 10659, 13380, 27392, 16151, 17152, 31426, 29111, 31830, 31679, 21890, 28100, 11755, 41940, 47077, 33888, 25497, 14482, 22597, 12213, 22703, 42455, 5463, 33316, 36855, 29128, 36377, 18542, 35170, 12213, 31878, 40130, 47216, 37707, 38385, 11418, 43328, 42483, 31329, 23575, 42959, 37869, 17475, 31129, 10461, 26110, 24790, 44940, 31167, 37792, 19910, 19732, 25073, 40523, 7018, 24898]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c399dd0>\n",
            "Constructing exemplars of class 90\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [24007, 22740, 40599, 1476, 23291, 9687, 25809, 46249, 49099, 39540, 15349, 20518, 14321, 27016, 32719, 42217, 10879, 7369, 5222, 34976, 47, 3442, 28622, 45708, 47140, 14866, 20896, 28312, 5304, 21114, 13515, 22385, 37367, 24580, 44748, 22638, 18880, 10482, 22122, 29355, 40811, 9241, 39115, 49712, 26282, 10960, 1956, 34171, 25837, 49313, 28929, 46835, 10960, 49313, 45818, 27734, 21610, 26196, 10106, 746, 41601, 3736, 10174, 791, 4581, 14866]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c35c310>\n",
            "Constructing exemplars of class 10\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [36971, 49077, 30717, 35787, 32162, 30225, 27957, 41961, 22586, 2237, 39855, 17980, 24639, 42813, 7941, 18361, 5193, 18346, 47288, 8048, 13343, 17344, 7406, 42371, 45942, 30498, 13766, 8456, 2127, 42493, 40883, 30225, 11858, 26744, 30042, 23295, 12547, 49516, 14231, 7476, 39040, 26742, 41451, 19503, 7299, 36528, 48795, 28748, 47936, 35241, 26550, 11486, 49415, 28091, 35434, 20233, 1587, 47240, 26106, 44786, 22387, 29045, 15693, 43599, 11486, 1579]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c99f350>\n",
            "Constructing exemplars of class 61\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [20826, 9804, 36734, 25017, 25187, 16330, 16273, 24139, 12562, 22836, 11591, 24156, 22651, 26660, 30244, 19790, 10268, 1552, 6864, 6869, 29425, 39329, 5120, 10392, 29475, 13844, 48512, 44155, 36796, 34818, 24139, 19121, 8709, 104, 49553, 544, 8980, 31935, 44144, 10277, 43495, 7377, 32226, 39247, 6614, 49581, 13110, 41364, 2513, 26601, 32360, 33262, 41422, 26660, 30334, 10277, 37139, 17650, 25239, 12029, 2789, 8865, 48512, 34237, 34475, 49725]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339e772310>\n",
            "Constructing exemplars of class 76\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [1475, 358, 34067, 40173, 18298, 44414, 38736, 18218, 39405, 48023, 36220, 37394, 29227, 44168, 20688, 4781, 5842, 19983, 48571, 34764, 18689, 20640, 14403, 34758, 34811, 41442, 27484, 48023, 44610, 40745, 31940, 3075, 40000, 5515, 4129, 23601, 9263, 18689, 35418, 15619, 49028, 44692, 12586, 14446, 40580, 28912, 20728, 10426, 3245, 34898, 40305, 22767, 30071, 41197, 49443, 10148, 32418, 4129, 5939, 38020, 4111, 32042, 4793, 45873, 10779, 10209]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c98d610>\n",
            "Constructing exemplars of class 64\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [14868, 41783, 44603, 15622, 14009, 17871, 3260, 6185, 46704, 26806, 18505, 173, 25533, 3220, 31797, 20255, 30513, 38436, 37690, 11190, 34409, 32467, 12769, 19318, 29860, 6366, 1373, 49819, 18354, 26611, 13657, 47076, 43917, 19458, 42025, 30209, 14271, 19439, 13835, 5917, 30209, 45275, 42840, 21703, 16397, 33172, 1376, 9474, 39317, 46759, 173, 46299, 10418, 12325, 1810, 10418, 27049, 22931, 12451, 23156, 19811, 10576, 46562, 21417, 17249, 11217]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c366a10>\n",
            "Constructing exemplars of class 32\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [38820, 40870, 16745, 39872, 24938, 26328, 29881, 9818, 26386, 1145, 3322, 42160, 25369, 42928, 9456, 26534, 12804, 29001, 12612, 23846, 48637, 25816, 48706, 22172, 20556, 12612, 13682, 7872, 25991, 27689, 549, 25067, 17680, 33534, 38454, 39950, 4306, 33263, 39019, 40185, 38754, 14494, 32960, 4048, 7455, 13648, 36306, 31411, 9886, 31879, 28059, 23088, 46813, 29195, 48183, 33187, 45661, 31175, 39382, 24837, 48706, 33705, 24010, 25369, 14405, 9133]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339d864a50>\n",
            "Constructing exemplars of class 24\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [17159, 30278, 46602, 5501, 4668, 20144, 23997, 25306, 35318, 18040, 7315, 1110, 6783, 21507, 21175, 24647, 25256, 39052, 44835, 16008, 23202, 17638, 32878, 45410, 35808, 24949, 3518, 21255, 32878, 37247, 31689, 32956, 21844, 48096, 43439, 47898, 37327, 34263, 6482, 37040, 35139, 24177, 49776, 41463, 17584, 19633, 42638, 20718, 1506, 37576, 5574, 8516, 21507, 8440, 1651, 27859, 31960, 41351, 957, 37130, 20144, 31342, 4308, 14595, 44831, 15147]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c226a50>\n",
            "Constructing exemplars of class 0\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [32233, 36817, 32128, 27576, 45171, 3780, 11048, 14513, 15085, 26702, 30731, 30172, 16996, 2640, 40715, 11314, 40607, 15105, 37056, 28648, 39949, 42631, 29405, 26247, 39303, 32826, 23780, 33773, 26819, 2126, 27350, 6851, 21781, 44364, 15085, 40598, 43575, 21870, 33950, 29582, 15701, 40486, 6241, 9236, 7155, 46274, 16353, 5239, 17727, 7234, 6851, 10037, 44429, 2936, 41259, 176, 38525, 31252, 44972, 35389, 29399, 1979, 39318, 8065, 45682, 35137]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.4 0.1220754086971283\n",
            "TEST GROUP:  0.369\n",
            "TEST ALL:  0.3416666666666667\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  4000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 95, 68, 64, 56, 42, 36, 34, 32, 30, 24, 22, 20, 18, 16, 10, 6, 4, 2, 72, 76, 80, 61, 83, 81, 79, 75, 67, 65, 63, 59, 82, 49, 47, 39, 23, 21, 7, 90, 0]\n",
            "TRAIN_SET CLASSES:  [95, 83, 63, 42, 30, 6, 2, 97, 72, 36]\n",
            "VALIDATION CLASSES:  [63, 42, 36, 97, 95, 30, 83, 72, 6, 2]\n",
            "GROUP:  4\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [95, 83, 63, 42, 30, 6, 2, 97, 72, 36]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "num classes till now:  40\n",
            "5\n",
            "Starting epoch 1/5, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.3499222695827484\n",
            "Train step - Step 10, Loss 0.2216702699661255\n",
            "Train step - Step 20, Loss 0.19988541305065155\n",
            "Train step - Step 30, Loss 0.20837430655956268\n",
            "Train step - Step 40, Loss 0.19468402862548828\n",
            "Train step - Step 50, Loss 0.19027912616729736\n",
            "Train epoch - Accuracy: 0.1645021645021645 Loss: 0.21463244940553391 Corrects: 862\n",
            "Starting epoch 2/5, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.19487030804157257\n",
            "Train step - Step 70, Loss 0.19478575885295868\n",
            "Train step - Step 80, Loss 0.19605349004268646\n",
            "Train step - Step 90, Loss 0.1974085420370102\n",
            "Train step - Step 100, Loss 0.1901780217885971\n",
            "Train epoch - Accuracy: 0.172005772005772 Loss: 0.1922725366608577 Corrects: 1148\n",
            "Starting epoch 3/5, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.18605127930641174\n",
            "Train step - Step 120, Loss 0.19892483949661255\n",
            "Train step - Step 130, Loss 0.1892576664686203\n",
            "Train step - Step 140, Loss 0.18869681656360626\n",
            "Train step - Step 150, Loss 0.1864238977432251\n",
            "Train step - Step 160, Loss 0.19479778409004211\n",
            "Train epoch - Accuracy: 0.173015873015873 Loss: 0.19051059945923254 Corrects: 1310\n",
            "Starting epoch 4/5, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.18930546939373016\n",
            "Train step - Step 180, Loss 0.18660981953144073\n",
            "Train step - Step 190, Loss 0.18613505363464355\n",
            "Train step - Step 200, Loss 0.1814662665128708\n",
            "Train step - Step 210, Loss 0.1858462244272232\n",
            "Train epoch - Accuracy: 0.1795093795093795 Loss: 0.19006205088906475 Corrects: 1416\n",
            "Starting epoch 5/5, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.1842668056488037\n",
            "Train step - Step 230, Loss 0.18505416810512543\n",
            "Train step - Step 240, Loss 0.19522976875305176\n",
            "Train step - Step 250, Loss 0.1819898933172226\n",
            "Train step - Step 260, Loss 0.18817420303821564\n",
            "Train step - Step 270, Loss 0.19249795377254486\n",
            "Train epoch - Accuracy: 0.17763347763347764 Loss: 0.1888795723044683 Corrects: 1594\n",
            "Training finished in 32.481855630874634 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0, 95, 83, 63, 42, 30, 6, 2, 97, 72, 36]\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c375f10>\n",
            "Constructing exemplars of class 95\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [45444, 13801, 35178, 46247, 43747, 44254, 16793, 46078, 24033, 47375, 33380, 31795, 29012, 11454, 43508, 5756, 94, 29681, 38855, 44573, 18052, 39789, 5657, 32897, 49681, 23225, 42567, 40371, 34057, 27842, 49256, 35440, 38447, 29194, 23873, 32805, 29648, 46276, 23030, 22169, 17231, 49508, 38952, 37946, 19111, 33847, 48605, 46247, 10805, 46111]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3664d0>\n",
            "Constructing exemplars of class 83\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [37432, 46671, 25416, 29197, 43615, 49471, 9298, 3688, 32150, 48064, 44727, 24713, 971, 21707, 41531, 21496, 1413, 19207, 23548, 42564, 24160, 36933, 38110, 25370, 49484, 43867, 10886, 41423, 22583, 48996, 11283, 38610, 29306, 34101, 48996, 21054, 13342, 27531, 41231, 49226, 21485, 27481, 49905, 14723, 29306, 27481, 46833, 23627, 15223, 21182]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339e7e1f90>\n",
            "Constructing exemplars of class 63\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [23674, 1181, 41040, 16862, 21062, 17225, 15298, 9447, 31587, 2887, 16004, 24060, 37307, 3091, 41334, 28707, 30491, 3027, 435, 25606, 8625, 16980, 19904, 48411, 40347, 10791, 2735, 12717, 18299, 5167, 30401, 15860, 38634, 36402, 46509, 5076, 6099, 49781, 26508, 11426, 30735, 47321, 35602, 21524, 4112, 6562, 4073, 28025, 38067, 39850]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339e741450>\n",
            "Constructing exemplars of class 42\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [22053, 36859, 46380, 28464, 40394, 14935, 35924, 4503, 10621, 18259, 11685, 12191, 25388, 23981, 30417, 39810, 4530, 32697, 35375, 49660, 3855, 4258, 28650, 20907, 22346, 29014, 12681, 24445, 33425, 33165, 12191, 39713, 49864, 12595, 42517, 3531, 250, 15152, 8426, 36214, 33280, 24445, 8426, 34760, 33917, 26098, 41036, 5035, 44048, 18459]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3511d0>\n",
            "Constructing exemplars of class 30\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [32574, 3959, 135, 41012, 18584, 4101, 30650, 8459, 33540, 45711, 36293, 21345, 11894, 31757, 39992, 40787, 23345, 41622, 33600, 45293, 35567, 35939, 29049, 40572, 28309, 9300, 3792, 47630, 48982, 20394, 33061, 6071, 2511, 46979, 6103, 43818, 17433, 35592, 48233, 15711, 42261, 33295, 42265, 42267, 6694, 32574, 18087, 42208, 40217, 13131]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c958f90>\n",
            "Constructing exemplars of class 6\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [27434, 31184, 17784, 38111, 34323, 27369, 47600, 25473, 17226, 19897, 34169, 40694, 15264, 22809, 12257, 714, 21415, 11982, 23470, 38520, 20433, 7856, 19195, 30004, 14373, 46365, 7096, 46016, 42130, 24253, 30398, 33064, 48240, 19313, 37621, 5535, 29127, 25257, 28017, 49107, 35216, 49856, 15269, 26366, 4739, 46065, 39127, 40740, 6226, 16601]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f33a2151090>\n",
            "Constructing exemplars of class 2\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [13261, 12762, 14499, 7802, 20415, 30742, 37797, 41119, 24149, 1804, 38345, 22121, 41309, 49862, 29307, 14048, 580, 19076, 22171, 16587, 31873, 37557, 9192, 30289, 17767, 35886, 8681, 42518, 20891, 10320, 14499, 5390, 16187, 11833, 40800, 22417, 18277, 40643, 33194, 27394, 14642, 25496, 41473, 15134, 27771, 20257, 11139, 2858, 18124, 18527]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c399f10>\n",
            "Constructing exemplars of class 97\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [13507, 28256, 15793, 22433, 14228, 15643, 44415, 20723, 16954, 8317, 11464, 3614, 16054, 19057, 22743, 40829, 20341, 43400, 11948, 40894, 27738, 16693, 1720, 15542, 46154, 13942, 16559, 18898, 8985, 5204, 19263, 48530, 48673, 6930, 31714, 29788, 15666, 34705, 16498, 10128, 45029, 41545, 45262, 12075, 39785, 46630, 44149, 22001, 32104, 36439]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c3e6710>\n",
            "Constructing exemplars of class 72\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [43893, 2463, 32996, 38358, 8662, 34074, 39313, 10555, 26513, 43501, 33192, 41207, 15650, 43989, 12090, 27203, 42269, 49124, 41289, 29124, 41205, 34748, 14080, 33944, 26087, 15371, 48930, 33192, 11779, 26513, 32064, 30264, 7909, 3644, 21839, 45617, 37995, 28591, 9011, 23013, 3632, 23323, 49124, 19155, 14114, 45538, 13102, 26283, 6532, 19463]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c65f890>\n",
            "Constructing exemplars of class 36\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [5518, 45214, 30117, 31740, 32402, 24960, 34791, 13607, 34243, 24767, 31599, 7818, 10506, 835, 35932, 37215, 3985, 15161, 34600, 16292, 42753, 20989, 31784, 39190, 5223, 33183, 49552, 12063, 356, 31867, 25775, 15368, 48846, 20508, 8859, 15266, 21898, 16090, 21085, 214, 12339, 21375, 25772, 25601, 9307, 36365, 12969, 36033, 4288, 44451]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.16 0.12240724265575409\n",
            "TEST GROUP:  0.174\n",
            "TEST ALL:  0.29875\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  5000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 80, 97, 93, 85, 81, 65, 61, 49, 21, 9, 96, 76, 83, 72, 68, 64, 56, 36, 32, 24, 20, 16, 4, 2, 6, 10, 18, 79, 75, 67, 63, 59, 55, 47, 39, 31, 23, 19, 7, 98, 94, 90, 82, 54, 42, 34, 30, 22, 0]\n",
            "TRAIN_SET CLASSES:  [55, 31, 19, 98, 94, 54, 93, 85, 9, 96]\n",
            "VALIDATION CLASSES:  [55, 54, 98, 96, 31, 94, 93, 85, 19, 9]\n",
            "GROUP:  5\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [55, 31, 19, 98, 94, 54, 93, 85, 9, 96]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  50\n",
            "5\n",
            "Starting epoch 1/5, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.2794016897678375\n",
            "Train step - Step 10, Loss 0.21458066999912262\n",
            "Train step - Step 20, Loss 0.2031850516796112\n",
            "Train step - Step 30, Loss 0.19893676042556763\n",
            "Train step - Step 40, Loss 0.19937297701835632\n",
            "Train step - Step 50, Loss 0.19884933531284332\n",
            "Train epoch - Accuracy: 0.09237410071942447 Loss: 0.2110676847601966 Corrects: 731\n",
            "Starting epoch 2/5, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.19256888329982758\n",
            "Train step - Step 70, Loss 0.19802996516227722\n",
            "Train step - Step 80, Loss 0.1936100870370865\n",
            "Train step - Step 90, Loss 0.20041754841804504\n",
            "Train step - Step 100, Loss 0.19804838299751282\n",
            "Train epoch - Accuracy: 0.10201438848920863 Loss: 0.19540657784012583 Corrects: 1136\n",
            "Starting epoch 3/5, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.1958869844675064\n",
            "Train step - Step 120, Loss 0.1944384127855301\n",
            "Train step - Step 130, Loss 0.19657917320728302\n",
            "Train step - Step 140, Loss 0.19930724799633026\n",
            "Train step - Step 150, Loss 0.19343560934066772\n",
            "Train step - Step 160, Loss 0.1909271627664566\n",
            "Train epoch - Accuracy: 0.11381294964028776 Loss: 0.19398411812970964 Corrects: 1287\n",
            "Starting epoch 4/5, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.19960711896419525\n",
            "Train step - Step 180, Loss 0.19213896989822388\n",
            "Train step - Step 190, Loss 0.19631388783454895\n",
            "Train step - Step 200, Loss 0.19276291131973267\n",
            "Train step - Step 210, Loss 0.19064150750637054\n",
            "Train epoch - Accuracy: 0.1227338129496403 Loss: 0.1928003161459518 Corrects: 1477\n",
            "Starting epoch 5/5, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.18779638409614563\n",
            "Train step - Step 230, Loss 0.18812400102615356\n",
            "Train step - Step 240, Loss 0.1887567788362503\n",
            "Train step - Step 250, Loss 0.1945568025112152\n",
            "Train step - Step 260, Loss 0.19487348198890686\n",
            "Train step - Step 270, Loss 0.19303111732006073\n",
            "Train epoch - Accuracy: 0.13223021582733813 Loss: 0.19220137341416996 Corrects: 1617\n",
            "Training finished in 33.01742148399353 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0, 95, 83, 63, 42, 30, 6, 2, 97, 72, 36, 55, 31, 19, 98, 94, 54, 93, 85, 9, 96]\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  40\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c399dd0>\n",
            "Constructing exemplars of class 55\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [24446, 45731, 1905, 38765, 40120, 23475, 48683, 46844, 21760, 30568, 1544, 24096, 42574, 29596, 46872, 45582, 2993, 9693, 11539, 19309, 24018, 37251, 39217, 3052, 23798, 43894, 36424, 31734, 17697, 8060, 41597, 4544, 27388, 7394, 38832, 4383, 29490, 49779, 38423, 24096]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c375650>\n",
            "Constructing exemplars of class 31\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [47198, 11256, 14564, 30859, 20503, 26056, 16115, 9730, 39085, 15461, 29317, 48911, 32374, 14450, 38882, 48403, 8073, 19931, 4946, 11777, 21784, 42090, 32948, 31113, 30862, 593, 8068, 31845, 46812, 14777, 44187, 1826, 19078, 32318, 30013, 21061, 520, 17696, 9753, 38083]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c980950>\n",
            "Constructing exemplars of class 19\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [4283, 23503, 7821, 48061, 15104, 33564, 351, 31384, 3607, 28983, 45169, 24816, 48937, 20664, 19498, 9878, 27752, 32415, 43221, 5783, 683, 6075, 38116, 43244, 19519, 15495, 40751, 25627, 24350, 25173, 49945, 49974, 32427, 39609, 16807, 4403, 10093, 18457, 35912, 11148]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339bb07bd0>\n",
            "Constructing exemplars of class 98\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [20046, 14930, 6657, 32017, 3677, 20881, 3624, 49923, 888, 49135, 39579, 5738, 12567, 42683, 33067, 12170, 31312, 18876, 29028, 5039, 2106, 41402, 6692, 9787, 40227, 27498, 43239, 40633, 33973, 45343, 7108, 29121, 2103, 41829, 25630, 47339, 6692, 45929, 12792, 14675]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339c966190>\n",
            "Constructing exemplars of class 94\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [13563, 23885, 39985, 39092, 21745, 21693, 11233, 6536, 20653, 49023, 16046, 15527, 1297, 5106, 2309, 9445, 46995, 47196, 44067, 29644, 29636, 1435, 27796, 5888, 31910, 9917, 45494, 31429, 43548, 41748, 45955, 7828, 9056, 41133, 12073, 23246, 27762, 31780, 39617, 48746]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339d8c0e10>\n",
            "Constructing exemplars of class 54\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [33810, 14713, 34847, 26599, 40826, 14184, 22238, 22496, 37779, 41410, 19730, 1104, 21722, 36591, 22573, 22588, 10893, 2642, 34910, 10385, 34648, 26899, 9239, 27603, 16372, 11851, 8433, 1295, 45211, 18311, 27243, 1577, 47082, 29281, 33653, 12468, 10229, 43546, 15914, 330]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339ca1a650>\n",
            "Constructing exemplars of class 93\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [5045, 16669, 12516, 31114, 47219, 17283, 45996, 10663, 12110, 45899, 29109, 34989, 10649, 31441, 23520, 3687, 28438, 8410, 36779, 2064, 13042, 45075, 23935, 99, 26905, 4822, 33055, 35432, 12776, 770, 32695, 35786, 27117, 14657, 31633, 29068, 2142, 49709, 17355, 10471]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339d8c0f10>\n",
            "Constructing exemplars of class 85\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [41809, 21627, 19128, 5445, 1372, 48883, 26079, 35358, 44349, 35933, 47574, 20228, 31055, 41943, 3862, 22154, 38961, 36440, 33290, 38121, 26698, 5799, 16156, 26533, 15001, 20927, 26242, 6620, 16640, 21233, 45654, 39362, 23388, 38502, 11559, 43708, 46360, 2221, 43283, 48161]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339bb07bd0>\n",
            "Constructing exemplars of class 9\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [41539, 33879, 35537, 9138, 32516, 24419, 41168, 4355, 3752, 5047, 44810, 16619, 14188, 1876, 43924, 21977, 41376, 15112, 31703, 36678, 13668, 20726, 40717, 13145, 20702, 47368, 8590, 7555, 41610, 24185, 9547, 34658, 8111, 35410, 1962, 9131, 2921, 32659, 19600, 4425]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7f339ca2ff90>\n",
            "Constructing exemplars of class 96\n",
            "lunghezza exemplar set:  40\n",
            "exemplar set:  [26096, 3681, 24249, 2145, 4177, 44424, 12292, 6839, 38465, 704, 29855, 10348, 2700, 19661, 22207, 4195, 8274, 20531, 47690, 39951, 6839, 28599, 15599, 3575, 2638, 38465, 2552, 28717, 20700, 21935, 25183, 18578, 8769, 18967, 12677, 33085, 46017, 15123, 13491, 18529]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.06 0.10472306609153748\n",
            "TEST GROUP:  0.152\n",
            "TEST ALL:  0.2636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KRvNYlfO1ICS",
        "outputId": "0227b47b-3ccc-4add-f379-05c74153e5c2"
      },
      "source": [
        "method = \"Closed world with Rejection\"\n",
        "print(\"metrics ClosedWord for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,5):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        " #writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics ClosedWord for seed 66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAJcCAYAAAAb0rWEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde5xVVf3/8dcH1FBEgRFRAcMAzQtCAWalhrdSs7T6esvMrKSLVqblV61v2vVrN83Kvv0o75qXvGVeSDLR/H5FBUPxgiKGckdRFFEUmM/vj70Hj+PMWWuYtfeZOef9fDzOY+acvc5ea1/OmTVrr/35mLsjIiIiUm961LoBIiIiIkVQJ0dERETqkjo5IiIiUpfUyREREZG6pE6OiIiI1CV1ckRERKQuqZNT58xsYzP7q5m9ZGZ/7sR6jjaz21O2rRbM7DYzO7agdbuZDS9i3e3U9zkzu6es+irqbXc7U7TJzPY0syeqLB+at2GDztQT2ZYzzOyP3WW9XYWZTTGzL9a6HSLq5HQRZvZpM5tmZq+Y2aL8j/EeCVb9H8BAoMndD1vflbj7Fe7+4QTteQszG5//wbqh1euj8tenRK7nLDO7PFTO3Q9090vWs61bm9kF+fFZYWazzOz7ZtZ7fdYnbXP3f7r7Di3PzWyume23vuszs4vN7I38s/WCmU02s3dHtuUn7t6pP9b5OT4/9XrriZm9y8xuzj9Xz5vZz1otP9LMHjezlWY2x8z2rFVbpXtRJ6cLMLOTgV8BPyHrkGwL/A44JMHq3wk86e5rEqyrKM8B7zezporXjgWeTFWBZdb7fDez/sC9wMbA+929D7A/0BcYlqaVXVsZIycF+pm7bwoMAhYAF9S4PZIzs42AycA/gK2AwcDlFcv3B34KHAf0AfYCni6/pdItubseNXwAmwOvAIdVKfMOsk7QwvzxK+Ad+bLxwHzgFGApsAg4Ll/2feANYHVexxeAs4DLK9Y9FHBgg/z558i+QFYA/waOrnj9nor3fQB4AHgp//mBimVTgB8C/5uv53Zgi3a2raX9vwdOyF/rSfaH6HvAlIqy5wHzgJeB6cCe+esHtNrOhyra8eO8Ha8Bw/PXvpgv/x/guor1/xS4A7A22vkjYCbQo8pxcmB4xXG9lKwD9wzw3Zb35u24K993zwNXV6zj3WRf+C8ATwCHVyxrAm7Kt//+fB/f005bLgFOyX8flLetZf8Oy9ff0p7jgafy124Ctmm1TScAs4F/t7GdydvUck7kr18GNOfH7xXgVN48Z48Fns334XeqHJeLgR9VPD8IWFnxfBvguvxY/Rv4esWys3jr52V34P+A5cBDwPiKZf2Bi8g+oy8CNwK987Y35+1/Ja+v9Xo/Djyar3cKsGPFsrnAt4CH83PmaqDXen7f9CLrQCzL63oAGFhxzl5A9h2ygOyc71nx3s8Dj+fb9jfgnRXL9gdm5e37Ldn5/cXINk0A/lll+f8BX1if7dVDD43k1N77yb54bqhS5jtkX66jgVHAbmR/NFtsRfYFNYisI3O+mfVz9zPJRoeudvdN3b3qf6/5ZZdfAwd6NlLxAWBGG+X6A7fkZZuAc4BbWo3EfJrsP68tgY3IvqSruRT4bP77R4BHyP5YVHqAbB/0B/4E/NnMern7pFbbOariPceQfYn2IetsVDoFGJnPI9mTbN8d6+5t5TrZD7je3ZsD29HiN2TH5F3Ah/JtOy5f9kOyjl8/sv9afwPr9v/kfNu2BI4EfmdmO+XvOx9YBWxN9gfn81Xqv4uss0Be/9Nk/wG3PP+nuzeb2T7AfwOH5+t9Briq1boOBd4H7MTbJW9T5Rvc/RiyjszH8mNbeRljD2AHYF/ge2a2Y5W6gXX7+CiyTh356N5fyTosg/J1nWRmH2njvYPIzvsfkZ2D3wKuM7MBeZHLgE2AncmO37nuvhI4EFiYt39Td1/Yar3bA1cCJwEDgFuBv+YjHC0OJ+vMbwfsSvZPx/o4luy8HEL22f0yWScMss7gGrJO+HuADwNfzNt4CHAG8Mm8jf/M24yZbQFcT/adtAUwB/hgxfZta2bLzWzbdtq0OzA3v0T/fD6fZ2T+3p7AWGCAmT1lZvPN7LdmtvF6br80GHVyaq8JeN6rX046GviBuy919+fIRmiOqVi+Ol++2t1vJftvcYc21hOjGdjFzDZ290Xu/mgbZT4KzHb3y9x9jbtfSfZf3Mcqylzk7k+6+2vANWSdk3a5+/8B/c1sB7IOwaVtlLnc3Zfldf6SbIQrtJ0Xu/uj+XtWt1rfq2T78Ryy/26/5u7z21oJ2XFaFKgLWPfFfCRwuruvcPe5wC9585itJruMuI27r3L3lom6BwNz3f2ivL3/IhthOCxf56eA77n7Snd/hGxkpD13AXvkf8T3An7Gm394PpQvh+zcutDdH3T314HTyS4dDq1Y13+7+wv5sWy9nUW0Kdb33f01d3+IrJMyqkrZb5nZcrKRxT1481iMAwa4+w/c/Q13fxr4A9nxa+0zwK3ufqu7N7v7ZGAacJCZbU3Wmfmyu7+YfxZjt+cI4BZ3n5yfo78guyz6gYoyv3b3he7+AlmnrOrnqYrVZOfycHdf6+7T3f1lMxtINsJ1Un4slwLn8uZ++DLZefB4/l31E2C0mb0zf9+j7n5t3v5fAYtbKnT3Z929r7s/206bBuf1/JpslOsW4C95J28gsCHZ3MI98+1+D2/9J0+kXerk1N4yYIvAfIdteOsoxDP5a+vW0aqT9CqwaUcbkv/XeQTZF9oiM7ulnQmardvT0qZBFc8XV/we257LgBOBvWljZMvMvpVPPnwp/4O1Odl/jtXMq7bQ3e8jG1Ewss5Ye5aRjVbE2ILsi7n1MWvZP6fm9d1vZo+aWcvoxzuB9+X/9S7Pt/FospG6AcAGrban9TGo3K45wEqyPwp7AjcDC/NOZGWH4i3H0t1fybe18li2tw+LalOsjpxjv3D3vmSXul7jzc7xO4FtWu3zM8j+uLb2TrIOZ2XZPcjOiyHAC+7+Yge3Ad5+DJrJ9mmHP0/5+fRK/mhrcu5lZJearjKzhWb2MzPbMN+2Dck+9y3b9v/IRqTIl59XsewFsnN4UN7+dedAPhJa9XPXymtklzhvc/c3yDp5TcCOvDnK9Jv8n67nyf4pOagD65cGpk5O7d0LvE52SaA9C8m+ZFpsy9sv5cRaSTak3mKryoXu/jd335/si3sW2X+1ofa0tGnBerapxWXAV8n+W361ckH+hX0q2bB9v/wP1ktkX7SQzdFoS3uvt6z3BLIRoYX5+tvzd+ATkZOXn+fN0ZoW6/aPuy929+PdfRvgS2SXpIaT/WG4K/+vt+Wxqbt/hWy+yBqyP6aV66zmLrL/gDdy9wX582PJLpO1XIZ8y7HML+c08dZj2d4+LKpNrVU9hh2RjyZ8g+wP9sZk+/zfrfZ5H3dv64/oPOCyVmV7u/vZ+bL+ZtZ3Pdrf+hgY2T7t8OfJ3XeuuCz2zzaWr3b377v7TmQjRQeTjZzOI/se2qJi2zZz953zt84DvtRq2zfOR2AXUXEOVLQ/1sO0s4/yTuP8VsuTnQ9S/9TJqTF3f4lsgu35ZnaomW1iZhua2YEVt1FeCXzXzAbk17+/R8XdBx00A9grv06+OdnlCQDMbKCZHZL/oXud7LJXW3NQbgW2t+y29w3M7Aiy+Ro3r2ebAHD3f5P9R/+dNhb3IfuD+hywgZl9D9isYvkSYGhH7qDK50L8iOwyxDHAqWbW3mWAc/L6LsmH6DGzQWZ2jpnt2mo71pKNCv3YzPrk5U8mP2ZmdpiZDc6Lv0j2pd1Mtv+2N7Nj8nNgQzMbZ2Y75uu8HjgrP0d2IuscVHMX2cjY3fnzKfnze/L1QXZuHWdmo83sHWSXIe7LL7FVVWCbWltCNrcpifwy00KyuVr3AyvM7D8tiynV08x2MbNxbbz1cuBjZvaRvFwvy24PH+zui4DbyDqs/fJj1zLfaAnQlH/e2nIN8FEz2zcfVTmF7PP3f6m2uYWZ7W1mI/NLjS+Tdcab8/bfDvzSzDYzsx5mNszMPpS/9ffA6Wa2c76ezc2sJSTFLcDOZvbJfET667T65yngcmB3M9svb9dJZP8oPJ4vvwj4mpltaWb9gG/Sye8aaRzq5HQB+fySk8muMz9H9l/TiWR3Z0D2h3ga2X88M4EH89fWp67JZHdnPEx2h1Lll0WPvB0LyYajPwR8pY11LCP7D/AUsksbpwIH50PJneLu93iriZm5vwGTyG4rf4ZssmvlkHhLoMNlZvZgqJ78y/hy4Kfu/pC7zya7THFZ/se+dbteIPvPdzVwn5mtILsT6yXySaytfI1s1Oxp4B6yycQX5svG5et4hezOpG+4+9PuvoJssueRZMdgMdkdXy3tOZHsMsViskmiFwU28y6yzmFLh+IeslG8lue4+9+B/yKb+7OI7C6ntuajtCd5m9rw32Sd/OVmFprAHuvnZOftBmTn8miyO6ueB/5Idin0Ldx9HllYhzN483P6bd78Hj2G7PyYRXan40n5+2aRdSafzrdhm1brfYKso/2bvP6PkU20fiPRtlbaCriWrIPzONnxuCxf9lmymwQeI+t8X0t+idbdbyA7F68ys5fJbgw4MF/2PHAYcDbZ98EIsjsagXUTj1+xdiYeV2z/7/N6DwE+XrH9PyS76eDJvM3/IrtrUiTIvM0bSURExMx+AAx292p3jYlIF6WRHBGRNuRzS3YiG+ERkW6oO0cwFREp0oNkc2NOrHVDRGT96HKViIiI1CVdrhIREZG61C0uVy3895yqw00fP+yOsprCikGDgmX6LAiHt1jTq1ewzAarViVpz4YrVyapK6ZMCqv6thVq5K16LV8eLBOzj2Ok2u6Y9rzW1BQsE3N+xYg5dzZetixJXaF9WOYxjzmeqdbT1cRsV0yZmGMhnTNt2gQLl0qqzMs6pW2bRnJERESkLqmTIyIiInWpJperzOwA4DygJ/DHPCy6iIiI1EDz2vYCjqfXo2fP8uoqraZcHrb7fLJomTsBR+Xh4EVERESSqcVIzm7AU+7+NICZXUUWxvuxGrRFRESk4TU3t5WmsBh1PZIDDOKtOYfm56+9hZlNMLNpZjbt8iuvKq1xIiIiUh+67C3k7j4RmAjhW8hFRERk/ZU5J4cNNyytqlqM5CwAhlQ8H5y/JiIiIpJMLUZyHgBGmNl2ZJ2bI4FP16AdIiIiAjQ3lziSU6LSOznuvsbMTgT+RnYL+YXu/mjZ7RAREZH6VpM5Oe5+K3BrbPn3/WBp1eWbJwoJHyNVqoAyw8anStmQqs2hEP6pwveXmTojVaqFmPUsGDcuWGbQAw8kqWv6CYcHy4w5/5pgmZBU53qqtBgx507MedFvzpxgmVTKTNmQoq6ulhoiJrVIjJjtuvSerpcSpHlteXdXlUkRj0VEJKlU/wyKdFaXvbtKREREylGvc3JqMpJjZhea2VIze6QW9YuIiEj9q9XlqouBA2pUt4iIiDSAWk08vtvMhtaibhEREXmrUoMBlqjLTjyuTOvwyhM31ro5IiIi0s102YnHlWkdhhx3r9I6iIiIFKTMBJ1l6rIjOSIiIiKd0WVHckRERKQc9TonpyadHDO7EhgPbGFm84Ez3f2C9V1fTITJ6WfsEiwz5ifhO9o3XrYsqk0hqaJ9PnnEjsEyo353T5K6UkWlTbHtqaI4x0Q5TXXMYyLkLh49OlhmuzvuCJaJCcb20Ff3CJYZc044mnGZ0adDXu3fP1gm5njGnKOpPg8xYs7TmHNn6JQpCVoT3vYNVq1KEhAwVVDBuePHB8uk2jcxPrtHeLumTSuhIQ2gVndXHVWLekWkayqzwyDFU8Tj7qdeR3I0J0dERETqUukjOWY2BLgUGAg4MNHdzyu7HSIiIpKp17uranG5ag1wirs/aGZ9gOlmNtndH6tBW0RERKROld7JcfdFwKL89xVm9jgwCFAnR0REpAY0J6cAeWqH9wD3tbFMEY9FREQajJntYGYzKh4vm9lJZtbfzCab2ez8Z7/QumrWyTGzTYHrgJPc/eXWy919oruPdfexm+5waPkNFBERaRDNzWtLe4S4+xPuPtrdRwNjgFeBG4DTgDvcfQRwR/68qpp0csxsQ7IOzhXufn0t2iAiIiJd3r7AHHd/BjgEuCR//RIgOAJSi7urDLgAeNzdzym7fhEREXmrMufkmNkEYELFSxPzfJVtORK4Mv99YD6vF2Ax2V3aVdXi7qoPAscAM81sRv7aGe5+aw3aIiIiIiWqTMBdjZltBHwcOL2NdbiZBZN31+LuqnsA68h7mmbP7nS9o855KljmxWHDgmViQsKnCmEfs55v73VxsMxl//xEsMygBx5I0p4U21Vm9NtU6TViUja8sckmwTI7XnddsEzMcXitqSlYhtfDKRBSHfMUxzQmtcFGr75aSltSridVXZvPmxcsE3M8l40YESwzcObMqstTpXWI+Vz1WbAgWCbU3lgxxyFVmwWAA4EH3X1J/nyJmW3t7ovMbGtgaWgFStApIiJJKa1D99NFgwEexZuXqgBuAo4Fzs5//iW0AqV1EBERkS7FzHoD+wOVNyedDexvZrOB/fLnVdVi4nEv4G7gHXn917r7mWW3Q0RERDJdLRigu68Emlq9tozsbqtotbhc9Tqwj7u/kt9Kfo+Z3ebuU2vQFhEREalTtZh47MAr+dMN80dwhrSIiIgUo6uN5KRSq2CAPfPbx5cCk929alqHZYvvLL+RIiIi0q3V5O4qd18LjDazvsANZraLuz/Sqsy6++hH73GpRnpEREQK0kXvruq0mt5d5e7LgTuBA2rZDhEREak/tbi7agCw2t2Xm9nGZLeI/bTsdoiIiEimXufk1OJy1dbAJWbWk2wk6Rp3v7kG7RAREZE6ZtnNTl1baE5OTKjtmJDwMSH+j9w9nP7gqqnjgmVSidmu1b17B8uUGWa8K6V1iJEqtcGSkSODZWJSmKTaPzHnTqow9v3mzOl0W1Kl4IgRk+Il5jNT5rnc1fZhWcqMrpwqHc/83XcPlnn+F9t3KP1RZ82afn9pnYF3j9mttG1TxGMRERGpS8pdJSIi0uCa1+ruqqTyWDn/MjPNxxEREZHkajmS8w3gcWCzGrZBRESk4TU31+fdVbWKeDwY+Cjwx1rULyIiIvWvVperfgWcCrR7EVBpHURERKQzahEM8GBgqbtPN7Px7ZVTWgcREZFy1GswwFqM5HwQ+LiZzQWuAvYxs8tr0A4RERGpY6WP5Lj76cDpAPlIzrfc/TNlt0NEREQyStApIiIi0o3UNBigu08BpoTKpUgDkCqc+c+O3yhYpt+8cJj7VGkUXmtqCpZZNmJEsEyZaR3KCnUfk0Zh4MyZJbQks9GrrwbLlJkG4NGjPxwsM+b8a4JlFo8eHSwTSuuQ6vOZKrVBqs9DmSkHYlIFDJ80KVgmJk1HzP4pK31LzD6OOeapjlWqVChl05wcERGRCGV27kSqUVoHERGRBlevIzk16eTkd1atANYCa9x9bC3aISIiIvWrliM5e7v78zWsX0RERNDdVSIiIiLdSq06OQ7cbmbTzWxCWwUq0zq8sODvJTdPRESkcTSvXVvao0y16uTs4e7vBQ4ETjCzvVoXcPeJ7j7W3cf2H7Rf+S0UERGRbq0mc3LcfUH+c6mZ3QDsBtxdi7aIiIg0uubm+ry7qvSRHDPrbWZ9Wn4HPgw8UnY7REREpL7VYiRnIHCDmbXU/yd3D4fhFBERkUI0r63Pu6vM3WvdhqBd9rumaiNThYQv0/k/eTZY5oQztg2WSRXGvh7FRF2NCcE+/YTDg2Vi0h/EmHn00cEyg6dODZYJpVGI9eKwYUnqKivE/4Jx44JlBj3wQJK6YnTHz2eqz02KY15WW2LFpGOI+TzEnBeP/P1wi2pUIvf89cbSOgN7fOzQ0rZNt5CLiEhSSusgXYXSOoiIiDQ4TTxOyMz6mtm1ZjbLzB43s/fXoh0iIiJSv2o1knMeMMnd/8PMNgI2qVE7REREGp4SdCZiZpsDewGfA3D3N4A3ym6HiIiI1LdaXK7aDngOuMjM/mVmf8zj5byF0jqIiIiUo7m5ubRHmWrRydkAeC/wP+7+HmAlcFrrQkrrICIiIp1Rizk584H57n5f/vxa2ujkiIiISDnqdU5O6SM57r4YmGdmO+Qv7Qs8VnY7REREpL7V6u6qrwFX5HdWPQ0cV6N2iIiINLx6HcmpVRbyGcDYVOuLCZGdKmz8kpEjg2UGzpwZLPPpC/YOlnn65O2CZZoeXBMss9WMGcEyMaHlU4VYDx2vVGHuUx3zURfcFCyTat8MnTIlWObJQz8YLDPm/M6nWkgpRV0x+y/VMY8Rs01lpmxIlUIixT7cYNWqYAqEPgsWdLoeKDf1Q6rz69HTXk6yHglTxGMREUkqJseTdC1l3/VUFuWuEhERkbpUi2CAOwBXV7z0LuB77v6rstsiIiIimpOTjLs/AYwGMLOewALghrLbISIiIvWt1nNy9gXmuPszNW6HiIhIw1IW8mIcCVzZ1gKldRAREZHOqFknJ4+R83Hgz20tV1oHERER6YxaXq46EHjQ3ZfUsA0iIiINr14nHtfyctVRtHOpSkRERKSzajKSY2a9gf2BL9WifhEREXlTvQYDrFVah5VAU2z5MkOjhwx64IEk6+k3Jxx2f8w54TLTpk0Ilhm9x9SoNpUlFBq9rPQRUF6Ye4hrT0yo+5g0EzFiotLGnKcxQvtw2YgRwXXEpEvZfN686DZ1VpkpJGKk+p5M8blJlbIhRqrjELOejZctS1LXqLM2ChfSVNQkan0LuYiIiNSY5uSIiIiIdCO1mpPzTeCLgAMzgePcvWuN/YqIiDQIjeQkYmaDgK8DY919F6AnWVBAERERkWRqNSdnA2BjM1sNbAIsrFE7REREGl693l1V+kiOuy8AfgE8CywCXnL321uXq0zr8Nxzd5fdTBEREenmanG5qh9wCLAdsA3Q28w+07pcZVqHAQP2KruZIiIiDaN57drSHmWqxd1V+wH/dvfn3H01cD3wgRq0Q0REROpYLebkPAvsbmabAK8B+wLTatAOERERAZqbdXdVEu5+H3At8CDZ7eM9gIllt0NERETqW63SOpwJnBlbfsnIkVWXx4R7T+XFYcOCZWJC4adKXdD3u88Gy7z4z9eCZcaNsyTtiRFaT0xY+Zi2vNYUnTmk03WlKhMj1XpSnacx6SFC4fBj6omRat/MHT8+WKZp9uxgmVTpDcrcP6nSQ4S+K8v8nkwlVV2pvptSal6ru6tERESCYv4ZFClDTTo5ZvYNM3vEzB41s5Nq0QYRERHJNDevLe0Rw8z6mtm1ZjbLzB43s/ebWX8zm2xms/Of/ULrqcUt5LsAxwO7AaOAg81seNntEBERkS7rPGCSu7+brK/wOHAacIe7jwDuyJ9XVYuRnB2B+9z9VXdfA9wFfLIG7RAREZEuxsw2B/YCLgBw9zfcfTlZjL1L8mKXAIeG1lWLTs4jwJ5m1pTfRn4QMKR1ocqIx688cWPpjRQREWkUZQYDrPz7nj8mtGrOdsBzwEVm9i8z+6OZ9QYGuvuivMxiYGBou0q/u8rdHzeznwK3AyuBGcDbLtK5+0TyW8uHHHevl9pIERERKUTl3/d2bAC8F/iau99nZufR6tKUu7uZBfsGNZl47O4XuPsYd98LeBF4shbtEBERkSxBZ1mPCPOB+XlcPchi670XWGJmWwPkP5eGVlSru6u2zH9uSzYf50+1aIeIiIh0Le6+GJhnZjvkL+0LPAbcBBybv3Ys8JfQumoSDBC4zsyagNXACfmEIhEREamBshNnRvgacIWZbQQ8DRxHNjBzjZl9AXgGODy0klpFPN6zI+WXfO2SqssHTnhvcB2pIme+7+y/B8s8dsyOSepK1eZ+/3VgsMxwJgXLpBLarlQRV2MiqpYp1TFPVVeqqL6v9u8fLBM6FqmilqeKSLvVjBnBMqnO0xhlnjsxEcdD295vzpwk7SlzH6cSs91PHnRQCS3p3tx9BjC2jUX7dmQ9tRrJERGROpWqwyXl6YIjOUkorYOIiIjUpcJGcszsQuBgYKm775K/1h+4GhgKzAUOd/cXi2qDiIiIhEXe9dTtFDmSczFwQKvXOhySWURERGR9FDaS4+53m9nQVi8fAozPf78EmAL8Z1FtEBERkTDNyUkjOiRzZdjn5usfK6d1IiIiUjdqdndVKCRzZdjnjR78stI6iIiIFKS5WSM5KXQ4JLOIiIjI+ih7JKclJPPZRIZkFhERkWI1r9XdVR1iZlcC9wI7mNn8PAzz2cD+ZjYb2C9/LiIiIpJckXdXHdXOog6FZAbY/tR9qi5f0ysc8jxVuPfpJ+0RLNNn1YIkdcXYfN68YJlU277NVU8Fyyw8cniwzIpBg6ouj0nHUGb6gzLFtCe0/wD6LAifgzGpC2LaE1NXinpSpTmJ0R3TCZSZ+iFkg1WrGjbqccxxmLrdf0Ws6Uedb4worYOIiKTVqB2c7kwTj0VERES6kSLn5FxoZkvN7JGK1w4zs0fNrNnM2souKiIiIiVrXru2tEeZyk7r8AjwSeDuAusVERERKTetg7s/DmBmRVUrIiIiHaQEnSWrTOvwwoK/17o5IiIi0s102burKtM67LLfNUrrICIiUhAl6BQRERHpRrrsSI6IiIiUQyM5HdRWWgcz+4SZzQfeD9xiZn8rqn4RERFpbLVI63BD6rrKDGd+9O+nBcvc9LGtk9QVs11PH/NasMzmU3cNltlw5cpgmZiUDav69g2WKSsNQIwy00PE7JuYdAIp9h/A6t69g2XKSm8w/ezw/1tjTkuT8iLmXE91fqVK07HxsmXBMq81NQXLpDp3Qsr8fHa1lCAxTjhj22CZaR8uoSEVdHeViIiISDeiOTkiIiINTnNyOqidtA4/N7NZZvawmd1gZuHxexEREZH1UHZah8nALu6+K/AkcHqB9YuIiEiE5ua1pT3KVFgnx93vBl5o9drt7r4mfzoVGFxU/SIiItLYajnx+PPAbe0tVFoHERER6YyaTDw2s+8Aa4Ar2iujtA4iIiLl8Dq9hbz0To6ZfQ44GNjX3dV5ERERkUKU2skxswOAU4EPufurZdYtIiIibWtmULIAACAASURBVOvRsz7D5pWa1gH4LdAHmGxmM8zs90XVLyIiIo2t7LQOF6zPulKElk8VsvvMm74XLDOGPySpK0bTlO2DZYZOmVRCSzIxx2ratAlVl48dOzFVc4LKDOVeVoqEWGWF+I8x5rQ08wFitqmrhfiPSdkQ055Ux7PMbQ8pMz1EmWkxUqWTSalHT6t1EwpRn+NTIiIi0vCU1kFERKTB9eihkZwOaSetww/zlA4zzOx2M9umqPpFRESksZWd1uHn7r6ru48GbgbCE1xERESkUD16WmmPUrerqBW3k9bh5YqnvQHFyREREZFC1CIY4I+BzwIvAXtXKTcBmACw7bZHM2DAXuU0UEREpMFoTk4i7v4ddx9CltLhxCrlJrr7WHcfqw6OiIiIdFQt7666ArgVOLOGbRAREWl4ipOTgJmNqHh6CDCrzPpFRESkcRQ2kpOndRgPbGFm88lGbA4ysx2AZuAZ4MtF1S8iIiJx6nVOTrdI61CWmFDbA3ZvdxrRm+u57H3BMqnCla8YNChYZvoZuwTLjPnJI8Eyqeyy3zVVl6+O2KZU4dVX9e0bLBNzrGLOndW9ewfLlBk2PiaMfUzKgSUjRwbLDHrggarLY45DqrQYMcdz7vjxwTKDp05NUleMmOMZ0+bhk8IpXlIdixSpC1KdozHHIdX51dXShjQ6pXUQEZGkumJuJmlMSusgIiLS4DTxuIPaSutQsewUM3Mz26Ko+kVERKSxlZ3WATMbAnwYeLbAukVERCRSjx5W2qPU7SpqxW2ldcidC5yKUjqIiIhIgUqdk2NmhwAL3P0hs+q9OaV1EBERKYfm5HSSmW0CnEFk5nGldRAREZHOKHMkZxiwHdAyijMYeNDMdnP3xSW2Q0RERCrU60hOaZ0cd58JbNny3MzmAmPd/fmy2iAiIiKNo9S0Du7epSMei4iINCKldeigdtI6VC4fWlTdRdr2C6OCZdZEBPtMFfp70P6nB8ss+WtMovfy0jqEtismvPqCceOCZZpmzw6WianrxWHDgmViQsvHlInx3uueCZZ58FPvDJaJSSERkzYklLIhRqqQ+jHnRUx7h06ZkqA15Yb4j0kzEdOeFMdig1WrgukhYupJlbIhJlVFjGUjRgTLDJw5M1hGEaHLo4jHIiKSVKpOhZSnXufkKHeViIiI1KVS0zqY2VlmtsDMZuSPg4qqX0REROL06NGjtEep21Xgui+mjbQOwLnuPjp/3Fpg/SIiItLAipx4fLeZDS1q/SIiIpKG5uSkc6KZPZxfzurXXiEzm2Bm08xs2nPP3V1m+0RERKQOlN3J+R+yyMejgUXAL9srqLQOIiIi0hml3kLu7ktafjezPwA3l1m/iIiIvF29BgMsdSTHzLauePoJyoxAJyIiIg2l1LQOwHgzGw04MBf4UlH1i4iISJx6nXhcdlqH9cpdFQqBnSoseoynDmjrrvi3Gj5pUgktyWxy0h7BMms+2rVCiKc4XjGh+a++KBwS/ojjmoJl+s2ZEywTE6b9oa+Gj9WYc/4eLHP/0TsEy2xAms/EhitXJllPWWJC6sdIlY6hzO+m2R/9aLDMyCuuKKElWcqGUDqUmLQOrzWFP58x6UlSpQ1JdX6VeV50V3kS7xXAWmCNu481s/7A1cBQsoGSw939xWrrUcRjERFJKibfm3QtPXpaaY8O2DuPqTc2f34acIe7jwDuyJ9X366O7woRERGR0h0CXJL/fglwaOgNpaZ1yF//mpnNMrNHzexnRdUvIiIicXr0sNIelXHw8seENprkwO1mNr1i+UB3X5T/vhgYGNquIm8hvxj4LXBpywtmtjdZT2yUu79uZlsWWL+IiIh0Me4+EZgYKLaHuy/I+wmTzWxWq3W4mXmorrLTOnwFONvdX8/LLC2qfhEREYnT1e6ucvcF+c+lZnYDsBuwxMy2dvdFeUiaYB+i7Dk52wN7mtl9ZnaXmY1rr2DlcNayxXeW2EQRERGpFTPrbWZ9Wn4HPkwWV+8m4Ni82LHAX0LrKjXicV5ff2B3YBxwjZm9y93fNuRUOZw1eo9Lg0NSIiIisn66WMTjgcANZgZZv+FP7j7JzB4g6zd8AXgGODy0orI7OfOB6/NOzf1m1gxsATxXcjtERESkC3L3p4FRbby+DNi3I+squ5NzI7A3cKeZbQ9sBDxfchtERESkQlebk5NK2WkdLgQuzG8rfwM4tq1LVSIiIiKdVXZaB4DPdHRdy0aMqLo8JtR2qjDt7zr63GAZJu0YLpNITNjzN/quLqEl6cRES41JtRCTsmHu+PHBMkOnTAmWiTl3dr5wWrBMjFQh4VcMGhQs8/I22wTLxLSnafbsTq8jRpn7ZuNl4bQhZaZ+iFnPqr59k9QVSpPQb86c4D6M+U6OSdkQs02p0jrEiNmurqiLzclJRhGPRUQkqZhOokgZSo14bGZXm9mM/DHXzGYUVb+IiIg0tlIjHrv7ES2/m9kvgZcKrF9EREQiaOJxB7UT8RgAy25+PxzYp6j6RUREpLHVak7OnsASd293NmJlxONXnrixxKaJiIg0ljITdJa6XaXW9qajgCurFXD3ie4+1t3HbrpDMJu6iIiIyFuUHQwQM9sA+CQwpuy6RURE5O169KzPm61rsVX7AbPcfX4N6hYREZEGUWrEY3e/ADiSwKUqERERKU+93l1l3SGrwg6H3FK1kTFRMVNFPI6JxhvTnphgWTFRfaV4Z3+7d7DMaT9fGSzT1SKhxkTLjonqGyNVVN+Qrhb9tl6lOJfLOidSSvV35PFPfSpYZuXpTaX2Om677JTSOgMHHvPL0rat9Dk5IiIi0rUorYOIiIhIN1LknJwLgYOBpe6+S/7aaOD3QC9gDfBVd7+/qDaIiIhIWL3OySlyJOdi4IBWr/0M+L67jwa+lz8XERERSa7stA4ObJb/vjmwsKj6RUREJI7m5KRxEvBzM5sH/AI4vb2ClWkdls+dVFoDRUREpD6U3cn5CvBNdx8CfBO4oL2ClWkd+g5tfdVLREREUunR00p7lLpdpdYGxwLX57//Gdit5PpFRESkQZTdyVkIfCj/fR+g3SzkIiIiIp1RaloH4HjgvDxJ5ypgQlH1i4iISJx6nXhc5N1VR7WzqMPZx1OElo8JtR0TEj6VVCkbLr0nvF0fO3bnYJkyU0iEQqPHHKuYtBgx6TVixKRsiEn3kWofp0pdEBOivqz0I3PHjw+WGTplSrBMd0zZkCrdR8w+HD4pfBNHqtQFZdVT5nkc057pJxweLDPm/GuCZThdYwApKK2DiIhIg1MwQBEREZFupOy0DqPI0jpsCswFjnb3l4tqg4iIiITV65ycstM6/BE4zd1HAjcA3y6wfhEREWlgZad12B64O/99MvA34L+KaoOIiIiEaU5OGo8Ch+S/HwYMaa9gZVqHZYvvLKVxIiIiUj/K7uR8HviqmU0H+gBvtFewMq1D01Z7l9ZAERGRRlOvaR1KvYXc3WcBHwYws+2Bj5ZZv4iIiDSOUjs5Zraluy81sx7Ad8nutBIREZEa0t1VHZSndbgX2MHM5pvZF4CjzOxJYBZZHquLiqpfREREGlst0jqc19F1pQghHhMKP6aexaNHB8ukSicQ4yNfGRcs89qQpmCZVCkHygoJH7OPU6U/iBGTeuSzH5oZLHPpXSODZVb37h0sE7NdMfswxbGKEZOyoTtKlbIhRtPsNPmOyzrmqeqJWU+ZaWtGXXBTaXWlpLurRERERLoR5a4SERFpcD161OeYR5FzcoaY2Z1m9piZPWpm38hf729mk81sdv6zX1FtEBERkcZVZNdtDXCKu+8E7A6cYGY7AacBd7j7COCO/LmIiIhIUkVOPF4ELMp/X2FmjwODyCIej8+LXQJMAf6zqHaIiIhIdZp43Al5Dqv3APcBA/MOEMBiYGA771mX1uG55+5uq4iIiIhIuwqfeGxmmwLXASe5+8tmb/YW3d3NzNt6n7tPBCYCjB07sc0yIiIi0nkKBrgezGxDsg7OFe5+ff7yEjPbOl++NbC0yDaIiIhIYypsJMeyIZsLgMfd/ZyKRTcBxwJn5z//UlQbREREJKxe5+QUebnqg8AxwEwzm5G/dgZZ5+aaPM3DM8DhBbZBREREGpS5d/3pLnbR8qqNHHP+NcF1lJVuINaSkeHw/QNnhtMApAobH7PtZaZJaFSHjpwSLHPjzPFJ6pp+1jbBMtv/IfzfXZlpTEJSfc672vdFjO7Y5hTqdbunTZtQ6tDK49N+VFpnYMex3y1t2+ozxKGIiIg0PKV1EBERaXC6u6qDqqR1OCx/3mxmY4uqX0RERBpbkSM5LWkdHjSzPsB0M5sMPAJ8Evh/BdYtIiIikXR3VQe1l9bB3ScDVAYFFBEREUmtFmkdYt+zLq0DUy4uqGUiIiLSo4eV9ihT6WkdYt9XmdYhdAu5iIiISGuFdnLaSesgIiIiXUi9zskp8u6q9tI6iIiIiBSuFmkd3gH8BhgA3GJmM9z9IwW2Q0RERBpQkXdX3QO0N/51Q0fWtf2N/1t1eZmh3F9ragqWiQlzv8kLLwTLxIhJD7G6d+9gmaFTpgTLpErZEEoPEVPP3PHjg2UGT50aLBNzXqwYNChYJuaYx5xf184+IFhm4l3LgmUmfCR8no45a2GwTKq0ISnqKTMdQ3dMAxBznr6xySbBMmWmkwlJdaxStTfm+3bQAw8Ey8Qcq7IpGKCIiIhIN6K0DiIiIg1OE487qEpah5+b2Swze9jMbjCzcGprERERkQ4q8nJVS1qHnYDdgRPMbCdgMrCLu+8KPAmcXmAbREREJKBegwEW1slx90Xu/mD++wqgJa3D7e6+Ji82FRhcVBtERESkcdU6rcPngdvaec+6tA7L504qtoEiIiINrEdPK+1R6nYVXUF7aR3M7Dtkl7SuaOt97j7R3ce6+9i+Q8O31YqIiIhUqklaBzP7HHAwsK+7Ky+ViIhIDfXo2fUiyphZT2AasMDdDzaz7YCrgCZgOnCMu79RbR2lp3UwswOAU4GPu/urRdUvIiIi3do3yObztvgpcK67DwdeBL4QWkGRXbeWtA77mNmM/HEQ8FugDzA5f+33BbZBREREArra3VVmNhj4KPDH/LkB+wDX5kUuAQ4Nrafdy1Vm9hug3UtJ7v71aiuuktbh1lCjWls2YkTV5UMjQuqHUglAXHjwmHQCMSkSNl4WDs0fY/FRk4Nldv7B0CR1pQqZH0qNEZPWIWYfpwrlvuHKlcEyMedXTHqNmPPiqx8Kr2cDwsdhwbhxwTIxIf5TpL2IWUfMvolJu5IqPUmqz0OMmPOr35w5Sep66oDwHMjhk8I3g4T2T8y+eXHYsGCZmPMi1XFomj07yXpSff93V2Y2AZhQ8dJEd5/YqtivyK769MmfNwHLK+7Ong8EvziqzcmZFtdcERGRN5WV20rSKfOup7xD07pTs46ZHQwsdffpZja+M3W128lx90taVbqJ5tCIiIhIwT4IfDyf4tIL2Aw4D+hrZhvkozmDgeBlnOCcHDN7v5k9BszKn48ys99FvK+9tA4/zFM6zDCz281sm9C6REREpDhdaU6Ou5/u7oPdfShwJPAPdz8auBP4j7zYscBfgtsVse2/Aj4CLMsrfwjYK+J97aV1+Lm77+ruo4Gbge9FrEtEREQa238CJ5vZU2RzdC4IvSEqTo67z8smNq+zNuI9i4BF+e8rzKwlrcNjFcV6U2Vys4iIiDQud58CTMl/fxrYrSPvj+nkzDOzDwCeB/drfd96UOu0Dmb2Y+CzwEvA3u28Z93s6977/4Beux7RkSpFREQkUtnpFsoSc7nqy8AJZLdqLQRG58+jtJXWwd2/4+5DyFI6nNjW+yrTOqiDIyIiIh0VHMlx9+eBo9dn5e2ldahwBVncnDPXZ/0iIiLSebFB+rqbmLur3mVmfzWz58xsqZn9xczeFfG+9tI6VEb2O4T8ri0RERGRlGLm5PwJOB/4RP78SOBK4H2B97WkdZhpZjPy184AvmBmOwDNwDNkl8NERESkRup1Tk5MJ2cTd7+s4vnlZvbt0JtSpnXYfN68jr7lbV4aMiRJPYOnTg2WSRVmPCZq6IDNnwmWgaERZcJShUZPFX4+JNU+TpUGIKY9S0aODJYZ9MADKZrDj394W7DMrw8dnKSu0H4OpX2AuP0XkzojlVSfhxgx52CqNBMxqTxi6kkR9TjVd0WqdDypjrkiQpenWu6q/vmvt5nZaWTpzR04gvXoqIiISGPQH/Hup17n5FQbyZlO1qlp2fIvVSxz4PSiGiUiIiLSWdVyV23XmRWb2RDgUmAgWadoorufV7H8FOAXwID8Di4RERGpgUaek4OZ7QLsRJYoCwB3vzTwtpa0Dg+aWR9guplNdvfH8g7Qh4Fn17PdIiIiIlUFOzlmdiYwnqyTcytwIHAP2ShNu9pL6wA8BpwLnEpEci0REREpVr2O5MREPP4PYF9gsbsfB4wCNu9IJZVpHczsEGBBnuiz2nsmmNk0M5v28pybO1KdiIiISNTlqtfcvdnM1pjZZsBSIHw/dq4yrQPZJawzyC5VVeXuE4GJAMOOuENJPEVERApSr3dXxYzkTDOzvsAfyO64ehC4N2blbaR1GAZsBzxkZnOBwcCDZrbVerRdREREpF0xuau+mv/6ezObBGzm7g+H3tdWWgd3nwlsWVFmLjBWd1eJiIjUTr3OyakWDPC91Za5+4OBdbeZ1sHdFUhQRERECmfubU93MbM7q7zP3X2fYprURlv+9HTVOTljzvl7WU1JJiZ8f9Ps2cEyZYaW725W9e0bLFNmuPwYLw4bFiyTKgVCzHaddGA4rP4v7tw5WCZVaoyQmP0XkyqgzGNejxp5/60YNChYJib1z4x7Plvq0EozE0ub+9qDCaVtW7VggHuX1QgRERGR1GImHq8XMxtiZnea2WNm9qiZfSN//SwzW2BmM/LHQUW1QURERBpXVMTj9dRmxON82bnu/osC6xYREZFIa9qZulKEjUq8EFdYJ6dKxGMRERGRwgUvV1nmM2b2vfz5tma2W0cqqYx4nL90opk9bGYXmlm/dt6zLuIx/7iyI9WJiIhIB6xxL+1Rppg5Ob8D3g8clT9fAZwfW0FlxGN3fxn4H7KggKPJRnp+2db73H2iu49197Hsc1RbRURERETaFXO56n3u/l4z+xeAu79oZhvFrLyNiMe4+5KK5X8AlJhKRESkhsoeYSlLzEjOajPrCTiAmQ0AmkNvaivicf761hXFPgE80qEWi4iIiESIGcn5NXADsKWZ/ZgsK/l3I97XZsRj4CgzG03WaZoLfKmjjRYREZF06nUkJyZ31RVmNh3YFzDgUHd/POJ99+TlW+twWofBk5eECyUQE6XzoZOHB8vs/Lv5wTIDZ86MalNITHTNuePHB8uMvOKKBK1JI1W01FRRf1NJFY03lWUjRgTL/OLO8P559LSXg2XGnBbVpKpijlVMNOgYrzU1lVZXqnMw5rtpzE/CA+cxkcJjvDRkSNXlMd+Bqb4LUq0nZt+kOi8kjWAnx8y2BV4F/lr5mrs/W2TDRESkewp1cKTrWVPrBhQk5nLVLWSXlgzoBWwHPAGEE9aIiIiI1EjM5aq3ZJLMs5N/NfQ+MxsCXAoMJOskTXT38/JlXwNOANYCt7j7qR1vuoiIiKTQsHNyWsvTNLwvomh7aR0GAocAo9z9dTPbsqNtEBEREQmJmZNzcsXTHsB7gYWh91VJ63A8cLa7v54vW7oe7RYREZFE6nUkJyZOTp+KxzvI5ugc0pFKWqV12B7Y08zuM7O7zGxcO+9Zl9bhlSdu7Eh1IiIiItVHcvIggH3c/VvrW0HrtA5mtgHQH9gdGAdcY2bvcn9rN9LdJwITAYYcd299djFFRES6gIYbyTGzDdx9LVlQv/XSVloHYD5wvWfuJ4uevMX61iEiIiLSlmojOfeTzb+ZYWY3AX8GVrYsrOi0tKm9tA7AjcDewJ1mtj2wEfD8+jVfREREpG0xd1f1ApYB+/BmvBwHqnZyaD+tw4XAhWb2CPAGcGzrS1UiIiJSnnq9XFWtk7NlfmfVI7zZuWkR3BtV0joAfCa6hYnEpD/YeNmyYJnBf21vk96UKjx4r+XLg2WePLZfsMyoc64LlolRVmj0mO2OEROaP+aYx2xTTMqGmPXEeOqAA4JlBk+dGiyzyQsvpGgOO5+9WbDMqsDpvrp37+A6YsLlxxyHmPXEnBeppDovNn9wcLDMqr7hlDMp0iRsPm9e8POXKp1FzHd7zDFPlc5CupZqnZyewKa03VGpzy6fiIh0Wsw/GNK1NGJah0Xu/oPSWiIiIiKSULVOTvi6TLU3t5PWwcyuBnbIi/UFlrv76M7UJSIiIuuvEefk7NvJdbeZ1sHdj2gpYGa/BF7qZD0iIiIib9NuJ8fdOzUrsUpah8dg3S3mh5PdtSUiIiI1Uq8jOTFpHTqtVVqHFnsCS9x9djvvUVoHERERWW8dzkLeUa3TOlQsOgq4sr33Ka2DiIhIOep1JKfQTk47aR3I81d9EhhTZP0iIiLSuArr5FRJ6wCwHzDL3cORqURERKRQ9TqSU+ScnJa0DvuY2Yz8cVC+7EiqXKoSERER6azCRnKqpXVw9891ZF0bvfpqp9sTE9Y7Jsx4TCj8mPWkSl3A4GuDRdb02j1YJiaUe5npKlKIOeZl1hVzXsTsv+GTJkW1KSTmOKRK5RFy/K43B8tctmCPJG1JlUahq0l1XqT4/uq1fHlwPamOQ6rPeapzJ1WaibLVa8TjUu6uEhGRxpEqL5VIZxV+d5WIiIh0bZqT00FmNsTM7jSzx8zsUTP7Rv76aDObms/RmWZmuxXVBhEREWlcRY7ktJnWAfgZ8H13vy2fiPwzYHyB7RAREZEGVOTE4/bSOjiwWV5sc2BhUW0QERGRMF2u6oRWaR1OAn5uZvOAXwCnt/OedWkdXp4TvvNCREREpFLhnZw20jp8Bfimuw8BvkkWMPBt3H2iu49197GbDTu46GaKiIg0rDXupT3KVGgnp520DscCLb//GdDEYxEREUmuFmkdFgIfAqYA+wBtZiEXERGRctTrnJwi765qSesw08xm5K+dARwPnJcn6VwFTCiwDSIiItKgapLWgQ5mH+83Z07nG5TIshEjgmUGT51aQksy5w3tHyxzYaLw6alSNoTWU1bah5TqNZ1AWW3+w8PheXcH3fhIsMw/Dg3XNf3k/YJlxpzz9/CKEon5XHW38ytVW2IiJ8eUmb97OLVNqrQYXTFlQwyldRARERHpRpTWQUREpMHV65ycWqR1GGVm95rZTDP7q5ltFlqXiIiISEfVIq3DH4FvuftdZvZ54NvAfxXYDhEREalCIzkd5O6L3P3B/PcVQEtah+2Bu/Nik4FPFdUGERER6V7MrJeZ3W9mD+VXgr6fv76dmd1nZk+Z2dVmtlFoXbVI6/AocEi+6DBgSDvvWZfW4bnn7m6riIiIiCTQxSIevw7s4+6jgNHAAWa2O/BT4Fx3Hw68CHwhtKJapHX4PPBVM5sO9AHeaOt9lWkdBgzYq+hmioiISBfgmVfypxvmDycLIHxt/volQDBoRKF3V7WV1sHdZwEfzpdvD3y0yDaIiIhIdWXOyTGzCbw1EPBEd5/YqkxPYDowHDgfmAMsd/eWkD7zyabAVFV6Wgcz29Ldl5pZD+C7wO+LaoOIiIh0LXmHZmKgzFpgtJn1BW4A3r0+dRV5uaolrcM+ZjYjfxwEHGVmTwKzyPJYXVRgG0RERKSbcvflwJ3A+4G+eUoogMFAMLx0rdI6nNeRdW1yxRNVl7969A4dWV2nDJ0yJcl6UqVImPjp7cPrWZUmTUKqdAvdMW1DCjHHfPHo0cEyMedgTKj715qagmVilBXG/h+HDg6W+cMvFwbLHH9KmpQNqfZxV0sDEJOSIWbbU6R2SJXOIibVTqrv5LL2TWpdKa2DmQ0AVrv7cjPbGNifbNLxncB/AFcBxwJ/Ca1LEY9FRCSprvhHXLqVrYFL8nk5PYBr3P1mM3sMuMrMfgT8i2xKTFXq5IiIiDS4rhQM0N0fJgs70/r1p4HdOrKuItM6JAvmIyIiItJRRU48ThbMR0RERIrTxYIBJlNkWodkwXxEREREOqrQiMdm1tPMZgBLyfJURQfzqUzrsOjqh4tspoiISEPTSM56cPe17j6a7H723ehAMJ/KtA5bH7FrYW0UERGR+lTK3VX5ve5vCeaTj+ZEBfMRERGR4nSlu6tSKvLuqgF5OGYqgvk8zpvBfCAymI+IiIhIRxU5kpMsmI+IiIgUpytFPE6pyLQOyYL5pEjb8OKwYcEyMVE6/33mX4Nldp3w3mCZVKkNNvrNvcEya45/22F4mzIjlE4/a5uqy8ecFQ7N39XEhHKPOeap0obEHM+ulE4gZt8sGTkyWOb4U8J1TT/++GCZMX/4Q7BMqn3cHdMAlNWeFYOCSaaj9vG/9903WGbH666LalMKMeeypKGIxyIiIg1Oc3JEREREupHCRnLMrBdwN/COvJ5r3f1MMzsROAkYBgxw9+eLaoOIiIiE1etITpGXq1rSOrxiZhsC95jZbcD/AjcDUwqsW0RERBpckROPHXhbWgd3/xeAmRVVtYiIiEi5aR3c/b4OvHddWofnnru7uEaKiIg0OKV1WA+t0zqY2S4deO+6tA4DBuxVXCNFRESkLpWd1uEA4JEy6hQREZE49TrxuOy0DrOKqk9ERESkUpGXq7YG7jSzh4EHyObk3GxmXzez+WSXsB42sz8W2AYREREJWFPio0zm3WCIqu93n63ayOGTJgXXkSp0+vRzwuHnd/7B0GCZVGkdYsKeP3nKo8EyY07um6I5SaQ6Vqv6hrcpVXj6mPXEHKuNly1LUleMVO2ZO358sEzMZzSFZJ/zs8P//23/m62DZcpMnRFzvsd876RaTyh1QdPs2cF1dLV0FmWm4Jg2bUKptyCfPP97pXUGzhn8g9K2TWkdb6R/xAAAIABJREFUREQkKeVm6n40J0dERESkG6lFWocrgLHAauB+4EvuvrqodoiIiEh1GsnpuJa0DqOA0cABZrY7cAXwbmAksDHwxQLbICIiIg2qFmkdbm0pY2b3k91lJSIiIjWikZz1UC2tQ5608xigzdsuKtM6vPHgn4pspoiIiNShQu+ucve1wOg8KOANZraLu7dEPP4dcLe7/7Od904EJkL4FnIRERFZfxrJ6QR3Xw60pHXAzM4EBgAnl1G/iIiINJ4i764aAKzO81a1pHX4qZl9EfgIsK+7NxdVv4iIiMQpOxJxWYq8XLU1cImZ9SQbMbomT+uwBngGuNfMAK539x9UW1Eo6uqCceMY9MADaVodsNWV+wfL9Fqepi0x0TVjyvBG/4jawv3NsqJ9brBqVVQ03hTRZGPq6TdnTqfrgXKj38aIOncixBzzMutKISaace+z/x5e0TE7JmhNnJeGDAmWiYlUnGIfD5w5MxgQcNmIEQycObPTdaX6nixr30BcVGlJo8i7qx4G3tPG68nrLKuDI+Uoq4Mj3UtXC/Ev7YuJeJyigyMSorQOIiIiDU4TjzvIzHqZ2f1m9pCZPWpm389fvyB/7WEzu9bMNi2qDSIiItK4ihzJaYl4/EoeE+ceM7sN+Ka7vwxgZucAJwJnF9gOERERqaJeR3JqEfG4pYNjZGkd6nPPioiISE3VJOKxmV0ELCbLYfWbdt67LuLxq49dV2QzRUREGtoa99IeZSq0k+Pua919NFl+qt3MbJf89eOAbYDHgSPaee9Edx/r7mM32elTRTZTRERE6lBNIh7nr60FrgLUgxEREakhjeR0kJkNyHNWURHx+AkzG56/ZsDHgVlFtUFEREQaV6kRj4FbgH+a2WaAAQ8BXymwDSIiIhJQr2kdzLvBbWPPLTi3aiMPPKR3cB1lpSSAuJDdMSHYU0UELXPb61HM8YwJCR9jwbhxwTIx50XM8YzZrsWjRwfLDJ46NUl7Unj2goeCZbb9wqgSWpJ56oADgmWGT5qUpK4yP+f1+J1S5uc8xrRpE6y0yoCDnjq1tM7ArcN/Vtq2KeKxiIgk1d06OFK/cXJKmXgsIiIiUrbCRnLMrBdwN/COvJ5r3f3MiuW/Bj7v7krrICIiUkP1OpJTeloHd59qZmOBfgXWLSIiIg2u9LQO+d1WPwc+DXyiqPpFREQkTr2O5NQircOJwE3uvijw3nVpHS69/N4imykiIiJ1qNC7q/KoxqPzoIA3mNlewGHA+Ij3TgQmQvgWchEREZHWSrmF3N2Xm9mdwN7AcOCpLOAxm5jZU+4+vIx2iIiIyNvpclUHtZPWYbq7b+XuQ919KPCqOjgiIiJShFLTOrj7zQXWJyIiIutBaR1qaIdDbqnayD4LFgTX8eKwYcEy/ebMCZbZ5qqngmUWHplmcComdPpDJ4fravq/LYJlhk6ZEtOkJKafsUvV5aPOCe/jmIiqXS30fKpzMEaqbS9rH64YNChYJuZz3h3NPProYJmRV1xRQksyZZ07ZZ5/c8ePD5bZasaMYJl6Tuuw26yTS+sM3P/uc5TWQUREuqeYzol0LZqTIyIiItKNlJ7WwcwuBj4EvJQX/Zy7h8cJRUREpBD1OpJTelqHfNm33f3aAusWERGRBld6Woei6hMREZH1U68jObVI6wDwYzN72MzONbN3tPPedWkdls+dVGQzRUREpA4V2slx97XuPhoYDOxmZrsApwPvBsYB/YH/bOe9E919rLuP7Tv0gCKbKSIi0tDWuJf2KFMpd1e5+3LgTuAAd1/kmdeBi4DdymiDiIiINJay0zrMMrOt89cMOBR4pKg2iIiISNiaEh9lKj2tg5n9w8wGAAbMAL5cYBtERESkQRV5d9XDwHvaeH2fjq5rw5UrO92eVOH7n/7y2GCZl0YOCZYZOHNmiuaw4GP/Cpb5+E82TFJXqhDrMWkbUkgVdTVmm1b17Rsss/GyZSmaEyVVqPvBU6cGy8Rseygcfqp909VSecTY8brrgmWmn7xfsMyo390TLBOz7fN33z1YJpQGJtU+fq2pKVgm5u9DmSkbuuM5CLq7SkRERKRbUSdHRERE6lKRE497mdn9ZvaQmT1qZt/PXzcz+7GZPWlmj5vZ14tqg4iIiIR1pVvIzWyImd1pZo/l/Ydv5K/3N7PJZjY7/9kvtK5apHXYERgCvNvdm81sywLbICIiIt3LGuAUd3/QzPoA081sMvD/27v3aDnKMt/j3x8JtyAJAcLFJK5gCKIQiBoQL2AIjAeRJejggArCAEbxIIpHFJxZOrKOHhQPeHRGmMhFPDCAEFEGMaJoBOYISYAAATQkEIRwNVwVEEme80fVhmbT3fVWp7p6796/z1q90l311lNPv33Ju6ur3udI4JqIOFXSScBJtJhrb0AvyjocC3wkItbm7R7tVg5mZmZWbCideBwRDwEP5fefkXQXMBE4EJiVNzsfWEDBIKcXZR2mAofkJRt+Lmlai21fKuvw+KpfdTNNMzMzq0nj/+/5bU6btlPIrtS+Edg6HwABPAxsXbSvbv5cRUSsAWbkkwJenpd12BB4PiJmSvogcC6wZ5Nt5wJzAXbe90dDZ4hpZmbWZ+o8ktP4/3s7kl4DzAM+GxFPZ3MIvxQjJBUmXXtZB+AB4Mf5qsuBXerIwczMzIaH/FzeecCFETEwZnikoWrCtmS/ErVVe1kH4CfA3nmzdwPLupWDmZmZFRtKZR3ysk/nAHdFxOkNq64AjsjvHwH8tChWL8o6XA9cKOkEshOTj+liDmZmZja8vBM4HLg9P68X4EvAqcCPJB0N3Af8Q1EgxRA6o7qVmTPnrnOSKVPP/22TTQrbPDJ9emGb7efPT8qpCsv326+wTcrU6NMvvLCKdIaUZyZOrCTOpqtWFbZJmco95b0zcdGipJyqUFXZi1uPfn9hm13PuaLt+qqmua9qSv2hNjV/Sj5fvvLhwjZf37f4e7Cq5/7E1Klt149fsaK2XIaalOe15PqPqbBRhcbccmxtg4Fn33xmbc/NMx6bmVmligY4ZnXp6tVVZmZmNvQNpXlyqtS1QY6kjYBryS4ZHw1cFhFfkXQdsGnebCtgYUQc1K08zMzMbGSqvaxDRLw0J46keSScHW1mZmbd069Hcrp2Tk5kmpV1AEDSWGA22SXlZmZmZpXqRVmHAQeRFdp6usW2L037/Nhj13YzTTMzsxEtYv3abnXq6iAnItZExAxgErB7XtZhwIeBi9psOzciZkbEzAkT9upmmmZmZtaHelHWAUlbArsDP6tj/2ZmZjbydPPqqgnA3yLiyYayDt/IVx8MXBkRw28WJzMzs36zdoNeZ9AVtZd1yNcdSjY9s5mZmVlXdG2QExG3AW9usW5WmVg3fWnntuvf+vWlhTFSpv5OafPU5MmFbVJUNV15SsmBlHIV/Th9espzSplaPqUkyEZPPpmUUxX7SinTkfK8UqycNauwTVHJBih+71T1/ls9bVphm61vv72wzVArD5HS5sRPvLU4zsRqvlOKntemq1ZVVlZlXXOB6l6rlM/exqtXF7YZkvr0SI7LOpiZWaXqGuCYFXFZBzMzs5HOR3LKkbSRpIWSbpV0h6Sv5sv3kXSzpCWSrpe0fbdyMDMzs5Gr9rIOwJnAgRFxl6RPAf8MHNnFPMzMzKydPj2S080TjwNoVtYhgLH58nHAg93KwczMzEaurp6Tk18+fhOwPfBvEXGjpGOAqyQ9BzwN7NFi2znAHAA+8QV4z4HdTNXMzGzk6tMjOb0o63ACsH9ETALOA05vse1LZR08wDEzM7Oyarm6Kp/1+DfAe4FdGwp1XgLMryMHMzMza8FHcsqRNEHSZvn9gbIOdwHjJO2QNxtYZmZmZlap2ss6SPo4ME/SWuAJ4Kgu5mBmZmZF+vRITu1lHSLicuDyMrFef8lz65xPyrTeKbN0TrrhhsI2dZZIeGCvNxa2ef38mwrb1DlFfVHpgqrKFlRV2qCqkg0TFy0qbFNnKYoNvvu7wjbbH77uryfUV9YhpWRDipRSKFWVY0iR0j8p5RhS8vnjObcWtnnd0bu2XT9+xYqknKuQ8r2dUmqhqjI6NrR4xmMzM6tUXQMcq1CfHslx7SozMzPrS70o6zA7L+uwVNL5knw0yczMzCpXd1mHXwDnA/tExDJJpwBHAOd0MQ8zMzNrxz9XlROZwWUd1gAvRMSyfPkvgb/vVg5mZmY2cnX1nBxJoyQtAR4lG9AsBEZLmpk3ORiY3GLbOZIWS1r89Ioru5mmmZnZyLZmg/puNaq1rAOwE3AocIakhcAzZEd3mm37UlmHsVMP6GaaZmZm1ofqLuuwX0R8C9gTQNJ7gB3abmxmZmbdFT4np5QWZR1+L2mrfNmGwBeBs7qVg5mZmY1cvSjrcJqkA/JlZ0bEr7uYg5mZmRXp06urelHW4UTgxDKx7pk9u+36tyZMc1/VtOjLjhhf2Gbrr1czlXuKbRbeW9imqP8grQ+rmqK+qExCVfupotxAqpQ4T0ydWtgmZfr5lGnsU0o/vPjxV308X+Wx6dMK22xx993F+yr4/FX1OjwyfXphm5TSD6unFT/vKRVN8V/V7MAp74sUrzu6uE1KzkXlWVLe6ymS3us1zsBc5/eOFfNEfGZmVqmU+nM2xPTpkRyXdTAzM7O+1PUjOfk5OYuBVRFxgKTtgIuBLYCbgMMj4oVu52FmZmYt+EhOxz4D3NXw+BvAGRGxPfAEkPALsJmZmVk53Z7xeBLwPuDs/LGA2cBleZPzgYO6mYOZmZkVWLtBfbcadftIzreBLwBr88dbAE9GxIv54weAppcENJZ14Lq5XU7TzMzM+k3XzsnJ58J5NCJukjSr7PYRMReYC6B/j6g4PTMzMxvQp+fkdPPE43cC75e0P7ARMBb4P8BmkkbnR3MmAdVMNmFmZmbWoJuTAZ4MnAyQH8n5fER8VNKlZNXHLwaOAH7arRzMzMwsgY/kVOaLwMWS/idwC3BO0QY7XHXVOu80ZYbJpFkxH3t7wt6WVrOvBCmTbm2zZEkl+0rJuap+rmJG0KcmTy5sM+7++2vJBeCFMWMK26TM3lrnLM1jHn+8kjh1SZnNOMX6f/lLJXGq+pxXFSdlVvcURa/5pqtW8ZZ59xXG+e3x72i7PuV1SHn/VfW9lNKmaEZ3SJsV2apRVxXyBcCC/P49wO517NeGp6H0n6aZlVfFAMesCi7rYGZmNtKt6c+fq7o+GaCkUZJukXRl/vg4ScslhaQtu71/MzMzG5nqOJIzMOPx2PzxfwFXkv98ZWZmZj0WPpJT2uAZjwEi4paIWNnN/ZqZmZl1+0jOwIzHm5bdUNIcYA7AVrscx2ZT9qs4NTMzMwP69hLyrh3JaZzxuJPtI2JuRMyMiJke4JiZmVlZtc54LOmCiDisi/s0MzOzsnwkp5yIODkiJkXEFOBQ4Nce4JiZmVldap8nR9LxZOfpbAPcJumqiDim7jzMzMws16fz5PRixuPvAN+pY79lJU2d/sbTCps8v9mUwjYpU3+neOpNLxa3YVxhm4mLivdVVzmBlCnPU/pvg2efrSROVVPqP7DHHoVttrj77sI2j0yfXthm4qKEFzRBVWUAil7Tqqbmr0pKeYi6ypOkxnl4xozCNimvZxXv94UffUNhm2enbV7YZlxF5TWqUufradXwjMdmZmYjnc/JMTMzMxs+un4kR9IoYDGwKiIOkHQhMBP4G7AQ+ERE/K3beZiZmVkLnvG4YwNlHQZcCOwITAc2BnzSsZmZmVWuq0dyGso6fA34HEBEXNWwfiEwqZs5mJmZWYE1o3qdQVd0+0jOQFmHtYNXSFofOByY32xDSXMkLZa0+MmVTZuYmZmZtdTLsg7fA66NiOuarXRZBzMzM1sXPSnrIOkrwATgE13cv5mZmSVYb+2rfnDpovp+Gqu9rIOkY4D/Bnw4IursVTMzMxtBejEZ4FnAfcDvJAH8OCJO6UEeZmZmBmjNmhr3Vt+RnF6UdejJLMtVTc2/w//eqbDN6OdXV7KvFPP2v6ywzSkHbFPJvp6ZOLGwTRXTxqeUWkgp/VCVlCnYU/LZ6dJLK9nX2AcfLGyTIuX13Hh18Xs5Jeei1zTl8znUyihUparnvn5CCYSU5/XE1KmFbcavWFHYpqj8SErpjJs+t29hm12/d31hmxRVveZVlaUZ6SSdCwyc27tzvmxz4BJgCrAS+IeIeKJdHM94bGZmlUqpr2ZDi9asqe2W6AfA4KuOTgKuiYhpwDX547Y8yDEzM7MhJSKuBR4ftPhA4Pz8/vnAQUVxelHW4Ryysg4ClgFHRsSfu52HmZmZNVfn1VWS5gBzGhbNjYi5CZtuHREP5fcfBrYu2qCO82MGyjqMzR+fEBFPA0g6HTgOOLWGPMzMzKzH8gFNyqCmXYyQFEXtelHWYWCAI7LaVYVJmpmZWffUe3VVxx6RtG1EPCRpW+DRog16UtZB0nlkh5p2BL7bbEOXdTAzM7MGVwBH5PePAH5atEFPyjpExD8CryX7GeuQZtu7rIOZmVk9htrVVZIuAn4HvEHSA5KOJju15e8k3Q3sS8KpLj0p6wAQEWskXUx2pOe8LuZhZmZmw0hEfLjFqn3KxOnaICciTgZOBpA0C/g8cLik7SNieX5OzvuB33crBzMzMytWb+2q+tQ9+7CA8yWNze/fChxbcw5mZmY2AtRe1oHsZ6xSUqaWL1LVlN1DbTruT953X2Gb11JNWYeUkg1VSJnCPuV1SHnNqyoVUFWbFFW9DlV8rqpSZxmFFHVOzV/Ve2fM44PnTeudrW+/vZLSKyklGw6bcV1hm4tv2G2dc0k11P6PSDVMrq4qzTMem5lZpeqsLWfWjgc5ZmZm1pdqL+vQsPw7wFER8Zpu52BmZmat+eeqzg2UdXiJpJnA+Br2bWZmZiNUVwc5DWUdzm5YNgo4jWx+HDMzM+ux9daure1W6/PqcvxmZR2OA65oqCTaVGNZh9UP/6abOZqZmVkf6to5OY1lHfLJAJH0WuBDwKyi7RurlM541w9dxNPMzKxL+vWcnFrLOgB3AH8FlmcTHjNG0vKI2L6LeZiZmdkIVGtZh8arq/Llf/YAx8zMrLf69UiO58kxMzOzvtSLsg6Ny5PmyBlKU77f9KWdC9vsevrywjZVPacXLv1iwr7mV7KvqjwzcWLb9eNXrKhkP3W+b6oqIVHVvlKsnjatsM3Wt99eyb6qUGepheE4NX/K65ny2ari81dn/6WUbDhk998Wtrlk4burSGfY6tcCnT6SY2ZmZn2p7irkZmZmNsT4nJwOSRol6RZJV+aPfyDpXklL8tuMbudgZmZmI08dR3IGyjqMbVh2YkRcVsO+zczMrICP5HSgWVkHMzMzszr0oqwDwNck3SbpDEkbNtuwsazDY49d2+U0zczMRi7XriqpsazDoFUnAzsCuwGbA02vgY6IuRExMyJmTpiwV7fSNDMzsz7VzSM5A2UdVgIXA7MlXRARD0Xmr8B5wO5dzMHMzMxGqLrLOhwmaduIeEhZ8aqDgKXdysHMzMyK9euJx72YJ+dCSRMAAUuAT/YgBzMzM+tztZd1iIjZZbcfc+Ef2q5/9qNvKIxR1ZT6k/5ThW2e22KLwjYbr15d2KaqnFfOmlXYZtINNxS2qap0QVVlG6qQku9QK6NQVXmDDZ59trBNipR8iqT0ccpzSnmvT1mwoLBNUekRSPsMV/WZSYmT8hmual8pcaoo31LVZy+lZMPy/fYrbJPSx8OxJAj075Ecl3UwM7NKpQwSzergsg5mZmYjnAt0dqhJWQdJ+pqkZZLuknR8t3MwMzOzkacXZR2OBCYDO0bEWklb1ZCDmZmZteBzcjrQoqzDscApEbEWICIe7WYOZmZmNjL1oqzDVOCQvGTDzyU1veSksazDQ5fc1uU0zczMRi6tWVPbrU69KOuwIfB8RMwEvg+c22z7xrIO2x6yS7fSNDMzsz7VzXNyBso67A9sBIyVdAHwAPDjvM3lZKUdzMzMrEd8dVVJEXFyREyKiCnAocCvI+Iw4CfA3nmzdwPLupWDmZmZjVy9mCfnVLLSDicAfwaO6UEOZmZmluvXq6t6UdbhSbIrrpI9/t/f1nb9RhRPo13V9OApU+FvumpVYZuUadFTpgff4u67C9u8MGZMYZuq+iclTtFsqFX1X4qUfKsqo5BS7iPFLt+/pbDNsg9tV9imzvIaVb2/iqRMu58ipWRDVerqmyoV5Tx+xQpW7bZbYZuhZPv58wvbfOyq4ouBf7i/Z0UZSjzjsZmZVapogGNDT78eyXHtKjMzM+tLXT+SI2kUsBhYFREHSLoO2DRfvRWwMCIO6nYeZmZmNrLUXtYhIvYcWCFpHvDTGnIwMzOzFnwJeQdalHUYWDcWmE12SbmZmZlZpbp9JGegrMOmTdYdBFwTEU8321DSHGAOwLY7fpzNJ+7btSTNzMxGMp94XFKbsg4DPgxc1Gr7xrIOHuCYmZlZWbWXdYiIwyRtCewOfKCL+zczM7MEPpJTUpuyDgAHA1dGxPCbBcvMzMyGhV5NBngoWXkHMzMz67F+vbqq9rIO+eNZdey3rJRSAfcc/lxhm52+XVwGoKqp3O/5+L2FbXY6dWxhm5TnXlXOKWUbilRVjiElTkrJhpWzZhW2qarkwJ2Hv7GwzWiKn9cTU6cWtnlq8uTCNimlRYr6MCWXlFILKa9Vne/1Ov1tk00K29TVPxMXLSqMkaKqz1VVr+e5H3xdYZtbv7R9YZtdT19eRTqWwGUdzMzMRjifk9MhSaMk3SLpyvzxPpJulrRE0vWSioe9ZmZmZiXVPuMxcCZwYETcJelTwD8DR9aQh5mZmTXhIzkdaDHjcfDygGcc8GA3czAzM7ORqRczHh8DXCXpOeBpYI9mG3rGYzMzs3r069VVvZjx+ARg/4iYBJwHnN5se894bGZmZuui7hmPfwbsGBE35m0uAeZ3MQczMzMr4HNySmo24zFwIDBO0g55s78jOynZzMzMrFK1zpMTES9K+jgwT9Ja4AngqDpzMDMzs5FBEdHrHApt+fllbZNMmXG1ill2IW0GzikLFhS2SZmNN2V20kemTy9sM+7++yvZV7/OFFskZTbe8StW1JBJpqr3ToqU99fWt99eyb6qUNV7NCVOSpuqXofhqKh/qvqueGbixMI2VX3/V/X+Onyv6wvbfOb0HyopqYpM3/ui2gYDt//mw7U9t65PBmhmZiNLymDArA4u62BmZjbC+RLyDjUp6zA7L+uwVNL5kjzQMjMzs8rV8XPVQFkHJK0HnA8cGhE7A/cBR9SQg5mZmbWgNWtqu9Wp7rIOWwAvRMSy/PEvgb/vZg5mZmY2MnX7SM5AWYeBH/v+BIyWNDN/fDAwudmGkuZIWixp8fO3XdLlNM3MzEYuH8kpqVlZh8iuVz8UOEPSQuAZoOkzbizrsNEuh3QrTTMzM+tTdZd1uCAiDgP2BJD0HmCHNjHMzMysy3x1VUnNyjpExGGStgKQtCHwReCsbuVgZmZmw4+k/ST9QdJySSd1GqcXl2+fmP+UtR5wZkT8ugc5mJmZWW4oFeiUNAr4N7L6lg8AiyRdERF3lo1VyyAnIhYAC/L7JwIn1rFfMzOr3+jnn/esx7YudgeWR8Q9AJIuJivwXXqQQ0QMuxswx3FGVpyhlIvj+DV3HL/mvYjTLzdgDrC44TZn0PqDgbMbHh8O/Gsn+xqutavmOM6IizOUcnGceuIMpVwcp544QymXoRinL0TD1dP5bW639jVcBzlmZmbWn1bxyjn0JuXLSvMgx8zMzIaSRcA0SdtJ2oDsCu0rOgk0XItjVnVoy3GGT5yhlIvj1BNnKOXiOPXEGUq5DMU4I0JEvCjpOOAXwCjg3Ii4o5NYyk/qMTMzM+sr/rnKzMzM+pIHOWZmZtaXht0gp4qpniVNlvQbSXdKukPSZ9Yhn1GSbpF0Zacx8jibSbpM0u8l3SXp7R3EOCF/PkslXSQpeTYuSedKelTS0oZlm0v6paS783/HdxDjtPw53SbpckmbdZJLw7r/ISkkbdlpHEmfznO6Q9I3O4kjaYakGyQtkbRY0u4FMZq+5zro41ZxSvVz0WcgtZ/bxSnTz22eV9l+3kjSQkm35nG+mi/fTtKN+ffGJfnJjJ3EuTD//lmavy/WLxujYf13JP25XR4FuUjS1yQtU/adcXyHcfaRdHPex9dL2r4op3y7V3z3le3jNnGS+7hVjIblSX3cJpdSfdwmTkd9bBXo9aRAJScQGgWsAF4PbADcCrypgzjbAm/J728KLOskTr7954D/AK5cx+d2PnBMfn8DYLOS208E7gU2zh//CDiyxPZ7AW8BljYs+yZwUn7/JOAbHcR4DzA6v/+Nohit4uTLJ5OdiHYfsGWHz2lv4FfAhvnjrTqMczXw3vz+/sCCTt5zHfRxqzil+rndZ6BMP7fJp1Q/t4lTtp8FvCa/vz5wI7BH/nk4NF9+FnBsh3H2z9cJuKhdnFYx8sczgf8L/Dnh/dcql38Efgisl9jHreIsA96YL/8U8IOinPK2r/juK9vHbeIk93GrGGX7uE0upfq4TZyO+ti3db8NtyM5L031HBEvAANTPZcSEQ9FxM35/WeAu8gGCaVImgS8Dzi77LaD4owj+4/0nDynFyLiyQ5CjQY2ljQaGAM8mLphRFwLPD5o8YFkgy/yfw8qGyMiro6IF/OHN5DNd9BJLgBnAF8Aks6WbxHnWODUiPhr3ubRDuMEMDa/P46Cvm7znivbx03jlO3ngs9Acj+3iVOqn9vEKdvPEREDf7mvn98CmA1cli9P6eemcSLiqnxdAAtp08+tYiiry3MaWR8XavOcjgVOiYi1ebuiPm4Vp1Qfw6u/+ySJkn3cLE6eZ3Ift4pRto9bxaFkH7eJU7qPrRrDbZAzEbi/4fEDdDA4aSRpCvBmsr9qyvo22YdoXWvUbwc8BpyXH+I8W9ImZQJExCo6B8K0AAAH2ElEQVTgW8AfgYeApyLi6nXMa+uIeCi//zCw9TrGOwr4eScbSjoQWBURt65jDjsAe+aH1X8rabcO43wWOE3S/WT9fnLqhoPecx33cZv3bql+boyzLv08KJ+O+3lQnNL9nP9UsAR4FPgl2dHfJxsGgUnfG4PjRMSNDevWJ5tqfn4HMY4Drmh43Qu1iDMVOETZz3g/lzStwzjHAFdJeiB/TqcmpDT4u28LOujjJnEac03q4xYxSvdxizil+7hFnE762Cow3AY5lZL0GmAe8NmIeLrktgcAj0bETRWkMprs55AzI+LNwF/Ifrook894sqMC2wGvBTaRdFgFuQHZX4EkHkFpRtI/AS8CF3aw7RjgS8CXO91/g9HA5mSH6U8EfpT/FVrWscAJETEZOIH8KFyRdu+5Mn3cKk7Zfm6Mk2/XUT83yaejfm4Sp3Q/R8SaiJhBdgRgd2DHss+nWRxJOzes/h5wbURcVzLGXsCHgO9WkMuGwPMRMRP4PnBuh3FOAPaPiEnAecDp7WJU9d2XEKewj5vFkPRaSvZxm1xK9XGbOKX62CoUQ+A3s9Qb8HbgFw2PTwZO7jDW+mTnHXyuw+3/F9lfKyvJ/gJ/Frigw1jbACsbHu8J/KxkjA8B5zQ8/hjwvZIxpvDK807+AGyb398W+EPZGPmyI4HfAWM6yQWYTvaX58r89iLZEattOnhO84G9Gx6vACZ0EOcpXp5nSsDTnbznOuzjpu/dsv08OE6n/dzieZXu5xZxSvfzoJhfJhtk/YmXz1l6xfdIiTifz+9/BfgJ+XkaJWN8hez7YqCP15L9BF86F+D3wHYNffNUh32zomHZ64A7C7Zr9t13Ydk+bhHngjJ93CLGE2X7uFUuZfu4RZyfle1j36q79TyBUslmfx3eQ3a0YuDE4506iCOyk8m+XVFes1j3E4+vA96Q3/8X4LSS278NuIPsXByR/Sb+6ZIxpvDK/8hP45UnxX6zgxj7AXeSMJBoF2fQupUknHjcIp9Pkv3GDtlPKveT/ydaMs5dwKz8/j7ATZ2858r2cZs4pfo55TOQ0s9t8inVz23ilO3nCeQn7QMb55+rA4BLeeVJsZ/qMM4xwP8jP8G/kxiD2qSceNwql1OBo/Lls4BFHcb5E7BDvvxoYF7Ke6hhvwMn15bq4zZxkvu4VYyyfdwml1J93CwO2f9bHfexb+t263kCpRPOzrxfRvaX4T91GONdZD8L3AYsyW/7r0NOTT9cJWPMICs5fxvZXzDjO4jxVbK/PJaSXVWwYYltLyI7l+dvZH+JHE32O/s1wN1kV8ps3kGM5WT/wQ3081md5DJo/UrSrq5qls8GZH+hLQVuBmZ3GOddwE1kA+0bgbd28p7roI9bxSnVzymfgZR+bpNPqX5uE6dsP+8C3JLHWQp8OV/+erKTWJeT/Wfc9rPRJs6LZN89Azl+uWyMQW1SBjmtctmM7CjB7WRH8HbtMM4H8hi3AguA15f43pjFywOCUn3cJk5yH7eKUbaP2+RSqo/bxOm4j31bt5vLOpiZmVlfGtEnHpuZmVn/8iDHzMzM+pIHOWZmZtaXPMgxMzOzvuRBjpmZmfUlD3LMhhBJa/JKxUslXZrP9txprB9IOji/f7akN7VpO0vSOzrYx0o1qVTeavmgNsnVofP2/yLp82VzNLORy4Mcs6HluYiYERE7Ay+QTar3krz4amkRcUxE3NmmySyg9CDHzGwo8yDHbOi6Dtg+P8pynaQrgDvzIounSVok6TZJn4CsErSkf5X0B0m/ArYaCCRpgaSZ+f39JN0s6VZJ1+QFMT8JnJAfRdpT0gRJ8/J9LJL0znzbLSRdLekOSWeTzVTclqSfSLop32bOoHVn5MuvkTQhXzZV0vx8m+skdVR7ysyso78Kzay78iM27+XlCsxvAXaOiHvzgcJTEbGbpA2B/5J0NVnl7jcAbyKrZn4ngwoK5gOJ7wN75bE2j4jHJZ1FNjPst/J2/wGcERHXS3odWU2pN5LVFLo+Ik6R9D6y2Z+LHJXvY2NgkaR5EbEa2ARYHBEnSBqo63QcMBf4ZETcLeltZIUaZ3fQjWY2wnmQYza0bCxpSX7/OrKq2+8AFkbEvfny9wC7DJxvA4wDpgF7ARdFxBrgQUm/bhJ/D7LKzvcCRMTjLfLYF3hTQ+HwsXmF8L2AD+bb/kzSEwnP6XhJH8jvT85zXU1WOPGSfPkFwI/zfbwDuLRh3xsm7MPM7FU8yDEbWp6LiBmNC/L/7P/SuIis+OovBrXbv8I81gP2iIjnm+SSTNIssgHT2yPiWUkLgI1aNI98v08O7gMzs074nByz4ecXwLGS1geQtIOkTYBrgUPyc3a2BfZusu0NwF6Stsu33Txf/gywaUO7q4FPDzyQNDDouBb4SL7svcD4glzHAU/kA5wdyY4kDVgPGDga9RGyn8GeBu6V9KF8H5K0a8E+zMya8iDHbPg5m+x8m5slLQX+neyo7OVk1czvBH5IVjX5FSLiMWAO2U9Dt/Lyz0X/CXxg4MRj4HhgZn5i8528fJXXV8kGSXeQ/Wz1x4Jc5wOjJd0FnEo2yBrwF2D3/DnMBk7Jl38UODrP7w7gwIQ+MTN7FVchNzMzs77kIzlmZmbWlzzIMTMzs77kQY6ZmZn1JQ9yzMzMrC95kGNmZmZ9yYMcMzMz60se5JiZmVlf+v8nvC8X7Uz7mgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7Wc2tTMTlFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1c6484-fa39-4c2b-f675-aa6d6c5710ae"
      },
      "source": [
        "closed_with_rej_acc = new_accuracies\n",
        "print(closed_with_rej_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.508, 0.406, 0.3416666666666667, 0.29875, 0.2636]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pXLJaU3CtVQ"
      },
      "source": [
        "### open world with rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNW9TdvxHEl2"
      },
      "source": [
        "def sequentialLearningiCaRLOpen(train_subsets, val_subsets, open_dataset, rejection = False, closed = True):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 5\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset in zip(train_subsets, val_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      print(\"BATCH SIZE: \", BATCH_SIZE )\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      old_net = copy.deepcopy(net)\n",
        "      old_net.to(DEVICE)\n",
        "      addOutputs(net,10)\n",
        "    \n",
        "    \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen, rejection=rejection, closed=closed) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      \n",
        "      # val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      # acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen, rejection=rejection, closed=closed)\n",
        "      # print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      if(group_id <=5):\n",
        "        \"\"\" modify batch size for the variation \"\"\"\n",
        "        num_classes_seen=100\n",
        "        test_loader = DataLoader(open_dataset, batch_size=1, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen, exemplars_set_tot, rejection, closed)\n",
        "        all_accuracies.append(acc_all)\n",
        "        print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, all_accuracies, all_preds_cm, all_labels_cm"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tY3UJjmjPji"
      },
      "source": [
        "def validate(net, val_dataloader, criterion, num_classes, exemplar_set, rejection = False, closed = False):\n",
        "    #counter for rejection part, known and unknown\n",
        "    rejection = True\n",
        "    if rejection==True:\n",
        "      n_sample_known = 0\n",
        "      n_sample_unknown = 0\n",
        "    net.eval()\n",
        "    exemplars_subset = []\n",
        "    exemplar_loader = []\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "    print(len(val_dataloader))\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, image, labels in val_dataloader:\n",
        "        print('ciclo immagini')\n",
        "        # Bring images and labels to GPU\n",
        "        image = image.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(image)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * image.size(0)\n",
        "            \n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "       \n",
        "        if rejection == True:\n",
        "          print('1')\n",
        "          #passare exemplar_set_tot\n",
        "          reject = True\n",
        "          feature = net.forward(image)\n",
        "          mean_image = []\n",
        "          cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "          for k, exemplar_set_class_k in exemplar_set.items():\n",
        "          # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "            if (exemplar_set_class_k != []):\n",
        "              exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "              exemplar_loader = torch.utils.data.DataLoader(exemplars_subset, shuffle = True, batch_size=1, num_workers=2)\n",
        "            dist = 0\n",
        "            for _, exemplar, labels  in exemplar_loader:\n",
        "              # print('exemplar: ', exemplar)\n",
        "              exemplar = exemplar.to(DEVICE)\n",
        "              feature_exemplar = net.forward(exemplar)\n",
        "              output = cos(feature_exemplar, feature)\n",
        "              dist += output.detach().cpu().numpy()\n",
        "            mean = dist / 40\n",
        "            # print(mean)\n",
        "            mean_image.append(mean)\n",
        "          print(mean_image)\n",
        "          normalized_vector = ( mean_image-mean_image.mean() ) / mean_image.std()\n",
        "          print(normalized_vector)\n",
        "          \n",
        "          for value in normalized_vector:\n",
        "            if (value < 0.5):\n",
        "              reject = False\n",
        "          if (reject == False):\n",
        "            n_sample_known += 1\n",
        "          else:\n",
        "            n_sample_unknown += 1\n",
        "        # if rejection == True:\n",
        "        #   prediction_batch = outputs.data.cpu().numpy()\n",
        "        #   print(\"OUTPUTS shape: \", outputs.data.shape)\n",
        "        #   print(\"LEN PRED BATCH:\", len(prediction_batch))\n",
        "        #   for i in range(len(prediction_batch)):\n",
        "        #     current_softmax = softmax(prediction_batch[i])\n",
        "        #     #print(max(current_softmax))\n",
        "        #     if max(current_softmax)>THRESHOLD:\n",
        "        #       n_sample_known += 1\n",
        "        #     else:\n",
        "        #       n_sample_unknown += 1\n",
        "        else:\n",
        "          #_, preds = classify(images, )\n",
        "          running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "          all_preds_cm.extend(preds.tolist())\n",
        "          all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    if rejection == True:\n",
        "      if closed == True:\n",
        "        acc = n_sample_known / float(len(val_dataloader.dataset))\n",
        "      else:\n",
        "        acc = n_sample_unknown / float(len(val_dataloader.dataset))\n",
        "    else:\n",
        "      acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes, exemplar_set, rejection = False, closed = True):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes, exemplar_set, rejection, closed)\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes, exemplar_set, rejection, closed)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs9uww-HEu08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "10bd569f-6485-458b-9c8a-076a9cd2b3c2"
      },
      "source": [
        "#open with rejection\n",
        "rejection = True\n",
        "closed = False\n",
        "# train\n",
        "net, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRLOpen(train_subsets, val_subsets, open_test, rejection=rejection, closed=closed)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BATCH SIZE:  128\n",
            "TRAIN:  4950\n",
            "TRAIN_SET CLASSES:  [67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "VALIDATION CLASSES:  [59, 56, 49, 39, 22, 20, 18, 4, 67, 65]\n",
            "GROUP:  1\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "Len TOTAL train susbset:  4950\n",
            "training\n",
            "num classes till now:  10\n",
            "1\n",
            "Starting epoch 1/1, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.7818266749382019\n",
            "Train step - Step 10, Loss 0.30527815222740173\n",
            "Train step - Step 20, Loss 0.2987963557243347\n",
            "Train step - Step 30, Loss 0.27880218625068665\n",
            "Train epoch - Accuracy: 0.8608080808080808 Loss: 0.36158826695548163 Corrects: 4261\n",
            "Training finished in 2.955033779144287 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fa7bf44cf10>\n",
            "Constructing exemplars of class 67\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [33991, 41583, 35158, 19900, 715, 15268, 22218, 1973, 48882, 37012, 48251, 28494, 965, 16878, 48251, 20899, 14591, 15270, 7029, 4970, 46666, 36708, 25463, 3540, 14591, 32082, 38172, 23985, 39844, 18423, 6666, 24336, 22584, 38422, 5107, 2816, 18713, 6928, 47514, 35429, 16820, 40570, 38326, 21582, 49439, 7961, 41855, 24092, 5730, 2062, 48251, 38671, 40180, 17396, 37065, 14637, 27043, 30023, 12908, 42295, 2771, 21582, 41643, 8475, 35785, 2974, 13266, 1088, 6314, 30015, 13909, 2326, 29480, 13905, 11730, 6815, 36708, 26975, 29022, 49262, 46443, 1027, 20109, 5107, 18701, 19244, 1796, 41392, 20057, 125, 23985, 49407, 34337, 32951, 9653, 21903, 27157, 41643, 30551, 23728, 34679, 46553, 34036, 32951, 23517, 37221, 10988, 42945, 26351, 44009, 7631, 17422, 6186, 25278, 18577, 46666, 9702, 3540, 19810, 9853, 14637, 33742, 46588, 47386, 18540, 22925, 46588, 18577, 14637, 24148, 26910, 28814, 32951, 10947, 8692, 14249, 21286, 38671, 35785, 35174, 40605, 26411, 27223, 46666, 36003, 32951, 3540, 14515, 8475, 6928, 6036, 13575, 17514, 40782, 16592, 13886, 47514, 31580, 30023, 15111, 14591, 22817, 18713, 47057, 41279, 18488, 7430, 2785, 37583, 48038, 38326, 11571, 49439, 38422, 2493, 1356, 7768, 33991, 12995, 13886, 8692, 29694, 6978, 15270, 7029, 23985, 17163, 18015, 715, 47266, 14999, 20145, 17411, 24720, 14591, 46619, 2816, 30991, 42872, 32082]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fa7bf459b50>\n",
            "Constructing exemplars of class 59\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [38991, 33062, 12113, 41971, 49165, 29628, 45267, 32609, 20713, 3316, 32778, 4018, 40744, 31388, 8624, 610, 36223, 11075, 16446, 27489, 517, 7980, 22642, 10773, 47618, 28325, 39641, 25142, 8624, 41434, 22797, 12832, 40985, 110, 45953, 19136, 4708, 38991, 31674, 22788, 35676, 32264, 37490, 46059, 41926, 14376, 11880, 24236, 12711, 46093, 6065, 16777, 13465, 28325, 6746, 11587, 45470, 10092, 4580, 10087, 29305, 33418, 13338, 39867, 39034, 10087, 45339, 16244, 18985, 39600, 19949, 14004, 36999, 43758, 45723, 11187, 17119, 30471, 43682, 16244, 39481, 8081, 13093, 10919, 25891, 47372, 24261, 34779, 14376, 24627, 35279, 30553, 40094, 42170, 849, 27341, 28878, 5672, 33685, 32372, 731, 40396, 35136, 4018, 40178, 18512, 16738, 38195, 26213, 46599, 12832, 30817, 45468, 46938, 26688, 9026, 17894, 44993, 13465, 49165, 24255, 19949, 2530, 45267, 41971, 19335, 19183, 15138, 1523, 48988, 27956, 10111, 610, 49299, 6883, 40985, 8578, 48169, 7664, 8113, 40396, 20481, 18467, 31607, 15418, 40769, 10773, 44865, 7065, 38962, 3165, 1160, 1616, 32372, 39641, 18938, 9679, 31388, 34560, 3820, 37533, 731, 49789, 517, 42114, 25112, 37490, 41971, 29873, 2530, 39867, 18250, 30470, 10167, 15798, 20713, 45723, 687, 44865, 36945, 29873, 16466, 17943, 1927, 19092, 36387, 13656, 25930, 11075, 30915, 13004, 28806, 39600, 15989, 36312, 37490, 31097, 6100, 49663, 517]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fa7bf980910>\n",
            "Constructing exemplars of class 39\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [28001, 2808, 22685, 21397, 156, 4155, 28010, 24205, 39348, 3745, 5344, 23374, 6542, 12135, 43077, 19080, 7493, 16316, 24205, 14473, 34507, 36912, 19899, 10790, 47358, 20453, 30940, 3970, 39686, 6542, 10645, 46513, 16449, 49899, 34395, 27250, 27032, 41901, 39804, 28623, 45884, 19303, 6407, 7295, 5673, 32307, 40525, 37628, 41688, 23370, 46629, 39348, 32852, 12184, 15263, 47593, 37794, 18253, 49899, 35280, 20687, 11500, 14346, 22227, 47014, 13219, 18785, 18875, 47583, 14815, 22685, 18522, 2823, 9660, 20120, 23492, 38861, 38041, 14438, 21575, 32439, 11500, 17931, 18522, 22648, 33518, 35634, 18875, 37062, 14438, 21409, 14473, 28132, 16211, 40488, 26365, 33832, 23011, 42970, 16610, 42224, 45631, 28649, 28163, 33506, 13996, 12786, 13745, 40353, 16303, 38395, 33368, 26139, 14172, 28004, 15148, 13077, 32802, 21506, 10, 5570, 15822, 30788, 2213, 17947, 41876, 47084, 18785, 49074, 36912, 37727, 3307, 18172, 735, 1446, 43777, 6124, 13077, 49887, 7999, 33859, 17472, 48671, 4922, 37794, 36070, 22175, 41455, 37727, 33421, 41876, 32685, 12386, 39225, 28623, 16212, 11883, 44046, 42129, 37794, 11775, 37780, 42191, 11265, 11082, 42159, 29686, 4999, 28163, 4999, 49155, 20120, 226, 47084, 28010, 32848, 8082, 39686, 37794, 34103, 5344, 40098, 156, 27032, 18253, 40640, 41341, 12263, 44285, 21343, 34070, 25284, 47583, 12025, 22654, 11427, 47340, 30202, 28010, 3758]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fa7bfa13f90>\n",
            "Constructing exemplars of class 22\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [3444, 8341, 24352, 47145, 18174, 20525, 28244, 25693, 36224, 16753, 5794, 24352, 29950, 8411, 632, 36314, 30371, 4228, 7341, 37478, 10240, 29172, 24590, 40972, 49320, 24165, 31742, 14594, 21333, 39093, 16072, 29641, 2135, 26429, 4522, 2752, 43562, 8942, 8140, 18718, 8603, 33527, 37754, 747, 5766, 29247, 47145, 27497, 8126, 39731, 48870, 4824, 7696, 38393, 5088, 6674, 26778, 44298, 26352, 23745, 8341, 10422, 19004, 8656, 39621, 17083, 8117, 9322, 49821, 19884, 3447, 35674, 9221, 15761, 8402, 10976, 46552, 747, 6669, 31477, 9828, 22774, 22036, 36224, 44298, 7341, 43566, 2503, 35935, 29950, 46997, 5794, 6300, 32438, 8016, 6027, 24042, 25186, 39147, 35840, 32180, 4915, 9221, 36314, 16283, 32438, 8016, 37250, 8389, 34817, 28257, 10976, 41988, 33620, 9681, 38022, 26767, 36897, 20915, 9171, 18718, 5402, 29247, 10291, 6544, 35824, 7499, 48818, 41765, 13447, 30411, 43809, 424, 36901, 31324, 46649, 24352, 41988, 7761, 26835, 8389, 43014, 31853, 41570, 16458, 18636, 10512, 5725, 12317, 3941, 33853, 12627, 31742, 19004, 25300, 44529, 39147, 35685, 48995, 11693, 19884, 31223, 6300, 2503, 10826, 5402, 15761, 7888, 49084, 25379, 14594, 27012, 18163, 44704, 39621, 13209, 6674, 41185, 19344, 10992, 43471, 19813, 30411, 37554, 47317, 21333, 33886, 8603, 35638, 15017, 18174, 41988, 6019, 9104, 35935, 31223, 2135, 26429, 4522, 17623]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fa7bf9713d0>\n",
            "Constructing exemplars of class 18\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [22390, 12585, 30738, 8936, 46622, 28047, 13732, 29345, 46753, 12619, 14811, 37016, 20543, 44061, 49129, 7149, 6745, 40615, 36761, 14128, 35793, 2778, 5809, 13417, 43832, 3556, 22438, 27169, 47192, 36707, 10065, 37129, 41405, 48346, 12968, 30306, 2671, 12886, 30555, 46329, 31340, 37924, 24602, 4386, 3114, 38974, 22390, 28378, 25591, 44655, 32431, 13732, 22322, 4107, 10127, 11574, 8583, 33432, 29345, 31265, 14283, 40615, 16584, 2811, 21335, 39583, 22397, 10829, 48761, 46329, 46578, 34411, 10536, 38921, 31736, 23148, 41532, 40907, 20143, 38719, 8405, 7386, 11385, 45663, 27989, 45188, 24650, 9645, 49129, 28519, 48346, 37016, 4461, 9100, 21960, 35968, 15174, 26947, 12619, 35637, 25531, 33432, 31256, 2593, 22416, 4384, 30885, 31274, 36037, 12968, 21190, 3556, 47407, 46925, 34994, 18889, 16819, 2819, 49759, 13030, 11574, 8583, 12886, 12660, 30216, 46386, 12619, 1273, 89, 23555, 5169, 5477, 37129, 44061, 9782, 46884, 407, 24601, 35004, 20068, 5966, 19627, 35061, 25531, 11276, 9705, 43287, 19796, 29811, 28378, 12660, 14107, 2125, 10693, 13172, 39440, 42724, 18683, 36385, 7649, 12619, 37966, 13030, 4384, 38379, 46854, 9774, 26947, 32203, 9414, 22981, 9718, 21190, 9705, 30580, 23705, 34889, 28047, 11132, 29345, 20068, 18932, 9645, 40615, 44274, 9782, 9622, 33465, 21960, 24650, 31512, 17146, 47750, 17833, 12009, 25714, 32963, 17055, 13606, 47407]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fa7bf994a90>\n",
            "Constructing exemplars of class 65\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [22323, 35969, 28855, 16121, 28962, 594, 25261, 33, 48829, 35503, 8666, 49493, 23766, 958, 6411, 19199, 33367, 7323, 48164, 30223, 20348, 47051, 6398, 20579, 41212, 16003, 16041, 30849, 25214, 36020, 21059, 22447, 46250, 29067, 47804, 45632, 25214, 14214, 36076, 32767, 23825, 14986, 9777, 39587, 36877, 44923, 42785, 6411, 19199, 1982, 6157, 20348, 32928, 21339, 23670, 32583, 40973, 46148, 25214, 28543, 10962, 17480, 26586, 35284, 3505, 38560, 19667, 44741, 39308, 26757, 4977, 9777, 16268, 16101, 43176, 20579, 38907, 4755, 1934, 18709, 44283, 31718, 21771, 11176, 14986, 47181, 9040, 7322, 26757, 18720, 30223, 12309, 7010, 31894, 3576, 39308, 16101, 36547, 8526, 21450, 16647, 48581, 848, 36776, 28375, 9715, 39714, 10177, 38560, 17603, 42643, 802, 29638, 19026, 7563, 6398, 35631, 18977, 46043, 21684, 23565, 42785, 6411, 46250, 24775, 2424, 45632, 23682, 27376, 11632, 24955, 25046, 46478, 38308, 30223, 47161, 35407, 10177, 42332, 13805, 20348, 49735, 22729, 21354, 22447, 23766, 42091, 7568, 23682, 26193, 292, 48126, 6411, 21450, 41727, 8123, 44923, 21366, 10704, 25214, 29737, 18550, 28532, 47181, 37635, 38262, 12304, 8899, 3576, 28855, 21515, 28543, 26586, 21684, 7114, 26241, 3602, 10704, 21639, 37992, 21726, 45229, 18977, 8666, 38799, 20016, 47051, 43396, 34206, 37153, 34302, 32834, 2972, 37105, 33597, 6254, 30989, 12309, 49140, 30989]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fa7bf971410>\n",
            "Constructing exemplars of class 49\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [45722, 10609, 4869, 617, 12011, 19312, 41817, 22247, 10758, 11122, 7441, 25906, 23913, 26204, 9463, 21001, 11081, 47186, 204, 19812, 24792, 2372, 23438, 38562, 15100, 6144, 27492, 17229, 46075, 32735, 24778, 33136, 18630, 11442, 13755, 39979, 22292, 6873, 38402, 3667, 20485, 33562, 33744, 6622, 27143, 41836, 1298, 27888, 41228, 38402, 8997, 18731, 39004, 41676, 36235, 41836, 46973, 23913, 12011, 39991, 29526, 22101, 42559, 41817, 17776, 38562, 8856, 12554, 46508, 8025, 2361, 29777, 8721, 26204, 22431, 14823, 1434, 11059, 36563, 617, 35615, 17524, 29893, 204, 27143, 416, 33744, 25318, 47556, 16075, 3750, 14406, 43305, 49043, 49274, 18630, 44090, 39004, 18797, 25906, 15342, 4869, 8987, 4264, 7587, 37237, 12555, 39435, 33136, 25906, 42230, 18731, 33106, 25068, 2737, 40851, 46075, 40318, 1787, 20753, 22074, 33031, 416, 30052, 2361, 16676, 45718, 3667, 13849, 27888, 30913, 18001, 11308, 6758, 41676, 2444, 26323, 41698, 45236, 42339, 8515, 4974, 31747, 19911, 38251, 31401, 39435, 35492, 47582, 45945, 35949, 5731, 34131, 11122, 45718, 46931, 26323, 18001, 39644, 1610, 16251, 18189, 42325, 12319, 42339, 28355, 33136, 1591, 27289, 26063, 47186, 8987, 19812, 47850, 39435, 45633, 33988, 43337, 18112, 27143, 7903, 15342, 23161, 25656, 8856, 16251, 35767, 8267, 4974, 7441, 36073, 2219, 45624, 12555, 39435, 5576, 29526, 25068, 18138, 19312]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fa7bf980bd0>\n",
            "Constructing exemplars of class 56\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [6595, 2853, 27676, 9811, 11727, 7751, 31482, 43815, 28791, 36898, 45845, 41541, 14334, 39675, 37352, 45627, 4430, 35585, 6522, 15636, 10274, 36578, 47151, 28854, 44648, 16068, 41541, 34018, 42482, 43212, 46633, 10390, 6522, 35585, 26765, 9259, 6916, 7492, 41264, 48484, 10274, 17919, 34242, 46688, 27809, 40795, 16238, 26349, 14752, 23863, 30262, 44648, 9811, 39054, 30678, 37023, 16238, 22829, 2327, 39945, 18368, 14724, 8628, 41553, 7156, 26303, 21522, 40439, 7619, 46633, 15649, 44808, 23461, 776, 7751, 13406, 689, 48881, 3426, 38459, 43813, 36165, 4868, 27001, 16068, 41541, 36491, 1701, 45218, 39221, 6165, 9259, 33738, 35585, 40710, 49638, 34776, 23382, 8628, 33660, 277, 47151, 45218, 25780, 12571, 25926, 41899, 34304, 43044, 47179, 28131, 7507, 46451, 4554, 45845, 21240, 39350, 19129, 689, 6838, 10070, 16420, 27457, 41541, 27588, 442, 471, 25519, 5954, 45311, 47479, 36669, 11756, 9811, 9176, 8663, 14392, 41820, 43650, 39599, 1822, 8914, 5429, 26153, 44648, 17799, 2909, 37622, 7640, 13908, 35565, 13189, 47438, 4395, 36578, 46309, 35792, 14423, 27997, 15456, 35305, 18867, 12819, 10239, 10357, 22460, 7964, 21642, 1410, 1167, 45621, 397, 5361, 47956, 43633, 41914, 31185, 49001, 43650, 29454, 12571, 40271, 18635, 18368, 32147, 13908, 6916, 30715, 40468, 29639, 26544, 2918, 10274, 269, 24629, 15774, 9050, 11760, 26517, 43212]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fa7bf44ae50>\n",
            "Constructing exemplars of class 20\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [7800, 5727, 43393, 4213, 40932, 49733, 24747, 15770, 25015, 15166, 4833, 29591, 8128, 14207, 17285, 12314, 43736, 47846, 16438, 38013, 34159, 49354, 19404, 17318, 45518, 7038, 1505, 26120, 17671, 36514, 16438, 26701, 23035, 37473, 44209, 3393, 45035, 13700, 25511, 3402, 44914, 24326, 44408, 43657, 12086, 15378, 31251, 10076, 30546, 10219, 45704, 31478, 19305, 34189, 1827, 19404, 509, 17671, 48989, 42719, 21939, 19671, 13094, 48133, 15330, 15770, 12086, 22243, 33242, 29519, 14029, 13794, 325, 30910, 12831, 26291, 9807, 44986, 28249, 3393, 43736, 31009, 28340, 40497, 24747, 5284, 39720, 5643, 2655, 39022, 22251, 30910, 4831, 49855, 2395, 19354, 19305, 44455, 48767, 26973, 5560, 28851, 41767, 2395, 28340, 17671, 32672, 30910, 16774, 46008, 44986, 27669, 3033, 38234, 25513, 30383, 41732, 1210, 2653, 20098, 24052, 5846, 44512, 31326, 34493, 24540, 25890, 29519, 32210, 40989, 37657, 41924, 22251, 22243, 26058, 37831, 22230, 20089, 17483, 37138, 16438, 9807, 28851, 41499, 23321, 3776, 43657, 28391, 5284, 17318, 40497, 31251, 25185, 41767, 5940, 43736, 34663, 26615, 31009, 25507, 46108, 42094, 37898, 19222, 24747, 10219, 4831, 34819, 4989, 36570, 10076, 24747, 46001, 17415, 36064, 34850, 23045, 22713, 14363, 26111, 9374, 44512, 4731, 30069, 44408, 14269, 42561, 28340, 5080, 42719, 14363, 33677, 22810, 28825, 7038, 14139, 10269, 37357, 7285, 3402]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fa7bf66fa10>\n",
            "Constructing exemplars of class 4\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [21579, 26580, 30185, 42844, 48332, 28682, 44872, 37055, 10066, 37771, 36232, 7524, 4899, 18449, 7932, 19094, 2429, 44162, 44258, 22002, 14058, 47596, 28576, 14727, 27242, 43897, 48414, 6854, 102, 37055, 7290, 22274, 33004, 18393, 8462, 7936, 35641, 10066, 49483, 22782, 13211, 47125, 2769, 16923, 41542, 48332, 19554, 6925, 16974, 7595, 41747, 4588, 42222, 35690, 23270, 20446, 40659, 825, 26299, 638, 44872, 15681, 45367, 40558, 21109, 3801, 31801, 25090, 3779, 41665, 27019, 14549, 43943, 11665, 40806, 7270, 42509, 22660, 1729, 45163, 47125, 45545, 16923, 49929, 35006, 27560, 37055, 48172, 32185, 4588, 40009, 48971, 21103, 6051, 40917, 36429, 15120, 7595, 8008, 49720, 33880, 1302, 8616, 19094, 29391, 42844, 9487, 15423, 36339, 44568, 48971, 24293, 38786, 11985, 32331, 29036, 44264, 9579, 20970, 11359, 45465, 31311, 10058, 4846, 5542, 25973, 36030, 29496, 14436, 33793, 45934, 37030, 7595, 1767, 12645, 47596, 45664, 5542, 20413, 28682, 37075, 37057, 7595, 34888, 41557, 33312, 19094, 39400, 15703, 13454, 43232, 35641, 32837, 20735, 24154, 41542, 32185, 30335, 43974, 34433, 24545, 15442, 488, 15117, 31801, 21579, 41626, 43151, 42509, 36063, 43660, 47133, 31043, 30288, 26476, 36371, 28205, 28682, 41403, 12963, 17304, 35361, 37049, 7017, 1929, 6854, 19181, 19801, 36124, 14826, 9380, 10882, 6051, 40917, 43151, 9579, 32331, 5519, 45780, 37434]\n",
            "5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ciclo immagini\n",
            "1\n",
            "[0.0, 0.0, 0.0, 0.0, array([2.4055023], dtype=float32), array([2.3638825], dtype=float32), array([2.4068599], dtype=float32), array([2.3655517], dtype=float32), array([2.3758178], dtype=float32), array([2.4138532], dtype=float32), array([2.3592186], dtype=float32), array([2.3951697], dtype=float32), array([2.4069576], dtype=float32), array([2.360907], dtype=float32), array([2.3623707], dtype=float32), array([2.395642], dtype=float32), array([2.3845034], dtype=float32), array([2.4192734], dtype=float32), array([3.2212708], dtype=float32), array([3.2533245], dtype=float32), array([1.1696795], dtype=float32), array([1.165406], dtype=float32), array([1.8031702], dtype=float32), array([1.7805498], dtype=float32), array([1.7711394], dtype=float32), array([1.7556934], dtype=float32), array([1.7827082], dtype=float32), array([1.7836301], dtype=float32), array([1.8254055], dtype=float32), array([1.8008496], dtype=float32), array([1.7622614], dtype=float32), array([1.7905105], dtype=float32), array([1.7463958], dtype=float32), array([1.7627394], dtype=float32), array([1.7847939], dtype=float32), array([1.7654871], dtype=float32), array([1.7578623], dtype=float32), array([1.7716877], dtype=float32), array([1.7896006], dtype=float32), array([1.2970065], dtype=float32), array([1.2907331], dtype=float32), array([1.2976203], dtype=float32), array([1.2700775], dtype=float32), array([1.3091242], dtype=float32), array([1.2863315], dtype=float32), array([1.292868], dtype=float32), array([1.2628095], dtype=float32), array([1.2979939], dtype=float32), array([1.2951105], dtype=float32), array([0.97085726], dtype=float32), array([0.98626965], dtype=float32), array([0.98251057], dtype=float32), array([1.0194643], dtype=float32), array([1.0048633], dtype=float32), array([1.0082635], dtype=float32), array([0.96680367], dtype=float32), array([2.3617032], dtype=float32), array([2.432063], dtype=float32), array([2.3930938], dtype=float32), array([2.4512496], dtype=float32), array([2.471545], dtype=float32), array([2.4110012], dtype=float32), array([2.412943], dtype=float32), array([2.4244866], dtype=float32), array([2.4608366], dtype=float32), array([2.6267495], dtype=float32), array([2.6275072], dtype=float32), array([1.735693], dtype=float32), array([1.7440742], dtype=float32), array([1.7224143], dtype=float32), array([1.7663593], dtype=float32), array([1.7253898], dtype=float32), array([1.7604078], dtype=float32), array([1.7519016], dtype=float32), array([1.7411636], dtype=float32), array([1.7246778], dtype=float32), array([1.7350123], dtype=float32), array([1.7373998], dtype=float32), array([1.7206933], dtype=float32), array([1.7301681], dtype=float32), array([1.7484354], dtype=float32), array([1.7360518], dtype=float32), array([1.7331278], dtype=float32), array([1.7371969], dtype=float32), array([1.693355], dtype=float32), array([1.7535101], dtype=float32), array([1.7620913], dtype=float32), array([1.7181346], dtype=float32), array([1.752938], dtype=float32), array([1.7299255], dtype=float32), array([1.7400706], dtype=float32), array([1.7724771], dtype=float32), array([1.7227414], dtype=float32), array([1.7455763], dtype=float32), array([1.742802], dtype=float32), array([1.7302748], dtype=float32), array([1.7189888], dtype=float32), array([1.7413769], dtype=float32), array([1.7251574], dtype=float32), array([1.7421792], dtype=float32)]\n",
            "[18.630825]\n",
            "ciclo immagini\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[array([1.8875507], dtype=float32), array([1.8630435], dtype=float32), array([1.8713691], dtype=float32), array([1.8870814], dtype=float32), array([0.03785958], dtype=float32), array([0.0430866], dtype=float32), array([0.04761378], dtype=float32), array([0.07575735], dtype=float32), array([0.06274681], dtype=float32), array([0.03946345], dtype=float32), array([0.04619192], dtype=float32), array([0.04756761], dtype=float32), array([0.04843986], dtype=float32), array([0.09126913], dtype=float32), array([0.04528502], dtype=float32), array([0.01854445], dtype=float32), array([0.03782889], dtype=float32), array([0.07103329], dtype=float32), array([-0.5390328], dtype=float32), array([-0.57357633], dtype=float32), array([-0.69359356], dtype=float32), array([-0.7080472], dtype=float32), array([-0.11536074], dtype=float32), array([-0.10935856], dtype=float32), array([-0.04426054], dtype=float32), array([-0.09189641], dtype=float32), array([-0.01493496], dtype=float32), array([-0.03990293], dtype=float32), array([-0.09546618], dtype=float32), array([-0.03156332], dtype=float32), array([-0.0372784], dtype=float32), array([-0.04033271], dtype=float32), array([-0.06306477], dtype=float32), array([-0.06789126], dtype=float32), array([-0.09647389], dtype=float32), array([-0.11852524], dtype=float32), array([-0.1118269], dtype=float32), array([-0.05157229], dtype=float32), array([-0.05272977], dtype=float32), array([0.62065756], dtype=float32), array([0.6056456], dtype=float32), array([0.5631276], dtype=float32), array([0.5979548], dtype=float32), array([0.620891], dtype=float32), array([0.5701858], dtype=float32), array([0.60553837], dtype=float32), array([0.5500886], dtype=float32), array([0.6635278], dtype=float32), array([0.58124924], dtype=float32), array([3.3806653], dtype=float32), array([3.394621], dtype=float32), array([3.3856215], dtype=float32), array([3.443359], dtype=float32), array([3.3843365], dtype=float32), array([3.3813694], dtype=float32), array([3.3967636], dtype=float32), array([0.95726967], dtype=float32), array([0.9598301], dtype=float32), array([0.96271497], dtype=float32), array([0.91749734], dtype=float32), array([0.9151524], dtype=float32), array([0.87827474], dtype=float32), array([0.9400145], dtype=float32), array([0.8672474], dtype=float32), array([0.9353029], dtype=float32), array([-0.16856124], dtype=float32), array([-0.14075099], dtype=float32), array([1.8514636], dtype=float32), array([1.902661], dtype=float32), array([1.8521969], dtype=float32), array([1.8862435], dtype=float32), array([1.8667519], dtype=float32), array([1.8523926], dtype=float32), array([1.8617132], dtype=float32), array([1.8487768], dtype=float32), array([1.8749828], dtype=float32), array([1.8818581], dtype=float32), array([1.8339083], dtype=float32), array([1.8559242], dtype=float32), array([1.8994653], dtype=float32), array([1.8755716], dtype=float32), array([1.896382], dtype=float32), array([1.9050137], dtype=float32), array([1.8875983], dtype=float32), array([1.9128845], dtype=float32), array([1.852573], dtype=float32), array([1.8640597], dtype=float32), array([1.8617508], dtype=float32), array([1.870953], dtype=float32), array([1.9197903], dtype=float32), array([1.8955972], dtype=float32), array([1.8729852], dtype=float32), array([1.8991083], dtype=float32), array([1.8746102], dtype=float32), array([1.893245], dtype=float32), array([1.8416599], dtype=float32), array([1.8695228], dtype=float32), array([1.8906031], dtype=float32), array([1.8549188], dtype=float32), array([1.9059913], dtype=float32)]\n",
            "14.968975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-6e4e79ac598e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclosed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequentialLearningiCaRLOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrejection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-5e2e2550143b>\u001b[0m in \u001b[0;36msequentialLearningiCaRLOpen\u001b[0;34m(train_subsets, val_subsets, open_dataset, rejection, closed)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mnum_classes_seen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0macc_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes_seen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexemplars_set_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mall_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TEST ALL: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-ce48ff669d3c>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(net, test_dataloader, num_classes, exemplar_set, rejection, closed)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexemplar_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexemplar_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexemplar_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_preds_cm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels_cm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-ce48ff669d3c>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(net, val_dataloader, criterion, num_classes, exemplar_set, rejection, closed)\u001b[0m\n\u001b[1;32m     64\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormalized_vector\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m               \u001b[0mreject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.float32' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51XqBQsXogw4"
      },
      "source": [
        "## Variation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEg6pKOSomU6"
      },
      "source": [
        "### Data Augmentation\n",
        "\n",
        "\n",
        "*   **brightness**: the intensity of the original image is altered by adding a random intensity value in the range [-63,63]\n",
        "*   **contrast normalization**: the contrast of the original image is altered by a random value in the range [0.2,1.8]\n",
        "*   **random cropping**: all the images (original, brightness and costrast) are randomly cropped\n",
        "*   **mirroring**: a mirror image is computed for all images (original, brightness, contrast and crops)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f5Osm7B45nd"
      },
      "source": [
        "HOW TO USE\n",
        "\n",
        "    for _, images, labels in train_dataloader: \n",
        "  \n",
        "        \"\"\"  \n",
        "        perform data augmentation \n",
        "        \"\"\" \n",
        "        # print(\"Dimension before data augmentation: \", images.shape) \n",
        "        # print(\"...performing data augmentation...\") \n",
        "        images, labels = data_augmentation_e2e(images,labels) \n",
        "  \n",
        "        # print(\"Dimension after data augmentation: \", images.shape) \n",
        "  \n",
        "  \n",
        "        images = torch.from_numpy(images) \n",
        "        labels = torch.from_numpy(labels) \n",
        "  \n",
        "  \n",
        "  \n",
        "        # Bring images and labels to GPU \n",
        "        images = images.to(DEVICE,dtype=torch.float32) \n",
        "        labels = labels.to(DEVICE,dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5j7CevMoisq"
      },
      "source": [
        "import tensorflow as tf\n",
        "def data_augmentation_e2e(img, lab):\n",
        "    \"\"\"\n",
        "        Realize the data augmentation in End-to-End paper\n",
        "        Parameters\n",
        "        ----------\n",
        "        img: the original images, size = (n, c, w, h)\n",
        "        lab: the original labels, size = (n)\n",
        "        Returns\n",
        "        ----------\n",
        "        img_aug: the original images, size = (n * 12, c, w, h)\n",
        "        lab_aug: the original labels, size = (n * 12)\n",
        "    \"\"\"\n",
        "    \n",
        "    shape = np.shape(img)\n",
        "    # print(\"IMG is: \",img)\n",
        "    # print(shape[0], 1, shape[1], shape[2], shape[3])\n",
        "    img_aug = np.zeros((shape[0], 6, shape[1], shape[2], shape[3]))\n",
        "    img_aug[:, 0, :, :, :] = img\n",
        "    lab_aug = np.zeros((shape[0], 6))\n",
        "    # print(\"IMG_AUG is: \", img_aug)\n",
        "\n",
        "    for i in range(shape[0]):\n",
        "        # np.random.seed(int(time.time()) % 1000)\n",
        "\n",
        "        # convert image from tensor to numpy\n",
        "        image=img.numpy()\n",
        "        im = image[i]\n",
        "      \n",
        "        # # brightness\n",
        "        brightness = (np.random.rand(1)-0.5)*2*63\n",
        "        im_temp = im + brightness\n",
        "\n",
        "        img_aug[i, 1] = im_temp\n",
        "\n",
        "\n",
        "        # constrast\n",
        "        constrast = (np.random.rand(1)-0.5)*2*0.8+1\n",
        "        m0 = np.mean(im[0])\n",
        "        m1 = np.mean(im[1])\n",
        "        m2 = np.mean(im[2])\n",
        "        im_temp = im\n",
        "        im_temp[0] = (im_temp[0]-m0)*constrast + m0\n",
        "        im_temp[1] = (im_temp[1]-m1)*constrast + m1\n",
        "        im_temp[2] = (im_temp[2]-m2)*constrast + m2\n",
        "        img_aug[i, 2] = im_temp\n",
        "\n",
        "        # crop\n",
        "        im_temp = img_aug[i, :3]\n",
        "        for j in range(3):\n",
        "            x_ = int(np.random.rand(1)*1000)%8\n",
        "            y_ = int(np.random.rand(1)*1000)%8\n",
        "            im_temp = np.zeros(shape=(shape[1], shape[2]+8, shape[3]+8))\n",
        "            im_temp[:, 4:-4, 4:-4] = img_aug[i, j]\n",
        "            img_aug[i, 3+j] = im_temp[:, x_:x_+shape[2], y_:y_+shape[3]]\n",
        "\n",
        "\n",
        "\n",
        "        # mirror\n",
        "        # for j in range(6):\n",
        "        #     im_temp = img_aug[i, j]\n",
        "        #     img_aug[i, 6 + j] = im_temp[:,-1::-1,:]\n",
        "\n",
        "        lab_aug[i, :] = lab[i]\n",
        "\n",
        "    # idx = np.where(img_aug>255)\n",
        "    # img_aug[idx] = 255\n",
        "    # idx = np.where(img_aug<0)\n",
        "    # img_aug[idx] = 0\n",
        "\n",
        "    img_aug = np.reshape(img_aug, newshape=(shape[0]*6, shape[1], shape[2], shape[3]))\n",
        "    img_aug = np.array(img_aug, dtype=np.float64)\n",
        "    lab_aug = np.reshape(lab_aug, newshape=(shape[0]*6))\n",
        "    lab_aug = np.array(lab_aug, dtype=np.float64)\n",
        "    return img_aug, lab_aug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nn5U7dCoygv"
      },
      "source": [
        "### prova"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NK2S4Xaozyd"
      },
      "source": [
        "train_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=4, drop_last=False)\n",
        "n = 1\n",
        "i = 0\n",
        "for _, images, labels in train_loader:\n",
        "  if i == 0:\n",
        "    print(images.shape, labels.shape)\n",
        "    print(images, labels)\n",
        "    images_combined, labels_combined = data_augmentation_e2e(images,labels)\n",
        "    print(images_combined.shape, labels_combined.shape)\n",
        "    print(images_combined, labels_combined)\n",
        "    i = 2\n",
        "  else:\n",
        "    break\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeUV-v0GW7yA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9knD34-NXADG",
        "outputId": "91be43d8-2635-4277-93e4-2687455f6f01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "input1 = torch.randn(1,64)\n",
        "input2 = torch.randn(1,64)\n",
        "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "output = cos(input1, input2)\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.2616581], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    }
  ]
}