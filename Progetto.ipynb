{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LWF funzionante.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoManca-FM/Project_MLDL/blob/main/Progetto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAfp19qRVa7l"
      },
      "source": [
        "### GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag2iq0PaVWC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29dd38c8-1718-41a6-f180-47cc7a566988"
      },
      "source": [
        "# Check GPU assigned\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jun 29 13:49:47 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OWtqytyzmMF"
      },
      "source": [
        "## Network, dataset, functions and parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itMjhanGVTUp"
      },
      "source": [
        "### LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6tCA_s2rru"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nHVORYbiZRx"
      },
      "source": [
        "### Resnet32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIdSy6PnibZA"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\"\"\"\n",
        "Credits to @hshustc\n",
        "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from torch.nn import init\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_relu=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.stride = stride\n",
        "        self.use_relu = use_relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "        if self.use_relu:\n",
        "            out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        assert stride == 2\n",
        "        self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat((x, x.mul(0)), 1)\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block=BasicBlock, depth=32, num_classes=0, channels=3):\n",
        "\n",
        "        super(CifarResNet, self).__init__()\n",
        "\n",
        "        # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
        "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
        "        layer_blocks = (depth - 2) // 6\n",
        "        bn = nn.BatchNorm2d\n",
        "        self.inplanes = 16\n",
        "\n",
        "        self.conv_1_3x3 = nn.Conv2d(channels, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn_1 = bn(self.inplanes)\n",
        "\n",
        "        self.stage_1 = self._make_layer(block, 16, layer_blocks, 1)\n",
        "        self.stage_2 = self._make_layer(block, 32, layer_blocks, 2)\n",
        "        self.stage_3 = self._make_layer(block, 64, layer_blocks, 2, last=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.relu = nn.ReLU()\n",
        "        #self.linear = nn.Linear(64, num_classes)\n",
        "        self.fcs = nn.ModuleList([nn.Linear(64, num_classes)])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, math.sqrt(1. / 64.))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, last=False):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = Downsample(self.\n",
        "inplanes, planes * block.expansion, stride)\n",
        "\n",
        "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        if last:\n",
        "            for i in range(1, blocks - 1):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            layers.append(block(self.inplanes, planes, use_relu=False))\n",
        "\n",
        "        else:\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv_1_3x3(x)\n",
        "\n",
        "        x = self.bn_1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.stage_1(x)\n",
        "        x = self.stage_2(x)\n",
        "        x = self.stage_3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        out = []\n",
        "        for fc in self.fcs:\n",
        "            out.append(fc(x))\n",
        "        out = torch.cat(out, dim=1)\n",
        "        return out\n",
        "\n",
        "    def addOutputNodes(self, num_classes):\n",
        "        self.fcs.append(nn.Linear(64, num_classes))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ga-SaCsjkYV"
      },
      "source": [
        "### Cifar100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHAyljuejmkd"
      },
      "source": [
        "# ref:\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py\n",
        "# https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR10\n",
        "# homework2 (caltech)\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# This is an handler class for the Cifar dataset\n",
        "class CIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    `CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "    This is a subclass of the `CIFAR100` Dataset.\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar-100-python'\n",
        "    train_file = 'train'\n",
        "    test_file = 'test'\n",
        "    meta_file = 'meta'\n",
        "\n",
        "    def __init__(self, root, split = 'train', transform = None):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              root (string): Root directory of the dataset where directory\n",
        "                  cifar-100-python exists.\n",
        "              split (string, optional): If 'train', creates dataset from training\n",
        "                  set, otherwise creates from test set.\n",
        "              transform (callable, optional): A function/transform that takes in a\n",
        "                  PIL image and returns a transformed version.\n",
        "        \"\"\"\n",
        "        super(CIFAR100, self).__init__(root, transform=transform)\n",
        "\n",
        "        self.split = split\n",
        "        if split == 'train':\n",
        "            filename = self.train_file\n",
        "        else:\n",
        "            filename = self.test_file\n",
        "\n",
        "        data_path = os.path.join(self.root, self.base_folder, filename)\n",
        "        data = None\n",
        "        labels = None\n",
        "\n",
        "        with open(data_path, 'rb') as f:\n",
        "            entry = pickle.load(f, encoding='latin1')\n",
        "            data = entry['data']\n",
        "            labels = entry['fine_labels']\n",
        "        \n",
        "        data = np.vstack(data).reshape(-1, 3, 32, 32)\n",
        "        data = data.transpose((0, 2, 3, 1))  # Convert to HWC\n",
        "        \n",
        "        labels = np.array(labels)\n",
        "\n",
        "        self.df = pd.DataFrame()\n",
        "        self.df['data'] = pd.Series(list(data))\n",
        "        self.df['labels'] = labels\n",
        "\n",
        "        self.data = self.df['data']\n",
        "        self.labels = self.df['labels']\n",
        "\n",
        "        self._load_meta()\n",
        "\n",
        "    def _load_meta(self):\n",
        "        meta_path = os.path.join(self.root, self.base_folder, self.meta_file)\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f, encoding='latin1')\n",
        "            self.label_names = meta['fine_label_names']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.df.loc[index, 'data'], self.df.loc[index, 'labels']\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img) # Return a PIL image\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return index, img, target\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def getTargets(self):\n",
        "        return set(self.labels)\n",
        "     \n",
        "    # test\n",
        "    def get_indices(self, labels):\n",
        "        return list(self.df[self.df['labels'].isin(labels)].index)\n",
        "\n",
        "    def split_classes(self, n_splits=10, seed=None, dictionary_of='dataframes'):\n",
        "        if dictionary_of not in ['dataframes','indices']:\n",
        "            raise ValueError(\"'dictionary_of' must be equal to 'dataframes' or 'indices'\")\n",
        "\n",
        "        all_classes = list(self.df['labels'].value_counts().index)\n",
        "        dictionary = {}\n",
        "        random.seed(seed)\n",
        "        random.shuffle(all_classes)\n",
        "        split_size = int(len(all_classes)/n_splits)\n",
        "        for j in range(n_splits):\n",
        "            if ((j+1)*split_size < len(all_classes)):\n",
        "                split_end = (j+1)*split_size\n",
        "            else:\n",
        "                split_end = None\n",
        "            subgroup = all_classes[j*split_size:split_end]\n",
        "            if dictionary_of == 'dataframes':\n",
        "                dictionary[j] = self.df[self.df['labels'].isin(subgroup)]\n",
        "            elif dictionary_of == 'indices':\n",
        "                dictionary[j] = list(self.df[self.df['labels'].isin(subgroup)].index)\n",
        "        return dictionary\n",
        "\n",
        "    def split_groups_in_train_validation(self, groups, ratio=0.5, seed=None):\n",
        "        groups_train_val = dict()\n",
        "        for k, subdf in groups.items():\n",
        "            train_indexes = []\n",
        "            val_indexes = []\n",
        "            split_labels = list(subdf['labels'].value_counts().index)\n",
        "            for l in split_labels:\n",
        "                indexes_to_sample = list(subdf[subdf['labels'] == l].index)\n",
        "                random.seed(seed)\n",
        "                train_samples = random.sample(indexes_to_sample, int(len(indexes_to_sample)*ratio))\n",
        "                train_indexes = train_indexes + train_samples\n",
        "                val_indexes = val_indexes + list(set(indexes_to_sample).difference(set(train_samples)))\n",
        "            groups_train_val[k] = {\n",
        "                'train': train_indexes,\n",
        "                'val': val_indexes\n",
        "            }\n",
        "        return groups_train_val\n",
        "    \n",
        "    def split_in_train_val_groups(self, n_splits=10, ratio=0.5, seed=None):\n",
        "        groups = self.split_classes(n_splits=n_splits, seed=seed)\n",
        "        return self.split_groups_in_train_validation(groups, ratio=ratio, seed=seed)\n",
        "\n",
        "    # given a tensors returns an image (used in exemplars)\n",
        "    #def tensorToImg(self, tensor):\n",
        "    #   return Variable(transform(Image.fromarray(img)), volatile=True)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_M1anVYkRnF"
      },
      "source": [
        "### Reverse Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j66mUOXekTlM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "__all__ = ['ReverseIndex']\n",
        "\n",
        "class ReverseIndex():\n",
        "\n",
        "    def __init__(self, dataset, splits, device='cuda'):\n",
        "\n",
        "        self.df = pd.DataFrame(columns=['group', 'labels'])\n",
        "\n",
        "        for k in splits.keys():\n",
        "            labels = list(dataset.df.loc[splits[k]['train'],'labels'].value_counts().index)\n",
        "            group = [k for i in range(len(labels))]\n",
        "            data = pd.DataFrame(np.array([group, labels]).T, columns=['group', 'labels'])\n",
        "            self.df = self.df.append(data, ignore_index=True)\n",
        "\n",
        "        self.df['nodes'] = self.df.index\n",
        "        self.device = device\n",
        "    \n",
        "    def _changeIndex(self, reverse_index, column):\n",
        "        reverse_index = reverse_index.set_index(column)\n",
        "        reverse_index[column] = reverse_index.index\n",
        "        return reverse_index\n",
        "\n",
        "    def getLabels(self, outputs):\n",
        "        outs = outputs.cpu().numpy()\n",
        "        reverse_index = self._changeIndex(self.df, 'nodes')\n",
        "        labels = reverse_index.loc[outs, 'labels']\n",
        "\n",
        "        labels = torch.tensor(list(labels))\n",
        "        return labels.to(self.device)\n",
        "\n",
        "    def getNodes(self, labels):\n",
        "        labels = labels.cpu().numpy()\n",
        "       \n",
        "        reverse_index = self._changeIndex(self.df, 'labels')\n",
        "\n",
        "        nodes = reverse_index.loc[labels, 'nodes']\n",
        "\n",
        "        nodes = torch.tensor(list(nodes))\n",
        "        return nodes.to(self.device)\n",
        "\n",
        "    def getGroups(self, distinct=True):\n",
        "        return self.df['group'].value_counts().index.sort_values()\n",
        "    \n",
        "    def getLabelsOfGroup(self, group):\n",
        "        return self.df.loc[self.df['group'] == group, 'labels']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVfOeOCuknB8"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QpAHzFkoW0"
      },
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Module\n",
        "\n",
        "# These are the default iCaRL hyper-parameters\n",
        "def getHyperparams():\n",
        "\tdictHyperparams = {\n",
        "\t\t\"LR\": 2,\n",
        "\t\t\"MOMENTUM\": 0.9,\n",
        "\t\t\"WEIGHT_DECAY\": 1e-5,\n",
        "\t\t\"NUM_EPOCHS\": 70,\n",
        "\t\t\"MILESTONES\": [49, 63],\n",
        "\t\t\"BATCH_SIZE\": 128,\n",
        "\t\t\"DEVICE\": 'cuda',\n",
        "\t\t\"GAMMA\": 0.2,\n",
        "\t\t\"SEED\": 66, #use 30, 42, 16\n",
        "\t\t\"LOG_FREQUENCY\": 10,\n",
        "\t\t\"NUM_CLASSES\": 100\n",
        "\t}\n",
        "\treturn dictHyperparams\n",
        "\n",
        "def getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize):\n",
        "\toptimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\tscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA, last_epoch=-1) \n",
        "\treturn optimizer, scheduler\n",
        "\n",
        "# the mean and the std have been found on the web as mean and std of cifar100\n",
        "# alternative (realistic): compute mean and std for the dataset\n",
        "def getTransformations():\n",
        "\t# Define transforms for training phase\n",
        "\ttrain_transform = transforms.Compose([transforms.RandomHorizontalFlip(), # Randomly flip the image with probability of 0.5\n",
        "\t                                      transforms.Pad(4), # Add padding\n",
        "\t                                      transforms.RandomCrop(32),# Crops a random squares of the image\n",
        "\t                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\t# Define transforms for the evaluation phase\n",
        "\teval_transform = transforms.Compose([\n",
        "\t                                      transforms.ToTensor(),\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\treturn train_transform, eval_transform\n",
        "\n",
        "# BCEWithLogits = Sigmoid + BCE, is the loss used in iCaRL\n",
        "def getLossCriterion():\n",
        "\tcriterion = nn.BCEWithLogitsLoss(reduction = 'mean') \n",
        "\treturn criterion\n",
        "\n",
        "# CrossEntropyLoss \n",
        "def computeLoss(criterion, outputs, labels):\n",
        "\treturn criterion(outputs, labels)\n",
        " \n",
        "# Loss L2\n",
        "def l2Loss (outputs, labels):\n",
        "  criterion = nn.MSELoss()\n",
        "  return criterion(outputs, labels)\n",
        "\n",
        "# Loss L1\n",
        "def l1Loss(outputs, labels):\n",
        "  criterion = nn.L1Loss()\n",
        "  return criterion(outputs, labels)\n",
        "\n",
        "# support BCE\n",
        "def _one_hot_encode(labels, n_classes, reverse_index, dtype=None, device='cuda'):\n",
        "\tbatch_size = len(labels)\n",
        "\tenconded = torch.zeros(batch_size, n_classes, dtype=dtype, device=device)\n",
        "\tlabels=map_to_outputs(labels, reverse_index)\n",
        "\tfor i, l in enumerate(labels):\n",
        "\t  enconded[i, l] = 1\n",
        "\treturn enconded\n",
        "\n",
        "def map_to_outputs(labels, reverse_index):\n",
        "\tif reverse_index is None:\n",
        "\t  return labels\n",
        "\tif type(labels) == int:\n",
        "\t  return int(reverse_index.getNodes(torch.tensor([labels])))\n",
        "\telif type(labels) == torch.Tensor:\n",
        "\t\treturn reverse_index.getNodes(labels)\n",
        "\n",
        "\n",
        "def plotAccuracyTrend(method, data_plot_line, seed):\n",
        "\tplt.figure(figsize=(20,7))\n",
        "\taccuracyDF=pd.DataFrame(data_plot_line, columns = ['Classes','Accuracy'])\n",
        "\tax = sns.lineplot(x=\"Classes\", y=\"Accuracy\",data=accuracyDF, marker = 'o')\n",
        "\tax.minorticks_on()\n",
        "\tax.set_xticks(np.arange(10,110,10))\n",
        "\tax.set_xlim(xmin=9, xmax=101)\n",
        "\tax.set_ylim(ymin=0, ymax=1)\n",
        "\tplt.legend(['Accuracy {}'.format(method)])\n",
        "\tax.grid(axis='y')\n",
        "\tplt.title(\"Accuracies against seen classes {} - seed: {}\".format(method, seed))\n",
        "\t\n",
        "\tfilename = \"acc_{}_{}.jpg\".format(method, seed) # ex. acc_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "def plotConfusionMatrix(method, confusionMatrixData, seed):\n",
        "\tfig,ax=plt.subplots(figsize=(10,10))\n",
        "\tsns.heatmap(confusionMatrixData,cmap='terrain',ax=ax)\n",
        "\tplt.ylabel('True label')\n",
        "\tplt.xlabel('Predicted label')\n",
        "\tplt.title(\"Confusion Matrix {} - seed: {}\".format(method, seed))\n",
        "\n",
        "\tfilename = \"cm_{}_{}.jpg\".format(method, seed) # ex. cm_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "# Write down the metrics (accuracy trand and confusion matrix)\n",
        "# this method is a shortcut when perfoming multiple tests with different splits (random_seed)\n",
        "# and allow us to plot on the same graph the data from multiple models (accuracy)\n",
        "def writeMetrics(method, seed, accuracies, confusionMatrixData):\n",
        "  data = {}\n",
        "  data['accuracies'] = []\n",
        "  data['cm'] = [] #cm line\n",
        "  i = 0\n",
        "  for classes_seen in range(10, 110, 10): #x axis on the plot\n",
        "    data['accuracies'].append({classes_seen : accuracies[i]}) \n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  for class_num in range(0,len(confusionMatrixData)): #rows of the cm\n",
        "    data['cm'].append({class_num : confusionMatrixData[i].tolist()}) \n",
        "    i += 1\n",
        "  \n",
        "  # dump to file\n",
        "  aus = method + '_' + str(seed)\n",
        "  filename = 'data_{}.json'.format(aus)\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "\n",
        "# Functions\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getResNet32():\n",
        "    net = CifarResNet()\n",
        "    # net.fc = nn.Linear(net.fc.in_features, output_size) # embedded in the class\n",
        "\n",
        "    criterion = getLossCriterion()\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return net, criterion, optimizer, scheduler\n",
        "\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getNet():\n",
        "    return getResNet32()\n",
        "\n",
        "def getSchedulerOptimizer(net):\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return optimizer, scheduler\n",
        "def softmax(x):\n",
        "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
        "    return f_x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfJj1z2LSxp0"
      },
      "source": [
        "### DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysghtAWOPYZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69f0d24-6d5d-4e3d-dbb8-c4a1daceebb9"
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "if not os.path.isdir('./{}'.format(\"$DATA_DIR/cifar-100-python\")):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-29 13:49:51--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  49.3MB/s    in 3.6s    \n",
            "\n",
            "2021-06-29 13:49:55 (44.9 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n",
            "mv: cannot move 'cifar-100-python' to 'DATA/cifar-100-python/cifar-100-python': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY10HfpTS1lq"
      },
      "source": [
        "### HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-pqSNg4_Ris"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100\n",
        "BATCH_SIZE = 128 \n",
        "LR = 0.1    \n",
        "MOMENTUM = 0.9     \n",
        "WEIGHT_DECAY = 1e-05\n",
        "NUM_EPOCHS = 70\n",
        "GAMMA = 0.2\n",
        "LOG_FREQUENCY = 10\n",
        "MILESTONES = [49,63]\n",
        "RANDOM_SEED = 66\n",
        "THRESHOLD = 0.5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW7WxV_QS4re"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_jHycn_1kk"
      },
      "source": [
        "train_transform, eval_transform = getTransformations()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPrwhTcIqUbA"
      },
      "source": [
        "### train & test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJKwvGljJj2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2832c58f-f70b-434f-a743-bc198bdfadfd"
      },
      "source": [
        "# Import dataset and apply transformations \n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# Check datasets length \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnooWwcyS_YN"
      },
      "source": [
        "### SPLIT DATA IN CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckn3H69iJj2X"
      },
      "source": [
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpcJvhxhJOLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b151c53-da9f-40dc-84cc-76d5facb1f7f"
      },
      "source": [
        "# TRAIN / VAL split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "print(outputs_labels_mapping)\n",
        "\n",
        "# TEST split\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.ReverseIndex object at 0x7fb96d4ee890>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MSItI0QVpn"
      },
      "source": [
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10):\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpWv5ZkhxTPJ"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4NQa-wNxWL1"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pisadUTPxcRP"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            # if (group_id > 1):\n",
        "            #   old_features = old_net.forward(images)\n",
        "            #   old_outputs = old_net.predict(old_features)\n",
        "            #   labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX_61nSuxh3v"
      },
      "source": [
        "### sequential learning fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM0GysHZxk1B"
      },
      "source": [
        "### Fine tuning\n",
        "def sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    old_net = None\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    #confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2X2IVZ-xrVT"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DGpd8lslxtUW",
        "outputId": "2a7a5171-8688-4b1f-fe52-f88f4b532a6a"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.7173749208450317\n",
            "Train step - Step 10, Loss 0.31387636065483093\n",
            "Train step - Step 20, Loss 0.2874338626861572\n",
            "Train step - Step 30, Loss 0.2654590606689453\n",
            "Train epoch - Accuracy: 0.318989898989899 Loss: 0.34560690457772725 Corrects: 1579\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.26789265871047974\n",
            "Train step - Step 50, Loss 0.258944034576416\n",
            "Train step - Step 60, Loss 0.2350231260061264\n",
            "Train step - Step 70, Loss 0.23865528404712677\n",
            "Train epoch - Accuracy: 0.44363636363636366 Loss: 0.24301170718790305 Corrects: 2196\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.23784291744232178\n",
            "Train step - Step 90, Loss 0.22167158126831055\n",
            "Train step - Step 100, Loss 0.22836807370185852\n",
            "Train step - Step 110, Loss 0.21694694459438324\n",
            "Train epoch - Accuracy: 0.48707070707070704 Loss: 0.22610946089330344 Corrects: 2411\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.21196965873241425\n",
            "Train step - Step 130, Loss 0.20575585961341858\n",
            "Train step - Step 140, Loss 0.22211872041225433\n",
            "Train step - Step 150, Loss 0.20619133114814758\n",
            "Train epoch - Accuracy: 0.5016161616161616 Loss: 0.21912972303954037 Corrects: 2483\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.2055768072605133\n",
            "Train step - Step 170, Loss 0.20553207397460938\n",
            "Train step - Step 180, Loss 0.22383907437324524\n",
            "Train step - Step 190, Loss 0.19269728660583496\n",
            "Train epoch - Accuracy: 0.5418181818181819 Loss: 0.20608459960932685 Corrects: 2682\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.19962547719478607\n",
            "Train step - Step 210, Loss 0.1956397444009781\n",
            "Train step - Step 220, Loss 0.23317204415798187\n",
            "Train step - Step 230, Loss 0.1870502084493637\n",
            "Train epoch - Accuracy: 0.5523232323232323 Loss: 0.19966931800047558 Corrects: 2734\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.19728612899780273\n",
            "Train step - Step 250, Loss 0.17967957258224487\n",
            "Train step - Step 260, Loss 0.1929226964712143\n",
            "Train step - Step 270, Loss 0.19188766181468964\n",
            "Train epoch - Accuracy: 0.5834343434343434 Loss: 0.19277908541939476 Corrects: 2888\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1998911201953888\n",
            "Train step - Step 290, Loss 0.18329913914203644\n",
            "Train step - Step 300, Loss 0.16586630046367645\n",
            "Train step - Step 310, Loss 0.17675089836120605\n",
            "Train epoch - Accuracy: 0.598989898989899 Loss: 0.1864176897809963 Corrects: 2965\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.17738604545593262\n",
            "Train step - Step 330, Loss 0.18055176734924316\n",
            "Train step - Step 340, Loss 0.17641305923461914\n",
            "Train step - Step 350, Loss 0.14969894289970398\n",
            "Train epoch - Accuracy: 0.6155555555555555 Loss: 0.17903075204955207 Corrects: 3047\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.1671830266714096\n",
            "Train step - Step 370, Loss 0.1687958985567093\n",
            "Train step - Step 380, Loss 0.159011110663414\n",
            "Train epoch - Accuracy: 0.6309090909090909 Loss: 0.1738420210462628 Corrects: 3123\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.15419088304042816\n",
            "Train step - Step 400, Loss 0.14775967597961426\n",
            "Train step - Step 410, Loss 0.16979841887950897\n",
            "Train step - Step 420, Loss 0.1525963395833969\n",
            "Train epoch - Accuracy: 0.6539393939393939 Loss: 0.1639658623993999 Corrects: 3237\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.19196446239948273\n",
            "Train step - Step 440, Loss 0.1466265171766281\n",
            "Train step - Step 450, Loss 0.17056730389595032\n",
            "Train step - Step 460, Loss 0.18163080513477325\n",
            "Train epoch - Accuracy: 0.6608080808080808 Loss: 0.16202940798167026 Corrects: 3271\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.176211878657341\n",
            "Train step - Step 480, Loss 0.1588747799396515\n",
            "Train step - Step 490, Loss 0.1342204362154007\n",
            "Train step - Step 500, Loss 0.14058950543403625\n",
            "Train epoch - Accuracy: 0.6767676767676768 Loss: 0.15383696049752862 Corrects: 3350\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.1555892676115036\n",
            "Train step - Step 520, Loss 0.13571906089782715\n",
            "Train step - Step 530, Loss 0.1515876054763794\n",
            "Train step - Step 540, Loss 0.1359780877828598\n",
            "Train epoch - Accuracy: 0.702020202020202 Loss: 0.14594818502363532 Corrects: 3475\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.15486030280590057\n",
            "Train step - Step 560, Loss 0.13412602245807648\n",
            "Train step - Step 570, Loss 0.15869688987731934\n",
            "Train step - Step 580, Loss 0.13754160702228546\n",
            "Train epoch - Accuracy: 0.7034343434343434 Loss: 0.14330182038172326 Corrects: 3482\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.13797789812088013\n",
            "Train step - Step 600, Loss 0.14471013844013214\n",
            "Train step - Step 610, Loss 0.1659015417098999\n",
            "Train step - Step 620, Loss 0.13035106658935547\n",
            "Train epoch - Accuracy: 0.712929292929293 Loss: 0.1402043603164981 Corrects: 3529\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.10688119381666183\n",
            "Train step - Step 640, Loss 0.10551120340824127\n",
            "Train step - Step 650, Loss 0.11231940239667892\n",
            "Train step - Step 660, Loss 0.12159333378076553\n",
            "Train epoch - Accuracy: 0.7434343434343434 Loss: 0.1296956703397963 Corrects: 3680\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.13172531127929688\n",
            "Train step - Step 680, Loss 0.142404243350029\n",
            "Train step - Step 690, Loss 0.12234494835138321\n",
            "Train step - Step 700, Loss 0.14414963126182556\n",
            "Train epoch - Accuracy: 0.756969696969697 Loss: 0.12469814313180519 Corrects: 3747\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.1333446353673935\n",
            "Train step - Step 720, Loss 0.15927483141422272\n",
            "Train step - Step 730, Loss 0.11001887172460556\n",
            "Train step - Step 740, Loss 0.12100914865732193\n",
            "Train epoch - Accuracy: 0.7450505050505051 Loss: 0.12714890332836093 Corrects: 3688\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.11506552994251251\n",
            "Train step - Step 760, Loss 0.10524898767471313\n",
            "Train step - Step 770, Loss 0.0984029471874237\n",
            "Train epoch - Accuracy: 0.7668686868686869 Loss: 0.11733168877435453 Corrects: 3796\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.12441835552453995\n",
            "Train step - Step 790, Loss 0.11353017389774323\n",
            "Train step - Step 800, Loss 0.13449843227863312\n",
            "Train step - Step 810, Loss 0.12670955061912537\n",
            "Train epoch - Accuracy: 0.7739393939393939 Loss: 0.11266988289175611 Corrects: 3831\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.10420570522546768\n",
            "Train step - Step 830, Loss 0.10050588846206665\n",
            "Train step - Step 840, Loss 0.09052333980798721\n",
            "Train step - Step 850, Loss 0.07840898633003235\n",
            "Train epoch - Accuracy: 0.7808080808080808 Loss: 0.11037116950509523 Corrects: 3865\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.13141851127147675\n",
            "Train step - Step 870, Loss 0.11617157608270645\n",
            "Train step - Step 880, Loss 0.11001942306756973\n",
            "Train step - Step 890, Loss 0.08741968870162964\n",
            "Train epoch - Accuracy: 0.7844444444444445 Loss: 0.10794938291564132 Corrects: 3883\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.11431687325239182\n",
            "Train step - Step 910, Loss 0.0919078066945076\n",
            "Train step - Step 920, Loss 0.1131000742316246\n",
            "Train step - Step 930, Loss 0.12487182766199112\n",
            "Train epoch - Accuracy: 0.7935353535353535 Loss: 0.10410453104310566 Corrects: 3928\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.0851389467716217\n",
            "Train step - Step 950, Loss 0.10327544063329697\n",
            "Train step - Step 960, Loss 0.09313042461872101\n",
            "Train step - Step 970, Loss 0.12224413454532623\n",
            "Train epoch - Accuracy: 0.8014141414141415 Loss: 0.10208582940125706 Corrects: 3967\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.10216508060693741\n",
            "Train step - Step 990, Loss 0.10625173896551132\n",
            "Train step - Step 1000, Loss 0.12439626455307007\n",
            "Train step - Step 1010, Loss 0.09492490440607071\n",
            "Train epoch - Accuracy: 0.805050505050505 Loss: 0.10121105768764863 Corrects: 3985\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.1075296625494957\n",
            "Train step - Step 1030, Loss 0.08558668196201324\n",
            "Train step - Step 1040, Loss 0.10448060184717178\n",
            "Train step - Step 1050, Loss 0.0987623855471611\n",
            "Train epoch - Accuracy: 0.8115151515151515 Loss: 0.09598487866647316 Corrects: 4017\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.11706822365522385\n",
            "Train step - Step 1070, Loss 0.08957835286855698\n",
            "Train step - Step 1080, Loss 0.1004997119307518\n",
            "Train step - Step 1090, Loss 0.10269360989332199\n",
            "Train epoch - Accuracy: 0.8151515151515152 Loss: 0.093790872918837 Corrects: 4035\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10780717432498932\n",
            "Train step - Step 1110, Loss 0.0829559937119484\n",
            "Train step - Step 1120, Loss 0.06946247071027756\n",
            "Train step - Step 1130, Loss 0.08679612725973129\n",
            "Train epoch - Accuracy: 0.8232323232323232 Loss: 0.09224864899811118 Corrects: 4075\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.0877915769815445\n",
            "Train step - Step 1150, Loss 0.07081689685583115\n",
            "Train step - Step 1160, Loss 0.09684779495000839\n",
            "Train epoch - Accuracy: 0.8305050505050505 Loss: 0.08737428996298048 Corrects: 4111\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.08674424141645432\n",
            "Train step - Step 1180, Loss 0.08422601222991943\n",
            "Train step - Step 1190, Loss 0.09191498905420303\n",
            "Train step - Step 1200, Loss 0.09191594272851944\n",
            "Train epoch - Accuracy: 0.834949494949495 Loss: 0.08547621402174535 Corrects: 4133\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.07304417341947556\n",
            "Train step - Step 1220, Loss 0.09637052565813065\n",
            "Train step - Step 1230, Loss 0.08423800766468048\n",
            "Train step - Step 1240, Loss 0.09754743427038193\n",
            "Train epoch - Accuracy: 0.8418181818181818 Loss: 0.0811596822106477 Corrects: 4167\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.08752617239952087\n",
            "Train step - Step 1260, Loss 0.08612791448831558\n",
            "Train step - Step 1270, Loss 0.09345199167728424\n",
            "Train step - Step 1280, Loss 0.06988094002008438\n",
            "Train epoch - Accuracy: 0.8432323232323232 Loss: 0.08195995536717501 Corrects: 4174\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.063926100730896\n",
            "Train step - Step 1300, Loss 0.09519979357719421\n",
            "Train step - Step 1310, Loss 0.0794561430811882\n",
            "Train step - Step 1320, Loss 0.09330239146947861\n",
            "Train epoch - Accuracy: 0.8434343434343434 Loss: 0.08210123597973525 Corrects: 4175\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.06706669181585312\n",
            "Train step - Step 1340, Loss 0.07936250418424606\n",
            "Train step - Step 1350, Loss 0.08014490455389023\n",
            "Train step - Step 1360, Loss 0.08957503736019135\n",
            "Train epoch - Accuracy: 0.8488888888888889 Loss: 0.07934777613541093 Corrects: 4202\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.08020991086959839\n",
            "Train step - Step 1380, Loss 0.09724964946508408\n",
            "Train step - Step 1390, Loss 0.06381719559431076\n",
            "Train step - Step 1400, Loss 0.06528539955615997\n",
            "Train epoch - Accuracy: 0.8549494949494949 Loss: 0.07585879642855037 Corrects: 4232\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.07746966183185577\n",
            "Train step - Step 1420, Loss 0.08617367595434189\n",
            "Train step - Step 1430, Loss 0.06970410794019699\n",
            "Train step - Step 1440, Loss 0.08160582929849625\n",
            "Train epoch - Accuracy: 0.8527272727272728 Loss: 0.07876499318715298 Corrects: 4221\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.06916437298059464\n",
            "Train step - Step 1460, Loss 0.06720501184463501\n",
            "Train step - Step 1470, Loss 0.06595558673143387\n",
            "Train step - Step 1480, Loss 0.0707656592130661\n",
            "Train epoch - Accuracy: 0.8654545454545455 Loss: 0.07132082007449082 Corrects: 4284\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.05659304931759834\n",
            "Train step - Step 1500, Loss 0.06974741071462631\n",
            "Train step - Step 1510, Loss 0.0784800574183464\n",
            "Train step - Step 1520, Loss 0.06901217252016068\n",
            "Train epoch - Accuracy: 0.8672727272727273 Loss: 0.07315080337753199 Corrects: 4293\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.07148046046495438\n",
            "Train step - Step 1540, Loss 0.08015324175357819\n",
            "Train step - Step 1550, Loss 0.07519340515136719\n",
            "Train epoch - Accuracy: 0.8682828282828283 Loss: 0.06976768631826748 Corrects: 4298\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.061872996389865875\n",
            "Train step - Step 1570, Loss 0.08952672779560089\n",
            "Train step - Step 1580, Loss 0.0770217701792717\n",
            "Train step - Step 1590, Loss 0.06781128793954849\n",
            "Train epoch - Accuracy: 0.8719191919191919 Loss: 0.06902587638659911 Corrects: 4316\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.0734851062297821\n",
            "Train step - Step 1610, Loss 0.055662430822849274\n",
            "Train step - Step 1620, Loss 0.08112241327762604\n",
            "Train step - Step 1630, Loss 0.07834818959236145\n",
            "Train epoch - Accuracy: 0.8739393939393939 Loss: 0.06590250587222551 Corrects: 4326\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.05307411774992943\n",
            "Train step - Step 1650, Loss 0.04475534334778786\n",
            "Train step - Step 1660, Loss 0.06450628489255905\n",
            "Train step - Step 1670, Loss 0.06469791382551193\n",
            "Train epoch - Accuracy: 0.8838383838383839 Loss: 0.06405000432874217 Corrects: 4375\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.055307772010564804\n",
            "Train step - Step 1690, Loss 0.06018352508544922\n",
            "Train step - Step 1700, Loss 0.05738571286201477\n",
            "Train step - Step 1710, Loss 0.07564640045166016\n",
            "Train epoch - Accuracy: 0.883030303030303 Loss: 0.06291912011124871 Corrects: 4371\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.0579797700047493\n",
            "Train step - Step 1730, Loss 0.05599530413746834\n",
            "Train step - Step 1740, Loss 0.07791357487440109\n",
            "Train step - Step 1750, Loss 0.07570113986730576\n",
            "Train epoch - Accuracy: 0.8860606060606061 Loss: 0.06122048602411241 Corrects: 4386\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.046970587223768234\n",
            "Train step - Step 1770, Loss 0.05448814854025841\n",
            "Train step - Step 1780, Loss 0.0570836178958416\n",
            "Train step - Step 1790, Loss 0.052846431732177734\n",
            "Train epoch - Accuracy: 0.8921212121212121 Loss: 0.05948092536342264 Corrects: 4416\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.0464363619685173\n",
            "Train step - Step 1810, Loss 0.056273628026247025\n",
            "Train step - Step 1820, Loss 0.0658160001039505\n",
            "Train step - Step 1830, Loss 0.0690997764468193\n",
            "Train epoch - Accuracy: 0.8955555555555555 Loss: 0.05688923456151076 Corrects: 4433\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.059149112552404404\n",
            "Train step - Step 1850, Loss 0.05253974348306656\n",
            "Train step - Step 1860, Loss 0.07070215791463852\n",
            "Train step - Step 1870, Loss 0.05269395932555199\n",
            "Train epoch - Accuracy: 0.8927272727272727 Loss: 0.05812157820571553 Corrects: 4419\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.04611649736762047\n",
            "Train step - Step 1890, Loss 0.06521129608154297\n",
            "Train step - Step 1900, Loss 0.0597667470574379\n",
            "Train step - Step 1910, Loss 0.07057886570692062\n",
            "Train epoch - Accuracy: 0.9002020202020202 Loss: 0.05608734115506663 Corrects: 4456\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.042010191828012466\n",
            "Train step - Step 1930, Loss 0.043184537440538406\n",
            "Train step - Step 1940, Loss 0.0397011898458004\n",
            "Train epoch - Accuracy: 0.9242424242424242 Loss: 0.04325935698518849 Corrects: 4575\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.039822064340114594\n",
            "Train step - Step 1960, Loss 0.036546554416418076\n",
            "Train step - Step 1970, Loss 0.04635452851653099\n",
            "Train step - Step 1980, Loss 0.033635109663009644\n",
            "Train epoch - Accuracy: 0.9432323232323232 Loss: 0.037418713830035144 Corrects: 4669\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.03440694510936737\n",
            "Train step - Step 2000, Loss 0.04546314477920532\n",
            "Train step - Step 2010, Loss 0.03715860843658447\n",
            "Train step - Step 2020, Loss 0.050867773592472076\n",
            "Train epoch - Accuracy: 0.9412121212121212 Loss: 0.03600090804876703 Corrects: 4659\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.02928098477423191\n",
            "Train step - Step 2040, Loss 0.03901729732751846\n",
            "Train step - Step 2050, Loss 0.037106212228536606\n",
            "Train step - Step 2060, Loss 0.03669671341776848\n",
            "Train epoch - Accuracy: 0.9452525252525252 Loss: 0.03400145272564406 Corrects: 4679\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.0314776748418808\n",
            "Train step - Step 2080, Loss 0.031674575060606\n",
            "Train step - Step 2090, Loss 0.018039535731077194\n",
            "Train step - Step 2100, Loss 0.0348820686340332\n",
            "Train epoch - Accuracy: 0.9468686868686869 Loss: 0.033380321872354755 Corrects: 4687\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.02144073322415352\n",
            "Train step - Step 2120, Loss 0.029142146930098534\n",
            "Train step - Step 2130, Loss 0.03788841888308525\n",
            "Train step - Step 2140, Loss 0.032097745686769485\n",
            "Train epoch - Accuracy: 0.9496969696969697 Loss: 0.03260654991022264 Corrects: 4701\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.027982238680124283\n",
            "Train step - Step 2160, Loss 0.020777106285095215\n",
            "Train step - Step 2170, Loss 0.030742371454834938\n",
            "Train step - Step 2180, Loss 0.0357779823243618\n",
            "Train epoch - Accuracy: 0.9543434343434344 Loss: 0.03025592699345916 Corrects: 4724\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.031895961612463\n",
            "Train step - Step 2200, Loss 0.025660132989287376\n",
            "Train step - Step 2210, Loss 0.038197994232177734\n",
            "Train step - Step 2220, Loss 0.025359375402331352\n",
            "Train epoch - Accuracy: 0.9525252525252526 Loss: 0.03145596713730783 Corrects: 4715\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.03523494675755501\n",
            "Train step - Step 2240, Loss 0.02871292270720005\n",
            "Train step - Step 2250, Loss 0.028518855571746826\n",
            "Train step - Step 2260, Loss 0.028394445776939392\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.031200148419599342 Corrects: 4703\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.026044586673378944\n",
            "Train step - Step 2280, Loss 0.03078872337937355\n",
            "Train step - Step 2290, Loss 0.03193800523877144\n",
            "Train step - Step 2300, Loss 0.05611391365528107\n",
            "Train epoch - Accuracy: 0.9555555555555556 Loss: 0.030103796654277377 Corrects: 4730\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.02573627233505249\n",
            "Train step - Step 2320, Loss 0.026890838518738747\n",
            "Train step - Step 2330, Loss 0.03133704885840416\n",
            "Train epoch - Accuracy: 0.953939393939394 Loss: 0.0305143504032884 Corrects: 4722\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.029058242216706276\n",
            "Train step - Step 2350, Loss 0.02426040545105934\n",
            "Train step - Step 2360, Loss 0.03160322457551956\n",
            "Train step - Step 2370, Loss 0.04511835798621178\n",
            "Train epoch - Accuracy: 0.9563636363636364 Loss: 0.029337084354324774 Corrects: 4734\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.031107379123568535\n",
            "Train step - Step 2390, Loss 0.029924381524324417\n",
            "Train step - Step 2400, Loss 0.024590641260147095\n",
            "Train step - Step 2410, Loss 0.020830800756812096\n",
            "Train epoch - Accuracy: 0.961010101010101 Loss: 0.02768125159346094 Corrects: 4757\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.02306819148361683\n",
            "Train step - Step 2430, Loss 0.05020679160952568\n",
            "Train step - Step 2440, Loss 0.032019324600696564\n",
            "Train step - Step 2450, Loss 0.035698115825653076\n",
            "Train epoch - Accuracy: 0.9587878787878787 Loss: 0.028257839812172784 Corrects: 4746\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.03360297158360481\n",
            "Train step - Step 2470, Loss 0.02808537892997265\n",
            "Train step - Step 2480, Loss 0.01882505789399147\n",
            "Train step - Step 2490, Loss 0.01958288811147213\n",
            "Train epoch - Accuracy: 0.96 Loss: 0.026811572033347504 Corrects: 4752\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.024240998551249504\n",
            "Train step - Step 2510, Loss 0.015550089068710804\n",
            "Train step - Step 2520, Loss 0.01995779201388359\n",
            "Train step - Step 2530, Loss 0.02513287402689457\n",
            "Train epoch - Accuracy: 0.962020202020202 Loss: 0.025361865840174934 Corrects: 4762\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.021166345104575157\n",
            "Train step - Step 2550, Loss 0.018641367554664612\n",
            "Train step - Step 2560, Loss 0.021410910412669182\n",
            "Train step - Step 2570, Loss 0.03057834319770336\n",
            "Train epoch - Accuracy: 0.9646464646464646 Loss: 0.023534335130167127 Corrects: 4775\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.029833734035491943\n",
            "Train step - Step 2590, Loss 0.018363453447818756\n",
            "Train step - Step 2600, Loss 0.026480138301849365\n",
            "Train step - Step 2610, Loss 0.028178563341498375\n",
            "Train epoch - Accuracy: 0.9660606060606061 Loss: 0.024363452725187695 Corrects: 4782\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.0342327281832695\n",
            "Train step - Step 2630, Loss 0.02055048942565918\n",
            "Train step - Step 2640, Loss 0.020426509901881218\n",
            "Train step - Step 2650, Loss 0.021581510081887245\n",
            "Train epoch - Accuracy: 0.9648484848484848 Loss: 0.025252315486320343 Corrects: 4776\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.03008689358830452\n",
            "Train step - Step 2670, Loss 0.0230241846293211\n",
            "Train step - Step 2680, Loss 0.019735518842935562\n",
            "Train step - Step 2690, Loss 0.01372586376965046\n",
            "Train epoch - Accuracy: 0.9644444444444444 Loss: 0.024579011685769966 Corrects: 4774\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.02005911059677601\n",
            "Train step - Step 2710, Loss 0.023675382137298584\n",
            "Train step - Step 2720, Loss 0.02118927799165249\n",
            "Train epoch - Accuracy: 0.9634343434343434 Loss: 0.024567253339772274 Corrects: 4769\n",
            "Training finished in 203.4930591583252 seconds\n",
            "EVALUATION:  0.78 0.12797264754772186\n",
            "TEST GROUP:  0.836\n",
            "TEST ALL:  0.836\n",
            "GROUP:  2\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.5198947787284851\n",
            "Train step - Step 10, Loss 0.18119554221630096\n",
            "Train step - Step 20, Loss 0.1347174048423767\n",
            "Train step - Step 30, Loss 0.10982789844274521\n",
            "Train epoch - Accuracy: 0.36808080808080806 Loss: 0.17154767145111102 Corrects: 1822\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.09604242444038391\n",
            "Train step - Step 50, Loss 0.09236812591552734\n",
            "Train step - Step 60, Loss 0.08287163078784943\n",
            "Train step - Step 70, Loss 0.08627394586801529\n",
            "Train epoch - Accuracy: 0.6482828282828282 Loss: 0.08684955637563359 Corrects: 3209\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.08100271224975586\n",
            "Train step - Step 90, Loss 0.07603686302900314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-754cea9f07ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequentialLearningFineTuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_subsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-31e6b3e7dcf7>\u001b[0m in \u001b[0;36msequentialLearningFineTuning\u001b[0;34m(train_subsets, val_subsets, test_subsets)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetSchedulerOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reset learning rate and step_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes_seen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# Validate on current group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-20cff53f0192>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Bring images and labels to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aakVopSdyeGr"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ0VbtGQygRH"
      },
      "source": [
        "method = \"finetuning\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ-J8aC_xx-c"
      },
      "source": [
        "## LWF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZBNivOix1FB"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdfIllqBx59v"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkzFiDVDyAm_"
      },
      "source": [
        "### sequential learning LWF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf7REFkIyEiQ"
      },
      "source": [
        "### LWF\n",
        "def sequentialLearningLWF(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZTwTS_4TcES"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkrMQy2TuUAb"
      },
      "source": [
        "# train\n",
        "# net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningLWF(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2qUxVyMTfB4"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_4xLfwcpDz"
      },
      "source": [
        "method = \"Learning Without Forgetting\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGHBWaLNXGeI"
      },
      "source": [
        "\"\"\"num_classes_seen = 100\n",
        "dif_accuracies=printAccuracyDifference(net,old_accuracies, num_classes_seen)\n",
        "dif_accuracies\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjGRIqDtNQP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nX10znUi6qd"
      },
      "source": [
        "## iCaRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKBdrJ8Csy2a"
      },
      "source": [
        "### classify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAPTOU9bszTY"
      },
      "source": [
        "def classify(batch_img, net, exemplar_sets):\n",
        "  \"\"\" classify images by nearest mean-of-exemplars \"\"\" \n",
        "  \"\"\" \n",
        "  class1: list_of_indices for images that represent class1\n",
        "  class2: list_of_indices for images that represent class2\n",
        "  ...\n",
        "  class100: list_of_indices for images that represent class 100\n",
        "  with Subset I retrieve the images from the train_dataset\n",
        "  iterate over it to calculate mean for each class\n",
        "  \"\"\"\n",
        "  net.eval()\n",
        "  classes_mean = []\n",
        "  for k, exemplars_indices in exemplar_sets.items():\n",
        "    features = []\n",
        "    class_images_set = Subset(train_dataset, exemplars_indices)\n",
        "    class_images = DataLoader(class_images_set, batch_size=BATCH_SIZE, num_workers=2)\n",
        "    for _, images, labels in class_images:\n",
        "      # for each class (paper from y=1...t) calculate features and then mean\n",
        "      feature = net.forward(images)\n",
        "      features.append(feature)\n",
        "    features_s = torch.cat(features)\n",
        "    class_mean = features_s.mean(0)\n",
        "    classes_mean.append(class_mean)\n",
        "    means_exemplars = torch.cat(classes_mean, dim=0)\n",
        "    means_exemplars = torch.stack([means_exemplars] * BATCH_SIZE)\n",
        "    means_exemplars = means_exemplars.transpose(1,2)\n",
        "  feature_images_to_classify = net.forward(batch_img)\n",
        "  # sono da normalizzare?\n",
        "  feature_images_to_classify = feature_images_to_classify.unsqueeze(2)\n",
        "  feature_images_to_classify = feature_images_to_classify.expand_as(means_exemplars) # expand_as to get the same dimension\n",
        "  preds = torch.argmin((feature_images_to_classify - means_exemplars).pow(2).sum(1), dim=1)\n",
        "  return preds"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5HRagcujHgM"
      },
      "source": [
        "### construct exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blwBqG6BjNsM"
      },
      "source": [
        "import sys\n",
        "\n",
        "def constructExemplarSet(net, Xclass, m):\n",
        "#Xclass contiene immagini e label della classe X\n",
        "  exemplars_set = []\n",
        "  feature_exemplars = []\n",
        "  indexes = []\n",
        "  features = [] \n",
        "  class_images = []\n",
        "  net.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    XtrainLoader = torch.utils.data.DataLoader(Xclass, shuffle = True, batch_size=1, num_workers=2)\n",
        "    for _, image, labels in XtrainLoader:\n",
        "      image = image.to(DEVICE)\n",
        "      class_images.append(image)\n",
        "      # per ogni immagine della classe x, prendiamo le rispettive feature e le uniamo nel vettore features, che contiene tutte quelle delle immagini della classe x\n",
        "      feature = net.forward(image)\n",
        "      #feature = net.predict(featuretmp)\n",
        "      feature = feature/np.linalg.norm(feature.cpu())\n",
        "      features.append(feature)\n",
        "\n",
        "    features = torch.cat(features, dim=0) #cat solve the problem of inequal size of tensors \n",
        "    current_class_mean = features.mean(0) # mu = media delle features delle immagini della classe\n",
        "\n",
        "    for k in range(1, m+1):\n",
        "      min = 100000\n",
        "      sum = 0\n",
        "      for j in range(k-1):\n",
        "        sum += feature_exemplars[j]\n",
        "      for x in range(len(Xclass)): \n",
        "        if (x not in indexes):\n",
        "          phiX = features[x]\n",
        "          val = current_class_mean - ((phiX + sum)/k)\n",
        "          val = np.linalg.norm(val.cpu().numpy()) ## NORMA \n",
        "          if (val < min):\n",
        "            min = val\n",
        "            feature_min = phiX\n",
        "            index_min = x\n",
        "      feature_exemplars.append(feature_min)\n",
        "      exemplars_set.append(Xclass[index_min][0])\n",
        "\n",
        "    print(\"lunghezza exemplar set: \", len(exemplars_set))\n",
        "    return exemplars_set"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy_HVL-tjZcb"
      },
      "source": [
        "### reduce exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7kuQ5DnjcCP"
      },
      "source": [
        "# DA CHIAMARE PER OGNI CLASSE k: riduce il numero di exemplars all'interno dell'exemplar_set della classe k \n",
        "# EXEMPLAR SET è una lista di indici che rappresentano la posizione dell'immagine selezionata per l'exemplar della classe corrente nel dataset di partenza\n",
        "def reduceExemplarSet(m, exemplars_set):\n",
        "  exemplars_new = []\n",
        "  for i in range(m):\n",
        "    if (exemplars_set != []):\n",
        "      exemplars_new.append(exemplars_set[i])\n",
        "\n",
        "  return exemplars_new"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMMlEpLsjfaj"
      },
      "source": [
        "### update representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRdA9RtDjhak"
      },
      "source": [
        "def updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot, old_net):\n",
        "  #exemplars_set_tot contiene tutti gli exemplars set ottenuti fino ad ora\n",
        "  #train_iter contiene tutti i dati (immagini + labels) delle classi nuove (s, .., t)\n",
        "  exemplars_subset = []\n",
        "  exemplars_indices = []\n",
        "  total_exemplars = []\n",
        "  labels_tot = []\n",
        "\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "    if (exemplar_set_class_k != []):\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "\n",
        "  if group_id > 1:\n",
        "    train_subset_total = torch.utils.data.ConcatDataset([train_subset, total_exemplars])\n",
        "  else:\n",
        "    train_subset_total = train_subset\n",
        "    \n",
        "  print(\"Len TOTAL train susbset: \", len(train_subset_total))\n",
        "  train_loader = torch.utils.data.DataLoader(train_subset_total, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  # train_loader è la concatenazione delle nuove classi con gli exemplar_sets calcolati fino a questo punto \n",
        "  print(\"training\")\n",
        "  #train(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)\n",
        "  trainWithOtherLosses(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)\n",
        "  #trainCEandL1(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac65-6QGjm9O"
      },
      "source": [
        "### incremental train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPU_32ArjpPH"
      },
      "source": [
        "import copy\n",
        "def incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_until_now):\n",
        "  print(\"Starting the update representation\")\n",
        "  exemplar_indices = None\n",
        "  num_classes = 10\n",
        "  new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "  print(\"NEW CLASSES: \", new_classes_examined)\n",
        "\n",
        "  updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net)\n",
        "                \n",
        "  iteration = group_id - 1\n",
        "  t = (num_classes * iteration) + num_classes # num_classes ricevute fino a questo momento \n",
        "  m = int(K/t) #casto ad intero ? per difetto o eccesso?\n",
        "  #s = num_classes * iteration\n",
        "\n",
        "  # REDUCING EXEMPLAR SET FOR EXISTING CLASSES\n",
        "  print(\"reducing exemplars for each class\")\n",
        "  print(total_classes_until_now)\n",
        "  for y in total_classes_until_now: #ci serve un set con tutte le classi fino ad ora viste\n",
        "    exemplar_y_new = reduceExemplarSet(m, exemplars_set_tot[y]) # valore associato alla chiave y che rappresenta il label della classe \n",
        "    print(\"REDUCED EXEMPLAR: \", len(exemplar_y_new))\n",
        "    exemplars_set_tot[y] = exemplar_y_new\n",
        "\n",
        "  \n",
        "  # CONSTRUCTION EXEMPLAR SET FOR NEW CLASSES\n",
        "  for y in new_classes_examined: # nuovi classi in arrivo di cui vogliamo costruire il set rappresentativo\n",
        "    images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data']\n",
        "    imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "    class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "    print(\"class train: \", class_train_subset)\n",
        "    print(\"Constructing exemplars of class\", y)\n",
        "    exemplars_set = constructExemplarSet(net, class_train_subset, m) # exemplar set è un set di indici\n",
        "    #devo recuperare dal dataset iniziale l'indice delle immagini dell'exemplar set creato \n",
        "    #for image in exemplars_set:\n",
        "     # exemplars_set = train_dataset.df.index[train_dataset.df['data'] == image].tolist()\n",
        "    exemplars_set_tot[y] = exemplars_set\n",
        "    print(\"exemplar set: \", exemplars_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDxmsQlWjp5D"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dQznGoEju9J"
      },
      "source": [
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes_till_now, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    print(\"num classes till now: \", num_classes_till_now)\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes_till_now, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            #if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcZUqVCjeMG4"
      },
      "source": [
        "### Train con CE + L1Loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl4zFMYGtAEJ"
      },
      "source": [
        "import copy\n",
        "def trainCEandL1(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    distillation_loss = 0\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "            # Classification LOSS\n",
        "            lr = 0.01\n",
        "            classification_loss = nn.CrossEntropyLoss()(outputs, labels) # BCE\n",
        "\n",
        "            # Distillation LOSS \n",
        "            if (group_id > 1):\n",
        "               old_features = old_net.forward(images)\n",
        "               old_outputs = old_net.predict(old_features)\n",
        "               #labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "               new_labels = torch.sigmoid(old_outputs)\n",
        "               new_outputs = outputs[:, 0:num_classes_till_previous_step]\n",
        "               lr = 1e-3\n",
        "               distillation_loss = l1Loss(new_outputs, new_labels) # L2\n",
        "               print(distillation_loss)\n",
        "\n",
        "            loss = classification_loss + distillation_loss\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrbPsdT2eRho"
      },
      "source": [
        "### Train con .. Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzTrBE-Dea1S"
      },
      "source": [
        "import copy\n",
        "def trainWithOtherLosses(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    distillation_loss = 0\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "            # Classification LOSS\n",
        "            classification_loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "            # Distillation LOSS \n",
        "            if (group_id > 1):\n",
        "               old_features = old_net.forward(images)\n",
        "               old_outputs = old_net.predict(old_features)\n",
        "               T = 2\n",
        "               beta = 0.25\n",
        "               distillation_loss = nn.KLDivLoss()(F.log_softmax(outputs[:, 0:num_classes_till_previous_step]/T, dim = 1), F.softmax(old_outputs.detach()/T, dim = 1)) * T * T * beta * num_classes_till_previous_step\n",
        "\n",
        "            loss = classification_loss + distillation_loss\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHpi410ljzWD"
      },
      "source": [
        "### test and validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQYB0-cLj0sP"
      },
      "source": [
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        #_, preds = classify(images, )\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTv6zBetj3Cb"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_cnELYdkap4"
      },
      "source": [
        "def sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 10\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      print(\"TEST SET LENGHT: \", len(test_set))\n",
        "      print(\"TEST CURRENT GROUP SET LENGHT: \", len(test_subset))\n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      test_classes = list(test_dataset.df.loc[test_set.indices, 'labels'].value_counts().index)\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TEST_SET CLASSES: \", test_classes)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTV1Gxq6yF5-"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-o1n4skl-77"
      },
      "source": [
        "##CLOSED AND OPEN WORLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lUUjCewmNLo"
      },
      "source": [
        "#Dataset divided into 2 halves, 50 for closed 50 for open (choose five different random division)\n",
        "#1) closed world\n",
        "#  1.1)without rejection -> standard incremental scenario (train and test using selected 10 classes) but with 50 classes\n",
        "#      iter = 0 -> 10 or 20 (he does so in BDOC) classes ? ask Dario\n",
        "#      next iters -> add 10 until 50\n",
        "#      result expected -> equal to incremental \n",
        "#\n",
        "#  1.2)with rejection -> same procedure of above but we implement a rejection technique that \n",
        "#      classify as unknown an object that doesn't belong to the classes seen in the training (for the alg follow BDOC)\n",
        "#      result expected -> idealistic the model should not reject any of the object because we've tested the model with classes seen in the training\n",
        "#\n",
        "#2) open world\n",
        "#    at each step -> test the model only on unknown samples (the second half of dataset)\n",
        "#    \n",
        "#    result expected -> idealist the model should reject all of the test objects"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFHiLXyLmaWu"
      },
      "source": [
        "###download and dividing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OJxXhGZmdDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6caf92c-5a70-4cfc-a5cb-4ba14ce0751b"
      },
      "source": [
        "# Import dataset and apply transformations \n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# Check datasets length \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZFLVy0Tmt3G"
      },
      "source": [
        "#closed and open world\n",
        "splits_of_10 = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "#first 5 splits to closed world\n",
        "closed = {k:splits_of_10[k] for k in range(5)}\n",
        "\n",
        "#last 5 to open (removing the train val splits)\n",
        "open = []\n",
        "for k in range(5,10):\n",
        "  for j in[\"train\", \"val\"]:\n",
        "    open += splits_of_10[k][j]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQKnRMfjEaA-"
      },
      "source": [
        "### Modified iCaRL for closed/open"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fIvHXxhzwNI"
      },
      "source": [
        "def updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot, old_net, rejection=False, closed=True):\n",
        "  #exemplars_set_tot contiene tutti gli exemplars set ottenuti fino ad ora\n",
        "  #train_iter contiene tutti i dati (immagini + labels) delle classi nuove (s, .., t)\n",
        "  exemplars_subset = []\n",
        "  exemplars_indices = []\n",
        "  total_exemplars = []\n",
        "  labels_tot = []\n",
        "\n",
        "  for k, exemplar_set_class_k in exemplars_set_tot.items():\n",
        "    # exemplar_set_class_k is the list of indices of images that belongs to the exemplar set selected for class k \n",
        "    if (exemplar_set_class_k != []):\n",
        "      exemplars_subset = Subset(train_dataset, exemplar_set_class_k)\n",
        "      total_exemplars = torch.utils.data.ConcatDataset([total_exemplars, exemplars_subset])\n",
        "\n",
        "  if group_id > 1:\n",
        "    train_subset_total = torch.utils.data.ConcatDataset([train_subset, total_exemplars])\n",
        "  else:\n",
        "    train_subset_total = train_subset\n",
        "    \n",
        "  print(\"Len TOTAL train susbset: \", len(train_subset_total))\n",
        "  train_loader = torch.utils.data.DataLoader(train_subset_total, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  # train_loader è la concatenazione delle nuove classi con gli exemplar_sets calcolati fino a questo punto \n",
        "  print(\"training\")\n",
        "  train(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id, old_net, rejection = rejection, closed = closed)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAJtrUjIzqz-"
      },
      "source": [
        "import copy\n",
        "def incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_until_now, rejection=False, closed=True):\n",
        "  print(\"Starting the update representation\")\n",
        "  exemplar_indices = None\n",
        "  num_classes = 10\n",
        "  new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "  print(\"NEW CLASSES: \", new_classes_examined)\n",
        "\n",
        "  updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, rejection=rejection, closed=closed)\n",
        "                \n",
        "  iteration = group_id - 1\n",
        "  t = (num_classes * iteration) + num_classes # num_classes ricevute fino a questo momento \n",
        "  m = int(K/t) #casto ad intero ? per difetto o eccesso?\n",
        "  #s = num_classes * iteration\n",
        "\n",
        "  # REDUCING EXEMPLAR SET FOR EXISTING CLASSES\n",
        "  print(\"reducing exemplars for each class\")\n",
        "  print(total_classes_until_now)\n",
        "  for y in total_classes_until_now: #ci serve un set con tutte le classi fino ad ora viste\n",
        "    exemplar_y_new = reduceExemplarSet(m, exemplars_set_tot[y]) # valore associato alla chiave y che rappresenta il label della classe \n",
        "    print(\"REDUCED EXEMPLAR: \", len(exemplar_y_new))\n",
        "    exemplars_set_tot[y] = exemplar_y_new\n",
        "\n",
        "  \n",
        "  # CONSTRUCTION EXEMPLAR SET FOR NEW CLASSES\n",
        "  for y in new_classes_examined: # nuovi classi in arrivo di cui vogliamo costruire il set rappresentativo\n",
        "    images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data']\n",
        "    imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "    class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "    print(\"class train: \", class_train_subset)\n",
        "    print(\"Constructing exemplars of class\", y)\n",
        "    exemplars_set = constructExemplarSet(net, class_train_subset, m) # exemplar set è un set di indici\n",
        "    #devo recuperare dal dataset iniziale l'indice delle immagini dell'exemplar set creato \n",
        "    #for image in exemplars_set:\n",
        "     # exemplars_set = train_dataset.df.index[train_dataset.df['data'] == image].tolist()\n",
        "    exemplars_set_tot[y] = exemplars_set\n",
        "    print(\"exemplar set: \", exemplars_set)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkpzD3lazndM"
      },
      "source": [
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes_till_now, group_id, old_net, num_epochs=NUM_EPOCHS, rejection=False, closed=True, threshold=THRESHOLD):    \n",
        "\n",
        "      \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "    print(\"num classes till now: \", num_classes_till_now)\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    print(num_epochs)\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        if rejection == True:\n",
        "          n_sample_known = 0\n",
        "          n_sample_unknown = 0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes_till_now, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            #if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            if rejection == True:\n",
        "              prediction_batch = outputs.data.cpu().numpy()\n",
        "              for i in range(len(prediction_batch)):\n",
        "                current_softmax = softmax(prediction_batch[i])\n",
        "                if max(current_softmax)>THRESHOLD:\n",
        "                  n_sample_known += 1\n",
        "                else:\n",
        "                  n_sample_unknown += 1\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        if rejection == True:\n",
        "          if closed == True:\n",
        "            epoch_acc = n_sample_known / float(len(train_dataloader.dataset))\n",
        "          else:\n",
        "            epoch_acc = n_sample_unknown / float(len(train_dataloader.dataset))\n",
        "        else:\n",
        "          epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0xrVtFyzj75"
      },
      "source": [
        "def validate(net, val_dataloader, criterion, num_classes, rejection = False, closed = False):\n",
        "    #counter for rejection part, known and unknown\n",
        "    if rejection==True:\n",
        "      n_sample_known = 0\n",
        "      n_sample_unknown = 0\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            \n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        if rejection == True:\n",
        "          prediction_batch = outputs.data.cpu().numpy()\n",
        "          for i in range(len(prediction_batch)):\n",
        "            current_softmax = softmax(prediction_batch[i])\n",
        "            if max(current_softmax)>THRESHOLD:\n",
        "              n_sample_known += 1\n",
        "            else:\n",
        "              n_sample_unknown += 1\n",
        "        else:\n",
        "          #_, preds = classify(images, )\n",
        "          running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "          all_preds_cm.extend(preds.tolist())\n",
        "          all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    if rejection == True:\n",
        "      if closed == True:\n",
        "        acc = n_sample_known / float(len(val_dataloader.dataset))\n",
        "      else:\n",
        "        acc = n_sampke_unknown / float(len(val_dataloader.dataset))\n",
        "    else:\n",
        "      acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes, rejection = False, closed = True):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes, rejection, closed)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcN9P3Gr1ICR"
      },
      "source": [
        "def sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets, rejection = False, closed = True):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 5\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    labels_train = []\n",
        "    total_classes_seen = []\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "\n",
        "      print(\"TRAIN: \", len(train_subset))\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      print(\"TEST SET LENGHT: \", len(test_set))\n",
        "      print(\"TEST CURRENT GROUP SET LENGHT: \", len(test_subset))\n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      test_classes = list(test_dataset.df.loc[test_set.indices, 'labels'].value_counts().index)\n",
        "      train_classes = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "      validation_classes = list(train_dataset.df.loc[val_subset.indices, 'labels'].value_counts().index)\n",
        "      for i in train_classes:\n",
        "        total_classes_seen.append(i)\n",
        "      print(\"TEST_SET CLASSES: \", test_classes)\n",
        "      print(\"TRAIN_SET CLASSES: \", train_classes)\n",
        "      print(\"VALIDATION CLASSES: \", validation_classes)\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot, old_net, total_classes_seen, rejection=rejection, closed=closed) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen, rejection=rejection, closed=closed)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen, rejection=rejection, closed=closed)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trLViwIa2_bY"
      },
      "source": [
        "### Closed World"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyhNorNImn9E",
        "outputId": "3740bed3-867b-45d8-de09-59ccf756eac1"
      },
      "source": [
        "# Reverse indexing for closed world\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, closed)\n",
        "print(outputs_labels_mapping.getGroups())\n",
        "\n",
        "# TEST split\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Int64Index([0, 1, 2, 3, 4], dtype='int64')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W6-U42kmn9F"
      },
      "source": [
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,5):\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMyXZhVemn8q"
      },
      "source": [
        "### Closed world without rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl8IKzFsocw5"
      },
      "source": [
        "#without rejection\n",
        "rejection = False\n",
        "closed = True\n",
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI36G5hjodD8"
      },
      "source": [
        "method = \"Closed world without Rejection\"\n",
        "print(\"metrics ClosedWord for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,5):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBFSAonO0mgQ"
      },
      "source": [
        "### Closed world with rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32Dv40KK1ICR",
        "outputId": "8215e9f9-cd5b-4fb5-e250-923fc50d5452"
      },
      "source": [
        "# train closed world with rejection\n",
        "rejection = True\n",
        "closed = True\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets, rejection=rejection, closed=closed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  1000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [67, 65, 59, 56, 49, 39, 22, 20, 18, 4]\n",
            "TRAIN_SET CLASSES:  [67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "VALIDATION CLASSES:  [59, 56, 49, 39, 22, 20, 18, 4, 67, 65]\n",
            "GROUP:  1\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "Len TOTAL train susbset:  4950\n",
            "training\n",
            "num classes till now:  10\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.7129717469215393\n",
            "Train step - Step 10, Loss 0.3073538839817047\n",
            "Train step - Step 20, Loss 0.2666463553905487\n",
            "Train step - Step 30, Loss 0.263442724943161\n",
            "Train epoch - Accuracy: 0.15878787878787878 Loss: 0.3405303882348417 Corrects: 1519\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.24745920300483704\n",
            "Train step - Step 50, Loss 0.23888666927814484\n",
            "Train step - Step 60, Loss 0.25233614444732666\n",
            "Train step - Step 70, Loss 0.24673597514629364\n",
            "Train epoch - Accuracy: 0.34505050505050505 Loss: 0.24282903482215573 Corrects: 2180\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.21642787754535675\n",
            "Train step - Step 90, Loss 0.21524553000926971\n",
            "Train step - Step 100, Loss 0.22190164029598236\n",
            "Train step - Step 110, Loss 0.2254035472869873\n",
            "Train epoch - Accuracy: 0.4216161616161616 Loss: 0.22903819093198488 Corrects: 2364\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.22766073048114777\n",
            "Train step - Step 130, Loss 0.225029855966568\n",
            "Train step - Step 140, Loss 0.1947578489780426\n",
            "Train step - Step 150, Loss 0.20058374106884003\n",
            "Train epoch - Accuracy: 0.49333333333333335 Loss: 0.21571704136602807 Corrects: 2554\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.20778270065784454\n",
            "Train step - Step 170, Loss 0.20208369195461273\n",
            "Train step - Step 180, Loss 0.21919575333595276\n",
            "Train step - Step 190, Loss 0.2135675996541977\n",
            "Train epoch - Accuracy: 0.5606060606060606 Loss: 0.20645653025068417 Corrects: 2650\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.19546841084957123\n",
            "Train step - Step 210, Loss 0.2103930562734604\n",
            "Train step - Step 220, Loss 0.18953628838062286\n",
            "Train step - Step 230, Loss 0.20455852150917053\n",
            "Train epoch - Accuracy: 0.6006060606060606 Loss: 0.20165898671655944 Corrects: 2687\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.19718711078166962\n",
            "Train step - Step 250, Loss 0.21017985045909882\n",
            "Train step - Step 260, Loss 0.18893304467201233\n",
            "Train step - Step 270, Loss 0.18222638964653015\n",
            "Train epoch - Accuracy: 0.617979797979798 Loss: 0.19372425444198377 Corrects: 2865\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1978043168783188\n",
            "Train step - Step 290, Loss 0.17621472477912903\n",
            "Train step - Step 300, Loss 0.20408406853675842\n",
            "Train step - Step 310, Loss 0.1918276995420456\n",
            "Train epoch - Accuracy: 0.6686868686868687 Loss: 0.19031395376330673 Corrects: 2897\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.19324056804180145\n",
            "Train step - Step 330, Loss 0.17526455223560333\n",
            "Train step - Step 340, Loss 0.1782958060503006\n",
            "Train step - Step 350, Loss 0.18929578363895416\n",
            "Train epoch - Accuracy: 0.6913131313131313 Loss: 0.18073841544714842 Corrects: 3027\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.1818878948688507\n",
            "Train step - Step 370, Loss 0.17415788769721985\n",
            "Train step - Step 380, Loss 0.20729608833789825\n",
            "Train epoch - Accuracy: 0.7169696969696969 Loss: 0.17323678760215488 Corrects: 3082\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.15040600299835205\n",
            "Train step - Step 400, Loss 0.17432105541229248\n",
            "Train step - Step 410, Loss 0.18147097527980804\n",
            "Train step - Step 420, Loss 0.15775418281555176\n",
            "Train epoch - Accuracy: 0.7347474747474747 Loss: 0.1731151535234066 Corrects: 3120\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.1724080890417099\n",
            "Train step - Step 440, Loss 0.154772087931633\n",
            "Train step - Step 450, Loss 0.14051632583141327\n",
            "Train step - Step 460, Loss 0.16924059391021729\n",
            "Train epoch - Accuracy: 0.7577777777777778 Loss: 0.1627591961200791 Corrects: 3286\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.14429610967636108\n",
            "Train step - Step 480, Loss 0.15541265904903412\n",
            "Train step - Step 490, Loss 0.13560816645622253\n",
            "Train step - Step 500, Loss 0.15359823405742645\n",
            "Train epoch - Accuracy: 0.7818181818181819 Loss: 0.15609632268698528 Corrects: 3327\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.13879401981830597\n",
            "Train step - Step 520, Loss 0.15403786301612854\n",
            "Train step - Step 530, Loss 0.15331827104091644\n",
            "Train step - Step 540, Loss 0.14623580873012543\n",
            "Train epoch - Accuracy: 0.7991919191919192 Loss: 0.15246754652924008 Corrects: 3398\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.13762083649635315\n",
            "Train step - Step 560, Loss 0.1746303141117096\n",
            "Train step - Step 570, Loss 0.13511574268341064\n",
            "Train step - Step 580, Loss 0.12735266983509064\n",
            "Train epoch - Accuracy: 0.8094949494949495 Loss: 0.14592910289162336 Corrects: 3444\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.14227905869483948\n",
            "Train step - Step 600, Loss 0.14480771124362946\n",
            "Train step - Step 610, Loss 0.14043785631656647\n",
            "Train step - Step 620, Loss 0.13456320762634277\n",
            "Train epoch - Accuracy: 0.8208080808080808 Loss: 0.14066492944654793 Corrects: 3543\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.13900920748710632\n",
            "Train step - Step 640, Loss 0.11766765266656876\n",
            "Train step - Step 650, Loss 0.18722748756408691\n",
            "Train step - Step 660, Loss 0.13037343323230743\n",
            "Train epoch - Accuracy: 0.8383838383838383 Loss: 0.14293768761735973 Corrects: 3517\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.13131263852119446\n",
            "Train step - Step 680, Loss 0.15935653448104858\n",
            "Train step - Step 690, Loss 0.1405581384897232\n",
            "Train step - Step 700, Loss 0.14364498853683472\n",
            "Train epoch - Accuracy: 0.8363636363636363 Loss: 0.1387201521011314 Corrects: 3548\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.1498812884092331\n",
            "Train step - Step 720, Loss 0.14735515415668488\n",
            "Train step - Step 730, Loss 0.11783070862293243\n",
            "Train step - Step 740, Loss 0.14439505338668823\n",
            "Train epoch - Accuracy: 0.8523232323232324 Loss: 0.1326135683782173 Corrects: 3619\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.11554808914661407\n",
            "Train step - Step 760, Loss 0.11413135379552841\n",
            "Train step - Step 770, Loss 0.12669236958026886\n",
            "Train epoch - Accuracy: 0.8537373737373737 Loss: 0.12731767723054596 Corrects: 3682\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.12931980192661285\n",
            "Train step - Step 790, Loss 0.14694622159004211\n",
            "Train step - Step 800, Loss 0.12685362994670868\n",
            "Train step - Step 810, Loss 0.1281389743089676\n",
            "Train epoch - Accuracy: 0.8729292929292929 Loss: 0.12280093781875842 Corrects: 3744\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.13057255744934082\n",
            "Train step - Step 830, Loss 0.11341796070337296\n",
            "Train step - Step 840, Loss 0.12708352506160736\n",
            "Train step - Step 850, Loss 0.13948126137256622\n",
            "Train epoch - Accuracy: 0.8767676767676768 Loss: 0.12249790840979778 Corrects: 3747\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.11454039067029953\n",
            "Train step - Step 870, Loss 0.11367668956518173\n",
            "Train step - Step 880, Loss 0.11342554539442062\n",
            "Train step - Step 890, Loss 0.11079321056604385\n",
            "Train epoch - Accuracy: 0.8870707070707071 Loss: 0.11786721051341355 Corrects: 3779\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.10858799517154694\n",
            "Train step - Step 910, Loss 0.11834799498319626\n",
            "Train step - Step 920, Loss 0.10590505599975586\n",
            "Train step - Step 930, Loss 0.13009992241859436\n",
            "Train epoch - Accuracy: 0.8848484848484849 Loss: 0.11634639514215064 Corrects: 3826\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.09604154527187347\n",
            "Train step - Step 950, Loss 0.12202145159244537\n",
            "Train step - Step 960, Loss 0.13081370294094086\n",
            "Train step - Step 970, Loss 0.1107260212302208\n",
            "Train epoch - Accuracy: 0.8953535353535353 Loss: 0.10941160281198194 Corrects: 3927\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.09917280822992325\n",
            "Train step - Step 990, Loss 0.08408848196268082\n",
            "Train step - Step 1000, Loss 0.10469148308038712\n",
            "Train step - Step 1010, Loss 0.12336184084415436\n",
            "Train epoch - Accuracy: 0.9111111111111111 Loss: 0.1073338475793299 Corrects: 3930\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.11461955308914185\n",
            "Train step - Step 1030, Loss 0.10571928322315216\n",
            "Train step - Step 1040, Loss 0.10274400562047958\n",
            "Train step - Step 1050, Loss 0.09951988607645035\n",
            "Train epoch - Accuracy: 0.9064646464646464 Loss: 0.10539515303240882 Corrects: 3945\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.10329502075910568\n",
            "Train step - Step 1070, Loss 0.11030161380767822\n",
            "Train step - Step 1080, Loss 0.07582666724920273\n",
            "Train step - Step 1090, Loss 0.0998583510518074\n",
            "Train epoch - Accuracy: 0.9177777777777778 Loss: 0.0992080882191658 Corrects: 3987\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.1091807410120964\n",
            "Train step - Step 1110, Loss 0.10328833013772964\n",
            "Train step - Step 1120, Loss 0.10374102741479874\n",
            "Train step - Step 1130, Loss 0.07982870191335678\n",
            "Train epoch - Accuracy: 0.9167676767676768 Loss: 0.09896320259029215 Corrects: 3963\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.11492772400379181\n",
            "Train step - Step 1150, Loss 0.09284977614879608\n",
            "Train step - Step 1160, Loss 0.09698130935430527\n",
            "Train epoch - Accuracy: 0.9161616161616162 Loss: 0.10087807445514081 Corrects: 3970\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.0798456072807312\n",
            "Train step - Step 1180, Loss 0.09043916314840317\n",
            "Train step - Step 1190, Loss 0.0874616801738739\n",
            "Train step - Step 1200, Loss 0.09916940331459045\n",
            "Train epoch - Accuracy: 0.9288888888888889 Loss: 0.09346261038021608 Corrects: 4092\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.07378334552049637\n",
            "Train step - Step 1220, Loss 0.08623585850000381\n",
            "Train step - Step 1230, Loss 0.09788971394300461\n",
            "Train step - Step 1240, Loss 0.08162578195333481\n",
            "Train epoch - Accuracy: 0.9319191919191919 Loss: 0.09382674536921762 Corrects: 4059\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.11297927051782608\n",
            "Train step - Step 1260, Loss 0.08491017669439316\n",
            "Train step - Step 1270, Loss 0.08206956833600998\n",
            "Train step - Step 1280, Loss 0.08000748604536057\n",
            "Train epoch - Accuracy: 0.9351515151515152 Loss: 0.08940958527603535 Corrects: 4130\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.08367051184177399\n",
            "Train step - Step 1300, Loss 0.08789476007223129\n",
            "Train step - Step 1310, Loss 0.06326350569725037\n",
            "Train step - Step 1320, Loss 0.08104230463504791\n",
            "Train epoch - Accuracy: 0.9428282828282828 Loss: 0.08235033937475898 Corrects: 4171\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.09270773828029633\n",
            "Train step - Step 1340, Loss 0.09743501991033554\n",
            "Train step - Step 1350, Loss 0.06402760744094849\n",
            "Train step - Step 1360, Loss 0.08421219140291214\n",
            "Train epoch - Accuracy: 0.9408080808080808 Loss: 0.08382808979713556 Corrects: 4180\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.0700470432639122\n",
            "Train step - Step 1380, Loss 0.06893398612737656\n",
            "Train step - Step 1390, Loss 0.08075902611017227\n",
            "Train step - Step 1400, Loss 0.08499731123447418\n",
            "Train epoch - Accuracy: 0.9428282828282828 Loss: 0.08458963126395688 Corrects: 4152\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.0795220136642456\n",
            "Train step - Step 1420, Loss 0.07924173027276993\n",
            "Train step - Step 1430, Loss 0.06955733150243759\n",
            "Train step - Step 1440, Loss 0.08134318888187408\n",
            "Train epoch - Accuracy: 0.9454545454545454 Loss: 0.08086818461165284 Corrects: 4197\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.06845570355653763\n",
            "Train step - Step 1460, Loss 0.0734158605337143\n",
            "Train step - Step 1470, Loss 0.070199154317379\n",
            "Train step - Step 1480, Loss 0.08204089850187302\n",
            "Train epoch - Accuracy: 0.9470707070707071 Loss: 0.0788863360671082 Corrects: 4205\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.08574999868869781\n",
            "Train step - Step 1500, Loss 0.07028436660766602\n",
            "Train step - Step 1510, Loss 0.06890442222356796\n",
            "Train step - Step 1520, Loss 0.09881951659917831\n",
            "Train epoch - Accuracy: 0.9492929292929293 Loss: 0.08073603755596913 Corrects: 4204\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.07114583253860474\n",
            "Train step - Step 1540, Loss 0.06477310508489609\n",
            "Train step - Step 1550, Loss 0.07020888477563858\n",
            "Train epoch - Accuracy: 0.9511111111111111 Loss: 0.07687617546681202 Corrects: 4232\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.06759702414274216\n",
            "Train step - Step 1570, Loss 0.07962369173765182\n",
            "Train step - Step 1580, Loss 0.061648596078157425\n",
            "Train step - Step 1590, Loss 0.09177841246128082\n",
            "Train epoch - Accuracy: 0.9565656565656566 Loss: 0.0732186144680688 Corrects: 4299\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.0630805641412735\n",
            "Train step - Step 1610, Loss 0.08780332654714584\n",
            "Train step - Step 1620, Loss 0.060536425560712814\n",
            "Train step - Step 1630, Loss 0.09516682475805283\n",
            "Train epoch - Accuracy: 0.955959595959596 Loss: 0.07125228912842393 Corrects: 4300\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.04254051670432091\n",
            "Train step - Step 1650, Loss 0.08491456508636475\n",
            "Train step - Step 1660, Loss 0.0429990291595459\n",
            "Train step - Step 1670, Loss 0.06598778814077377\n",
            "Train epoch - Accuracy: 0.9608080808080808 Loss: 0.06847903282052338 Corrects: 4313\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.06713943183422089\n",
            "Train step - Step 1690, Loss 0.053463466465473175\n",
            "Train step - Step 1700, Loss 0.05825401470065117\n",
            "Train step - Step 1710, Loss 0.07120367139577866\n",
            "Train epoch - Accuracy: 0.9636363636363636 Loss: 0.06437299113683026 Corrects: 4396\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.07951600849628448\n",
            "Train step - Step 1730, Loss 0.09134059399366379\n",
            "Train step - Step 1740, Loss 0.054582882672548294\n",
            "Train step - Step 1750, Loss 0.06488721817731857\n",
            "Train epoch - Accuracy: 0.9587878787878787 Loss: 0.06887944131186514 Corrects: 4319\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.05546596273779869\n",
            "Train step - Step 1770, Loss 0.04130192846059799\n",
            "Train step - Step 1780, Loss 0.07017122954130173\n",
            "Train step - Step 1790, Loss 0.07837430387735367\n",
            "Train epoch - Accuracy: 0.9652525252525253 Loss: 0.06667942977011806 Corrects: 4347\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.061023153364658356\n",
            "Train step - Step 1810, Loss 0.06746115535497665\n",
            "Train step - Step 1820, Loss 0.06285827606916428\n",
            "Train step - Step 1830, Loss 0.06393901258707047\n",
            "Train epoch - Accuracy: 0.963030303030303 Loss: 0.06597033879371604 Corrects: 4358\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.046916987746953964\n",
            "Train step - Step 1850, Loss 0.07791300863027573\n",
            "Train step - Step 1860, Loss 0.071982741355896\n",
            "Train step - Step 1870, Loss 0.05675208196043968\n",
            "Train epoch - Accuracy: 0.9656565656565657 Loss: 0.060711432421448254 Corrects: 4418\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.07104899734258652\n",
            "Train step - Step 1890, Loss 0.029578030109405518\n",
            "Train step - Step 1900, Loss 0.055041104555130005\n",
            "Train step - Step 1910, Loss 0.07562176883220673\n",
            "Train epoch - Accuracy: 0.9670707070707071 Loss: 0.05892347928851542 Corrects: 4436\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.06366488337516785\n",
            "Train step - Step 1930, Loss 0.05007033422589302\n",
            "Train step - Step 1940, Loss 0.04778220131993294\n",
            "Train epoch - Accuracy: 0.9717171717171718 Loss: 0.04624423565587612 Corrects: 4558\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.037026967853307724\n",
            "Train step - Step 1960, Loss 0.04202381893992424\n",
            "Train step - Step 1970, Loss 0.0492984913289547\n",
            "Train step - Step 1980, Loss 0.050122808665037155\n",
            "Train epoch - Accuracy: 0.9759595959595959 Loss: 0.03998027484073784 Corrects: 4624\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.03445024415850639\n",
            "Train step - Step 2000, Loss 0.03433879092335701\n",
            "Train step - Step 2010, Loss 0.03543202206492424\n",
            "Train step - Step 2020, Loss 0.05362358316779137\n",
            "Train epoch - Accuracy: 0.9765656565656565 Loss: 0.03901909349994226 Corrects: 4632\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.031019091606140137\n",
            "Train step - Step 2040, Loss 0.04528169706463814\n",
            "Train step - Step 2050, Loss 0.03718123957514763\n",
            "Train step - Step 2060, Loss 0.03513204678893089\n",
            "Train epoch - Accuracy: 0.9775757575757575 Loss: 0.03651137043129314 Corrects: 4674\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.035689275711774826\n",
            "Train step - Step 2080, Loss 0.026729991659522057\n",
            "Train step - Step 2090, Loss 0.04092613235116005\n",
            "Train step - Step 2100, Loss 0.03821713477373123\n",
            "Train epoch - Accuracy: 0.9828282828282828 Loss: 0.03620744348776461 Corrects: 4684\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.0380810908973217\n",
            "Train step - Step 2120, Loss 0.03097846545279026\n",
            "Train step - Step 2130, Loss 0.039104875177145004\n",
            "Train step - Step 2140, Loss 0.03543740138411522\n",
            "Train epoch - Accuracy: 0.983030303030303 Loss: 0.034869624445534715 Corrects: 4690\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.04255911335349083\n",
            "Train step - Step 2160, Loss 0.033110540360212326\n",
            "Train step - Step 2170, Loss 0.05032482370734215\n",
            "Train step - Step 2180, Loss 0.03802185878157616\n",
            "Train epoch - Accuracy: 0.982020202020202 Loss: 0.03440835596786605 Corrects: 4691\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.03096712939441204\n",
            "Train step - Step 2200, Loss 0.02969198301434517\n",
            "Train step - Step 2210, Loss 0.02935841493308544\n",
            "Train step - Step 2220, Loss 0.03001468814909458\n",
            "Train epoch - Accuracy: 0.984040404040404 Loss: 0.032663977746710635 Corrects: 4701\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.024106016382575035\n",
            "Train step - Step 2240, Loss 0.04114753007888794\n",
            "Train step - Step 2250, Loss 0.03918920084834099\n",
            "Train step - Step 2260, Loss 0.03304389864206314\n",
            "Train epoch - Accuracy: 0.983030303030303 Loss: 0.03339001115373891 Corrects: 4704\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.026070674881339073\n",
            "Train step - Step 2280, Loss 0.04676560312509537\n",
            "Train step - Step 2290, Loss 0.03261227533221245\n",
            "Train step - Step 2300, Loss 0.028247226029634476\n",
            "Train epoch - Accuracy: 0.9824242424242424 Loss: 0.032628892453933 Corrects: 4701\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.02818862907588482\n",
            "Train step - Step 2320, Loss 0.03181951865553856\n",
            "Train step - Step 2330, Loss 0.022734077647328377\n",
            "Train epoch - Accuracy: 0.9878787878787879 Loss: 0.030223828475884718 Corrects: 4747\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.02049989439547062\n",
            "Train step - Step 2350, Loss 0.028865201398730278\n",
            "Train step - Step 2360, Loss 0.03943115845322609\n",
            "Train step - Step 2370, Loss 0.031276024878025055\n",
            "Train epoch - Accuracy: 0.9846464646464647 Loss: 0.03257578891711404 Corrects: 4708\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.03519546613097191\n",
            "Train step - Step 2390, Loss 0.030469302088022232\n",
            "Train step - Step 2400, Loss 0.03230798989534378\n",
            "Train step - Step 2410, Loss 0.03626507148146629\n",
            "Train epoch - Accuracy: 0.9850505050505051 Loss: 0.03061991383180474 Corrects: 4723\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.03817214071750641\n",
            "Train step - Step 2430, Loss 0.03243224695324898\n",
            "Train step - Step 2440, Loss 0.02937345579266548\n",
            "Train step - Step 2450, Loss 0.052793949842453\n",
            "Train epoch - Accuracy: 0.9880808080808081 Loss: 0.030368272537233853 Corrects: 4723\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.0387638621032238\n",
            "Train step - Step 2470, Loss 0.02427780069410801\n",
            "Train step - Step 2480, Loss 0.028937894850969315\n",
            "Train step - Step 2490, Loss 0.02785852923989296\n",
            "Train epoch - Accuracy: 0.9860606060606061 Loss: 0.028875334782883375 Corrects: 4753\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.02166069485247135\n",
            "Train step - Step 2510, Loss 0.030202705413103104\n",
            "Train step - Step 2520, Loss 0.03184792399406433\n",
            "Train step - Step 2530, Loss 0.02230757288634777\n",
            "Train epoch - Accuracy: 0.9882828282828283 Loss: 0.0279593915876114 Corrects: 4749\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.025618908926844597\n",
            "Train step - Step 2550, Loss 0.03968783840537071\n",
            "Train step - Step 2560, Loss 0.025734303519129753\n",
            "Train step - Step 2570, Loss 0.029478464275598526\n",
            "Train epoch - Accuracy: 0.9872727272727273 Loss: 0.02719902207905596 Corrects: 4762\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.028580380603671074\n",
            "Train step - Step 2590, Loss 0.026058131828904152\n",
            "Train step - Step 2600, Loss 0.03031555749475956\n",
            "Train step - Step 2610, Loss 0.0219555851072073\n",
            "Train epoch - Accuracy: 0.9864646464646465 Loss: 0.02685321000940872 Corrects: 4767\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.02985372021794319\n",
            "Train step - Step 2630, Loss 0.026584668084979057\n",
            "Train step - Step 2640, Loss 0.023030996322631836\n",
            "Train step - Step 2650, Loss 0.018181294202804565\n",
            "Train epoch - Accuracy: 0.9892929292929293 Loss: 0.026636416828918336 Corrects: 4754\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.030310992151498795\n",
            "Train step - Step 2670, Loss 0.04066663607954979\n",
            "Train step - Step 2680, Loss 0.020677560940384865\n",
            "Train step - Step 2690, Loss 0.026789428666234016\n",
            "Train epoch - Accuracy: 0.9868686868686869 Loss: 0.027080691061688193 Corrects: 4761\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.02683667279779911\n",
            "Train step - Step 2710, Loss 0.041440825909376144\n",
            "Train step - Step 2720, Loss 0.02242247574031353\n",
            "Train epoch - Accuracy: 0.9878787878787879 Loss: 0.025990610756356308 Corrects: 4770\n",
            "Training finished in 192.9745135307312 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4]\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96f7a8ad0>\n",
            "Constructing exemplars of class 67\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [30517, 49104, 21616, 27071, 40581, 32655, 4293, 22434, 49391, 5399, 43781, 6919, 5372, 14515, 27948, 1973, 2974, 21286, 36337, 46443, 30949, 41907, 23457, 5543, 10946, 4752, 18901, 284, 28814, 30805, 47844, 11811, 6186, 37145, 26453, 44900, 29006, 33264, 32328, 17514, 42399, 21903, 32943, 39682, 29022, 39931, 19621, 12149, 39089, 13995, 34332, 20414, 31039, 3928, 48076, 36337, 11730, 40782, 40906, 6455, 17514, 49244, 36938, 12570, 9653, 8812, 29694, 30023, 11730, 25356, 32224, 1356, 49094, 7029, 284, 37583, 34337, 20145, 20984, 43635, 20422, 48523, 394, 6036, 39502, 29694, 18830, 13905, 42295, 16820, 20899, 41279, 2062, 49340, 37221, 27223, 15596, 28060, 8812, 49783, 8206, 30391, 3017, 40465, 4516, 21582, 37338, 21917, 30391, 36834, 30805, 37583, 2860, 27157, 14157, 22925, 23836, 3928, 40581, 13909, 8323, 20057, 29022, 45719, 4680, 49795, 25356, 40605, 5543, 4752, 394, 49407, 1247, 37012, 26856, 10946, 33742, 11761, 4677, 6815, 48882, 18981, 16820, 11761, 7752, 24336, 47104, 11050, 41721, 42945, 8484, 3915, 19194, 40285, 23728, 8560, 29694, 14999, 37221, 16592, 16047, 4970, 14843, 23517, 45635, 27223, 43880, 28494, 18981, 23633, 37561, 30805, 7768, 45719, 22584, 11730, 13886, 5415, 24092, 27380, 32655, 25966, 37700, 49104, 38197, 44686, 37329, 6815, 2974, 18812, 28814, 284, 47266, 36834, 40906, 8812, 14637, 4226, 4411, 32328]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cc68790>\n",
            "Constructing exemplars of class 59\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [43884, 28590, 16709, 40744, 3952, 25629, 41926, 10167, 3500, 31388, 15381, 49931, 14835, 34935, 35190, 982, 18985, 8081, 26727, 19949, 26430, 3952, 13465, 32372, 25774, 19092, 18757, 43758, 44335, 16650, 27858, 6746, 21649, 17404, 32609, 13656, 28860, 40094, 11254, 12113, 19474, 33936, 27950, 20812, 27167, 6100, 24255, 43682, 19261, 45267, 43463, 16080, 30471, 7320, 18985, 26849, 5672, 36701, 21144, 12254, 41307, 35676, 26688, 1616, 11555, 3740, 14004, 3995, 12907, 46093, 36312, 10664, 40178, 3711, 5848, 2290, 26636, 36917, 47618, 5774, 43758, 21, 37074, 29968, 22797, 4708, 30328, 24236, 8190, 16608, 39993, 3165, 28860, 47618, 17404, 8676, 10664, 15381, 13873, 38064, 13178, 34163, 49299, 16295, 12318, 20812, 9727, 40094, 30109, 6746, 27858, 49401, 6283, 8324, 38216, 8578, 36917, 34504, 30817, 20207, 22751, 4018, 6065, 13004, 22642, 16001, 16709, 31097, 10406, 41678, 49979, 15798, 3711, 20670, 3952, 9679, 10664, 49165, 49141, 38957, 15810, 17427, 37719, 16250, 16086, 13541, 19949, 44367, 23044, 28279, 7061, 13178, 45723, 42912, 41678, 43682, 34536, 19474, 7320, 44335, 43758, 5774, 43534, 11004, 31388, 15381, 982, 19092, 13830, 46093, 567, 33062, 10920, 47268, 17507, 44865, 40320, 29870, 42114, 38195, 46599, 45659, 22866, 10111, 13541, 11710, 43682, 41678, 13105, 973, 16244, 46599, 17404, 31388, 15381, 35106, 12254, 49156, 42330, 31685]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96f7deb10>\n",
            "Constructing exemplars of class 39\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [28472, 49693, 45328, 35009, 30760, 21506, 22227, 3495, 15263, 10125, 19522, 47593, 46661, 45072, 30760, 15822, 49074, 218, 47788, 16211, 24205, 29734, 11500, 5525, 7999, 21997, 31970, 10953, 25467, 29450, 46350, 47340, 11265, 12697, 5414, 32634, 7600, 20997, 218, 47456, 32835, 34395, 22654, 47583, 36912, 21202, 16610, 15132, 33518, 20897, 40196, 28132, 21397, 735, 14438, 13489, 30307, 3495, 31593, 7584, 5887, 39348, 49899, 42637, 46962, 20919, 15156, 25887, 36185, 11500, 18785, 24768, 4922, 8082, 24442, 10384, 6639, 33859, 8422, 44790, 11427, 6639, 17472, 19303, 10645, 25887, 27986, 12786, 2830, 11265, 12697, 38041, 39712, 35280, 15936, 10386, 21774, 49887, 16449, 38861, 18357, 22648, 30229, 17467, 29160, 38834, 32685, 37909, 40196, 44582, 40353, 11117, 24369, 37919, 23943, 32307, 7600, 3965, 2808, 22654, 30158, 14172, 14096, 21397, 18785, 5320, 3965, 33518, 28132, 19303, 2808, 30725, 49155, 12931, 20430, 11082, 47014, 5414, 32634, 9764, 29457, 37929, 30686, 46877, 32848, 24442, 10384, 37727, 25537, 5466, 38834, 46089, 23943, 10386, 11797, 12386, 44046, 44582, 23610, 40454, 33832, 7584, 4519, 38592, 14659, 45109, 37881, 25284, 39069, 47358, 49952, 44582, 10340, 18749, 23943, 5525, 30647, 33859, 38041, 17307, 47014, 5414, 48448, 26858, 45674, 4457, 11179, 6006, 33832, 33238, 49860, 49920, 34501, 25177, 29450, 11179, 24442, 45038, 18206, 15263]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cc68790>\n",
            "Constructing exemplars of class 22\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [6019, 13977, 40349, 24147, 20770, 21510, 27820, 31223, 2135, 24415, 32438, 30469, 45561, 18636, 44557, 10003, 8411, 46821, 15680, 46997, 26474, 19401, 13977, 29380, 14507, 239, 36122, 43809, 19434, 9171, 35892, 37554, 49533, 8389, 40431, 47317, 36224, 33620, 24448, 44433, 19434, 33335, 4915, 30254, 22774, 6771, 15680, 35194, 45986, 12317, 25304, 25178, 40972, 4147, 12726, 24147, 48379, 8088, 40972, 12872, 22566, 49821, 13150, 25693, 46821, 27911, 36979, 25693, 2503, 9204, 13084, 20180, 28257, 48737, 12627, 3444, 31853, 33886, 17083, 3293, 41981, 13630, 35840, 747, 31223, 31053, 38235, 22036, 37219, 21749, 41413, 17542, 38509, 35616, 18718, 26352, 40034, 46817, 43346, 16272, 21481, 35732, 16283, 26352, 4807, 35226, 20372, 35236, 14460, 31976, 48676, 49821, 16552, 41413, 44721, 29172, 40349, 39731, 29380, 44472, 29247, 47463, 47110, 35226, 44918, 8656, 7888, 2053, 2584, 44714, 41791, 8656, 32169, 29950, 48379, 43346, 29021, 2503, 13615, 13630, 12429, 27820, 34873, 19434, 17243, 47193, 31571, 45561, 15318, 43346, 24415, 5102, 3941, 40292, 31292, 36397, 29950, 49069, 39070, 28257, 35732, 31857, 10652, 6771, 13209, 6369, 10817, 46230, 35926, 9204, 8436, 47193, 17251, 35250, 38451, 21200, 13084, 18163, 872, 43947, 34997, 9221, 21578, 5828, 33877, 45699, 43809, 26835, 24204, 46649, 47193, 10936, 5402, 28665, 46552, 24147, 48485, 38509, 48676, 42014]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cc44350>\n",
            "Constructing exemplars of class 18\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [39571, 45833, 31274, 8346, 24650, 38535, 20174, 44591, 31962, 46712, 44061, 30654, 8936, 38448, 42724, 16536, 7149, 20386, 38379, 46329, 2125, 44118, 17430, 37193, 37924, 28829, 25182, 13403, 25982, 18031, 28456, 31289, 46386, 9100, 18541, 42406, 10419, 36707, 43870, 17833, 22322, 18503, 2560, 7955, 19492, 44061, 38976, 2560, 47192, 4461, 43649, 32952, 2125, 38448, 17430, 27981, 9670, 33131, 28389, 46854, 22115, 12541, 33432, 18932, 14977, 31539, 41165, 13100, 27698, 9718, 16536, 9414, 21442, 17907, 14633, 21078, 40085, 34411, 31289, 21348, 38719, 16584, 32209, 17228, 35004, 48346, 29210, 42724, 2593, 12008, 11808, 25714, 38719, 7908, 13269, 43649, 28048, 31256, 48813, 37181, 49433, 32500, 36385, 23148, 35651, 26711, 44061, 49342, 31340, 36886, 36525, 40207, 43586, 46386, 27120, 5205, 20143, 75, 10462, 23148, 39583, 5520, 3343, 14102, 40907, 46367, 17267, 32209, 13030, 27981, 35004, 23958, 22390, 38976, 14512, 9654, 25395, 36687, 28519, 17833, 20871, 37638, 32775, 40498, 21106, 46622, 7649, 18932, 33131, 42573, 23148, 32101, 9774, 5477, 38776, 40946, 20261, 36525, 34175, 32792, 25714, 2125, 18836, 21552, 27698, 22416, 21877, 13030, 21770, 22322, 27943, 38976, 20543, 13269, 39132, 9654, 24601, 35247, 34023, 1273, 1005, 26792, 8346, 38379, 12886, 21942, 35267, 17748, 12660, 21442, 42724, 33131, 6745, 22467, 46925, 9654, 18889, 46578, 1454, 33587]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cb62c10>\n",
            "Constructing exemplars of class 65\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [3105, 10962, 19912, 10695, 42332, 3799, 11415, 21059, 19161, 17959, 38262, 37635, 41727, 16875, 2175, 41695, 9979, 44363, 17603, 43135, 7024, 16041, 46170, 7762, 40512, 10695, 6708, 30303, 35969, 13533, 14372, 9040, 38489, 15842, 17905, 48625, 7789, 39907, 17978, 45463, 29638, 20934, 20016, 48581, 24775, 35284, 47181, 27378, 48829, 20579, 396, 16039, 6411, 37662, 46848, 42091, 43785, 29675, 25843, 28855, 13399, 9073, 4158, 32767, 20348, 802, 1835, 22801, 38799, 31883, 9777, 33229, 25365, 43296, 18691, 18498, 29737, 4550, 7707, 35733, 33043, 48126, 7777, 19294, 8225, 43574, 8531, 9010, 18988, 41236, 9979, 47804, 39954, 3799, 14712, 42242, 49572, 13560, 35425, 26182, 30661, 40512, 28532, 16875, 46478, 10695, 32212, 38680, 43365, 17905, 34206, 29067, 34559, 9040, 11569, 10695, 35862, 45707, 41695, 42785, 6695, 38560, 43365, 20321, 29091, 1313, 38799, 42353, 7114, 10962, 32583, 31038, 39954, 30351, 49802, 16003, 34105, 19667, 14712, 22123, 49735, 26376, 30989, 40984, 22341, 39907, 21675, 15789, 44985, 1982, 8526, 11149, 33835, 43365, 21726, 15842, 21445, 26757, 3904, 8526, 6708, 33366, 33781, 10579, 33472, 2175, 7323, 28375, 32045, 463, 9073, 3838, 12674, 42979, 42701, 23330, 33043, 48711, 22323, 24050, 22195, 12479, 25655, 30989, 23825, 17388, 28962, 33596, 25106, 22720, 20934, 39196, 29091, 1313, 11569, 46190, 5103, 16647, 44459, 7024]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cc52f90>\n",
            "Constructing exemplars of class 49\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [47461, 28844, 41320, 36073, 14533, 29526, 47253, 7188, 10609, 28201, 16676, 44090, 16668, 24792, 38236, 13957, 49274, 16676, 13850, 33031, 29526, 47253, 9045, 6094, 8987, 416, 46508, 479, 44525, 29777, 14246, 13859, 6403, 47041, 27276, 10680, 17524, 6898, 14536, 13849, 6351, 41228, 35448, 12189, 3127, 19486, 8025, 32135, 15623, 23528, 6898, 38938, 12713, 1434, 47247, 617, 19812, 46075, 26063, 18001, 34270, 4264, 30549, 13797, 29491, 9798, 23219, 11736, 45762, 32260, 11694, 15491, 43337, 42122, 13215, 20485, 12189, 16075, 17790, 37546, 10203, 31975, 36563, 5061, 34940, 23821, 46931, 10680, 14890, 35671, 12555, 33332, 2361, 11397, 41037, 17737, 49972, 18189, 9798, 14543, 49610, 25621, 40851, 48987, 30596, 479, 26777, 29526, 2265, 48754, 416, 8181, 4974, 22278, 11731, 47556, 12713, 10137, 32592, 43713, 38909, 30549, 27276, 37237, 18189, 44670, 33562, 38562, 2737, 16676, 20211, 17790, 88, 25318, 25906, 36847, 35448, 45506, 23219, 14929, 40747, 47556, 3029, 12189, 1097, 8515, 36523, 31350, 47029, 15401, 5444, 5731, 27122, 16075, 27170, 14890, 14823, 34111, 22247, 19048, 15089, 15563, 16421, 34940, 5061, 49274, 12554, 14890, 9798, 35400, 32386, 18138, 30826, 47029, 34975, 19316, 49966, 32135, 2444, 19316, 25656, 40851, 12631, 30821, 26063, 36373, 7785, 49659, 40890, 27289, 46767, 41037, 49922, 34118, 34801, 16421, 21976, 18189, 25127, 17737]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cc52b50>\n",
            "Constructing exemplars of class 56\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [37440, 9102, 46309, 48881, 48820, 49875, 19986, 33423, 30079, 23447, 5433, 47455, 36578, 15140, 36541, 5351, 47360, 38889, 1410, 36491, 34304, 30715, 4319, 24562, 31465, 14334, 22401, 6916, 47423, 45836, 43869, 24629, 31644, 23447, 12013, 7536, 21570, 26410, 4215, 34242, 42786, 5361, 38779, 37544, 38176, 7640, 38346, 46755, 12571, 11760, 15000, 30453, 22627, 48466, 23949, 4554, 13600, 41820, 15649, 37426, 25780, 26492, 13908, 30262, 5190, 13749, 34183, 43212, 1529, 18984, 7156, 7619, 2762, 12819, 3044, 29975, 15432, 4422, 26289, 37730, 36623, 10856, 29524, 6916, 27069, 6595, 4787, 26492, 47336, 39221, 23080, 20482, 4395, 38865, 40813, 13189, 6163, 35565, 39547, 10390, 17919, 7492, 26303, 40439, 45845, 34378, 25059, 7156, 2196, 29110, 4857, 40733, 39721, 31586, 16420, 27718, 30076, 6766, 26289, 43374, 47360, 24599, 1822, 46633, 16963, 26765, 42287, 14825, 24108, 44416, 17799, 11376, 6595, 4787, 2255, 36541, 35305, 36623, 18183, 9811, 4387, 12972, 39892, 26153, 11305, 15023, 29286, 38889, 6310, 21799, 1533, 4319, 31465, 46688, 36669, 7289, 2764, 47805, 42345, 23309, 5351, 25519, 27676, 2255, 33799, 42786, 35792, 43001, 36374, 9102, 25104, 43001, 3426, 31574, 11753, 4387, 12499, 28828, 39547, 5190, 13749, 48484, 24629, 30678, 35585, 25519, 2196, 20305, 41011, 24108, 41553, 33738, 1783, 47805, 6522, 8628, 2843, 471, 14825, 27038]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96d4ee890>\n",
            "Constructing exemplars of class 20\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [4774, 509, 32522, 8882, 23898, 28340, 22243, 19354, 32262, 36064, 36588, 23000, 33915, 40053, 26405, 7205, 41235, 17362, 134, 6667, 22243, 28825, 10234, 34596, 44209, 25198, 40770, 40989, 14269, 28555, 39151, 31968, 37831, 12862, 44408, 34159, 15904, 10076, 33677, 42123, 388, 30425, 37808, 16774, 24540, 31478, 46878, 32210, 3120, 47921, 19622, 23654, 9374, 26538, 48876, 39084, 49604, 17285, 10716, 5231, 15143, 46897, 4831, 29591, 5727, 36514, 38996, 17169, 16775, 45022, 7813, 16301, 10219, 18768, 20631, 5643, 29097, 44209, 14029, 44177, 13760, 31785, 22251, 44861, 32210, 8279, 10397, 14089, 17671, 6038, 34663, 47108, 7285, 6279, 14329, 47829, 26701, 44512, 37694, 29429, 26407, 30425, 19855, 23654, 6667, 42375, 9289, 19444, 9508, 25839, 28555, 2693, 43013, 17483, 28851, 33915, 24052, 34420, 7606, 10076, 4768, 40494, 43657, 7890, 22865, 3033, 40053, 48234, 32395, 43710, 39084, 41235, 34663, 40770, 989, 24540, 34247, 37694, 37538, 32210, 5643, 47846, 16462, 28391, 31968, 2747, 26615, 33153, 3776, 38425, 388, 42094, 2653, 16301, 1827, 13700, 44177, 22520, 7800, 9374, 33829, 12599, 20089, 32210, 18131, 19855, 16775, 29097, 19634, 14139, 475, 41449, 41520, 35018, 13794, 33814, 2655, 16006, 48767, 48340, 10076, 26407, 30827, 26613, 47391, 1210, 21615, 10252, 29303, 49222, 26701, 19622, 28980, 33915, 41732, 31197, 38996, 41235, 42719, 49630]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cb62c10>\n",
            "Constructing exemplars of class 4\n",
            "lunghezza exemplar set:  200\n",
            "exemplar set:  [42050, 18400, 19597, 20699, 22660, 25090, 10066, 35641, 15063, 28953, 38364, 10099, 27607, 1929, 3107, 22002, 43786, 26319, 44264, 35028, 31043, 48926, 27560, 10518, 9218, 47133, 7017, 11359, 30771, 13836, 7290, 24635, 15681, 43286, 32837, 4588, 5880, 48332, 11359, 33174, 13211, 37055, 36429, 10518, 36030, 5493, 14436, 45436, 19620, 23361, 42937, 10983, 9380, 37049, 43786, 26319, 6834, 22843, 37381, 46523, 6514, 16201, 13695, 28584, 46335, 38786, 22782, 44237, 44104, 17304, 13253, 43490, 35390, 45465, 21951, 44200, 41403, 14691, 39748, 42222, 48043, 26476, 19094, 41904, 30335, 3654, 19094, 20078, 26476, 4865, 11798, 40736, 13029, 2769, 15442, 24923, 29840, 37381, 32185, 17704, 32066, 7524, 7270, 9380, 19554, 34784, 4472, 8292, 6051, 39748, 6834, 22843, 7841, 27342, 18449, 47410, 19967, 10983, 38786, 19597, 42333, 25134, 14549, 49252, 44568, 36753, 19620, 28139, 42042, 37381, 16016, 33267, 28682, 44162, 14549, 22782, 35576, 3080, 31801, 42509, 825, 24809, 24923, 8462, 7031, 13503, 3067, 35649, 26299, 42661, 35006, 3107, 3230, 48926, 10417, 3423, 45151, 33004, 36340, 15063, 21069, 49929, 41304, 17693, 40753, 34974, 24923, 36801, 3822, 41626, 26714, 14826, 39730, 30378, 33174, 35006, 26319, 37381, 8292, 5677, 33267, 2139, 24545, 20620, 37075, 6565, 29036, 3470, 20413, 25090, 43286, 32837, 389, 20699, 33329, 27560, 6591, 37381, 10074, 22002]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.98 0.11484958231449127\n",
            "TEST GROUP:  0.982\n",
            "TEST ALL:  0.813\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  2000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [82, 81, 7, 16, 18, 20, 21, 22, 34, 39, 47, 49, 56, 59, 65, 67, 68, 79, 80, 4]\n",
            "TRAIN_SET CLASSES:  [79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "VALIDATION CLASSES:  [47, 34, 21, 16, 82, 81, 80, 79, 7, 68]\n",
            "GROUP:  2\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  20\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.45061907172203064\n",
            "Train step - Step 10, Loss 0.18950989842414856\n",
            "Train step - Step 20, Loss 0.16002149879932404\n",
            "Train step - Step 30, Loss 0.15456916391849518\n",
            "Train step - Step 40, Loss 0.1446455717086792\n",
            "Train step - Step 50, Loss 0.14673921465873718\n",
            "Train epoch - Accuracy: 0.7415827338129496 Loss: 0.17422316341520214 Corrects: 2161\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.13609114289283752\n",
            "Train step - Step 70, Loss 0.13374410569667816\n",
            "Train step - Step 80, Loss 0.13330690562725067\n",
            "Train step - Step 90, Loss 0.11923724412918091\n",
            "Train step - Step 100, Loss 0.1318753957748413\n",
            "Train epoch - Accuracy: 0.7552517985611511 Loss: 0.13136173039889165 Corrects: 2616\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.13073013722896576\n",
            "Train step - Step 120, Loss 0.13018444180488586\n",
            "Train step - Step 130, Loss 0.11693806946277618\n",
            "Train step - Step 140, Loss 0.13358284533023834\n",
            "Train step - Step 150, Loss 0.13928700983524323\n",
            "Train step - Step 160, Loss 0.1109718307852745\n",
            "Train epoch - Accuracy: 0.7647482014388489 Loss: 0.12377123254666225 Corrects: 2903\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.12406094372272491\n",
            "Train step - Step 180, Loss 0.11621620506048203\n",
            "Train step - Step 190, Loss 0.11441566795110703\n",
            "Train step - Step 200, Loss 0.11541779339313507\n",
            "Train step - Step 210, Loss 0.12401342391967773\n",
            "Train epoch - Accuracy: 0.7798561151079136 Loss: 0.11882921394684332 Corrects: 3116\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11557307094335556\n",
            "Train step - Step 230, Loss 0.11380542814731598\n",
            "Train step - Step 240, Loss 0.09639990329742432\n",
            "Train step - Step 250, Loss 0.12197407335042953\n",
            "Train step - Step 260, Loss 0.11492263525724411\n",
            "Train step - Step 270, Loss 0.10873141139745712\n",
            "Train epoch - Accuracy: 0.7815827338129496 Loss: 0.11432878502410093 Corrects: 3416\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.12648385763168335\n",
            "Train step - Step 290, Loss 0.10769688338041306\n",
            "Train step - Step 300, Loss 0.10981549322605133\n",
            "Train step - Step 310, Loss 0.1097974181175232\n",
            "Train step - Step 320, Loss 0.12060785293579102\n",
            "Train epoch - Accuracy: 0.8028776978417266 Loss: 0.11279192822657043 Corrects: 3520\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.10446856170892715\n",
            "Train step - Step 340, Loss 0.11329728364944458\n",
            "Train step - Step 350, Loss 0.1296980232000351\n",
            "Train step - Step 360, Loss 0.10190858691930771\n",
            "Train step - Step 370, Loss 0.11042549461126328\n",
            "Train step - Step 380, Loss 0.10569888353347778\n",
            "Train epoch - Accuracy: 0.8102158273381295 Loss: 0.11038568267719351 Corrects: 3600\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.12281662225723267\n",
            "Train step - Step 400, Loss 0.09931965172290802\n",
            "Train step - Step 410, Loss 0.11860644072294235\n",
            "Train step - Step 420, Loss 0.1106986552476883\n",
            "Train step - Step 430, Loss 0.11194741725921631\n",
            "Train epoch - Accuracy: 0.8132374100719425 Loss: 0.11044506404039671 Corrects: 3679\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.09675692021846771\n",
            "Train step - Step 450, Loss 0.09995874017477036\n",
            "Train step - Step 460, Loss 0.11627989262342453\n",
            "Train step - Step 470, Loss 0.09987204521894455\n",
            "Train step - Step 480, Loss 0.10544645041227341\n",
            "Train step - Step 490, Loss 0.10817378759384155\n",
            "Train epoch - Accuracy: 0.8176978417266187 Loss: 0.10671299931814344 Corrects: 3806\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.09911828488111496\n",
            "Train step - Step 510, Loss 0.10266812145709991\n",
            "Train step - Step 520, Loss 0.1081223264336586\n",
            "Train step - Step 530, Loss 0.09948735684156418\n",
            "Train step - Step 540, Loss 0.098422110080719\n",
            "Train epoch - Accuracy: 0.822158273381295 Loss: 0.1059078549052314 Corrects: 3868\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.11974121630191803\n",
            "Train step - Step 560, Loss 0.09936954826116562\n",
            "Train step - Step 570, Loss 0.10861660540103912\n",
            "Train step - Step 580, Loss 0.09822187572717667\n",
            "Train step - Step 590, Loss 0.09876324981451035\n",
            "Train step - Step 600, Loss 0.1112392321228981\n",
            "Train epoch - Accuracy: 0.8225899280575539 Loss: 0.10484651148104839 Corrects: 3985\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.09766906499862671\n",
            "Train step - Step 620, Loss 0.10983617603778839\n",
            "Train step - Step 630, Loss 0.10831105709075928\n",
            "Train step - Step 640, Loss 0.09217667579650879\n",
            "Train step - Step 650, Loss 0.09902913868427277\n",
            "Train epoch - Accuracy: 0.8316546762589928 Loss: 0.10348186379713978 Corrects: 4036\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10565982013940811\n",
            "Train step - Step 670, Loss 0.10271191596984863\n",
            "Train step - Step 680, Loss 0.10199731588363647\n",
            "Train step - Step 690, Loss 0.1138785108923912\n",
            "Train step - Step 700, Loss 0.10066025704145432\n",
            "Train step - Step 710, Loss 0.1057879701256752\n",
            "Train epoch - Accuracy: 0.8355395683453237 Loss: 0.10212557664878077 Corrects: 4112\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10089939087629318\n",
            "Train step - Step 730, Loss 0.10556299984455109\n",
            "Train step - Step 740, Loss 0.10437367111444473\n",
            "Train step - Step 750, Loss 0.10794856399297714\n",
            "Train step - Step 760, Loss 0.10601267963647842\n",
            "Train epoch - Accuracy: 0.8402877697841726 Loss: 0.10127415904895865 Corrects: 4164\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.08672056347131729\n",
            "Train step - Step 780, Loss 0.10551247745752335\n",
            "Train step - Step 790, Loss 0.08707576245069504\n",
            "Train step - Step 800, Loss 0.09491825103759766\n",
            "Train step - Step 810, Loss 0.10012269020080566\n",
            "Train step - Step 820, Loss 0.10315064340829849\n",
            "Train epoch - Accuracy: 0.8451798561151079 Loss: 0.10072508760707842 Corrects: 4215\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.09760046750307083\n",
            "Train step - Step 840, Loss 0.09680107980966568\n",
            "Train step - Step 850, Loss 0.09111880511045456\n",
            "Train step - Step 860, Loss 0.09351795166730881\n",
            "Train step - Step 870, Loss 0.10070234537124634\n",
            "Train epoch - Accuracy: 0.8506474820143884 Loss: 0.0985617358735997 Corrects: 4286\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.09417375177145004\n",
            "Train step - Step 890, Loss 0.09293433278799057\n",
            "Train step - Step 900, Loss 0.10268024355173111\n",
            "Train step - Step 910, Loss 0.10530070215463638\n",
            "Train step - Step 920, Loss 0.10865598171949387\n",
            "Train step - Step 930, Loss 0.10239113867282867\n",
            "Train epoch - Accuracy: 0.8539568345323741 Loss: 0.09769566777798769 Corrects: 4386\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.0914679765701294\n",
            "Train step - Step 950, Loss 0.10307283699512482\n",
            "Train step - Step 960, Loss 0.08310604840517044\n",
            "Train step - Step 970, Loss 0.09784524142742157\n",
            "Train step - Step 980, Loss 0.08986169099807739\n",
            "Train epoch - Accuracy: 0.8510791366906475 Loss: 0.09776717506295486 Corrects: 4394\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.08684080094099045\n",
            "Train step - Step 1000, Loss 0.09635156393051147\n",
            "Train step - Step 1010, Loss 0.08342693001031876\n",
            "Train step - Step 1020, Loss 0.08917318284511566\n",
            "Train step - Step 1030, Loss 0.09566399455070496\n",
            "Train step - Step 1040, Loss 0.08876568078994751\n",
            "Train epoch - Accuracy: 0.8532374100719424 Loss: 0.09599240851488045 Corrects: 4443\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.10379542410373688\n",
            "Train step - Step 1060, Loss 0.08907819539308548\n",
            "Train step - Step 1070, Loss 0.09277530759572983\n",
            "Train step - Step 1080, Loss 0.10115861892700195\n",
            "Train step - Step 1090, Loss 0.10387396812438965\n",
            "Train epoch - Accuracy: 0.8627338129496402 Loss: 0.09580625979162806 Corrects: 4453\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.08822039514780045\n",
            "Train step - Step 1110, Loss 0.09166744351387024\n",
            "Train step - Step 1120, Loss 0.0893406793475151\n",
            "Train step - Step 1130, Loss 0.09772597998380661\n",
            "Train step - Step 1140, Loss 0.09863168001174927\n",
            "Train step - Step 1150, Loss 0.10116241127252579\n",
            "Train epoch - Accuracy: 0.8658992805755396 Loss: 0.09485541515213122 Corrects: 4524\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.09939604252576828\n",
            "Train step - Step 1170, Loss 0.09774871915578842\n",
            "Train step - Step 1180, Loss 0.10471035540103912\n",
            "Train step - Step 1190, Loss 0.08574088662862778\n",
            "Train step - Step 1200, Loss 0.08606389164924622\n",
            "Train epoch - Accuracy: 0.8717985611510791 Loss: 0.09435155688858718 Corrects: 4628\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.0989154800772667\n",
            "Train step - Step 1220, Loss 0.09817422926425934\n",
            "Train step - Step 1230, Loss 0.0943419337272644\n",
            "Train step - Step 1240, Loss 0.09574402123689651\n",
            "Train step - Step 1250, Loss 0.09368766844272614\n",
            "Train step - Step 1260, Loss 0.09907811135053635\n",
            "Train epoch - Accuracy: 0.8702158273381295 Loss: 0.09353677999630249 Corrects: 4589\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.106044702231884\n",
            "Train step - Step 1280, Loss 0.10918673127889633\n",
            "Train step - Step 1290, Loss 0.09424122422933578\n",
            "Train step - Step 1300, Loss 0.08463280647993088\n",
            "Train step - Step 1310, Loss 0.09828191250562668\n",
            "Train epoch - Accuracy: 0.8768345323741007 Loss: 0.09293113844857799 Corrects: 4632\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.08823347091674805\n",
            "Train step - Step 1330, Loss 0.08553383499383926\n",
            "Train step - Step 1340, Loss 0.09009511768817902\n",
            "Train step - Step 1350, Loss 0.09560927003622055\n",
            "Train step - Step 1360, Loss 0.08729241043329239\n",
            "Train step - Step 1370, Loss 0.0890762209892273\n",
            "Train epoch - Accuracy: 0.8761151079136691 Loss: 0.09156585740528518 Corrects: 4682\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.0966082364320755\n",
            "Train step - Step 1390, Loss 0.08921661227941513\n",
            "Train step - Step 1400, Loss 0.08765973895788193\n",
            "Train step - Step 1410, Loss 0.0846896544098854\n",
            "Train step - Step 1420, Loss 0.09456072002649307\n",
            "Train epoch - Accuracy: 0.8807194244604316 Loss: 0.09062239766120911 Corrects: 4766\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.09623239189386368\n",
            "Train step - Step 1440, Loss 0.08757082372903824\n",
            "Train step - Step 1450, Loss 0.09289305657148361\n",
            "Train step - Step 1460, Loss 0.1024484857916832\n",
            "Train step - Step 1470, Loss 0.08028896898031235\n",
            "Train step - Step 1480, Loss 0.09455931186676025\n",
            "Train epoch - Accuracy: 0.8837410071942446 Loss: 0.09044275095136903 Corrects: 4836\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.07976363599300385\n",
            "Train step - Step 1500, Loss 0.09075479954481125\n",
            "Train step - Step 1510, Loss 0.08848512172698975\n",
            "Train step - Step 1520, Loss 0.08609136939048767\n",
            "Train step - Step 1530, Loss 0.08718577772378922\n",
            "Train epoch - Accuracy: 0.8835971223021583 Loss: 0.08995099288739747 Corrects: 4850\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.08960425108671188\n",
            "Train step - Step 1550, Loss 0.09027928858995438\n",
            "Train step - Step 1560, Loss 0.07607783377170563\n",
            "Train step - Step 1570, Loss 0.0858301967382431\n",
            "Train step - Step 1580, Loss 0.07723497599363327\n",
            "Train step - Step 1590, Loss 0.08369159698486328\n",
            "Train epoch - Accuracy: 0.8917985611510791 Loss: 0.08900862416989512 Corrects: 4857\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.08499843627214432\n",
            "Train step - Step 1610, Loss 0.0925946831703186\n",
            "Train step - Step 1620, Loss 0.09259632974863052\n",
            "Train step - Step 1630, Loss 0.07706525176763535\n",
            "Train step - Step 1640, Loss 0.09763442724943161\n",
            "Train epoch - Accuracy: 0.8863309352517985 Loss: 0.08845010787677422 Corrects: 4907\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.08774832636117935\n",
            "Train step - Step 1660, Loss 0.0848984643816948\n",
            "Train step - Step 1670, Loss 0.07307357341051102\n",
            "Train step - Step 1680, Loss 0.09312722831964493\n",
            "Train step - Step 1690, Loss 0.09701209515333176\n",
            "Train step - Step 1700, Loss 0.09353866428136826\n",
            "Train epoch - Accuracy: 0.8926618705035971 Loss: 0.08846511540224226 Corrects: 4932\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.08899318426847458\n",
            "Train step - Step 1720, Loss 0.08715497702360153\n",
            "Train step - Step 1730, Loss 0.0946565568447113\n",
            "Train step - Step 1740, Loss 0.0844496414065361\n",
            "Train step - Step 1750, Loss 0.08916131407022476\n",
            "Train epoch - Accuracy: 0.899568345323741 Loss: 0.0855672799833387 Corrects: 4990\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.0782499834895134\n",
            "Train step - Step 1770, Loss 0.08308451622724533\n",
            "Train step - Step 1780, Loss 0.08893879503011703\n",
            "Train step - Step 1790, Loss 0.088591568171978\n",
            "Train step - Step 1800, Loss 0.07776105403900146\n",
            "Train step - Step 1810, Loss 0.08752893656492233\n",
            "Train epoch - Accuracy: 0.8912230215827338 Loss: 0.08647015107192582 Corrects: 5011\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.08594565838575363\n",
            "Train step - Step 1830, Loss 0.08599863201379776\n",
            "Train step - Step 1840, Loss 0.08810216188430786\n",
            "Train step - Step 1850, Loss 0.0809936597943306\n",
            "Train step - Step 1860, Loss 0.09352321922779083\n",
            "Train epoch - Accuracy: 0.9014388489208633 Loss: 0.0851440616370105 Corrects: 5035\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.09037754684686661\n",
            "Train step - Step 1880, Loss 0.08527474850416183\n",
            "Train step - Step 1890, Loss 0.0883122906088829\n",
            "Train step - Step 1900, Loss 0.08515224605798721\n",
            "Train step - Step 1910, Loss 0.0877663865685463\n",
            "Train step - Step 1920, Loss 0.09025685489177704\n",
            "Train epoch - Accuracy: 0.9046043165467625 Loss: 0.085451259278565 Corrects: 5105\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.0781296044588089\n",
            "Train step - Step 1940, Loss 0.08235552906990051\n",
            "Train step - Step 1950, Loss 0.07416684925556183\n",
            "Train step - Step 1960, Loss 0.09024631977081299\n",
            "Train step - Step 1970, Loss 0.08421026915311813\n",
            "Train epoch - Accuracy: 0.9030215827338129 Loss: 0.08424420084241482 Corrects: 5128\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.08515026420354843\n",
            "Train step - Step 1990, Loss 0.07735027372837067\n",
            "Train step - Step 2000, Loss 0.09091674536466599\n",
            "Train step - Step 2010, Loss 0.08992914110422134\n",
            "Train step - Step 2020, Loss 0.08231045305728912\n",
            "Train step - Step 2030, Loss 0.07910002768039703\n",
            "Train epoch - Accuracy: 0.9093525179856116 Loss: 0.08451972441493179 Corrects: 5130\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.08355240523815155\n",
            "Train step - Step 2050, Loss 0.07909995317459106\n",
            "Train step - Step 2060, Loss 0.08290400356054306\n",
            "Train step - Step 2070, Loss 0.08545942604541779\n",
            "Train step - Step 2080, Loss 0.08963319659233093\n",
            "Train epoch - Accuracy: 0.9086330935251798 Loss: 0.08327116492197668 Corrects: 5176\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.07305429875850677\n",
            "Train step - Step 2100, Loss 0.08713551610708237\n",
            "Train step - Step 2110, Loss 0.08006815612316132\n",
            "Train step - Step 2120, Loss 0.08071880787611008\n",
            "Train step - Step 2130, Loss 0.09317087382078171\n",
            "Train step - Step 2140, Loss 0.08166628330945969\n",
            "Train epoch - Accuracy: 0.9143884892086331 Loss: 0.08339160346941982 Corrects: 5218\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.08767753094434738\n",
            "Train step - Step 2160, Loss 0.08057203143835068\n",
            "Train step - Step 2170, Loss 0.08257921785116196\n",
            "Train step - Step 2180, Loss 0.08701374381780624\n",
            "Train step - Step 2190, Loss 0.08664591610431671\n",
            "Train epoch - Accuracy: 0.9066187050359712 Loss: 0.08346441656565495 Corrects: 5166\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.08032957464456558\n",
            "Train step - Step 2210, Loss 0.08465868979692459\n",
            "Train step - Step 2220, Loss 0.07792582362890244\n",
            "Train step - Step 2230, Loss 0.07487418502569199\n",
            "Train step - Step 2240, Loss 0.07128167152404785\n",
            "Train step - Step 2250, Loss 0.08554165810346603\n",
            "Train epoch - Accuracy: 0.9165467625899281 Loss: 0.08130588210958371 Corrects: 5273\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.08809053897857666\n",
            "Train step - Step 2270, Loss 0.07704170793294907\n",
            "Train step - Step 2280, Loss 0.08174679428339005\n",
            "Train step - Step 2290, Loss 0.08076333999633789\n",
            "Train step - Step 2300, Loss 0.0864299014210701\n",
            "Train epoch - Accuracy: 0.9096402877697841 Loss: 0.08222129150045862 Corrects: 5310\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.0804247036576271\n",
            "Train step - Step 2320, Loss 0.08530741184949875\n",
            "Train step - Step 2330, Loss 0.08728205412626266\n",
            "Train step - Step 2340, Loss 0.08973328024148941\n",
            "Train step - Step 2350, Loss 0.0780038982629776\n",
            "Train step - Step 2360, Loss 0.08125681430101395\n",
            "Train epoch - Accuracy: 0.9133812949640288 Loss: 0.08114717252391705 Corrects: 5308\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.08015783876180649\n",
            "Train step - Step 2380, Loss 0.09210287034511566\n",
            "Train step - Step 2390, Loss 0.0705195814371109\n",
            "Train step - Step 2400, Loss 0.0754726231098175\n",
            "Train step - Step 2410, Loss 0.07951049506664276\n",
            "Train epoch - Accuracy: 0.9151079136690647 Loss: 0.0806982881495421 Corrects: 5358\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.07098525017499924\n",
            "Train step - Step 2430, Loss 0.08567839860916138\n",
            "Train step - Step 2440, Loss 0.07457418739795685\n",
            "Train step - Step 2450, Loss 0.08303363621234894\n",
            "Train step - Step 2460, Loss 0.0809204950928688\n",
            "Train step - Step 2470, Loss 0.08345627784729004\n",
            "Train epoch - Accuracy: 0.9156834532374101 Loss: 0.08183565137412051 Corrects: 5291\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.0750420093536377\n",
            "Train step - Step 2490, Loss 0.08222158998250961\n",
            "Train step - Step 2500, Loss 0.07892324030399323\n",
            "Train step - Step 2510, Loss 0.08077090233564377\n",
            "Train step - Step 2520, Loss 0.07862215489149094\n",
            "Train epoch - Accuracy: 0.920863309352518 Loss: 0.0804478059869876 Corrects: 5353\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.07582611590623856\n",
            "Train step - Step 2540, Loss 0.08909783512353897\n",
            "Train step - Step 2550, Loss 0.08206717669963837\n",
            "Train step - Step 2560, Loss 0.08126766234636307\n",
            "Train step - Step 2570, Loss 0.07855264097452164\n",
            "Train step - Step 2580, Loss 0.08122061938047409\n",
            "Train epoch - Accuracy: 0.9189928057553957 Loss: 0.07993441841585174 Corrects: 5401\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.07984977960586548\n",
            "Train step - Step 2600, Loss 0.07247669249773026\n",
            "Train step - Step 2610, Loss 0.06509585678577423\n",
            "Train step - Step 2620, Loss 0.07313039153814316\n",
            "Train step - Step 2630, Loss 0.07970406860113144\n",
            "Train epoch - Accuracy: 0.9218705035971223 Loss: 0.08028933197474308 Corrects: 5375\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.0838751420378685\n",
            "Train step - Step 2650, Loss 0.0759241133928299\n",
            "Train step - Step 2660, Loss 0.07438012212514877\n",
            "Train step - Step 2670, Loss 0.08637256175279617\n",
            "Train step - Step 2680, Loss 0.07657237350940704\n",
            "Train step - Step 2690, Loss 0.08078847080469131\n",
            "Train epoch - Accuracy: 0.9194244604316547 Loss: 0.07830877823795346 Corrects: 5474\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.0677584707736969\n",
            "Train step - Step 2710, Loss 0.075705885887146\n",
            "Train step - Step 2720, Loss 0.06968232244253159\n",
            "Train step - Step 2730, Loss 0.06961432844400406\n",
            "Train step - Step 2740, Loss 0.073256716132164\n",
            "Train epoch - Accuracy: 0.9284892086330935 Loss: 0.07384972161955113 Corrects: 5600\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.07547511160373688\n",
            "Train step - Step 2760, Loss 0.07510822266340256\n",
            "Train step - Step 2770, Loss 0.06446193903684616\n",
            "Train step - Step 2780, Loss 0.06735425442457199\n",
            "Train step - Step 2790, Loss 0.08588508516550064\n",
            "Train step - Step 2800, Loss 0.07631555944681168\n",
            "Train epoch - Accuracy: 0.9346762589928057 Loss: 0.07167565816169162 Corrects: 5609\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.06268750131130219\n",
            "Train step - Step 2820, Loss 0.07991518825292587\n",
            "Train step - Step 2830, Loss 0.06452809274196625\n",
            "Train step - Step 2840, Loss 0.07022807747125626\n",
            "Train step - Step 2850, Loss 0.07226832211017609\n",
            "Train epoch - Accuracy: 0.9458992805755395 Loss: 0.06980154877407946 Corrects: 5641\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.06808673590421677\n",
            "Train step - Step 2870, Loss 0.0669170543551445\n",
            "Train step - Step 2880, Loss 0.06903699040412903\n",
            "Train step - Step 2890, Loss 0.06882738322019577\n",
            "Train step - Step 2900, Loss 0.07609628885984421\n",
            "Train step - Step 2910, Loss 0.07421897351741791\n",
            "Train epoch - Accuracy: 0.9362589928057554 Loss: 0.07032493793492695 Corrects: 5637\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.07155109196901321\n",
            "Train step - Step 2930, Loss 0.07279249280691147\n",
            "Train step - Step 2940, Loss 0.06823454052209854\n",
            "Train step - Step 2950, Loss 0.07043514400720596\n",
            "Train step - Step 2960, Loss 0.07333838939666748\n",
            "Train epoch - Accuracy: 0.9424460431654677 Loss: 0.0697903840061572 Corrects: 5683\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.06546493619680405\n",
            "Train step - Step 2980, Loss 0.06673426926136017\n",
            "Train step - Step 2990, Loss 0.061902452260255814\n",
            "Train step - Step 3000, Loss 0.06868483871221542\n",
            "Train step - Step 3010, Loss 0.06753941625356674\n",
            "Train step - Step 3020, Loss 0.0675111934542656\n",
            "Train epoch - Accuracy: 0.9437410071942446 Loss: 0.06970707436688513 Corrects: 5686\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.08236676454544067\n",
            "Train step - Step 3040, Loss 0.06735413521528244\n",
            "Train step - Step 3050, Loss 0.062327075749635696\n",
            "Train step - Step 3060, Loss 0.07166611403226852\n",
            "Train step - Step 3070, Loss 0.08135131001472473\n",
            "Train epoch - Accuracy: 0.9392805755395683 Loss: 0.06945767539653847 Corrects: 5675\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.06764494627714157\n",
            "Train step - Step 3090, Loss 0.06828731298446655\n",
            "Train step - Step 3100, Loss 0.07257628440856934\n",
            "Train step - Step 3110, Loss 0.06917746365070343\n",
            "Train step - Step 3120, Loss 0.06705846637487411\n",
            "Train step - Step 3130, Loss 0.07756900042295456\n",
            "Train epoch - Accuracy: 0.9408633093525179 Loss: 0.06916789855674016 Corrects: 5711\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.07205694913864136\n",
            "Train step - Step 3150, Loss 0.060764580965042114\n",
            "Train step - Step 3160, Loss 0.06929911673069\n",
            "Train step - Step 3170, Loss 0.07258902490139008\n",
            "Train step - Step 3180, Loss 0.07027377933263779\n",
            "Train epoch - Accuracy: 0.9443165467625899 Loss: 0.06877351867209236 Corrects: 5726\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.06543561071157455\n",
            "Train step - Step 3200, Loss 0.06681136786937714\n",
            "Train step - Step 3210, Loss 0.0649978443980217\n",
            "Train step - Step 3220, Loss 0.06837234646081924\n",
            "Train step - Step 3230, Loss 0.06402935832738876\n",
            "Train step - Step 3240, Loss 0.07621540129184723\n",
            "Train epoch - Accuracy: 0.9451798561151079 Loss: 0.06884849651897554 Corrects: 5696\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.07269330322742462\n",
            "Train step - Step 3260, Loss 0.06251183897256851\n",
            "Train step - Step 3270, Loss 0.07738302648067474\n",
            "Train step - Step 3280, Loss 0.07636144012212753\n",
            "Train step - Step 3290, Loss 0.0782107338309288\n",
            "Train epoch - Accuracy: 0.9451798561151079 Loss: 0.06959593646603522 Corrects: 5702\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.061691947281360626\n",
            "Train step - Step 3310, Loss 0.06344880908727646\n",
            "Train step - Step 3320, Loss 0.06788548082113266\n",
            "Train step - Step 3330, Loss 0.06338222324848175\n",
            "Train step - Step 3340, Loss 0.06723703444004059\n",
            "Train step - Step 3350, Loss 0.07540037482976913\n",
            "Train epoch - Accuracy: 0.9431654676258993 Loss: 0.06865214216194565 Corrects: 5718\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.06945093721151352\n",
            "Train step - Step 3370, Loss 0.06609045714139938\n",
            "Train step - Step 3380, Loss 0.06642892211675644\n",
            "Train step - Step 3390, Loss 0.07373305410146713\n",
            "Train step - Step 3400, Loss 0.06437788903713226\n",
            "Train epoch - Accuracy: 0.9435971223021583 Loss: 0.06872937124838932 Corrects: 5689\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.06768279522657394\n",
            "Train step - Step 3420, Loss 0.07101553678512573\n",
            "Train step - Step 3430, Loss 0.07380123436450958\n",
            "Train step - Step 3440, Loss 0.06760076433420181\n",
            "Train step - Step 3450, Loss 0.07720186561346054\n",
            "Train step - Step 3460, Loss 0.0697837546467781\n",
            "Train epoch - Accuracy: 0.9461870503597122 Loss: 0.06891588951400715 Corrects: 5715\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.06339257955551147\n",
            "Train step - Step 3480, Loss 0.07524116337299347\n",
            "Train step - Step 3490, Loss 0.06576047837734222\n",
            "Train step - Step 3500, Loss 0.06702102720737457\n",
            "Train step - Step 3510, Loss 0.06439776718616486\n",
            "Train epoch - Accuracy: 0.9479136690647482 Loss: 0.06754183055899984 Corrects: 5784\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.05878809839487076\n",
            "Train step - Step 3530, Loss 0.06593772768974304\n",
            "Train step - Step 3540, Loss 0.0595695786178112\n",
            "Train step - Step 3550, Loss 0.06635630130767822\n",
            "Train step - Step 3560, Loss 0.06672505289316177\n",
            "Train step - Step 3570, Loss 0.06666796654462814\n",
            "Train epoch - Accuracy: 0.9466187050359712 Loss: 0.06730071496406047 Corrects: 5759\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.06547164171934128\n",
            "Train step - Step 3590, Loss 0.06881364434957504\n",
            "Train step - Step 3600, Loss 0.06844448298215866\n",
            "Train step - Step 3610, Loss 0.06484246999025345\n",
            "Train step - Step 3620, Loss 0.059539973735809326\n",
            "Train epoch - Accuracy: 0.9437410071942446 Loss: 0.06686811508463442 Corrects: 5748\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.06518266350030899\n",
            "Train step - Step 3640, Loss 0.06615139544010162\n",
            "Train step - Step 3650, Loss 0.0635361522436142\n",
            "Train step - Step 3660, Loss 0.062106240540742874\n",
            "Train step - Step 3670, Loss 0.06663314998149872\n",
            "Train step - Step 3680, Loss 0.06507553905248642\n",
            "Train epoch - Accuracy: 0.9461870503597122 Loss: 0.06687802926241923 Corrects: 5814\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.06121518835425377\n",
            "Train step - Step 3700, Loss 0.07057400792837143\n",
            "Train step - Step 3710, Loss 0.06595613062381744\n",
            "Train step - Step 3720, Loss 0.07121067494153976\n",
            "Train step - Step 3730, Loss 0.07074454426765442\n",
            "Train epoch - Accuracy: 0.9444604316546763 Loss: 0.06756015016235035 Corrects: 5809\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.0720551535487175\n",
            "Train step - Step 3750, Loss 0.06325965374708176\n",
            "Train step - Step 3760, Loss 0.06345977634191513\n",
            "Train step - Step 3770, Loss 0.06761999428272247\n",
            "Train step - Step 3780, Loss 0.06840270012617111\n",
            "Train step - Step 3790, Loss 0.06186717748641968\n",
            "Train epoch - Accuracy: 0.9418705035971223 Loss: 0.06732885898874819 Corrects: 5767\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.05987396463751793\n",
            "Train step - Step 3810, Loss 0.06298452615737915\n",
            "Train step - Step 3820, Loss 0.06900646537542343\n",
            "Train step - Step 3830, Loss 0.06476882100105286\n",
            "Train step - Step 3840, Loss 0.07348614186048508\n",
            "Train epoch - Accuracy: 0.9446043165467626 Loss: 0.06764037425783899 Corrects: 5789\n",
            "Training finished in 442.1255216598511 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16]\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  100\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96d316dd0>\n",
            "Constructing exemplars of class 79\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [42535, 31030, 17351, 21411, 3813, 5507, 18041, 37186, 18651, 42104, 42516, 21744, 7571, 20588, 26446, 34348, 30501, 21825, 36864, 9378, 44087, 30339, 34350, 2104, 21744, 32653, 12495, 26221, 26421, 29031, 40923, 34348, 38175, 48539, 7571, 11692, 26456, 18210, 47414, 20711, 16514, 47453, 37891, 36984, 4237, 6720, 11087, 29964, 31950, 37280, 20711, 44802, 31788, 36672, 3086, 3333, 14467, 26221, 5485, 26236, 19003, 13402, 9152, 36036, 20486, 15717, 48101, 13365, 37142, 29642, 17492, 45618, 38711, 49842, 28204, 37186, 32267, 6490, 46786, 43923, 7758, 6513, 45693, 7775, 29356, 4827, 18210, 24351, 34453, 31489, 9378, 31950, 19875, 40529, 43154, 41497, 13411, 10631, 17337, 232]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96d731910>\n",
            "Constructing exemplars of class 47\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [39661, 1121, 10124, 17620, 45705, 22286, 17087, 1006, 37848, 28400, 5901, 7059, 43269, 36974, 22915, 29308, 42525, 49526, 34809, 48495, 1737, 10428, 9817, 25611, 7121, 12758, 26494, 42078, 35554, 47672, 7361, 38500, 18966, 31137, 47283, 5276, 47460, 48147, 2278, 36759, 6318, 32645, 40579, 9555, 27674, 43237, 14110, 48716, 39354, 28065, 8193, 43756, 6515, 14039, 44062, 21096, 23407, 28836, 25313, 33014, 40579, 21654, 4240, 14964, 35823, 49096, 48254, 25919, 12721, 33001, 5363, 29535, 8193, 38867, 28346, 29308, 29023, 38946, 10799, 23074, 12243, 28400, 1670, 16654, 36230, 19882, 41117, 1683, 31892, 8844, 32293, 36178, 15326, 47992, 29035, 3281, 14296, 47460, 5276, 31137]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cebab10>\n",
            "Constructing exemplars of class 7\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [14903, 38132, 45984, 961, 3723, 28111, 3963, 20361, 19432, 26593, 20561, 41370, 1054, 30061, 49282, 46952, 36663, 30918, 31307, 38408, 45809, 24582, 31829, 10729, 31942, 35498, 15243, 9229, 47435, 34181, 27520, 4966, 20596, 8444, 3239, 23752, 21694, 42172, 13297, 18441, 35159, 14701, 31764, 42099, 36995, 43273, 23198, 8444, 21694, 40006, 47101, 1321, 37445, 31399, 9862, 10407, 46130, 20181, 26443, 45534, 17791, 16537, 7672, 7938, 31101, 23393, 24755, 15202, 43315, 23354, 8368, 805, 309, 49808, 18920, 19272, 42898, 27726, 9779, 20569, 28243, 3963, 36892, 35498, 10190, 15852, 12202, 44445, 40099, 42898, 11952, 43273, 377, 25231, 34424, 29867, 30796, 30814, 21893, 37071]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb974a3a850>\n",
            "Constructing exemplars of class 82\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [20923, 39822, 4419, 43429, 26471, 41778, 7959, 32944, 34267, 35285, 29348, 11501, 22254, 43699, 21695, 35369, 42485, 34608, 21991, 44260, 26553, 45482, 41719, 13309, 11084, 10283, 26965, 25724, 36690, 22835, 13204, 35336, 7586, 31815, 47957, 23681, 33522, 24178, 42901, 15141, 47737, 3438, 24715, 5275, 47737, 20824, 8518, 34673, 22254, 4019, 25615, 8099, 10923, 36727, 42693, 39264, 44523, 24715, 5275, 47737, 7959, 18937, 45757, 11296, 34267, 33477, 36690, 4019, 25615, 26921, 10283, 39637, 12210, 26976, 7442, 11470, 43081, 42656, 36688, 15403, 9891, 8331, 26852, 45366, 33522, 8170, 19830, 25724, 25997, 28254, 15304, 32847, 49368, 8207, 15808, 8170, 35967, 43442, 44112, 1919]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96ccb58d0>\n",
            "Constructing exemplars of class 34\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [3680, 40019, 34833, 28935, 41853, 11424, 43839, 13074, 4001, 35024, 33386, 36943, 46674, 13758, 32593, 34288, 1076, 19311, 40584, 34877, 48745, 10153, 26427, 14829, 43149, 27664, 32593, 27418, 43839, 14233, 34833, 4681, 44729, 11646, 49724, 36335, 40438, 15165, 439, 24929, 35524, 15314, 26238, 6560, 5124, 40779, 6849, 28939, 21112, 28672, 27731, 41170, 33555, 8824, 12039, 23687, 8643, 42579, 39585, 47152, 17247, 4875, 35397, 21112, 49918, 13673, 14859, 31750, 4292, 10238, 34833, 22490, 15972, 48806, 1076, 12039, 42036, 43297, 10565, 33411, 41093, 46674, 18745, 48048, 27932, 33876, 24499, 35836, 34859, 38168, 11594, 21803, 20461, 32901, 43775, 29936, 34288, 17613, 6782, 22548]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96ccd2e10>\n",
            "Constructing exemplars of class 81\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [40881, 18735, 38594, 42524, 7023, 30059, 37290, 25384, 27886, 1412, 40369, 38668, 19211, 45585, 18325, 21919, 21965, 48255, 22025, 4538, 21025, 47627, 2596, 20967, 13258, 2377, 21151, 40965, 11247, 37252, 1395, 22966, 46294, 24135, 7612, 10042, 49232, 48948, 18774, 13060, 13364, 9593, 48004, 19430, 36497, 34515, 47627, 6337, 18363, 43537, 7428, 10180, 783, 2841, 3401, 20278, 9106, 22692, 31727, 783, 37364, 975, 42926, 10584, 26308, 6209, 45865, 34460, 20400, 39373, 15252, 8321, 14600, 31466, 33201, 38079, 21151, 12195, 32141, 41707, 16857, 19916, 24481, 20504, 29951, 21829, 40563, 13364, 12638, 15119, 7428, 33718, 22191, 24756, 9956, 11575, 6946, 39868, 43513, 43892]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cdbc190>\n",
            "Constructing exemplars of class 21\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [39490, 8700, 46014, 12042, 33008, 42679, 24657, 22884, 23580, 25276, 11210, 23567, 17598, 7680, 45054, 26004, 41772, 33967, 354, 46772, 42653, 29113, 4825, 29463, 27947, 32397, 43531, 28523, 27185, 47810, 15584, 25309, 412, 25124, 11483, 40977, 42101, 40270, 49881, 39786, 32762, 27211, 26790, 44550, 21758, 18137, 34870, 12294, 5943, 5596, 15103, 10726, 1702, 24902, 29589, 15837, 30797, 40466, 30372, 15071, 39563, 36592, 8922, 42479, 13623, 20131, 44201, 35934, 47810, 23144, 28247, 26684, 11483, 45356, 2031, 5596, 6133, 49739, 13358, 46521, 7159, 5029, 7224, 48854, 28124, 37667, 40052, 19919, 45457, 45054, 17038, 37046, 42053, 16783, 34948, 12294, 412, 27118, 24295, 4810]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cd39cd0>\n",
            "Constructing exemplars of class 80\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [49861, 44869, 10443, 36436, 40095, 37298, 24684, 1512, 40626, 1646, 44580, 20818, 39365, 29481, 47287, 20882, 36203, 30605, 31804, 4501, 31195, 12061, 2938, 25422, 1081, 7741, 15026, 15116, 12901, 15974, 5728, 8094, 49167, 19341, 25263, 27586, 33998, 41737, 1090, 10210, 11768, 9935, 12901, 28931, 8777, 6113, 38774, 28030, 36436, 25618, 10646, 34553, 3429, 22449, 12061, 41326, 2385, 43611, 31244, 1223, 30532, 42529, 32387, 3924, 23436, 40307, 32587, 6047, 46381, 48899, 22336, 39805, 361, 45030, 20495, 23564, 48557, 35914, 29400, 39933, 36216, 7599, 39480, 37907, 6047, 22633, 43266, 45998, 29481, 21105, 31266, 26008, 9844, 8777, 42990, 44580, 20701, 39656, 10514, 3319]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cdd0390>\n",
            "Constructing exemplars of class 68\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [37785, 22527, 36824, 24020, 20363, 16762, 13044, 40312, 14793, 13212, 12280, 12826, 1831, 30336, 6778, 23007, 3170, 8710, 28636, 20014, 17132, 34368, 19149, 16427, 22591, 39862, 12280, 8660, 8951, 29282, 43955, 20739, 34184, 14511, 33068, 19950, 14068, 17859, 47885, 10673, 4342, 34470, 895, 10601, 37377, 16146, 34184, 7073, 47710, 39776, 48550, 9591, 21392, 40926, 8660, 24365, 33864, 31919, 42352, 25904, 40017, 33068, 7073, 45199, 43823, 30478, 36461, 48311, 25899, 49464, 40793, 42651, 41709, 45997, 28797, 17614, 35991, 11153, 32181, 1245, 15046, 15345, 31044, 26591, 30834, 13154, 19543, 21729, 32740, 45020, 8412, 36099, 39098, 49932, 14511, 28636, 454, 36703, 13299, 9030]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96ced1a10>\n",
            "Constructing exemplars of class 16\n",
            "lunghezza exemplar set:  100\n",
            "exemplar set:  [30251, 11817, 37388, 23203, 12350, 38638, 28120, 26760, 3948, 8028, 29901, 48588, 30078, 5810, 24151, 20703, 10611, 22084, 47194, 35299, 19760, 22464, 34239, 17812, 45065, 44758, 28374, 26475, 3742, 14425, 6600, 37520, 21461, 20087, 38554, 46580, 8890, 22297, 29271, 22451, 33914, 35299, 19928, 17899, 27976, 47194, 16145, 6304, 44130, 9711, 16732, 19025, 36714, 37217, 573, 48582, 20761, 28715, 32487, 25541, 2616, 6178, 30215, 21588, 47233, 37286, 2676, 9166, 41919, 6965, 39201, 41484, 25135, 25870, 48588, 44376, 16704, 40311, 23301, 10793, 18572, 9075, 13725, 23965, 27470, 3350, 37111, 17743, 20797, 41382, 21953, 16590, 6409, 20465, 31620, 48929, 6178, 36421, 8157, 20549]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.9 0.1533845216035843\n",
            "TEST GROUP:  0.928\n",
            "TEST ALL:  0.712\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  3000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [81, 79, 4, 10, 16, 18, 20, 22, 24, 32, 34, 56, 64, 68, 76, 80, 82, 90, 7, 21, 23, 39, 47, 49, 59, 61, 65, 67, 75, 0]\n",
            "TRAIN_SET CLASSES:  [75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "VALIDATION CLASSES:  [61, 32, 90, 24, 23, 76, 75, 10, 0, 64]\n",
            "GROUP:  3\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  30\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.3846653401851654\n",
            "Train step - Step 10, Loss 0.1450684517621994\n",
            "Train step - Step 20, Loss 0.15228049457073212\n",
            "Train step - Step 30, Loss 0.13677839934825897\n",
            "Train step - Step 40, Loss 0.13149291276931763\n",
            "Train step - Step 50, Loss 0.12393473833799362\n",
            "Train epoch - Accuracy: 0.6899280575539568 Loss: 0.15670299451556996 Corrects: 1932\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12045542150735855\n",
            "Train step - Step 70, Loss 0.1105114072561264\n",
            "Train step - Step 80, Loss 0.10607095807790756\n",
            "Train step - Step 90, Loss 0.11844657361507416\n",
            "Train step - Step 100, Loss 0.10735305398702621\n",
            "Train epoch - Accuracy: 0.7046043165467626 Loss: 0.1191634808620103 Corrects: 2308\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.1146884486079216\n",
            "Train step - Step 120, Loss 0.12139478325843811\n",
            "Train step - Step 130, Loss 0.111526720225811\n",
            "Train step - Step 140, Loss 0.11826331168413162\n",
            "Train step - Step 150, Loss 0.11779379099607468\n",
            "Train step - Step 160, Loss 0.1093992367386818\n",
            "Train epoch - Accuracy: 0.7105035971223022 Loss: 0.11425889390192444 Corrects: 2460\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.1192820742726326\n",
            "Train step - Step 180, Loss 0.11374711990356445\n",
            "Train step - Step 190, Loss 0.11241795867681503\n",
            "Train step - Step 200, Loss 0.10570447891950607\n",
            "Train step - Step 210, Loss 0.10995011776685715\n",
            "Train epoch - Accuracy: 0.7152517985611511 Loss: 0.11196015194594432 Corrects: 2749\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.10684698820114136\n",
            "Train step - Step 230, Loss 0.10704987496137619\n",
            "Train step - Step 240, Loss 0.10521380603313446\n",
            "Train step - Step 250, Loss 0.11054693162441254\n",
            "Train step - Step 260, Loss 0.10511048138141632\n",
            "Train step - Step 270, Loss 0.11016801744699478\n",
            "Train epoch - Accuracy: 0.7279136690647482 Loss: 0.11049375099243877 Corrects: 2900\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.11652529239654541\n",
            "Train step - Step 290, Loss 0.10489818453788757\n",
            "Train step - Step 300, Loss 0.11018060892820358\n",
            "Train step - Step 310, Loss 0.10766603797674179\n",
            "Train step - Step 320, Loss 0.11271870881319046\n",
            "Train epoch - Accuracy: 0.74 Loss: 0.10890195249653549 Corrects: 3031\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11493059247732162\n",
            "Train step - Step 340, Loss 0.10515302419662476\n",
            "Train step - Step 350, Loss 0.09904475510120392\n",
            "Train step - Step 360, Loss 0.1075025200843811\n",
            "Train step - Step 370, Loss 0.10597210377454758\n",
            "Train step - Step 380, Loss 0.10901432484388351\n",
            "Train epoch - Accuracy: 0.7418705035971223 Loss: 0.10776115166626388 Corrects: 3141\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.10969241708517075\n",
            "Train step - Step 400, Loss 0.1086372584104538\n",
            "Train step - Step 410, Loss 0.10266637802124023\n",
            "Train step - Step 420, Loss 0.10226792842149734\n",
            "Train step - Step 430, Loss 0.11812062561511993\n",
            "Train epoch - Accuracy: 0.7394244604316547 Loss: 0.10671728612707673 Corrects: 3249\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10497238487005234\n",
            "Train step - Step 450, Loss 0.10611435770988464\n",
            "Train step - Step 460, Loss 0.09376516938209534\n",
            "Train step - Step 470, Loss 0.10382652282714844\n",
            "Train step - Step 480, Loss 0.1104901134967804\n",
            "Train step - Step 490, Loss 0.10409184545278549\n",
            "Train epoch - Accuracy: 0.7471942446043165 Loss: 0.1061454516434841 Corrects: 3319\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.10980655252933502\n",
            "Train step - Step 510, Loss 0.10671667009592056\n",
            "Train step - Step 520, Loss 0.10343528538942337\n",
            "Train step - Step 530, Loss 0.10539216548204422\n",
            "Train step - Step 540, Loss 0.09954217076301575\n",
            "Train epoch - Accuracy: 0.7551079136690647 Loss: 0.10466810740584093 Corrects: 3442\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.10136996954679489\n",
            "Train step - Step 560, Loss 0.10661538690328598\n",
            "Train step - Step 570, Loss 0.10504375398159027\n",
            "Train step - Step 580, Loss 0.1082836464047432\n",
            "Train step - Step 590, Loss 0.10060853511095047\n",
            "Train step - Step 600, Loss 0.10186148434877396\n",
            "Train epoch - Accuracy: 0.7542446043165467 Loss: 0.10405024411866991 Corrects: 3473\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.10644602030515671\n",
            "Train step - Step 620, Loss 0.10736733675003052\n",
            "Train step - Step 630, Loss 0.10690145939588547\n",
            "Train step - Step 640, Loss 0.1168069839477539\n",
            "Train step - Step 650, Loss 0.10042976588010788\n",
            "Train epoch - Accuracy: 0.75568345323741 Loss: 0.10296095028841239 Corrects: 3543\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10281194001436234\n",
            "Train step - Step 670, Loss 0.10118823498487473\n",
            "Train step - Step 680, Loss 0.1082841157913208\n",
            "Train step - Step 690, Loss 0.10797936469316483\n",
            "Train step - Step 700, Loss 0.09998300671577454\n",
            "Train step - Step 710, Loss 0.09254755079746246\n",
            "Train epoch - Accuracy: 0.7623021582733813 Loss: 0.10315881356275339 Corrects: 3660\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10175521671772003\n",
            "Train step - Step 730, Loss 0.10408187657594681\n",
            "Train step - Step 740, Loss 0.11232264339923859\n",
            "Train step - Step 750, Loss 0.10226178914308548\n",
            "Train step - Step 760, Loss 0.09679199010133743\n",
            "Train epoch - Accuracy: 0.7631654676258993 Loss: 0.10240903304420787 Corrects: 3664\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10656853020191193\n",
            "Train step - Step 780, Loss 0.0963171198964119\n",
            "Train step - Step 790, Loss 0.10717621445655823\n",
            "Train step - Step 800, Loss 0.09858091920614243\n",
            "Train step - Step 810, Loss 0.09644339233636856\n",
            "Train step - Step 820, Loss 0.10240669548511505\n",
            "Train epoch - Accuracy: 0.7614388489208633 Loss: 0.10225033405444604 Corrects: 3694\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.10473312437534332\n",
            "Train step - Step 840, Loss 0.10290583223104477\n",
            "Train step - Step 850, Loss 0.10309308022260666\n",
            "Train step - Step 860, Loss 0.10995156317949295\n",
            "Train step - Step 870, Loss 0.09877729415893555\n",
            "Train epoch - Accuracy: 0.7717985611510791 Loss: 0.10142589792502012 Corrects: 3790\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.10446568578481674\n",
            "Train step - Step 890, Loss 0.10131040960550308\n",
            "Train step - Step 900, Loss 0.09682722389698029\n",
            "Train step - Step 910, Loss 0.09185279905796051\n",
            "Train step - Step 920, Loss 0.10704629123210907\n",
            "Train step - Step 930, Loss 0.10528639703989029\n",
            "Train epoch - Accuracy: 0.7762589928057554 Loss: 0.09988861868278585 Corrects: 3862\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.10786179453134537\n",
            "Train step - Step 950, Loss 0.10437612980604172\n",
            "Train step - Step 960, Loss 0.09561403840780258\n",
            "Train step - Step 970, Loss 0.09594950824975967\n",
            "Train step - Step 980, Loss 0.10203056782484055\n",
            "Train epoch - Accuracy: 0.7748201438848921 Loss: 0.10050102031917023 Corrects: 3898\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.10001175105571747\n",
            "Train step - Step 1000, Loss 0.09822896867990494\n",
            "Train step - Step 1010, Loss 0.09856801480054855\n",
            "Train step - Step 1020, Loss 0.09555689990520477\n",
            "Train step - Step 1030, Loss 0.10819580405950546\n",
            "Train step - Step 1040, Loss 0.09435253590345383\n",
            "Train epoch - Accuracy: 0.7861870503597123 Loss: 0.09903521612608175 Corrects: 3999\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.10289093852043152\n",
            "Train step - Step 1060, Loss 0.10064354538917542\n",
            "Train step - Step 1070, Loss 0.10485077649354935\n",
            "Train step - Step 1080, Loss 0.09286388009786606\n",
            "Train step - Step 1090, Loss 0.10066413134336472\n",
            "Train epoch - Accuracy: 0.7840287769784172 Loss: 0.09895683044581105 Corrects: 3953\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10564889013767242\n",
            "Train step - Step 1110, Loss 0.0948813408613205\n",
            "Train step - Step 1120, Loss 0.10378748923540115\n",
            "Train step - Step 1130, Loss 0.09167797118425369\n",
            "Train step - Step 1140, Loss 0.11053922027349472\n",
            "Train step - Step 1150, Loss 0.10465242713689804\n",
            "Train epoch - Accuracy: 0.782589928057554 Loss: 0.09848914994824705 Corrects: 4026\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10184039175510406\n",
            "Train step - Step 1170, Loss 0.09679556638002396\n",
            "Train step - Step 1180, Loss 0.10784845054149628\n",
            "Train step - Step 1190, Loss 0.09956850856542587\n",
            "Train step - Step 1200, Loss 0.0954941138625145\n",
            "Train epoch - Accuracy: 0.7943884892086331 Loss: 0.0978311903185124 Corrects: 4074\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.09285583347082138\n",
            "Train step - Step 1220, Loss 0.09247567504644394\n",
            "Train step - Step 1230, Loss 0.0895221084356308\n",
            "Train step - Step 1240, Loss 0.10531531274318695\n",
            "Train step - Step 1250, Loss 0.10420569032430649\n",
            "Train step - Step 1260, Loss 0.09644374996423721\n",
            "Train epoch - Accuracy: 0.797841726618705 Loss: 0.09770521672723963 Corrects: 4124\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.10033229738473892\n",
            "Train step - Step 1280, Loss 0.1011989489197731\n",
            "Train step - Step 1290, Loss 0.09431327134370804\n",
            "Train step - Step 1300, Loss 0.09936966001987457\n",
            "Train step - Step 1310, Loss 0.10109502822160721\n",
            "Train epoch - Accuracy: 0.8014388489208633 Loss: 0.09769449109224965 Corrects: 4138\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.10715793818235397\n",
            "Train step - Step 1330, Loss 0.09358934313058853\n",
            "Train step - Step 1340, Loss 0.0994206890463829\n",
            "Train step - Step 1350, Loss 0.09755831211805344\n",
            "Train step - Step 1360, Loss 0.09140674769878387\n",
            "Train step - Step 1370, Loss 0.09237226843833923\n",
            "Train epoch - Accuracy: 0.8031654676258992 Loss: 0.09724083182623061 Corrects: 4161\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.09906797856092453\n",
            "Train step - Step 1390, Loss 0.09507527947425842\n",
            "Train step - Step 1400, Loss 0.09218184649944305\n",
            "Train step - Step 1410, Loss 0.09650599956512451\n",
            "Train step - Step 1420, Loss 0.10492520779371262\n",
            "Train epoch - Accuracy: 0.7953956834532374 Loss: 0.09620857416297035 Corrects: 4186\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.09351741522550583\n",
            "Train step - Step 1440, Loss 0.09777852892875671\n",
            "Train step - Step 1450, Loss 0.0963672399520874\n",
            "Train step - Step 1460, Loss 0.10119880735874176\n",
            "Train step - Step 1470, Loss 0.10024663805961609\n",
            "Train step - Step 1480, Loss 0.09654643386602402\n",
            "Train epoch - Accuracy: 0.796978417266187 Loss: 0.0966861418425608 Corrects: 4222\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.09740082919597626\n",
            "Train step - Step 1500, Loss 0.09801936894655228\n",
            "Train step - Step 1510, Loss 0.09915919601917267\n",
            "Train step - Step 1520, Loss 0.0948297530412674\n",
            "Train step - Step 1530, Loss 0.09342066943645477\n",
            "Train epoch - Accuracy: 0.8023021582733812 Loss: 0.09677051924115462 Corrects: 4305\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.09510485827922821\n",
            "Train step - Step 1550, Loss 0.10403388738632202\n",
            "Train step - Step 1560, Loss 0.09417694061994553\n",
            "Train step - Step 1570, Loss 0.10799847543239594\n",
            "Train step - Step 1580, Loss 0.0951734408736229\n",
            "Train step - Step 1590, Loss 0.0946204885840416\n",
            "Train epoch - Accuracy: 0.8090647482014388 Loss: 0.09594779034527086 Corrects: 4331\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.09965574741363525\n",
            "Train step - Step 1610, Loss 0.0960303544998169\n",
            "Train step - Step 1620, Loss 0.09530998021364212\n",
            "Train step - Step 1630, Loss 0.09606163203716278\n",
            "Train step - Step 1640, Loss 0.09029335528612137\n",
            "Train epoch - Accuracy: 0.8054676258992806 Loss: 0.095226691860518 Corrects: 4357\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.09097375720739365\n",
            "Train step - Step 1660, Loss 0.10374206304550171\n",
            "Train step - Step 1670, Loss 0.09745883196592331\n",
            "Train step - Step 1680, Loss 0.09518866240978241\n",
            "Train step - Step 1690, Loss 0.10144491493701935\n",
            "Train step - Step 1700, Loss 0.09712940454483032\n",
            "Train epoch - Accuracy: 0.8156834532374101 Loss: 0.09426912511424194 Corrects: 4381\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.07724220305681229\n",
            "Train step - Step 1720, Loss 0.09814298152923584\n",
            "Train step - Step 1730, Loss 0.09058493375778198\n",
            "Train step - Step 1740, Loss 0.09632150828838348\n",
            "Train step - Step 1750, Loss 0.08713524788618088\n",
            "Train epoch - Accuracy: 0.8110791366906475 Loss: 0.09464759619544735 Corrects: 4382\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.09020301699638367\n",
            "Train step - Step 1770, Loss 0.09422875940799713\n",
            "Train step - Step 1780, Loss 0.09639712423086166\n",
            "Train step - Step 1790, Loss 0.08653464913368225\n",
            "Train step - Step 1800, Loss 0.09251286834478378\n",
            "Train step - Step 1810, Loss 0.09113632887601852\n",
            "Train epoch - Accuracy: 0.8235971223021583 Loss: 0.09444891896179254 Corrects: 4463\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.10171860456466675\n",
            "Train step - Step 1830, Loss 0.0888073518872261\n",
            "Train step - Step 1840, Loss 0.08537252992391586\n",
            "Train step - Step 1850, Loss 0.09386765956878662\n",
            "Train step - Step 1860, Loss 0.09717041999101639\n",
            "Train epoch - Accuracy: 0.8188489208633094 Loss: 0.09330220369126299 Corrects: 4475\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.0940612331032753\n",
            "Train step - Step 1880, Loss 0.0913662239909172\n",
            "Train step - Step 1890, Loss 0.09613784402608871\n",
            "Train step - Step 1900, Loss 0.08594653010368347\n",
            "Train step - Step 1910, Loss 0.09804730117321014\n",
            "Train step - Step 1920, Loss 0.09125667065382004\n",
            "Train epoch - Accuracy: 0.8175539568345324 Loss: 0.09275757707923436 Corrects: 4514\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.09677878022193909\n",
            "Train step - Step 1940, Loss 0.0908399149775505\n",
            "Train step - Step 1950, Loss 0.09037716686725616\n",
            "Train step - Step 1960, Loss 0.09416501224040985\n",
            "Train step - Step 1970, Loss 0.09217698127031326\n",
            "Train epoch - Accuracy: 0.8250359712230216 Loss: 0.09352317070360663 Corrects: 4567\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.09083142131567001\n",
            "Train step - Step 1990, Loss 0.08952715247869492\n",
            "Train step - Step 2000, Loss 0.09140972048044205\n",
            "Train step - Step 2010, Loss 0.0936424657702446\n",
            "Train step - Step 2020, Loss 0.08901949971914291\n",
            "Train step - Step 2030, Loss 0.0830497294664383\n",
            "Train epoch - Accuracy: 0.8264748201438848 Loss: 0.09275889792673879 Corrects: 4562\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.09683415293693542\n",
            "Train step - Step 2050, Loss 0.08966664224863052\n",
            "Train step - Step 2060, Loss 0.08899933099746704\n",
            "Train step - Step 2070, Loss 0.09505323320627213\n",
            "Train step - Step 2080, Loss 0.09722834080457687\n",
            "Train epoch - Accuracy: 0.8303597122302159 Loss: 0.09243279982813828 Corrects: 4580\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.09613365679979324\n",
            "Train step - Step 2100, Loss 0.0962015688419342\n",
            "Train step - Step 2110, Loss 0.08768615126609802\n",
            "Train step - Step 2120, Loss 0.09508015960454941\n",
            "Train step - Step 2130, Loss 0.1009371355175972\n",
            "Train step - Step 2140, Loss 0.08530370146036148\n",
            "Train epoch - Accuracy: 0.8282014388489208 Loss: 0.09197358742463503 Corrects: 4644\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.09207121282815933\n",
            "Train step - Step 2160, Loss 0.08920130133628845\n",
            "Train step - Step 2170, Loss 0.09103941172361374\n",
            "Train step - Step 2180, Loss 0.09763190895318985\n",
            "Train step - Step 2190, Loss 0.08546837419271469\n",
            "Train epoch - Accuracy: 0.8312230215827339 Loss: 0.09195957170759174 Corrects: 4630\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.0941610112786293\n",
            "Train step - Step 2210, Loss 0.08724714070558548\n",
            "Train step - Step 2220, Loss 0.08908674865961075\n",
            "Train step - Step 2230, Loss 0.0955904945731163\n",
            "Train step - Step 2240, Loss 0.08430782705545425\n",
            "Train step - Step 2250, Loss 0.09766533970832825\n",
            "Train epoch - Accuracy: 0.8310791366906475 Loss: 0.0918012124257122 Corrects: 4669\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.09422769397497177\n",
            "Train step - Step 2270, Loss 0.09768284112215042\n",
            "Train step - Step 2280, Loss 0.09290606528520584\n",
            "Train step - Step 2290, Loss 0.09053782373666763\n",
            "Train step - Step 2300, Loss 0.09546447545289993\n",
            "Train epoch - Accuracy: 0.836115107913669 Loss: 0.09169921414672041 Corrects: 4702\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.0891842246055603\n",
            "Train step - Step 2320, Loss 0.09282156080007553\n",
            "Train step - Step 2330, Loss 0.09548663347959518\n",
            "Train step - Step 2340, Loss 0.08951010555028915\n",
            "Train step - Step 2350, Loss 0.0935014933347702\n",
            "Train step - Step 2360, Loss 0.09183546155691147\n",
            "Train epoch - Accuracy: 0.8463309352517986 Loss: 0.09058978864186101 Corrects: 4708\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.09128474444150925\n",
            "Train step - Step 2380, Loss 0.0944410115480423\n",
            "Train step - Step 2390, Loss 0.09125387668609619\n",
            "Train step - Step 2400, Loss 0.08633780479431152\n",
            "Train step - Step 2410, Loss 0.0924985334277153\n",
            "Train epoch - Accuracy: 0.840863309352518 Loss: 0.09058060122050828 Corrects: 4774\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.0928611159324646\n",
            "Train step - Step 2430, Loss 0.08681097626686096\n",
            "Train step - Step 2440, Loss 0.09838036447763443\n",
            "Train step - Step 2450, Loss 0.0945514589548111\n",
            "Train step - Step 2460, Loss 0.0911717489361763\n",
            "Train step - Step 2470, Loss 0.08987337350845337\n",
            "Train epoch - Accuracy: 0.8474820143884892 Loss: 0.08996068109282487 Corrects: 4812\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.09765870124101639\n",
            "Train step - Step 2490, Loss 0.08597968518733978\n",
            "Train step - Step 2500, Loss 0.091509610414505\n",
            "Train step - Step 2510, Loss 0.08657623082399368\n",
            "Train step - Step 2520, Loss 0.09019868075847626\n",
            "Train epoch - Accuracy: 0.84 Loss: 0.09047968905606715 Corrects: 4795\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.08975113928318024\n",
            "Train step - Step 2540, Loss 0.09677978605031967\n",
            "Train step - Step 2550, Loss 0.08021768182516098\n",
            "Train step - Step 2560, Loss 0.08787008374929428\n",
            "Train step - Step 2570, Loss 0.09197864681482315\n",
            "Train step - Step 2580, Loss 0.09223712980747223\n",
            "Train epoch - Accuracy: 0.8461870503597122 Loss: 0.09019945920585729 Corrects: 4788\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.09090573340654373\n",
            "Train step - Step 2600, Loss 0.08685161173343658\n",
            "Train step - Step 2610, Loss 0.0920916423201561\n",
            "Train step - Step 2620, Loss 0.09750092029571533\n",
            "Train step - Step 2630, Loss 0.08363315463066101\n",
            "Train epoch - Accuracy: 0.8402877697841726 Loss: 0.0896000499691037 Corrects: 4834\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.08618507534265518\n",
            "Train step - Step 2650, Loss 0.08832709491252899\n",
            "Train step - Step 2660, Loss 0.08948695659637451\n",
            "Train step - Step 2670, Loss 0.08546823263168335\n",
            "Train step - Step 2680, Loss 0.08786419779062271\n",
            "Train step - Step 2690, Loss 0.08378816395998001\n",
            "Train epoch - Accuracy: 0.8490647482014388 Loss: 0.08896480126989831 Corrects: 4829\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.08829141408205032\n",
            "Train step - Step 2710, Loss 0.08296947926282883\n",
            "Train step - Step 2720, Loss 0.08924172818660736\n",
            "Train step - Step 2730, Loss 0.08476810902357101\n",
            "Train step - Step 2740, Loss 0.08062021434307098\n",
            "Train epoch - Accuracy: 0.8525179856115108 Loss: 0.08660245834923476 Corrects: 4944\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.08288819342851639\n",
            "Train step - Step 2760, Loss 0.08600146323442459\n",
            "Train step - Step 2770, Loss 0.07967504113912582\n",
            "Train step - Step 2780, Loss 0.08018405735492706\n",
            "Train step - Step 2790, Loss 0.08394278585910797\n",
            "Train step - Step 2800, Loss 0.081295445561409\n",
            "Train epoch - Accuracy: 0.8635971223021582 Loss: 0.08562212529799922 Corrects: 5002\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.08383683115243912\n",
            "Train step - Step 2820, Loss 0.08421926200389862\n",
            "Train step - Step 2830, Loss 0.08991947770118713\n",
            "Train step - Step 2840, Loss 0.08332838118076324\n",
            "Train step - Step 2850, Loss 0.07959441095590591\n",
            "Train epoch - Accuracy: 0.8637410071942446 Loss: 0.08578767143779521 Corrects: 4978\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.08633840084075928\n",
            "Train step - Step 2870, Loss 0.07887344807386398\n",
            "Train step - Step 2880, Loss 0.09443635493516922\n",
            "Train step - Step 2890, Loss 0.08765694499015808\n",
            "Train step - Step 2900, Loss 0.08384574949741364\n",
            "Train step - Step 2910, Loss 0.07910388708114624\n",
            "Train epoch - Accuracy: 0.8692086330935251 Loss: 0.0851059867473815 Corrects: 5003\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.08440558612346649\n",
            "Train step - Step 2930, Loss 0.08300904184579849\n",
            "Train step - Step 2940, Loss 0.09161124378442764\n",
            "Train step - Step 2950, Loss 0.08343212306499481\n",
            "Train step - Step 2960, Loss 0.08572003245353699\n",
            "Train epoch - Accuracy: 0.863453237410072 Loss: 0.0849718350343567 Corrects: 4997\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.09087467193603516\n",
            "Train step - Step 2980, Loss 0.08559111505746841\n",
            "Train step - Step 2990, Loss 0.08102377504110336\n",
            "Train step - Step 3000, Loss 0.0866449847817421\n",
            "Train step - Step 3010, Loss 0.08421319723129272\n",
            "Train step - Step 3020, Loss 0.07395721226930618\n",
            "Train epoch - Accuracy: 0.8637410071942446 Loss: 0.08501116855324602 Corrects: 5074\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.08524828404188156\n",
            "Train step - Step 3040, Loss 0.0842253565788269\n",
            "Train step - Step 3050, Loss 0.08783514052629471\n",
            "Train step - Step 3060, Loss 0.08187658339738846\n",
            "Train step - Step 3070, Loss 0.08630526065826416\n",
            "Train epoch - Accuracy: 0.8696402877697842 Loss: 0.08425521848441886 Corrects: 5028\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.08784846216440201\n",
            "Train step - Step 3090, Loss 0.08632520586252213\n",
            "Train step - Step 3100, Loss 0.08139855414628983\n",
            "Train step - Step 3110, Loss 0.0834374949336052\n",
            "Train step - Step 3120, Loss 0.08634596318006516\n",
            "Train step - Step 3130, Loss 0.08758136630058289\n",
            "Train epoch - Accuracy: 0.8628776978417266 Loss: 0.08472442984366589 Corrects: 5036\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.08652595430612564\n",
            "Train step - Step 3150, Loss 0.08286133408546448\n",
            "Train step - Step 3160, Loss 0.0825687125325203\n",
            "Train step - Step 3170, Loss 0.08199302107095718\n",
            "Train step - Step 3180, Loss 0.08421654999256134\n",
            "Train epoch - Accuracy: 0.865179856115108 Loss: 0.08462033649571508 Corrects: 5046\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.08588839322328568\n",
            "Train step - Step 3200, Loss 0.08392787724733353\n",
            "Train step - Step 3210, Loss 0.09083309024572372\n",
            "Train step - Step 3220, Loss 0.08693808317184448\n",
            "Train step - Step 3230, Loss 0.08208300918340683\n",
            "Train step - Step 3240, Loss 0.08814836293458939\n",
            "Train epoch - Accuracy: 0.8646043165467626 Loss: 0.08504874756867938 Corrects: 5029\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.08036813139915466\n",
            "Train step - Step 3260, Loss 0.08749669045209885\n",
            "Train step - Step 3270, Loss 0.08514095842838287\n",
            "Train step - Step 3280, Loss 0.08290672302246094\n",
            "Train step - Step 3290, Loss 0.08454474061727524\n",
            "Train epoch - Accuracy: 0.8614388489208633 Loss: 0.08459919900345288 Corrects: 5026\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.08838577568531036\n",
            "Train step - Step 3310, Loss 0.08504081517457962\n",
            "Train step - Step 3320, Loss 0.08526606112718582\n",
            "Train step - Step 3330, Loss 0.08476167917251587\n",
            "Train step - Step 3340, Loss 0.08845189958810806\n",
            "Train step - Step 3350, Loss 0.08950600028038025\n",
            "Train epoch - Accuracy: 0.86431654676259 Loss: 0.08497403925485748 Corrects: 5041\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.09047146141529083\n",
            "Train step - Step 3370, Loss 0.08989682793617249\n",
            "Train step - Step 3380, Loss 0.08402378112077713\n",
            "Train step - Step 3390, Loss 0.08673365414142609\n",
            "Train step - Step 3400, Loss 0.08038549870252609\n",
            "Train epoch - Accuracy: 0.8664748201438849 Loss: 0.08403951669982869 Corrects: 5106\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.08634886890649796\n",
            "Train step - Step 3420, Loss 0.08799728751182556\n",
            "Train step - Step 3430, Loss 0.07948580384254456\n",
            "Train step - Step 3440, Loss 0.08007704466581345\n",
            "Train step - Step 3450, Loss 0.08277302980422974\n",
            "Train step - Step 3460, Loss 0.08323697000741959\n",
            "Train epoch - Accuracy: 0.8692086330935251 Loss: 0.08436239725394215 Corrects: 5084\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.08375437557697296\n",
            "Train step - Step 3480, Loss 0.08480574935674667\n",
            "Train step - Step 3490, Loss 0.0842151939868927\n",
            "Train step - Step 3500, Loss 0.08059054613113403\n",
            "Train step - Step 3510, Loss 0.07969268411397934\n",
            "Train epoch - Accuracy: 0.8730935251798562 Loss: 0.08374641240929528 Corrects: 5051\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.09205381572246552\n",
            "Train step - Step 3530, Loss 0.07662969082593918\n",
            "Train step - Step 3540, Loss 0.08666625618934631\n",
            "Train step - Step 3550, Loss 0.08169194310903549\n",
            "Train step - Step 3560, Loss 0.08170382678508759\n",
            "Train step - Step 3570, Loss 0.08051559329032898\n",
            "Train epoch - Accuracy: 0.8682014388489209 Loss: 0.08318272015602468 Corrects: 5080\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.08425548672676086\n",
            "Train step - Step 3590, Loss 0.08441639691591263\n",
            "Train step - Step 3600, Loss 0.08321984857320786\n",
            "Train step - Step 3610, Loss 0.08477092534303665\n",
            "Train step - Step 3620, Loss 0.09166219085454941\n",
            "Train epoch - Accuracy: 0.8686330935251798 Loss: 0.08347911402047109 Corrects: 5120\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.07761464267969131\n",
            "Train step - Step 3640, Loss 0.09186136722564697\n",
            "Train step - Step 3650, Loss 0.08209384977817535\n",
            "Train step - Step 3660, Loss 0.08712388575077057\n",
            "Train step - Step 3670, Loss 0.0809502974152565\n",
            "Train step - Step 3680, Loss 0.08073017746210098\n",
            "Train epoch - Accuracy: 0.8733812949640288 Loss: 0.08349695726693106 Corrects: 5091\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.08410041034221649\n",
            "Train step - Step 3700, Loss 0.08515587449073792\n",
            "Train step - Step 3710, Loss 0.08050340414047241\n",
            "Train step - Step 3720, Loss 0.08430567383766174\n",
            "Train step - Step 3730, Loss 0.08989584445953369\n",
            "Train epoch - Accuracy: 0.8705035971223022 Loss: 0.08391798930631267 Corrects: 5094\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.08786600083112717\n",
            "Train step - Step 3750, Loss 0.08532831072807312\n",
            "Train step - Step 3760, Loss 0.0868954211473465\n",
            "Train step - Step 3770, Loss 0.07918920367956161\n",
            "Train step - Step 3780, Loss 0.08475690335035324\n",
            "Train step - Step 3790, Loss 0.08797158300876617\n",
            "Train epoch - Accuracy: 0.8738129496402878 Loss: 0.08341310239309888 Corrects: 5143\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.08743316680192947\n",
            "Train step - Step 3810, Loss 0.07908768206834793\n",
            "Train step - Step 3820, Loss 0.08402910083532333\n",
            "Train step - Step 3830, Loss 0.08730814605951309\n",
            "Train step - Step 3840, Loss 0.08986754715442657\n",
            "Train epoch - Accuracy: 0.8658992805755396 Loss: 0.08361501257196605 Corrects: 5108\n",
            "Training finished in 443.83919191360474 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0]\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  66\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96ccd2690>\n",
            "Constructing exemplars of class 75\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [38527, 22569, 45546, 43927, 33079, 12811, 31135, 5110, 30274, 48889, 30659, 42320, 44017, 3793, 17901, 45567, 23645, 28323, 35116, 26802, 33792, 43588, 37476, 7975, 41578, 29627, 17148, 17261, 41891, 31322, 24553, 33079, 42645, 38177, 19242, 23222, 48402, 34761, 31034, 4013, 49259, 41150, 17969, 46431, 49431, 6931, 23676, 37366, 30295, 45706, 25503, 49738, 9609, 41489, 47430, 15095, 28861, 959, 48308, 31536, 43588, 9048, 24380, 30895, 12573, 41475]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96ccb5710>\n",
            "Constructing exemplars of class 23\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [47419, 14234, 20602, 23524, 46262, 49951, 84, 46261, 4262, 35170, 19568, 46682, 21173, 2245, 26110, 10461, 14313, 8246, 18815, 14076, 9617, 29428, 17993, 6013, 39191, 34071, 46766, 28979, 24932, 40707, 18501, 15823, 20530, 13380, 48569, 31329, 16241, 28324, 15019, 7437, 42947, 29299, 37360, 20547, 26570, 20018, 42483, 20018, 8479, 29533, 47555, 1486, 18055, 15386, 28486, 35485, 40278, 13688, 1680, 30044, 17222, 42279, 12264, 14822, 28613, 534]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96ccb5ad0>\n",
            "Constructing exemplars of class 90\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [17130, 18494, 16389, 4214, 15550, 3452, 42010, 7934, 4025, 32719, 39041, 49099, 6941, 37862, 41336, 17938, 49007, 35260, 17921, 30027, 40679, 17731, 9383, 33582, 12387, 24426, 5638, 1603, 7728, 28622, 1108, 43282, 15415, 32694, 1171, 49468, 39736, 9687, 5179, 20835, 30870, 7934, 41467, 3205, 35260, 21263, 6566, 40943, 44892, 29515, 25941, 6577, 18494, 14375, 1744, 5638, 4725, 7953, 29404, 27850, 49007, 11327, 27319, 25738, 43825, 38878]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96ccb5190>\n",
            "Constructing exemplars of class 10\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [6427, 23324, 7459, 48795, 21994, 4972, 37486, 44752, 3198, 889, 48000, 4673, 22069, 41596, 23133, 8707, 22934, 45944, 48000, 24793, 27581, 7712, 1173, 10788, 47655, 26651, 17323, 49077, 24296, 40049, 10577, 41461, 48223, 43313, 7476, 19042, 31047, 11185, 23498, 36528, 47341, 22474, 12816, 42496, 43754, 29346, 23512, 23675, 42380, 26231, 39752, 17207, 18383, 12266, 40123, 45944, 48000, 27581, 385, 2237, 7006, 47187, 40975, 20186, 15222, 30127]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cc39590>\n",
            "Constructing exemplars of class 61\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [43495, 9841, 13110, 10675, 12390, 43031, 30626, 42726, 18717, 33775, 43995, 8150, 43031, 23390, 23761, 11216, 14463, 48165, 31525, 34062, 27801, 45377, 34219, 16636, 22651, 19893, 8150, 1251, 37483, 6869, 19121, 36394, 45301, 3249, 12737, 23134, 13468, 24977, 6788, 26821, 11666, 13544, 31406, 9804, 1130, 1566, 45745, 22177, 13468, 39065, 47311, 2833, 1742, 15565, 18717, 10071, 22836, 9381, 13544, 21261, 17615, 49039, 11216, 30613, 5701, 43109]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96ce61b90>\n",
            "Constructing exemplars of class 76\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [36024, 2581, 47839, 21633, 33759, 46936, 18913, 46486, 9093, 18689, 7053, 11970, 46698, 18536, 27484, 22488, 10228, 30128, 608, 358, 10490, 18437, 13722, 28492, 12192, 28994, 44691, 19135, 34811, 35418, 23722, 47839, 28912, 30014, 14387, 14286, 5515, 45282, 20626, 8923, 35788, 39871, 32760, 33448, 24200, 43780, 22936, 44511, 6063, 27033, 19983, 48541, 44427, 47095, 9107, 11446, 358, 36376, 38331, 5773, 13003, 49057, 40915, 19167, 6433, 29249]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96d29fb50>\n",
            "Constructing exemplars of class 64\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [13440, 44983, 16397, 19241, 37154, 2305, 24550, 16308, 22374, 49715, 42507, 22874, 3893, 13835, 36233, 43917, 17249, 46715, 36925, 4866, 35380, 33211, 35131, 3150, 45275, 46422, 81, 3583, 33219, 15892, 21703, 3150, 32457, 20796, 35175, 26582, 7916, 1058, 29760, 38432, 1810, 49819, 3260, 32926, 2845, 6251, 5917, 3018, 9646, 20796, 19439, 27879, 49788, 19065, 25440, 20056, 48596, 11217, 357, 1537, 794, 10113, 35568, 31261, 31078, 28009]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cdd0cd0>\n",
            "Constructing exemplars of class 32\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [38754, 24162, 11696, 29140, 24197, 13648, 35436, 30464, 26386, 34095, 35156, 15627, 20799, 14545, 42722, 36094, 22172, 25731, 2222, 39717, 15973, 17352, 549, 33531, 24420, 37199, 30733, 25731, 14635, 25816, 21436, 24420, 7869, 16613, 40693, 14343, 24454, 31506, 7556, 18676, 2566, 34849, 23738, 21249, 13648, 38597, 3322, 7455, 25067, 46813, 33178, 18358, 8098, 26260, 42392, 37199, 43419, 24218, 13368, 28687, 33140, 25160, 24420, 22098, 23660, 36236]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96d706110>\n",
            "Constructing exemplars of class 24\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [14741, 35205, 37576, 21507, 3348, 41081, 25306, 1921, 43402, 23506, 35385, 21166, 7617, 48844, 16435, 24470, 33233, 44519, 13314, 8867, 21178, 27264, 24634, 44595, 7749, 37576, 8516, 35318, 23202, 21175, 24949, 44784, 18861, 16435, 48844, 42655, 47577, 32956, 1651, 1959, 28757, 2687, 24967, 48515, 36471, 43926, 22828, 7676, 5087, 6828, 4308, 30448, 15911, 25256, 4301, 19862, 26630, 32878, 27710, 17584, 26546, 35490, 6266, 5032, 44831, 26741]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96d29ff50>\n",
            "Constructing exemplars of class 0\n",
            "lunghezza exemplar set:  66\n",
            "exemplar set:  [17384, 3757, 40264, 13252, 36693, 9470, 3520, 4480, 17882, 6803, 16637, 459, 35389, 33376, 25705, 29399, 8045, 4480, 16520, 29405, 43584, 31148, 27576, 15105, 37402, 7143, 43994, 38787, 8409, 176, 41172, 11402, 8012, 9062, 45940, 7923, 4251, 11349, 35389, 29582, 34110, 24382, 4691, 35665, 48469, 16520, 21290, 44972, 45096, 13528, 2, 3594, 46840, 36535, 46358, 17727, 11065, 16242, 23399, 31492, 42441, 46295, 4251, 12775, 35971, 43946]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.84 0.11504831910133362\n",
            "TEST GROUP:  0.892\n",
            "TEST ALL:  0.6573333333333333\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  4000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [97, 95, 68, 64, 56, 42, 36, 34, 32, 30, 24, 22, 20, 18, 16, 10, 6, 4, 2, 72, 76, 80, 61, 83, 81, 79, 75, 67, 65, 63, 59, 82, 49, 47, 39, 23, 21, 7, 90, 0]\n",
            "TRAIN_SET CLASSES:  [95, 83, 63, 42, 30, 6, 2, 97, 72, 36]\n",
            "VALIDATION CLASSES:  [63, 42, 36, 97, 95, 30, 83, 72, 6, 2]\n",
            "GROUP:  4\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [95, 83, 63, 42, 30, 6, 2, 97, 72, 36]\n",
            "Len TOTAL train susbset:  6930\n",
            "training\n",
            "num classes till now:  40\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.27499741315841675\n",
            "Train step - Step 10, Loss 0.1472535878419876\n",
            "Train step - Step 20, Loss 0.1464959681034088\n",
            "Train step - Step 30, Loss 0.14012764394283295\n",
            "Train step - Step 40, Loss 0.13045378029346466\n",
            "Train step - Step 50, Loss 0.11970837414264679\n",
            "Train epoch - Accuracy: 0.591053391053391 Loss: 0.148888004737843 Corrects: 1552\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.12425189465284348\n",
            "Train step - Step 70, Loss 0.13367794454097748\n",
            "Train step - Step 80, Loss 0.11948688328266144\n",
            "Train step - Step 90, Loss 0.12160535901784897\n",
            "Train step - Step 100, Loss 0.12108492106199265\n",
            "Train epoch - Accuracy: 0.5971139971139972 Loss: 0.12420020560595553 Corrects: 1790\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.126724511384964\n",
            "Train step - Step 120, Loss 0.11694305390119553\n",
            "Train step - Step 130, Loss 0.11937270313501358\n",
            "Train step - Step 140, Loss 0.11402425915002823\n",
            "Train step - Step 150, Loss 0.11758067458868027\n",
            "Train step - Step 160, Loss 0.11807060241699219\n",
            "Train epoch - Accuracy: 0.5878787878787879 Loss: 0.12049309069890136 Corrects: 1921\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.11533873528242111\n",
            "Train step - Step 180, Loss 0.1138635203242302\n",
            "Train step - Step 190, Loss 0.10945258289575577\n",
            "Train step - Step 200, Loss 0.11623682826757431\n",
            "Train step - Step 210, Loss 0.1105542704463005\n",
            "Train epoch - Accuracy: 0.5867243867243868 Loss: 0.11870346951364266 Corrects: 2070\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.12521350383758545\n",
            "Train step - Step 230, Loss 0.1252364218235016\n",
            "Train step - Step 240, Loss 0.1154940128326416\n",
            "Train step - Step 250, Loss 0.11445563286542892\n",
            "Train step - Step 260, Loss 0.11231513321399689\n",
            "Train step - Step 270, Loss 0.11465805023908615\n",
            "Train epoch - Accuracy: 0.5926406926406926 Loss: 0.11757019154867225 Corrects: 2143\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.1241070032119751\n",
            "Train step - Step 290, Loss 0.1217266246676445\n",
            "Train step - Step 300, Loss 0.1232309564948082\n",
            "Train step - Step 310, Loss 0.11317727714776993\n",
            "Train step - Step 320, Loss 0.11225295066833496\n",
            "Train epoch - Accuracy: 0.5874458874458874 Loss: 0.11733710104424888 Corrects: 2210\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.11563152074813843\n",
            "Train step - Step 340, Loss 0.11618492752313614\n",
            "Train step - Step 350, Loss 0.12097201496362686\n",
            "Train step - Step 360, Loss 0.12471314519643784\n",
            "Train step - Step 370, Loss 0.11378251761198044\n",
            "Train step - Step 380, Loss 0.11431347578763962\n",
            "Train epoch - Accuracy: 0.5945165945165946 Loss: 0.1165257584631529 Corrects: 2369\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.1140720471739769\n",
            "Train step - Step 400, Loss 0.11681491136550903\n",
            "Train step - Step 410, Loss 0.11429848521947861\n",
            "Train step - Step 420, Loss 0.11487691849470139\n",
            "Train step - Step 430, Loss 0.1110067367553711\n",
            "Train epoch - Accuracy: 0.5888888888888889 Loss: 0.11549771094391012 Corrects: 2429\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.11330924183130264\n",
            "Train step - Step 450, Loss 0.11521593481302261\n",
            "Train step - Step 460, Loss 0.11340649425983429\n",
            "Train step - Step 470, Loss 0.11262648552656174\n",
            "Train step - Step 480, Loss 0.11372174322605133\n",
            "Train step - Step 490, Loss 0.11291593313217163\n",
            "Train epoch - Accuracy: 0.5935064935064935 Loss: 0.11538201331834257 Corrects: 2540\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.11931860446929932\n",
            "Train step - Step 510, Loss 0.12274219840765\n",
            "Train step - Step 520, Loss 0.11733346432447433\n",
            "Train step - Step 530, Loss 0.11994650214910507\n",
            "Train step - Step 540, Loss 0.1177298054099083\n",
            "Train epoch - Accuracy: 0.597979797979798 Loss: 0.11420841096842616 Corrects: 2600\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.10861867666244507\n",
            "Train step - Step 560, Loss 0.10835965722799301\n",
            "Train step - Step 570, Loss 0.11510562896728516\n",
            "Train step - Step 580, Loss 0.11230041831731796\n",
            "Train step - Step 590, Loss 0.11809702217578888\n",
            "Train step - Step 600, Loss 0.10936850309371948\n",
            "Train epoch - Accuracy: 0.6038961038961039 Loss: 0.11380075827821509 Corrects: 2618\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.1141366958618164\n",
            "Train step - Step 620, Loss 0.11394000053405762\n",
            "Train step - Step 630, Loss 0.1103454977273941\n",
            "Train step - Step 640, Loss 0.12121953815221786\n",
            "Train step - Step 650, Loss 0.11812671273946762\n",
            "Train epoch - Accuracy: 0.6053391053391053 Loss: 0.1134487531057856 Corrects: 2676\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10705896466970444\n",
            "Train step - Step 670, Loss 0.11238928884267807\n",
            "Train step - Step 680, Loss 0.11175687611103058\n",
            "Train step - Step 690, Loss 0.11115604639053345\n",
            "Train step - Step 700, Loss 0.11497878283262253\n",
            "Train step - Step 710, Loss 0.11090490967035294\n",
            "Train epoch - Accuracy: 0.6090909090909091 Loss: 0.11275291722832304 Corrects: 2743\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.11050095409154892\n",
            "Train step - Step 730, Loss 0.11417140811681747\n",
            "Train step - Step 740, Loss 0.11446543037891388\n",
            "Train step - Step 750, Loss 0.12066030502319336\n",
            "Train step - Step 760, Loss 0.12098212540149689\n",
            "Train epoch - Accuracy: 0.6128427128427129 Loss: 0.11346537564998065 Corrects: 2809\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.11536908149719238\n",
            "Train step - Step 780, Loss 0.11801841109991074\n",
            "Train step - Step 790, Loss 0.10880254954099655\n",
            "Train step - Step 800, Loss 0.10836098343133926\n",
            "Train step - Step 810, Loss 0.11055836826562881\n",
            "Train step - Step 820, Loss 0.11211974918842316\n",
            "Train epoch - Accuracy: 0.6194805194805195 Loss: 0.11247840363311905 Corrects: 2856\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.11149817705154419\n",
            "Train step - Step 840, Loss 0.11111759394407272\n",
            "Train step - Step 850, Loss 0.10959000885486603\n",
            "Train step - Step 860, Loss 0.12081928551197052\n",
            "Train step - Step 870, Loss 0.10469973087310791\n",
            "Train epoch - Accuracy: 0.6197691197691197 Loss: 0.11224540710277199 Corrects: 2897\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.11925661563873291\n",
            "Train step - Step 890, Loss 0.10940071195363998\n",
            "Train step - Step 900, Loss 0.10744871199131012\n",
            "Train step - Step 910, Loss 0.11668437719345093\n",
            "Train step - Step 920, Loss 0.10904170572757721\n",
            "Train step - Step 930, Loss 0.11185438930988312\n",
            "Train epoch - Accuracy: 0.6252525252525253 Loss: 0.11195251423688161 Corrects: 2909\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.11680369824171066\n",
            "Train step - Step 950, Loss 0.11337918043136597\n",
            "Train step - Step 960, Loss 0.10701122134923935\n",
            "Train step - Step 970, Loss 0.12212564796209335\n",
            "Train step - Step 980, Loss 0.1105276569724083\n",
            "Train epoch - Accuracy: 0.6271284271284271 Loss: 0.11184429260559413 Corrects: 2969\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.11039989441633224\n",
            "Train step - Step 1000, Loss 0.10487736761569977\n",
            "Train step - Step 1010, Loss 0.11200644820928574\n",
            "Train step - Step 1020, Loss 0.11062409728765488\n",
            "Train step - Step 1030, Loss 0.11667286604642868\n",
            "Train step - Step 1040, Loss 0.11473806202411652\n",
            "Train epoch - Accuracy: 0.6203463203463203 Loss: 0.11100218757492468 Corrects: 3016\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.10303539037704468\n",
            "Train step - Step 1060, Loss 0.10985255241394043\n",
            "Train step - Step 1070, Loss 0.1060832291841507\n",
            "Train step - Step 1080, Loss 0.10372447222471237\n",
            "Train step - Step 1090, Loss 0.11522247642278671\n",
            "Train epoch - Accuracy: 0.6320346320346321 Loss: 0.11102303382638212 Corrects: 2989\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.1056329607963562\n",
            "Train step - Step 1110, Loss 0.11196606606245041\n",
            "Train step - Step 1120, Loss 0.1186068058013916\n",
            "Train step - Step 1130, Loss 0.1118285208940506\n",
            "Train step - Step 1140, Loss 0.10955824702978134\n",
            "Train step - Step 1150, Loss 0.11070891469717026\n",
            "Train epoch - Accuracy: 0.6356421356421357 Loss: 0.11065963622811553 Corrects: 3097\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10709574073553085\n",
            "Train step - Step 1170, Loss 0.11760611832141876\n",
            "Train step - Step 1180, Loss 0.10532712191343307\n",
            "Train step - Step 1190, Loss 0.10696157068014145\n",
            "Train step - Step 1200, Loss 0.11802666634321213\n",
            "Train epoch - Accuracy: 0.6376623376623377 Loss: 0.11077643705136848 Corrects: 3124\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.10304462909698486\n",
            "Train step - Step 1220, Loss 0.11361713707447052\n",
            "Train step - Step 1230, Loss 0.11493634432554245\n",
            "Train step - Step 1240, Loss 0.10595276206731796\n",
            "Train step - Step 1250, Loss 0.11201059818267822\n",
            "Train step - Step 1260, Loss 0.10870998352766037\n",
            "Train epoch - Accuracy: 0.6463203463203463 Loss: 0.11003524226715726 Corrects: 3208\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.10607092827558517\n",
            "Train step - Step 1280, Loss 0.10924508422613144\n",
            "Train step - Step 1290, Loss 0.11544694006443024\n",
            "Train step - Step 1300, Loss 0.108611561357975\n",
            "Train step - Step 1310, Loss 0.11739625036716461\n",
            "Train epoch - Accuracy: 0.6467532467532467 Loss: 0.10958054197944088 Corrects: 3190\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.11087117344141006\n",
            "Train step - Step 1330, Loss 0.12052923440933228\n",
            "Train step - Step 1340, Loss 0.11168620735406876\n",
            "Train step - Step 1350, Loss 0.10450366884469986\n",
            "Train step - Step 1360, Loss 0.10492829233407974\n",
            "Train step - Step 1370, Loss 0.11027172952890396\n",
            "Train epoch - Accuracy: 0.6464646464646465 Loss: 0.10953130840954155 Corrects: 3245\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.11301936954259872\n",
            "Train step - Step 1390, Loss 0.10690116137266159\n",
            "Train step - Step 1400, Loss 0.10499005764722824\n",
            "Train step - Step 1410, Loss 0.10446076840162277\n",
            "Train step - Step 1420, Loss 0.10816838592290878\n",
            "Train epoch - Accuracy: 0.6457431457431457 Loss: 0.10966225794985525 Corrects: 3228\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.10596666485071182\n",
            "Train step - Step 1440, Loss 0.11321790516376495\n",
            "Train step - Step 1450, Loss 0.11061988025903702\n",
            "Train step - Step 1460, Loss 0.10744454711675644\n",
            "Train step - Step 1470, Loss 0.11004854738712311\n",
            "Train step - Step 1480, Loss 0.11334818601608276\n",
            "Train epoch - Accuracy: 0.6509379509379509 Loss: 0.10899708719136567 Corrects: 3361\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.10923375189304352\n",
            "Train step - Step 1500, Loss 0.11276870965957642\n",
            "Train step - Step 1510, Loss 0.10764439404010773\n",
            "Train step - Step 1520, Loss 0.10377676784992218\n",
            "Train step - Step 1530, Loss 0.10809832811355591\n",
            "Train epoch - Accuracy: 0.651082251082251 Loss: 0.10921030308820125 Corrects: 3317\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.10709637403488159\n",
            "Train step - Step 1550, Loss 0.10788553953170776\n",
            "Train step - Step 1560, Loss 0.11150366067886353\n",
            "Train step - Step 1570, Loss 0.10647427290678024\n",
            "Train step - Step 1580, Loss 0.10771480947732925\n",
            "Train step - Step 1590, Loss 0.10509573668241501\n",
            "Train epoch - Accuracy: 0.6588744588744588 Loss: 0.10849449141932084 Corrects: 3398\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.11428695172071457\n",
            "Train step - Step 1610, Loss 0.10792527347803116\n",
            "Train step - Step 1620, Loss 0.10457336902618408\n",
            "Train step - Step 1630, Loss 0.10878864675760269\n",
            "Train step - Step 1640, Loss 0.11172343790531158\n",
            "Train epoch - Accuracy: 0.6526695526695526 Loss: 0.10845042068032819 Corrects: 3392\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1650, Loss 0.09810905903577805\n",
            "Train step - Step 1660, Loss 0.10358636826276779\n",
            "Train step - Step 1670, Loss 0.10604498535394669\n",
            "Train step - Step 1680, Loss 0.11275022476911545\n",
            "Train step - Step 1690, Loss 0.10899656265974045\n",
            "Train step - Step 1700, Loss 0.10700786113739014\n",
            "Train epoch - Accuracy: 0.6577200577200577 Loss: 0.1088045627394796 Corrects: 3443\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1710, Loss 0.10905716568231583\n",
            "Train step - Step 1720, Loss 0.10869412869215012\n",
            "Train step - Step 1730, Loss 0.1098422035574913\n",
            "Train step - Step 1740, Loss 0.11060430854558945\n",
            "Train step - Step 1750, Loss 0.10924609005451202\n",
            "Train epoch - Accuracy: 0.6646464646464646 Loss: 0.10837195108377229 Corrects: 3491\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.100197434425354\n",
            "Train step - Step 1770, Loss 0.10699079185724258\n",
            "Train step - Step 1780, Loss 0.10618772357702255\n",
            "Train step - Step 1790, Loss 0.10496836155653\n",
            "Train step - Step 1800, Loss 0.10863742977380753\n",
            "Train step - Step 1810, Loss 0.10588915646076202\n",
            "Train epoch - Accuracy: 0.6645021645021645 Loss: 0.10783907692411761 Corrects: 3457\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1820, Loss 0.1075805202126503\n",
            "Train step - Step 1830, Loss 0.11584758758544922\n",
            "Train step - Step 1840, Loss 0.10237995535135269\n",
            "Train step - Step 1850, Loss 0.10933785885572433\n",
            "Train step - Step 1860, Loss 0.11603932827711105\n",
            "Train epoch - Accuracy: 0.6660894660894661 Loss: 0.10741201081403234 Corrects: 3562\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1870, Loss 0.10761499404907227\n",
            "Train step - Step 1880, Loss 0.101218581199646\n",
            "Train step - Step 1890, Loss 0.09979543834924698\n",
            "Train step - Step 1900, Loss 0.10249347984790802\n",
            "Train step - Step 1910, Loss 0.11363871395587921\n",
            "Train step - Step 1920, Loss 0.11720883846282959\n",
            "Train epoch - Accuracy: 0.6766233766233766 Loss: 0.1071534387606047 Corrects: 3508\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1930, Loss 0.11237209290266037\n",
            "Train step - Step 1940, Loss 0.10348069667816162\n",
            "Train step - Step 1950, Loss 0.10176656395196915\n",
            "Train step - Step 1960, Loss 0.11803143471479416\n",
            "Train step - Step 1970, Loss 0.11370215564966202\n",
            "Train epoch - Accuracy: 0.6792207792207792 Loss: 0.10722351818201689 Corrects: 3616\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1980, Loss 0.10289566963911057\n",
            "Train step - Step 1990, Loss 0.10251529514789581\n",
            "Train step - Step 2000, Loss 0.10828280448913574\n",
            "Train step - Step 2010, Loss 0.10364451259374619\n",
            "Train step - Step 2020, Loss 0.11456959694623947\n",
            "Train step - Step 2030, Loss 0.10861563682556152\n",
            "Train epoch - Accuracy: 0.6796536796536796 Loss: 0.1071832894590854 Corrects: 3612\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 2040, Loss 0.10407116264104843\n",
            "Train step - Step 2050, Loss 0.10874946415424347\n",
            "Train step - Step 2060, Loss 0.12227959930896759\n",
            "Train step - Step 2070, Loss 0.10888671875\n",
            "Train step - Step 2080, Loss 0.10598231852054596\n",
            "Train epoch - Accuracy: 0.670995670995671 Loss: 0.10650978881817359 Corrects: 3590\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 2090, Loss 0.10909932851791382\n",
            "Train step - Step 2100, Loss 0.11294084787368774\n",
            "Train step - Step 2110, Loss 0.09934338182210922\n",
            "Train step - Step 2120, Loss 0.09907577931880951\n",
            "Train step - Step 2130, Loss 0.09231620281934738\n",
            "Train step - Step 2140, Loss 0.10466134548187256\n",
            "Train epoch - Accuracy: 0.6844155844155844 Loss: 0.10629814437027446 Corrects: 3694\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 2150, Loss 0.09987626224756241\n",
            "Train step - Step 2160, Loss 0.1031867265701294\n",
            "Train step - Step 2170, Loss 0.10007589310407639\n",
            "Train step - Step 2180, Loss 0.10552000254392624\n",
            "Train step - Step 2190, Loss 0.10094638913869858\n",
            "Train epoch - Accuracy: 0.6906204906204906 Loss: 0.10627481731845531 Corrects: 3704\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 2200, Loss 0.10607117414474487\n",
            "Train step - Step 2210, Loss 0.10156949609518051\n",
            "Train step - Step 2220, Loss 0.10783928632736206\n",
            "Train step - Step 2230, Loss 0.11441278457641602\n",
            "Train step - Step 2240, Loss 0.10951515287160873\n",
            "Train step - Step 2250, Loss 0.0997564047574997\n",
            "Train epoch - Accuracy: 0.6992784992784993 Loss: 0.10593833813677618 Corrects: 3715\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 2260, Loss 0.11333413422107697\n",
            "Train step - Step 2270, Loss 0.10011863708496094\n",
            "Train step - Step 2280, Loss 0.10894647985696793\n",
            "Train step - Step 2290, Loss 0.10441479831933975\n",
            "Train step - Step 2300, Loss 0.10519098490476608\n",
            "Train epoch - Accuracy: 0.6898989898989899 Loss: 0.10589560697562078 Corrects: 3722\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 2310, Loss 0.10379264503717422\n",
            "Train step - Step 2320, Loss 0.10753718763589859\n",
            "Train step - Step 2330, Loss 0.10794909298419952\n",
            "Train step - Step 2340, Loss 0.10277392715215683\n",
            "Train step - Step 2350, Loss 0.10324586927890778\n",
            "Train step - Step 2360, Loss 0.1044124960899353\n",
            "Train epoch - Accuracy: 0.6937950937950937 Loss: 0.10577328782878052 Corrects: 3775\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 2370, Loss 0.10961329936981201\n",
            "Train step - Step 2380, Loss 0.10412044823169708\n",
            "Train step - Step 2390, Loss 0.10315082222223282\n",
            "Train step - Step 2400, Loss 0.10476122051477432\n",
            "Train step - Step 2410, Loss 0.10331372171640396\n",
            "Train epoch - Accuracy: 0.6978354978354978 Loss: 0.10564512743105275 Corrects: 3744\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 2420, Loss 0.09869197010993958\n",
            "Train step - Step 2430, Loss 0.11438355594873428\n",
            "Train step - Step 2440, Loss 0.09758701175451279\n",
            "Train step - Step 2450, Loss 0.10395631939172745\n",
            "Train step - Step 2460, Loss 0.10082840919494629\n",
            "Train step - Step 2470, Loss 0.10387208312749863\n",
            "Train epoch - Accuracy: 0.6955266955266955 Loss: 0.10491325935459068 Corrects: 3824\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 2480, Loss 0.09641370922327042\n",
            "Train step - Step 2490, Loss 0.09879686683416367\n",
            "Train step - Step 2500, Loss 0.10678062587976456\n",
            "Train step - Step 2510, Loss 0.10800132900476456\n",
            "Train step - Step 2520, Loss 0.10380493849515915\n",
            "Train epoch - Accuracy: 0.6958152958152958 Loss: 0.10542768855373581 Corrects: 3761\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 2530, Loss 0.10065632313489914\n",
            "Train step - Step 2540, Loss 0.10673642158508301\n",
            "Train step - Step 2550, Loss 0.1112421303987503\n",
            "Train step - Step 2560, Loss 0.11609597504138947\n",
            "Train step - Step 2570, Loss 0.1029263287782669\n",
            "Train step - Step 2580, Loss 0.10486260801553726\n",
            "Train epoch - Accuracy: 0.7028860028860029 Loss: 0.10552468561334871 Corrects: 3879\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 2590, Loss 0.10397541522979736\n",
            "Train step - Step 2600, Loss 0.10877116024494171\n",
            "Train step - Step 2610, Loss 0.11226220428943634\n",
            "Train step - Step 2620, Loss 0.11097785085439682\n",
            "Train step - Step 2630, Loss 0.09994706511497498\n",
            "Train epoch - Accuracy: 0.7080808080808081 Loss: 0.10455706548510176 Corrects: 3889\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 2640, Loss 0.10466431826353073\n",
            "Train step - Step 2650, Loss 0.10453547537326813\n",
            "Train step - Step 2660, Loss 0.10047781467437744\n",
            "Train step - Step 2670, Loss 0.10082647949457169\n",
            "Train step - Step 2680, Loss 0.10485859215259552\n",
            "Train step - Step 2690, Loss 0.10745485126972198\n",
            "Train epoch - Accuracy: 0.7103896103896103 Loss: 0.10392467038541989 Corrects: 3958\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.10766973346471786\n",
            "Train step - Step 2710, Loss 0.10204656422138214\n",
            "Train step - Step 2720, Loss 0.10422923415899277\n",
            "Train step - Step 2730, Loss 0.09895437210798264\n",
            "Train step - Step 2740, Loss 0.10169859230518341\n",
            "Train epoch - Accuracy: 0.7037518037518038 Loss: 0.10313027428858208 Corrects: 3915\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2750, Loss 0.09912931174039841\n",
            "Train step - Step 2760, Loss 0.10117173194885254\n",
            "Train step - Step 2770, Loss 0.10185213387012482\n",
            "Train step - Step 2780, Loss 0.09700018912553787\n",
            "Train step - Step 2790, Loss 0.09956633299589157\n",
            "Train step - Step 2800, Loss 0.10760384798049927\n",
            "Train epoch - Accuracy: 0.715007215007215 Loss: 0.10181780237312096 Corrects: 3986\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2810, Loss 0.10078034549951553\n",
            "Train step - Step 2820, Loss 0.10941942781209946\n",
            "Train step - Step 2830, Loss 0.10126989334821701\n",
            "Train step - Step 2840, Loss 0.10240162909030914\n",
            "Train step - Step 2850, Loss 0.10157077759504318\n",
            "Train epoch - Accuracy: 0.717027417027417 Loss: 0.1018009396880048 Corrects: 4054\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2860, Loss 0.0994599312543869\n",
            "Train step - Step 2870, Loss 0.10382936149835587\n",
            "Train step - Step 2880, Loss 0.1032150536775589\n",
            "Train step - Step 2890, Loss 0.09779570251703262\n",
            "Train step - Step 2900, Loss 0.10662069171667099\n",
            "Train step - Step 2910, Loss 0.09720142185688019\n",
            "Train epoch - Accuracy: 0.7121212121212122 Loss: 0.10154505810911349 Corrects: 4046\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2920, Loss 0.10179954022169113\n",
            "Train step - Step 2930, Loss 0.10389574617147446\n",
            "Train step - Step 2940, Loss 0.10732387751340866\n",
            "Train step - Step 2950, Loss 0.10168948024511337\n",
            "Train step - Step 2960, Loss 0.1054091677069664\n",
            "Train epoch - Accuracy: 0.7076479076479076 Loss: 0.10172479977296373 Corrects: 4000\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2970, Loss 0.10007718950510025\n",
            "Train step - Step 2980, Loss 0.11344672739505768\n",
            "Train step - Step 2990, Loss 0.09831654280424118\n",
            "Train step - Step 3000, Loss 0.1067856177687645\n",
            "Train step - Step 3010, Loss 0.09914971888065338\n",
            "Train step - Step 3020, Loss 0.10465603321790695\n",
            "Train epoch - Accuracy: 0.7213564213564213 Loss: 0.1015993481741151 Corrects: 4016\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3030, Loss 0.11229854077100754\n",
            "Train step - Step 3040, Loss 0.10116880387067795\n",
            "Train step - Step 3050, Loss 0.10538122802972794\n",
            "Train step - Step 3060, Loss 0.11263282597064972\n",
            "Train step - Step 3070, Loss 0.09973376989364624\n",
            "Train epoch - Accuracy: 0.7210678210678211 Loss: 0.10153959440161484 Corrects: 4017\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3080, Loss 0.09893239289522171\n",
            "Train step - Step 3090, Loss 0.10757733881473541\n",
            "Train step - Step 3100, Loss 0.10135293006896973\n",
            "Train step - Step 3110, Loss 0.10053505748510361\n",
            "Train step - Step 3120, Loss 0.0998266190290451\n",
            "Train step - Step 3130, Loss 0.09991418570280075\n",
            "Train epoch - Accuracy: 0.7265512265512265 Loss: 0.10146010712025658 Corrects: 4066\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3140, Loss 0.10428672283887863\n",
            "Train step - Step 3150, Loss 0.09847814589738846\n",
            "Train step - Step 3160, Loss 0.10237272083759308\n",
            "Train step - Step 3170, Loss 0.10839851200580597\n",
            "Train step - Step 3180, Loss 0.10403268784284592\n",
            "Train epoch - Accuracy: 0.7210678210678211 Loss: 0.10160307712412156 Corrects: 4043\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3190, Loss 0.10846047848463058\n",
            "Train step - Step 3200, Loss 0.1024194285273552\n",
            "Train step - Step 3210, Loss 0.0982256531715393\n",
            "Train step - Step 3220, Loss 0.10645385086536407\n",
            "Train step - Step 3230, Loss 0.102350614964962\n",
            "Train step - Step 3240, Loss 0.09961528331041336\n",
            "Train epoch - Accuracy: 0.7194805194805195 Loss: 0.10112843129670981 Corrects: 4051\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3250, Loss 0.09965778142213821\n",
            "Train step - Step 3260, Loss 0.10644900798797607\n",
            "Train step - Step 3270, Loss 0.095891073346138\n",
            "Train step - Step 3280, Loss 0.11201559752225876\n",
            "Train step - Step 3290, Loss 0.09853684157133102\n",
            "Train epoch - Accuracy: 0.7264069264069264 Loss: 0.10096974193533778 Corrects: 4076\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3300, Loss 0.10250519961118698\n",
            "Train step - Step 3310, Loss 0.10304977744817734\n",
            "Train step - Step 3320, Loss 0.10831167548894882\n",
            "Train step - Step 3330, Loss 0.09742549806833267\n",
            "Train step - Step 3340, Loss 0.11044102162122726\n",
            "Train step - Step 3350, Loss 0.09784425050020218\n",
            "Train epoch - Accuracy: 0.7217893217893218 Loss: 0.10132023676219269 Corrects: 4081\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3360, Loss 0.10302183777093887\n",
            "Train step - Step 3370, Loss 0.0960879921913147\n",
            "Train step - Step 3380, Loss 0.09937643259763718\n",
            "Train step - Step 3390, Loss 0.09701886773109436\n",
            "Train step - Step 3400, Loss 0.10362811386585236\n",
            "Train epoch - Accuracy: 0.7277056277056277 Loss: 0.10102956236526908 Corrects: 4052\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 3410, Loss 0.10466285049915314\n",
            "Train step - Step 3420, Loss 0.10389584302902222\n",
            "Train step - Step 3430, Loss 0.10059972107410431\n",
            "Train step - Step 3440, Loss 0.10139300674200058\n",
            "Train step - Step 3450, Loss 0.09938917309045792\n",
            "Train step - Step 3460, Loss 0.09976781159639359\n",
            "Train epoch - Accuracy: 0.7216450216450216 Loss: 0.10118413940394595 Corrects: 4085\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 3470, Loss 0.10451815277338028\n",
            "Train step - Step 3480, Loss 0.1002892404794693\n",
            "Train step - Step 3490, Loss 0.10408749431371689\n",
            "Train step - Step 3500, Loss 0.09268230944871902\n",
            "Train step - Step 3510, Loss 0.1016550287604332\n",
            "Train epoch - Accuracy: 0.7340548340548341 Loss: 0.10068295609641385 Corrects: 4122\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3520, Loss 0.10020718723535538\n",
            "Train step - Step 3530, Loss 0.09605744481086731\n",
            "Train step - Step 3540, Loss 0.09863730520009995\n",
            "Train step - Step 3550, Loss 0.10601618140935898\n",
            "Train step - Step 3560, Loss 0.10063965618610382\n",
            "Train step - Step 3570, Loss 0.09959079325199127\n",
            "Train epoch - Accuracy: 0.7288600288600289 Loss: 0.10065110376773975 Corrects: 4124\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3580, Loss 0.09569450467824936\n",
            "Train step - Step 3590, Loss 0.09810227155685425\n",
            "Train step - Step 3600, Loss 0.10282838344573975\n",
            "Train step - Step 3610, Loss 0.09970398992300034\n",
            "Train step - Step 3620, Loss 0.09957081079483032\n",
            "Train epoch - Accuracy: 0.7212121212121212 Loss: 0.10028293328453797 Corrects: 4175\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3630, Loss 0.10135553032159805\n",
            "Train step - Step 3640, Loss 0.09638223797082901\n",
            "Train step - Step 3650, Loss 0.09546566009521484\n",
            "Train step - Step 3660, Loss 0.10483518987894058\n",
            "Train step - Step 3670, Loss 0.10051170736551285\n",
            "Train step - Step 3680, Loss 0.0955401286482811\n",
            "Train epoch - Accuracy: 0.729004329004329 Loss: 0.10072346233143263 Corrects: 4099\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3690, Loss 0.09849809855222702\n",
            "Train step - Step 3700, Loss 0.1053435355424881\n",
            "Train step - Step 3710, Loss 0.10370764881372452\n",
            "Train step - Step 3720, Loss 0.10060101002454758\n",
            "Train step - Step 3730, Loss 0.10171067714691162\n",
            "Train epoch - Accuracy: 0.7304473304473305 Loss: 0.10050191194636138 Corrects: 4139\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3740, Loss 0.0949556976556778\n",
            "Train step - Step 3750, Loss 0.09527958929538727\n",
            "Train step - Step 3760, Loss 0.0981421023607254\n",
            "Train step - Step 3770, Loss 0.10870931297540665\n",
            "Train step - Step 3780, Loss 0.10562574863433838\n",
            "Train step - Step 3790, Loss 0.09825437515974045\n",
            "Train epoch - Accuracy: 0.7277056277056277 Loss: 0.10047858615028221 Corrects: 4140\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 3800, Loss 0.09878730028867722\n",
            "Train step - Step 3810, Loss 0.09740237146615982\n",
            "Train step - Step 3820, Loss 0.09582536667585373\n",
            "Train step - Step 3830, Loss 0.10278196632862091\n",
            "Train step - Step 3840, Loss 0.09865792095661163\n",
            "Train epoch - Accuracy: 0.7261183261183262 Loss: 0.10043469928688817 Corrects: 4111\n",
            "Training finished in 445.6256136894226 seconds\n",
            "reducing exemplars for each class\n",
            "[67, 59, 39, 22, 18, 65, 49, 56, 20, 4, 79, 47, 7, 82, 34, 81, 21, 80, 68, 16, 75, 23, 90, 10, 61, 76, 64, 32, 24, 0, 95, 83, 63, 42, 30, 6, 2, 97, 72, 36]\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  50\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "REDUCED EXEMPLAR:  0\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cd39650>\n",
            "Constructing exemplars of class 95\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [13640, 5280, 49683, 44719, 30445, 11471, 49154, 25442, 26551, 17792, 894, 5756, 46247, 26713, 26558, 8550, 19758, 2731, 36988, 44719, 43465, 25472, 21413, 545, 22104, 49406, 29358, 42800, 49109, 28647, 151, 44975, 22789, 46072, 45444, 12433, 33883, 27030, 10805, 6813, 28170, 33380, 41176, 41872, 4311, 1838, 38855, 34057, 25472, 4281]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cc65a50>\n",
            "Constructing exemplars of class 83\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [23627, 22554, 40384, 49471, 45906, 48781, 3482, 32394, 42061, 47818, 28180, 23406, 7356, 17556, 27409, 24301, 25370, 49602, 7972, 34520, 18906, 39164, 43361, 44457, 2813, 13119, 48135, 43615, 3523, 27459, 16633, 22096, 25975, 9854, 37693, 5488, 49484, 8763, 21674, 26089, 23716, 10634, 28180, 34977, 6204, 35794, 33665, 39898, 7196, 15003]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96ccb5ad0>\n",
            "Constructing exemplars of class 63\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [6697, 790, 20417, 21216, 23052, 31174, 2887, 29735, 32414, 4745, 7592, 26075, 8828, 28707, 2228, 38634, 32859, 45083, 11994, 42838, 30401, 26897, 47740, 47599, 4963, 1044, 33750, 1339, 40370, 2900, 34981, 24060, 11576, 29561, 10625, 23409, 17092, 45401, 24112, 19791, 29802, 28602, 16980, 15860, 36922, 16538, 29735, 15824, 43262, 42504]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cb5b590>\n",
            "Constructing exemplars of class 42\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [36932, 3631, 43911, 1515, 6887, 39810, 41656, 812, 32542, 9659, 17558, 42575, 37318, 20460, 14265, 27885, 30923, 23959, 36608, 1780, 28333, 11445, 42472, 36214, 35973, 10126, 12600, 8361, 33116, 19115, 26883, 4277, 21648, 10819, 37471, 25562, 33076, 20282, 15695, 28333, 17923, 14472, 16667, 9141, 37986, 36932, 3630, 15695, 8953, 8397]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96d854910>\n",
            "Constructing exemplars of class 30\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [15711, 42267, 21345, 20125, 20279, 22742, 41677, 21636, 42993, 35905, 3297, 23432, 22882, 17858, 37821, 23337, 40968, 27353, 24811, 7520, 1711, 39411, 48439, 5127, 33540, 22882, 33684, 41101, 44160, 22281, 24992, 10671, 28222, 37101, 20545, 42156, 26581, 42023, 7261, 6192, 30454, 32684, 24379, 48114, 33508, 28430, 41512, 37648, 16711, 25962]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96d2ab810>\n",
            "Constructing exemplars of class 6\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [34991, 8561, 35217, 36474, 31837, 38990, 10231, 26420, 7856, 28857, 34256, 46819, 34169, 30651, 35001, 17234, 21593, 2121, 12234, 23470, 34872, 40008, 3223, 34509, 16585, 39127, 15472, 40401, 37838, 29768, 7856, 36903, 1320, 14747, 22537, 43299, 13220, 40819, 19497, 24102, 15546, 38520, 34629, 30651, 4459, 43188, 9929, 23387, 25499, 31837]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cd35610>\n",
            "Constructing exemplars of class 2\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [22665, 20458, 2925, 35048, 22125, 27863, 1696, 7854, 27432, 1198, 30024, 9321, 22388, 41867, 23451, 17910, 18725, 12363, 14263, 38518, 7877, 9773, 44740, 41030, 27810, 13969, 5417, 10320, 8100, 38793, 6975, 19186, 40511, 7877, 49862, 33986, 23970, 24759, 22754, 18548, 27595, 38575, 24149, 21617, 23970, 11042, 34551, 22662, 39442, 42302]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96d98e050>\n",
            "Constructing exemplars of class 97\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [39014, 41517, 37409, 9099, 42263, 23916, 15643, 49314, 37032, 46225, 21715, 41368, 47882, 20999, 36343, 10823, 1687, 25750, 3008, 13865, 10128, 48633, 35010, 29174, 45577, 12766, 47258, 44149, 42894, 34741, 20564, 48042, 12863, 25750, 27222, 12109, 29182, 8985, 13118, 15793, 41638, 20073, 32405, 47882, 39561, 41097, 6079, 9206, 27949, 34301]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96cb5e5d0>\n",
            "Constructing exemplars of class 72\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [25396, 26087, 10379, 14668, 24536, 40880, 21482, 28976, 19463, 29566, 46721, 19651, 42792, 16252, 30016, 40163, 4328, 34178, 30298, 47692, 26587, 46810, 34381, 35526, 37279, 31631, 10847, 47542, 366, 62, 7850, 22639, 13661, 486, 8830, 14668, 26525, 15010, 19369, 29965, 17688, 49968, 32324, 4582, 5144, 22826, 12623, 21453, 40125, 24386]\n",
            "class train:  <torch.utils.data.dataset.Subset object at 0x7fb96e8b7c50>\n",
            "Constructing exemplars of class 36\n",
            "lunghezza exemplar set:  50\n",
            "exemplar set:  [8743, 3610, 40990, 11245, 47195, 35573, 43436, 12593, 46729, 31867, 41985, 3806, 14164, 36795, 42634, 49755, 43004, 47171, 29981, 16453, 25772, 16169, 39027, 15907, 9965, 29226, 46692, 12626, 17516, 8788, 9740, 18756, 19130, 15368, 36456, 29871, 18221, 3271, 22040, 13153, 45176, 857, 19273, 42456, 24960, 38917, 3509, 40361, 43978, 17929]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EVALUATION:  0.7 0.115640789270401\n",
            "TEST GROUP:  0.752\n",
            "TEST ALL:  0.57025\n",
            "TRAIN:  4950\n",
            "TEST SET LENGHT:  5000\n",
            "TEST CURRENT GROUP SET LENGHT:  1000\n",
            "TEST_SET CLASSES:  [95, 80, 97, 93, 85, 81, 65, 61, 49, 21, 9, 96, 76, 83, 72, 68, 64, 56, 36, 32, 24, 20, 16, 4, 2, 6, 10, 18, 79, 75, 67, 63, 59, 55, 47, 39, 31, 23, 19, 7, 98, 94, 90, 82, 54, 42, 34, 30, 22, 0]\n",
            "TRAIN_SET CLASSES:  [55, 31, 19, 98, 94, 54, 93, 85, 9, 96]\n",
            "VALIDATION CLASSES:  [55, 54, 98, 96, 31, 94, 93, 85, 19, 9]\n",
            "GROUP:  5\n",
            "Starting the update representation\n",
            "NEW CLASSES:  [55, 31, 19, 98, 94, 54, 93, 85, 9, 96]\n",
            "Len TOTAL train susbset:  6950\n",
            "training\n",
            "num classes till now:  50\n",
            "70\n",
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.2826979458332062\n",
            "Train step - Step 10, Loss 0.1438816487789154\n",
            "Train step - Step 20, Loss 0.1379355788230896\n",
            "Train step - Step 30, Loss 0.12791785597801208\n",
            "Train step - Step 40, Loss 0.12635664641857147\n",
            "Train step - Step 50, Loss 0.12149056792259216\n",
            "Train epoch - Accuracy: 0.502589928057554 Loss: 0.1458743736576691 Corrects: 1410\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 60, Loss 0.1274026483297348\n",
            "Train step - Step 70, Loss 0.11563771218061447\n",
            "Train step - Step 80, Loss 0.12893298268318176\n",
            "Train step - Step 90, Loss 0.1203140914440155\n",
            "Train step - Step 100, Loss 0.11703498661518097\n",
            "Train epoch - Accuracy: 0.5188489208633094 Loss: 0.12031922526496777 Corrects: 1652\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 110, Loss 0.1143544390797615\n",
            "Train step - Step 120, Loss 0.1194443330168724\n",
            "Train step - Step 130, Loss 0.11375769972801208\n",
            "Train step - Step 140, Loss 0.12041904777288437\n",
            "Train step - Step 150, Loss 0.11548735946416855\n",
            "Train step - Step 160, Loss 0.11235523223876953\n",
            "Train epoch - Accuracy: 0.5267625899280576 Loss: 0.11709900354309906 Corrects: 1816\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 170, Loss 0.11206938326358795\n",
            "Train step - Step 180, Loss 0.1141943633556366\n",
            "Train step - Step 190, Loss 0.11295375227928162\n",
            "Train step - Step 200, Loss 0.11312047392129898\n",
            "Train step - Step 210, Loss 0.10830529034137726\n",
            "Train epoch - Accuracy: 0.5309352517985612 Loss: 0.11623231988159015 Corrects: 1986\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 220, Loss 0.11882679909467697\n",
            "Train step - Step 230, Loss 0.11272617429494858\n",
            "Train step - Step 240, Loss 0.11237397789955139\n",
            "Train step - Step 250, Loss 0.10972971469163895\n",
            "Train step - Step 260, Loss 0.11519746482372284\n",
            "Train step - Step 270, Loss 0.10526692122220993\n",
            "Train epoch - Accuracy: 0.5300719424460432 Loss: 0.11457447790842262 Corrects: 2156\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.11869210749864578\n",
            "Train step - Step 290, Loss 0.12510420382022858\n",
            "Train step - Step 300, Loss 0.11453350633382797\n",
            "Train step - Step 310, Loss 0.11213038861751556\n",
            "Train step - Step 320, Loss 0.11045600473880768\n",
            "Train epoch - Accuracy: 0.5353956834532374 Loss: 0.11393273985857587 Corrects: 2286\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 330, Loss 0.10660106688737869\n",
            "Train step - Step 340, Loss 0.1113235130906105\n",
            "Train step - Step 350, Loss 0.11294358968734741\n",
            "Train step - Step 360, Loss 0.11073493957519531\n",
            "Train step - Step 370, Loss 0.11328225582838058\n",
            "Train step - Step 380, Loss 0.11248426139354706\n",
            "Train epoch - Accuracy: 0.5365467625899281 Loss: 0.11327681235057845 Corrects: 2370\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.1097562164068222\n",
            "Train step - Step 400, Loss 0.10853466391563416\n",
            "Train step - Step 410, Loss 0.10797977447509766\n",
            "Train step - Step 420, Loss 0.11230636388063431\n",
            "Train step - Step 430, Loss 0.1182076707482338\n",
            "Train epoch - Accuracy: 0.5434532374100719 Loss: 0.11273255646014385 Corrects: 2501\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 440, Loss 0.10539310425519943\n",
            "Train step - Step 450, Loss 0.10803503543138504\n",
            "Train step - Step 460, Loss 0.10982567816972733\n",
            "Train step - Step 470, Loss 0.11046002060174942\n",
            "Train step - Step 480, Loss 0.10621959716081619\n",
            "Train step - Step 490, Loss 0.11527106165885925\n",
            "Train epoch - Accuracy: 0.5411510791366907 Loss: 0.11242955425660388 Corrects: 2563\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 500, Loss 0.1149085983633995\n",
            "Train step - Step 510, Loss 0.11672463268041611\n",
            "Train step - Step 520, Loss 0.11108209192752838\n",
            "Train step - Step 530, Loss 0.11364737898111343\n",
            "Train step - Step 540, Loss 0.10893331468105316\n",
            "Train epoch - Accuracy: 0.5486330935251799 Loss: 0.11179686729856532 Corrects: 2693\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.11563108116388321\n",
            "Train step - Step 560, Loss 0.11771915853023529\n",
            "Train step - Step 570, Loss 0.11302562803030014\n",
            "Train step - Step 580, Loss 0.11214570701122284\n",
            "Train step - Step 590, Loss 0.10710547864437103\n",
            "Train step - Step 600, Loss 0.10923108458518982\n",
            "Train epoch - Accuracy: 0.5466187050359712 Loss: 0.11173778599543537 Corrects: 2696\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 610, Loss 0.11573464423418045\n",
            "Train step - Step 620, Loss 0.11235742270946503\n",
            "Train step - Step 630, Loss 0.10574956983327866\n",
            "Train step - Step 640, Loss 0.11553182452917099\n",
            "Train step - Step 650, Loss 0.11863473802804947\n",
            "Train epoch - Accuracy: 0.5592805755395683 Loss: 0.11116664639908633 Corrects: 2786\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 660, Loss 0.10393872857093811\n",
            "Train step - Step 670, Loss 0.10319549590349197\n",
            "Train step - Step 680, Loss 0.11326137185096741\n",
            "Train step - Step 690, Loss 0.10806213319301605\n",
            "Train step - Step 700, Loss 0.11071968078613281\n",
            "Train step - Step 710, Loss 0.11191688477993011\n",
            "Train epoch - Accuracy: 0.5588489208633094 Loss: 0.11098565807874254 Corrects: 2810\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 720, Loss 0.10762744396924973\n",
            "Train step - Step 730, Loss 0.11074447631835938\n",
            "Train step - Step 740, Loss 0.11221512407064438\n",
            "Train step - Step 750, Loss 0.11468776315450668\n",
            "Train step - Step 760, Loss 0.1168489158153534\n",
            "Train epoch - Accuracy: 0.5692086330935252 Loss: 0.11008634295609357 Corrects: 2917\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 770, Loss 0.10826639086008072\n",
            "Train step - Step 780, Loss 0.10616718232631683\n",
            "Train step - Step 790, Loss 0.10755034536123276\n",
            "Train step - Step 800, Loss 0.11118976026773453\n",
            "Train step - Step 810, Loss 0.11510318517684937\n",
            "Train step - Step 820, Loss 0.11011800169944763\n",
            "Train epoch - Accuracy: 0.5682014388489208 Loss: 0.11046866491115351 Corrects: 2952\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 830, Loss 0.10845836251974106\n",
            "Train step - Step 840, Loss 0.11044618487358093\n",
            "Train step - Step 850, Loss 0.10831732302904129\n",
            "Train step - Step 860, Loss 0.11279638111591339\n",
            "Train step - Step 870, Loss 0.11138061434030533\n",
            "Train epoch - Accuracy: 0.5712230215827339 Loss: 0.11009267452166235 Corrects: 2986\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 880, Loss 0.10694114118814468\n",
            "Train step - Step 890, Loss 0.11339683085680008\n",
            "Train step - Step 900, Loss 0.12019907683134079\n",
            "Train step - Step 910, Loss 0.10988935083150864\n",
            "Train step - Step 920, Loss 0.10777490586042404\n",
            "Train step - Step 930, Loss 0.1092212051153183\n",
            "Train epoch - Accuracy: 0.5723741007194244 Loss: 0.11011481842548727 Corrects: 2996\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.10985984653234482\n",
            "Train step - Step 950, Loss 0.11265330016613007\n",
            "Train step - Step 960, Loss 0.107309490442276\n",
            "Train step - Step 970, Loss 0.10791625827550888\n",
            "Train step - Step 980, Loss 0.10298750549554825\n",
            "Train epoch - Accuracy: 0.5781294964028777 Loss: 0.10948545793835207 Corrects: 3100\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 990, Loss 0.10609830915927887\n",
            "Train step - Step 1000, Loss 0.11501889675855637\n",
            "Train step - Step 1010, Loss 0.10844554007053375\n",
            "Train step - Step 1020, Loss 0.11411355435848236\n",
            "Train step - Step 1030, Loss 0.10334598273038864\n",
            "Train step - Step 1040, Loss 0.11360438913106918\n",
            "Train epoch - Accuracy: 0.5848920863309353 Loss: 0.1098430862186624 Corrects: 3135\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 1050, Loss 0.11062493920326233\n",
            "Train step - Step 1060, Loss 0.11595083028078079\n",
            "Train step - Step 1070, Loss 0.11172061413526535\n",
            "Train step - Step 1080, Loss 0.1083303838968277\n",
            "Train step - Step 1090, Loss 0.10063633322715759\n",
            "Train epoch - Accuracy: 0.5892086330935252 Loss: 0.10944617116837192 Corrects: 3189\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.10876664519309998\n",
            "Train step - Step 1110, Loss 0.10845017433166504\n",
            "Train step - Step 1120, Loss 0.10709312558174133\n",
            "Train step - Step 1130, Loss 0.10955420881509781\n",
            "Train step - Step 1140, Loss 0.11469849944114685\n",
            "Train step - Step 1150, Loss 0.10598636418581009\n",
            "Train epoch - Accuracy: 0.5913669064748202 Loss: 0.1089376296242364 Corrects: 3265\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 1160, Loss 0.10945633053779602\n",
            "Train step - Step 1170, Loss 0.11609697341918945\n",
            "Train step - Step 1180, Loss 0.10545377433300018\n",
            "Train step - Step 1190, Loss 0.10934165865182877\n",
            "Train step - Step 1200, Loss 0.11365299671888351\n",
            "Train epoch - Accuracy: 0.5900719424460432 Loss: 0.10859976057740424 Corrects: 3262\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.1078062430024147\n",
            "Train step - Step 1220, Loss 0.10353626310825348\n",
            "Train step - Step 1230, Loss 0.1070321723818779\n",
            "Train step - Step 1240, Loss 0.10830924659967422\n",
            "Train step - Step 1250, Loss 0.10679739713668823\n",
            "Train step - Step 1260, Loss 0.1090002954006195\n",
            "Train epoch - Accuracy: 0.5933812949640288 Loss: 0.10844355173676992 Corrects: 3295\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 1270, Loss 0.11280006915330887\n",
            "Train step - Step 1280, Loss 0.11530641466379166\n",
            "Train step - Step 1290, Loss 0.1118263378739357\n",
            "Train step - Step 1300, Loss 0.1100429892539978\n",
            "Train step - Step 1310, Loss 0.11106863617897034\n",
            "Train epoch - Accuracy: 0.5984172661870504 Loss: 0.10874286173058928 Corrects: 3299\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 1320, Loss 0.11182616651058197\n",
            "Train step - Step 1330, Loss 0.1129150390625\n",
            "Train step - Step 1340, Loss 0.1103309616446495\n",
            "Train step - Step 1350, Loss 0.10834269970655441\n",
            "Train step - Step 1360, Loss 0.11458177119493484\n",
            "Train step - Step 1370, Loss 0.10723864287137985\n",
            "Train epoch - Accuracy: 0.6 Loss: 0.10869419454670638 Corrects: 3372\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 1380, Loss 0.10846506059169769\n",
            "Train step - Step 1390, Loss 0.10689326375722885\n",
            "Train step - Step 1400, Loss 0.1064450815320015\n",
            "Train step - Step 1410, Loss 0.1024724468588829\n",
            "Train step - Step 1420, Loss 0.1136416420340538\n",
            "Train epoch - Accuracy: 0.6031654676258993 Loss: 0.10837616338146676 Corrects: 3437\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1430, Loss 0.10490083694458008\n",
            "Train step - Step 1440, Loss 0.1079883947968483\n",
            "Train step - Step 1450, Loss 0.10467750579118729\n",
            "Train step - Step 1460, Loss 0.10024800896644592\n",
            "Train step - Step 1470, Loss 0.102628193795681\n",
            "Train step - Step 1480, Loss 0.1027984619140625\n",
            "Train epoch - Accuracy: 0.6053237410071942 Loss: 0.10821359488818286 Corrects: 3438\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.10747386515140533\n",
            "Train step - Step 1500, Loss 0.10909537971019745\n",
            "Train step - Step 1510, Loss 0.10659187287092209\n",
            "Train step - Step 1520, Loss 0.1102721095085144\n",
            "Train step - Step 1530, Loss 0.10462624579668045\n",
            "Train epoch - Accuracy: 0.6100719424460431 Loss: 0.10803313512167485 Corrects: 3458\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1540, Loss 0.10663978010416031\n",
            "Train step - Step 1550, Loss 0.11052616685628891\n",
            "Train step - Step 1560, Loss 0.09987485408782959\n",
            "Train step - Step 1570, Loss 0.10720903426408768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KRvNYlfO1ICS",
        "outputId": "768f275a-983b-4359-8857-27d16ab2a0d9"
      },
      "source": [
        "method = \"Closed world with Rejection\"\n",
        "print(\"metrics ClosedWord for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,5):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics ClosedWord for seed 66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e11c493e5986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# write down json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mwriteMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusionMatrixData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracies' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pXLJaU3CtVQ"
      },
      "source": [
        "### open world with rejection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNW9TdvxHEl2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs9uww-HEu08"
      },
      "source": [
        "#without rejection\n",
        "rejection = False\n",
        "closed = False\n",
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}