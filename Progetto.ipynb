{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LWF funzionante.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoManca-FM/Project_MLDL/blob/main/Progetto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAfp19qRVa7l"
      },
      "source": [
        "### GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag2iq0PaVWC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42002a58-5ff3-4490-80ca-696926f6f5e8"
      },
      "source": [
        "# Check GPU assigned\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OWtqytyzmMF"
      },
      "source": [
        "## Network, dataset, functions and parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itMjhanGVTUp"
      },
      "source": [
        "### LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6tCA_s2rru"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nHVORYbiZRx"
      },
      "source": [
        "### Resnet32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIdSy6PnibZA"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\"\"\"\n",
        "Credits to @hshustc\n",
        "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "from torch.nn import init\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_relu=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.stride = stride\n",
        "        self.use_relu = use_relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "        if self.use_relu:\n",
        "            out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        assert stride == 2\n",
        "        self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat((x, x.mul(0)), 1)\n",
        "\n",
        "\n",
        "class CifarResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block=BasicBlock, depth=32, num_classes=0, channels=3):\n",
        "\n",
        "        super(CifarResNet, self).__init__()\n",
        "\n",
        "        # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
        "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
        "        layer_blocks = (depth - 2) // 6\n",
        "        bn = nn.BatchNorm2d\n",
        "        self.inplanes = 16\n",
        "\n",
        "        self.conv_1_3x3 = nn.Conv2d(channels, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn_1 = bn(self.inplanes)\n",
        "\n",
        "        self.stage_1 = self._make_layer(block, 16, layer_blocks, 1)\n",
        "        self.stage_2 = self._make_layer(block, 32, layer_blocks, 2)\n",
        "        self.stage_3 = self._make_layer(block, 64, layer_blocks, 2, last=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.relu = nn.ReLU()\n",
        "        #self.linear = nn.Linear(64, num_classes)\n",
        "        self.fcs = nn.ModuleList([nn.Linear(64, num_classes)])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, math.sqrt(1. / 64.))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, last=False):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = Downsample(self.\n",
        "inplanes, planes * block.expansion, stride)\n",
        "\n",
        "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        if last:\n",
        "            for i in range(1, blocks - 1):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            layers.append(block(self.inplanes, planes, use_relu=False))\n",
        "\n",
        "        else:\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv_1_3x3(x)\n",
        "\n",
        "        x = self.bn_1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.stage_1(x)\n",
        "        x = self.stage_2(x)\n",
        "        x = self.stage_3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        out = []\n",
        "        for fc in self.fcs:\n",
        "            out.append(fc(x))\n",
        "        out = torch.cat(out, dim=1)\n",
        "        return out\n",
        "\n",
        "    def addOutputNodes(self, num_classes):\n",
        "        self.fcs.append(nn.Linear(64, num_classes))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ga-SaCsjkYV"
      },
      "source": [
        "### Cifar100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHAyljuejmkd"
      },
      "source": [
        "# ref:\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py\n",
        "# https://pytorch.org/docs/stable/_modules/torchvision/datasets/cifar.html#CIFAR10\n",
        "# homework2 (caltech)\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# This is an handler class for the Cifar dataset\n",
        "class CIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    `CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "    This is a subclass of the `CIFAR100` Dataset.\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar-100-python'\n",
        "    train_file = 'train'\n",
        "    test_file = 'test'\n",
        "    meta_file = 'meta'\n",
        "\n",
        "    def __init__(self, root, split = 'train', transform = None):\n",
        "        \"\"\"\n",
        "          Args:\n",
        "              root (string): Root directory of the dataset where directory\n",
        "                  cifar-100-python exists.\n",
        "              split (string, optional): If 'train', creates dataset from training\n",
        "                  set, otherwise creates from test set.\n",
        "              transform (callable, optional): A function/transform that takes in a\n",
        "                  PIL image and returns a transformed version.\n",
        "        \"\"\"\n",
        "        super(CIFAR100, self).__init__(root, transform=transform)\n",
        "\n",
        "        self.split = split\n",
        "        if split == 'train':\n",
        "            filename = self.train_file\n",
        "        else:\n",
        "            filename = self.test_file\n",
        "\n",
        "        data_path = os.path.join(self.root, self.base_folder, filename)\n",
        "        data = None\n",
        "        labels = None\n",
        "\n",
        "        with open(data_path, 'rb') as f:\n",
        "            entry = pickle.load(f, encoding='latin1')\n",
        "            data = entry['data']\n",
        "            labels = entry['fine_labels']\n",
        "        \n",
        "        data = np.vstack(data).reshape(-1, 3, 32, 32)\n",
        "        data = data.transpose((0, 2, 3, 1))  # Convert to HWC\n",
        "        \n",
        "        labels = np.array(labels)\n",
        "\n",
        "        self.df = pd.DataFrame()\n",
        "        self.df['data'] = pd.Series(list(data))\n",
        "        self.df['labels'] = labels\n",
        "\n",
        "        self.data = self.df['data']\n",
        "        self.labels = self.df['labels']\n",
        "\n",
        "        self._load_meta()\n",
        "\n",
        "    def _load_meta(self):\n",
        "        meta_path = os.path.join(self.root, self.base_folder, self.meta_file)\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f, encoding='latin1')\n",
        "            self.label_names = meta['fine_label_names']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.df.loc[index, 'data'], self.df.loc[index, 'labels']\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img) # Return a PIL image\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return index, img, target\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def getTargets(self):\n",
        "        return set(self.labels)\n",
        "     \n",
        "    # test\n",
        "    def get_indices(self, labels):\n",
        "        return list(self.df[self.df['labels'].isin(labels)].index)\n",
        "\n",
        "    def split_classes(self, n_splits=10, seed=None, dictionary_of='dataframes'):\n",
        "        if dictionary_of not in ['dataframes','indices']:\n",
        "            raise ValueError(\"'dictionary_of' must be equal to 'dataframes' or 'indices'\")\n",
        "\n",
        "        all_classes = list(self.df['labels'].value_counts().index)\n",
        "        dictionary = {}\n",
        "        random.seed(seed)\n",
        "        random.shuffle(all_classes)\n",
        "        split_size = int(len(all_classes)/n_splits)\n",
        "        for j in range(n_splits):\n",
        "            if ((j+1)*split_size < len(all_classes)):\n",
        "                split_end = (j+1)*split_size\n",
        "            else:\n",
        "                split_end = None\n",
        "            subgroup = all_classes[j*split_size:split_end]\n",
        "            if dictionary_of == 'dataframes':\n",
        "                dictionary[j] = self.df[self.df['labels'].isin(subgroup)]\n",
        "            elif dictionary_of == 'indices':\n",
        "                dictionary[j] = list(self.df[self.df['labels'].isin(subgroup)].index)\n",
        "        return dictionary\n",
        "\n",
        "    def split_groups_in_train_validation(self, groups, ratio=0.5, seed=None):\n",
        "        groups_train_val = dict()\n",
        "        for k, subdf in groups.items():\n",
        "            train_indexes = []\n",
        "            val_indexes = []\n",
        "            split_labels = list(subdf['labels'].value_counts().index)\n",
        "            for l in split_labels:\n",
        "                indexes_to_sample = list(subdf[subdf['labels'] == l].index)\n",
        "                random.seed(seed)\n",
        "                train_samples = random.sample(indexes_to_sample, int(len(indexes_to_sample)*ratio))\n",
        "                train_indexes = train_indexes + train_samples\n",
        "                val_indexes = val_indexes + list(set(indexes_to_sample).difference(set(train_samples)))\n",
        "            groups_train_val[k] = {\n",
        "                'train': train_indexes,\n",
        "                'val': val_indexes\n",
        "            }\n",
        "        return groups_train_val\n",
        "    \n",
        "    def split_in_train_val_groups(self, n_splits=10, ratio=0.5, seed=None):\n",
        "        groups = self.split_classes(n_splits=n_splits, seed=seed)\n",
        "        return self.split_groups_in_train_validation(groups, ratio=ratio, seed=seed)\n",
        "\n",
        "    # given a tensors returns an image (used in exemplars)\n",
        "    #def tensorToImg(self, tensor):\n",
        "    #   return Variable(transform(Image.fromarray(img)), volatile=True)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_M1anVYkRnF"
      },
      "source": [
        "### Reverse Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j66mUOXekTlM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "__all__ = ['ReverseIndex']\n",
        "\n",
        "class ReverseIndex():\n",
        "\n",
        "    def __init__(self, dataset, splits, device='cuda'):\n",
        "\n",
        "        self.df = pd.DataFrame(columns=['group', 'labels'])\n",
        "\n",
        "        for k in splits.keys():\n",
        "            labels = list(dataset.df.loc[splits[k]['train'],'labels'].value_counts().index)\n",
        "            group = [k for i in range(len(labels))]\n",
        "            data = pd.DataFrame(np.array([group, labels]).T, columns=['group', 'labels'])\n",
        "            self.df = self.df.append(data, ignore_index=True)\n",
        "\n",
        "        self.df['nodes'] = self.df.index\n",
        "        self.device = device\n",
        "    \n",
        "    def _changeIndex(self, reverse_index, column):\n",
        "        reverse_index = reverse_index.set_index(column)\n",
        "        reverse_index[column] = reverse_index.index\n",
        "        return reverse_index\n",
        "\n",
        "    def getLabels(self, outputs):\n",
        "        outs = outputs.cpu().numpy()\n",
        "        reverse_index = self._changeIndex(self.df, 'nodes')\n",
        "        labels = reverse_index.loc[outs, 'labels']\n",
        "\n",
        "        labels = torch.tensor(list(labels))\n",
        "        return labels.to(self.device)\n",
        "\n",
        "    def getNodes(self, labels):\n",
        "        labels = labels.cpu().numpy()\n",
        "       \n",
        "        reverse_index = self._changeIndex(self.df, 'labels')\n",
        "\n",
        "        nodes = reverse_index.loc[labels, 'nodes']\n",
        "\n",
        "        nodes = torch.tensor(list(nodes))\n",
        "        return nodes.to(self.device)\n",
        "\n",
        "    def getGroups(self, distinct=True):\n",
        "        return self.df['group'].value_counts().index.sort_values()\n",
        "    \n",
        "    def getLabelsOfGroup(self, group):\n",
        "        return self.df.loc[self.df['group'] == group, 'labels']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVfOeOCuknB8"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4QpAHzFkoW0"
      },
      "source": [
        "from torchvision import transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import Module\n",
        "\n",
        "# These are the default iCaRL hyper-parameters\n",
        "def getHyperparams():\n",
        "\tdictHyperparams = {\n",
        "\t\t\"LR\": 2,\n",
        "\t\t\"MOMENTUM\": 0.9,\n",
        "\t\t\"WEIGHT_DECAY\": 1e-5,\n",
        "\t\t\"NUM_EPOCHS\": 70,\n",
        "\t\t\"MILESTONES\": [49, 63],\n",
        "\t\t\"BATCH_SIZE\": 128,\n",
        "\t\t\"DEVICE\": 'cuda',\n",
        "\t\t\"GAMMA\": 0.2,\n",
        "\t\t\"SEED\": 66, #use 30, 42, 16\n",
        "\t\t\"LOG_FREQUENCY\": 10,\n",
        "\t\t\"NUM_CLASSES\": 100\n",
        "\t}\n",
        "\treturn dictHyperparams\n",
        "\n",
        "def getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize):\n",
        "\toptimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\tscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA, last_epoch=-1) \n",
        "\treturn optimizer, scheduler\n",
        "\n",
        "# the mean and the std have been found on the web as mean and std of cifar100\n",
        "# alternative (realistic): compute mean and std for the dataset\n",
        "def getTransformations():\n",
        "\t# Define transforms for training phase\n",
        "\ttrain_transform = transforms.Compose([transforms.RandomHorizontalFlip(), # Randomly flip the image with probability of 0.5\n",
        "\t                                      transforms.Pad(4), # Add padding\n",
        "\t                                      transforms.RandomCrop(32),# Crops a random squares of the image\n",
        "\t                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\t# Define transforms for the evaluation phase\n",
        "\teval_transform = transforms.Compose([\n",
        "\t                                      transforms.ToTensor(),\n",
        "\t                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) \n",
        "\t])\n",
        "\treturn train_transform, eval_transform\n",
        "\n",
        "# BCEWithLogits = Sigmoid + BCE, is the loss used in iCaRL\n",
        "def getLossCriterion():\n",
        "\tcriterion = nn.BCEWithLogitsLoss(reduction = 'mean') # for classification: Cross Entropy\n",
        "\treturn criterion\n",
        "\n",
        "def computeLoss(criterion, outputs, labels):\n",
        "\treturn criterion(outputs, labels)\n",
        "\n",
        "# support BCE\n",
        "def _one_hot_encode(labels, n_classes, reverse_index, dtype=None, device='cuda'):\n",
        "\tbatch_size = len(labels)\n",
        "\tenconded = torch.zeros(batch_size, n_classes, dtype=dtype, device=device)\n",
        "\tlabels=map_to_outputs(labels, reverse_index)\n",
        "\tfor i, l in enumerate(labels):\n",
        "\t  enconded[i, l] = 1\n",
        "\treturn enconded\n",
        "\n",
        "def map_to_outputs(labels, reverse_index):\n",
        "\tif reverse_index is None:\n",
        "\t  return labels\n",
        "\tif type(labels) == int:\n",
        "\t  return int(reverse_index.getNodes(torch.tensor([labels])))\n",
        "\telif type(labels) == torch.Tensor:\n",
        "\t\treturn reverse_index.getNodes(labels)\n",
        "\n",
        "\n",
        "def plotAccuracyTrend(method, data_plot_line, seed):\n",
        "\tplt.figure(figsize=(20,7))\n",
        "\taccuracyDF=pd.DataFrame(data_plot_line, columns = ['Classes','Accuracy'])\n",
        "\tax = sns.lineplot(x=\"Classes\", y=\"Accuracy\",data=accuracyDF, marker = 'o')\n",
        "\tax.minorticks_on()\n",
        "\tax.set_xticks(np.arange(10,110,10))\n",
        "\tax.set_xlim(xmin=9, xmax=101)\n",
        "\tax.set_ylim(ymin=0, ymax=1)\n",
        "\tplt.legend(['Accuracy {}'.format(method)])\n",
        "\tax.grid(axis='y')\n",
        "\tplt.title(\"Accuracies against seen classes {} - seed: {}\".format(method, seed))\n",
        "\t\n",
        "\tfilename = \"acc_{}_{}.jpg\".format(method, seed) # ex. acc_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "def plotConfusionMatrix(method, confusionMatrixData, seed):\n",
        "\tfig,ax=plt.subplots(figsize=(10,10))\n",
        "\tsns.heatmap(confusionMatrixData,cmap='terrain',ax=ax)\n",
        "\tplt.ylabel('True label')\n",
        "\tplt.xlabel('Predicted label')\n",
        "\tplt.title(\"Confusion Matrix {} - seed: {}\".format(method, seed))\n",
        "\n",
        "\tfilename = \"cm_{}_{}.jpg\".format(method, seed) # ex. cm_lwf_30\n",
        "\tplt.savefig(filename, format='png', dpi=300)\n",
        "\tplt.show()\n",
        "\n",
        "# Write down the metrics (accuracy trand and confusion matrix)\n",
        "# this method is a shortcut when perfoming multiple tests with different splits (random_seed)\n",
        "# and allow us to plot on the same graph the data from multiple models (accuracy)\n",
        "def writeMetrics(method, seed, accuracies, confusionMatrixData):\n",
        "  data = {}\n",
        "  data['accuracies'] = []\n",
        "  data['cm'] = [] #cm line\n",
        "  i = 0\n",
        "  for classes_seen in range(10, 110, 10): #x axis on the plot\n",
        "    data['accuracies'].append({classes_seen : accuracies[i]}) \n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  for class_num in range(0,len(confusionMatrixData)): #rows of the cm\n",
        "    data['cm'].append({class_num : confusionMatrixData[i].tolist()}) \n",
        "    i += 1\n",
        "  \n",
        "  # dump to file\n",
        "  aus = method + '_' + str(seed)\n",
        "  filename = 'data_{}.json'.format(aus)\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "\n",
        "# Functions\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getResNet32():\n",
        "    net = CifarResNet()\n",
        "    # net.fc = nn.Linear(net.fc.in_features, output_size) # embedded in the class\n",
        "\n",
        "    criterion = getLossCriterion()\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return net, criterion, optimizer, scheduler\n",
        "\n",
        "def addOutputs(net, num):\n",
        "    net.addOutputNodes(num)\n",
        "\n",
        "def getNet():\n",
        "    return getResNet32()\n",
        "\n",
        "def getSchedulerOptimizer(net):\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer, scheduler = getOptimizerScheduler(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, parameters_to_optimize)\n",
        "    return optimizer, scheduler"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfJj1z2LSxp0"
      },
      "source": [
        "### DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysghtAWOPYZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c50ee9-1444-4ace-af3f-de6825a906bc"
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "if not os.path.isdir('./{}'.format(\"$DATA_DIR/cifar-100-python\")):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-07 15:17:43--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  46.9MB/s    in 3.8s    \n",
            "\n",
            "2021-06-07 15:17:48 (41.9 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY10HfpTS1lq"
      },
      "source": [
        "### HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-pqSNg4_Ris"
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "NUM_CLASSES = 100\n",
        "BATCH_SIZE = 128 \n",
        "LR = 0.1         \n",
        "MOMENTUM = 0.9     \n",
        "WEIGHT_DECAY = 1e-05\n",
        "NUM_EPOCHS = 1 \n",
        "GAMMA = 0.2\n",
        "LOG_FREQUENCY = 10\n",
        "MILESTONES = [49,63]\n",
        "RANDOM_SEED = 1993"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW7WxV_QS4re"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c_jHycn_1kk"
      },
      "source": [
        "train_transform, eval_transform = getTransformations()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPrwhTcIqUbA"
      },
      "source": [
        "### train & test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJKwvGljJj2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f41468d6-91d5-4c83-8d5d-dc6f484df260"
      },
      "source": [
        "# Import dataset and apply transformations \n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# Check datasets length \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnooWwcyS_YN"
      },
      "source": [
        "### SPLIT DATA IN CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckn3H69iJj2X"
      },
      "source": [
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpcJvhxhJOLc"
      },
      "source": [
        "# TRAIN / VAL split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "\n",
        "# TEST split\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MSItI0QVpn"
      },
      "source": [
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10):\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpWv5ZkhxTPJ"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4NQa-wNxWL1"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pisadUTPxcRP"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "  \n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            # if (group_id > 1):\n",
        "            #   old_features = old_net.forward(images)\n",
        "            #   old_outputs = old_net.predict(old_features)\n",
        "            #   labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX_61nSuxh3v"
      },
      "source": [
        "### sequential learning fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM0GysHZxk1B"
      },
      "source": [
        "### Fine tuning\n",
        "def sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    old_net = None\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    #confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2X2IVZ-xrVT"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGpd8lslxtUW",
        "outputId": "2b44f991-1d78-4855-bea7-87d6a9132ff4"
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningFineTuning(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/70, LR = [0.1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 0.767611026763916\n",
            "Train step - Step 10, Loss 0.3055065870285034\n",
            "Train step - Step 20, Loss 0.2840235233306885\n",
            "Train step - Step 30, Loss 0.2555047571659088\n",
            "Train epoch - Accuracy: 0.29434343434343435 Loss: 0.34861349815070025 Corrects: 1457\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.2527182996273041\n",
            "Train step - Step 50, Loss 0.2743365466594696\n",
            "Train step - Step 60, Loss 0.24633298814296722\n",
            "Train step - Step 70, Loss 0.2645392119884491\n",
            "Train epoch - Accuracy: 0.3987878787878788 Loss: 0.24913429067592427 Corrects: 1974\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.2458353489637375\n",
            "Train step - Step 90, Loss 0.22875867784023285\n",
            "Train step - Step 100, Loss 0.24039344489574432\n",
            "Train step - Step 110, Loss 0.2521066963672638\n",
            "Train epoch - Accuracy: 0.4446464646464646 Loss: 0.2393503865148082 Corrects: 2201\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.2374119758605957\n",
            "Train step - Step 130, Loss 0.22671543061733246\n",
            "Train step - Step 140, Loss 0.22434572875499725\n",
            "Train step - Step 150, Loss 0.2108890265226364\n",
            "Train epoch - Accuracy: 0.4676767676767677 Loss: 0.2291866770477006 Corrects: 2315\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.21194438636302948\n",
            "Train step - Step 170, Loss 0.21664921939373016\n",
            "Train step - Step 180, Loss 0.22545400261878967\n",
            "Train step - Step 190, Loss 0.20940791070461273\n",
            "Train epoch - Accuracy: 0.4822222222222222 Loss: 0.22285805300630704 Corrects: 2387\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.2194495052099228\n",
            "Train step - Step 210, Loss 0.22435317933559418\n",
            "Train step - Step 220, Loss 0.22630193829536438\n",
            "Train step - Step 230, Loss 0.2118579000234604\n",
            "Train epoch - Accuracy: 0.5153535353535353 Loss: 0.21604947575415023 Corrects: 2551\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.2210996150970459\n",
            "Train step - Step 250, Loss 0.200479656457901\n",
            "Train step - Step 260, Loss 0.18607263267040253\n",
            "Train step - Step 270, Loss 0.19961336255073547\n",
            "Train epoch - Accuracy: 0.5345454545454545 Loss: 0.20708442610321623 Corrects: 2646\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.2128882259130478\n",
            "Train step - Step 290, Loss 0.20366787910461426\n",
            "Train step - Step 300, Loss 0.22154922783374786\n",
            "Train step - Step 310, Loss 0.22403739392757416\n",
            "Train epoch - Accuracy: 0.5412121212121213 Loss: 0.20425522707327448 Corrects: 2679\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.20081201195716858\n",
            "Train step - Step 330, Loss 0.18710796535015106\n",
            "Train step - Step 340, Loss 0.19191424548625946\n",
            "Train step - Step 350, Loss 0.1904316246509552\n",
            "Train epoch - Accuracy: 0.5567676767676768 Loss: 0.19814761435142672 Corrects: 2756\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.19875626266002655\n",
            "Train step - Step 370, Loss 0.20463135838508606\n",
            "Train step - Step 380, Loss 0.1826702207326889\n",
            "Train epoch - Accuracy: 0.5492929292929293 Loss: 0.19759756876362694 Corrects: 2719\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.1880079060792923\n",
            "Train step - Step 400, Loss 0.1761585772037506\n",
            "Train step - Step 410, Loss 0.19462056457996368\n",
            "Train step - Step 420, Loss 0.18261544406414032\n",
            "Train epoch - Accuracy: 0.5654545454545454 Loss: 0.1914005540476905 Corrects: 2799\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.188619926571846\n",
            "Train step - Step 440, Loss 0.19580481946468353\n",
            "Train step - Step 450, Loss 0.20786015689373016\n",
            "Train step - Step 460, Loss 0.2021411508321762\n",
            "Train epoch - Accuracy: 0.5888888888888889 Loss: 0.18595228883955214 Corrects: 2915\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.1783556193113327\n",
            "Train step - Step 480, Loss 0.18291021883487701\n",
            "Train step - Step 490, Loss 0.17217327654361725\n",
            "Train step - Step 500, Loss 0.1719745695590973\n",
            "Train epoch - Accuracy: 0.6038383838383838 Loss: 0.1825932342054868 Corrects: 2989\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.2037982940673828\n",
            "Train step - Step 520, Loss 0.166042298078537\n",
            "Train step - Step 530, Loss 0.18999306857585907\n",
            "Train step - Step 540, Loss 0.16880714893341064\n",
            "Train epoch - Accuracy: 0.6012121212121212 Loss: 0.18076736577231475 Corrects: 2976\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.161343052983284\n",
            "Train step - Step 560, Loss 0.19617584347724915\n",
            "Train step - Step 570, Loss 0.1797410100698471\n",
            "Train step - Step 580, Loss 0.17382049560546875\n",
            "Train epoch - Accuracy: 0.6232323232323232 Loss: 0.17416856027612782 Corrects: 3085\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.1599704921245575\n",
            "Train step - Step 600, Loss 0.17038653790950775\n",
            "Train step - Step 610, Loss 0.1722535341978073\n",
            "Train step - Step 620, Loss 0.1509934961795807\n",
            "Train epoch - Accuracy: 0.63010101010101 Loss: 0.17045772668087122 Corrects: 3119\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.15959201753139496\n",
            "Train step - Step 640, Loss 0.1857101023197174\n",
            "Train step - Step 650, Loss 0.1644548624753952\n",
            "Train step - Step 660, Loss 0.18616260588169098\n",
            "Train epoch - Accuracy: 0.6418181818181818 Loss: 0.1659425301804687 Corrects: 3177\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.1493934839963913\n",
            "Train step - Step 680, Loss 0.16844038665294647\n",
            "Train step - Step 690, Loss 0.17953501641750336\n",
            "Train step - Step 700, Loss 0.16625748574733734\n",
            "Train epoch - Accuracy: 0.6496969696969697 Loss: 0.16359886820870218 Corrects: 3216\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.16083469986915588\n",
            "Train step - Step 720, Loss 0.16493770480155945\n",
            "Train step - Step 730, Loss 0.16669994592666626\n",
            "Train step - Step 740, Loss 0.14499220252037048\n",
            "Train epoch - Accuracy: 0.6602020202020202 Loss: 0.15953951586376536 Corrects: 3268\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.1483915150165558\n",
            "Train step - Step 760, Loss 0.18057504296302795\n",
            "Train step - Step 770, Loss 0.1825159639120102\n",
            "Train epoch - Accuracy: 0.6652525252525252 Loss: 0.15876834649630267 Corrects: 3293\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.15373972058296204\n",
            "Train step - Step 790, Loss 0.14376501739025116\n",
            "Train step - Step 800, Loss 0.18242628872394562\n",
            "Train step - Step 810, Loss 0.1378394067287445\n",
            "Train epoch - Accuracy: 0.6682828282828283 Loss: 0.15505649876714958 Corrects: 3308\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.1297774314880371\n",
            "Train step - Step 830, Loss 0.1524980515241623\n",
            "Train step - Step 840, Loss 0.1506941318511963\n",
            "Train step - Step 850, Loss 0.15540675818920135\n",
            "Train epoch - Accuracy: 0.6812121212121212 Loss: 0.1519573307579214 Corrects: 3372\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.13424502313137054\n",
            "Train step - Step 870, Loss 0.15309564769268036\n",
            "Train step - Step 880, Loss 0.15897302329540253\n",
            "Train step - Step 890, Loss 0.15569522976875305\n",
            "Train epoch - Accuracy: 0.6868686868686869 Loss: 0.148031689223617 Corrects: 3400\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.1327926367521286\n",
            "Train step - Step 910, Loss 0.13251470029354095\n",
            "Train step - Step 920, Loss 0.13004253804683685\n",
            "Train step - Step 930, Loss 0.13533596694469452\n",
            "Train epoch - Accuracy: 0.6963636363636364 Loss: 0.1440224988111342 Corrects: 3447\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.1455235332250595\n",
            "Train step - Step 950, Loss 0.14498889446258545\n",
            "Train step - Step 960, Loss 0.13298237323760986\n",
            "Train step - Step 970, Loss 0.139012411236763\n",
            "Train epoch - Accuracy: 0.7113131313131313 Loss: 0.1410865326391326 Corrects: 3521\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.1449199765920639\n",
            "Train step - Step 990, Loss 0.125033438205719\n",
            "Train step - Step 1000, Loss 0.15103010833263397\n",
            "Train step - Step 1010, Loss 0.14698369801044464\n",
            "Train epoch - Accuracy: 0.7111111111111111 Loss: 0.13918141982170065 Corrects: 3520\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.14627908170223236\n",
            "Train step - Step 1030, Loss 0.11172983795404434\n",
            "Train step - Step 1040, Loss 0.13937900960445404\n",
            "Train step - Step 1050, Loss 0.1467704325914383\n",
            "Train epoch - Accuracy: 0.7183838383838383 Loss: 0.1361691221986154 Corrects: 3556\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.13681188225746155\n",
            "Train step - Step 1070, Loss 0.15181498229503632\n",
            "Train step - Step 1080, Loss 0.15618866682052612\n",
            "Train step - Step 1090, Loss 0.12844620645046234\n",
            "Train epoch - Accuracy: 0.713939393939394 Loss: 0.13841984666357138 Corrects: 3534\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.1270679533481598\n",
            "Train step - Step 1110, Loss 0.16798816621303558\n",
            "Train step - Step 1120, Loss 0.13143497705459595\n",
            "Train step - Step 1130, Loss 0.1359071284532547\n",
            "Train epoch - Accuracy: 0.7282828282828283 Loss: 0.1331657945206671 Corrects: 3605\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.11902417987585068\n",
            "Train step - Step 1150, Loss 0.11873867362737656\n",
            "Train step - Step 1160, Loss 0.12367868423461914\n",
            "Train epoch - Accuracy: 0.7361616161616161 Loss: 0.12830214732223086 Corrects: 3644\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.11188191175460815\n",
            "Train step - Step 1180, Loss 0.12881885468959808\n",
            "Train step - Step 1190, Loss 0.12403557449579239\n",
            "Train step - Step 1200, Loss 0.15595345199108124\n",
            "Train epoch - Accuracy: 0.7458585858585859 Loss: 0.1277258534955256 Corrects: 3692\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.11178221553564072\n",
            "Train step - Step 1220, Loss 0.11067507416009903\n",
            "Train step - Step 1230, Loss 0.12654636800289154\n",
            "Train step - Step 1240, Loss 0.10075562447309494\n",
            "Train epoch - Accuracy: 0.7581818181818182 Loss: 0.12079872093718462 Corrects: 3753\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.11354684084653854\n",
            "Train step - Step 1260, Loss 0.13100717961788177\n",
            "Train step - Step 1270, Loss 0.11979227513074875\n",
            "Train step - Step 1280, Loss 0.12430484592914581\n",
            "Train epoch - Accuracy: 0.7553535353535353 Loss: 0.12299975345532099 Corrects: 3739\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.14646445214748383\n",
            "Train step - Step 1300, Loss 0.11100027710199356\n",
            "Train step - Step 1310, Loss 0.10693354904651642\n",
            "Train step - Step 1320, Loss 0.11762652546167374\n",
            "Train epoch - Accuracy: 0.7674747474747474 Loss: 0.11733839429388143 Corrects: 3799\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.10363650321960449\n",
            "Train step - Step 1340, Loss 0.1130787655711174\n",
            "Train step - Step 1350, Loss 0.1379060596227646\n",
            "Train step - Step 1360, Loss 0.14313898980617523\n",
            "Train epoch - Accuracy: 0.7640404040404041 Loss: 0.11834214120200186 Corrects: 3782\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.11001982539892197\n",
            "Train step - Step 1380, Loss 0.10001325607299805\n",
            "Train step - Step 1390, Loss 0.12546668946743011\n",
            "Train step - Step 1400, Loss 0.12907181680202484\n",
            "Train epoch - Accuracy: 0.7561616161616161 Loss: 0.11912461498771051 Corrects: 3743\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.1308506280183792\n",
            "Train step - Step 1420, Loss 0.11254481226205826\n",
            "Train step - Step 1430, Loss 0.11309249699115753\n",
            "Train step - Step 1440, Loss 0.10305257141590118\n",
            "Train epoch - Accuracy: 0.7694949494949495 Loss: 0.11494217787728166 Corrects: 3809\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.09614896029233932\n",
            "Train step - Step 1460, Loss 0.11210034042596817\n",
            "Train step - Step 1470, Loss 0.10799568146467209\n",
            "Train step - Step 1480, Loss 0.11846452206373215\n",
            "Train epoch - Accuracy: 0.776969696969697 Loss: 0.1090032665085311 Corrects: 3846\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.10952156037092209\n",
            "Train step - Step 1500, Loss 0.12141718715429306\n",
            "Train step - Step 1510, Loss 0.10894111543893814\n",
            "Train step - Step 1520, Loss 0.09546183794736862\n",
            "Train epoch - Accuracy: 0.7854545454545454 Loss: 0.10711824673895884 Corrects: 3888\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.0970182716846466\n",
            "Train step - Step 1540, Loss 0.11929245293140411\n",
            "Train step - Step 1550, Loss 0.09755498915910721\n",
            "Train epoch - Accuracy: 0.7913131313131313 Loss: 0.10404397698965939 Corrects: 3917\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.09529400616884232\n",
            "Train step - Step 1570, Loss 0.14100365340709686\n",
            "Train step - Step 1580, Loss 0.09726891666650772\n",
            "Train step - Step 1590, Loss 0.10429747402667999\n",
            "Train epoch - Accuracy: 0.7864646464646464 Loss: 0.10573700643248028 Corrects: 3893\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.1220022588968277\n",
            "Train step - Step 1610, Loss 0.10925435274839401\n",
            "Train step - Step 1620, Loss 0.10504128783941269\n",
            "Train step - Step 1630, Loss 0.10665321350097656\n",
            "Train epoch - Accuracy: 0.7997979797979798 Loss: 0.10055167252066159 Corrects: 3959\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.10297422856092453\n",
            "Train step - Step 1650, Loss 0.10935431718826294\n",
            "Train step - Step 1660, Loss 0.11341118812561035\n",
            "Train step - Step 1670, Loss 0.11647369712591171\n",
            "Train epoch - Accuracy: 0.796969696969697 Loss: 0.10274452474683222 Corrects: 3945\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.08454947918653488\n",
            "Train step - Step 1690, Loss 0.10947953909635544\n",
            "Train step - Step 1700, Loss 0.08325108140707016\n",
            "Train step - Step 1710, Loss 0.07999908924102783\n",
            "Train epoch - Accuracy: 0.8109090909090909 Loss: 0.09729910804165734 Corrects: 4014\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.0967489629983902\n",
            "Train step - Step 1730, Loss 0.1068764477968216\n",
            "Train step - Step 1740, Loss 0.10271306335926056\n",
            "Train step - Step 1750, Loss 0.114781454205513\n",
            "Train epoch - Accuracy: 0.8016161616161617 Loss: 0.09862018324208982 Corrects: 3968\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.09966225922107697\n",
            "Train step - Step 1770, Loss 0.07720647007226944\n",
            "Train step - Step 1780, Loss 0.09544558823108673\n",
            "Train step - Step 1790, Loss 0.08719921857118607\n",
            "Train epoch - Accuracy: 0.822020202020202 Loss: 0.09222229959687801 Corrects: 4069\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.09645551443099976\n",
            "Train step - Step 1810, Loss 0.09624357521533966\n",
            "Train step - Step 1820, Loss 0.08667024970054626\n",
            "Train step - Step 1830, Loss 0.09086474031209946\n",
            "Train epoch - Accuracy: 0.821010101010101 Loss: 0.09237819242658037 Corrects: 4064\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.09767164289951324\n",
            "Train step - Step 1850, Loss 0.09276225417852402\n",
            "Train step - Step 1860, Loss 0.08490593731403351\n",
            "Train step - Step 1870, Loss 0.09260158985853195\n",
            "Train epoch - Accuracy: 0.816969696969697 Loss: 0.09136466069956019 Corrects: 4044\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.1015307679772377\n",
            "Train step - Step 1890, Loss 0.08381729573011398\n",
            "Train step - Step 1900, Loss 0.09543962776660919\n",
            "Train step - Step 1910, Loss 0.07473569363355637\n",
            "Train epoch - Accuracy: 0.8107070707070707 Loss: 0.0953116624433585 Corrects: 4013\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.06154933199286461\n",
            "Train step - Step 1930, Loss 0.060101110488176346\n",
            "Train step - Step 1940, Loss 0.060836780816316605\n",
            "Train epoch - Accuracy: 0.8634343434343434 Loss: 0.07311292726584155 Corrects: 4274\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.06942512840032578\n",
            "Train step - Step 1960, Loss 0.07010430097579956\n",
            "Train step - Step 1970, Loss 0.049065735191106796\n",
            "Train step - Step 1980, Loss 0.07144930213689804\n",
            "Train epoch - Accuracy: 0.8785858585858586 Loss: 0.06598696447984137 Corrects: 4349\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.05649774149060249\n",
            "Train step - Step 2000, Loss 0.06138712912797928\n",
            "Train step - Step 2010, Loss 0.05927126482129097\n",
            "Train step - Step 2020, Loss 0.06899639219045639\n",
            "Train epoch - Accuracy: 0.8870707070707071 Loss: 0.06366226668309684 Corrects: 4391\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.05331916734576225\n",
            "Train step - Step 2040, Loss 0.06714595854282379\n",
            "Train step - Step 2050, Loss 0.05181267485022545\n",
            "Train step - Step 2060, Loss 0.0438183955848217\n",
            "Train epoch - Accuracy: 0.895959595959596 Loss: 0.05940094818521027 Corrects: 4435\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.047994859516620636\n",
            "Train step - Step 2080, Loss 0.06112179160118103\n",
            "Train step - Step 2090, Loss 0.06562960147857666\n",
            "Train step - Step 2100, Loss 0.04807380586862564\n",
            "Train epoch - Accuracy: 0.8955555555555555 Loss: 0.05813769307702479 Corrects: 4433\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.04686129838228226\n",
            "Train step - Step 2120, Loss 0.05487583950161934\n",
            "Train step - Step 2130, Loss 0.04855211451649666\n",
            "Train step - Step 2140, Loss 0.04490039870142937\n",
            "Train epoch - Accuracy: 0.9040404040404041 Loss: 0.05659200810874351 Corrects: 4475\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.0546119287610054\n",
            "Train step - Step 2160, Loss 0.06371113657951355\n",
            "Train step - Step 2170, Loss 0.05515799671411514\n",
            "Train step - Step 2180, Loss 0.05823478102684021\n",
            "Train epoch - Accuracy: 0.9024242424242425 Loss: 0.05556321816612976 Corrects: 4467\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.07555873692035675\n",
            "Train step - Step 2200, Loss 0.06298745423555374\n",
            "Train step - Step 2210, Loss 0.05196355655789375\n",
            "Train step - Step 2220, Loss 0.0651683583855629\n",
            "Train epoch - Accuracy: 0.9038383838383839 Loss: 0.05714471035834515 Corrects: 4474\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.042365990579128265\n",
            "Train step - Step 2240, Loss 0.052081432193517685\n",
            "Train step - Step 2250, Loss 0.053845155984163284\n",
            "Train step - Step 2260, Loss 0.04561092332005501\n",
            "Train epoch - Accuracy: 0.9076767676767676 Loss: 0.0547286997238795 Corrects: 4493\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.07596009969711304\n",
            "Train step - Step 2280, Loss 0.05890824273228645\n",
            "Train step - Step 2290, Loss 0.07505764812231064\n",
            "Train step - Step 2300, Loss 0.06092827394604683\n",
            "Train epoch - Accuracy: 0.9050505050505051 Loss: 0.054641181313329276 Corrects: 4480\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.07577773183584213\n",
            "Train step - Step 2320, Loss 0.059476327151060104\n",
            "Train step - Step 2330, Loss 0.04484550282359123\n",
            "Train epoch - Accuracy: 0.9042424242424243 Loss: 0.05488927788204617 Corrects: 4476\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.04297078773379326\n",
            "Train step - Step 2350, Loss 0.060686349868774414\n",
            "Train step - Step 2360, Loss 0.04798252508044243\n",
            "Train step - Step 2370, Loss 0.06523225456476212\n",
            "Train epoch - Accuracy: 0.9143434343434343 Loss: 0.051984320863930865 Corrects: 4526\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.04312660172581673\n",
            "Train step - Step 2390, Loss 0.048522334545850754\n",
            "Train step - Step 2400, Loss 0.06112672761082649\n",
            "Train step - Step 2410, Loss 0.04545595869421959\n",
            "Train epoch - Accuracy: 0.9109090909090909 Loss: 0.052969564694647836 Corrects: 4509\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.04998675361275673\n",
            "Train step - Step 2430, Loss 0.04801177233457565\n",
            "Train step - Step 2440, Loss 0.05545699596405029\n",
            "Train step - Step 2450, Loss 0.04757986217737198\n",
            "Train epoch - Accuracy: 0.914949494949495 Loss: 0.049558305473941744 Corrects: 4529\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.04576239734888077\n",
            "Train step - Step 2470, Loss 0.0413895919919014\n",
            "Train step - Step 2480, Loss 0.04847611114382744\n",
            "Train step - Step 2490, Loss 0.054551269859075546\n",
            "Train epoch - Accuracy: 0.9218181818181819 Loss: 0.04783695155171433 Corrects: 4563\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.0540720634162426\n",
            "Train step - Step 2510, Loss 0.04216070845723152\n",
            "Train step - Step 2520, Loss 0.054631829261779785\n",
            "Train step - Step 2530, Loss 0.07392524927854538\n",
            "Train epoch - Accuracy: 0.9296969696969697 Loss: 0.045625086047432636 Corrects: 4602\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.04222388193011284\n",
            "Train step - Step 2550, Loss 0.04434574395418167\n",
            "Train step - Step 2560, Loss 0.040665533393621445\n",
            "Train step - Step 2570, Loss 0.03880590200424194\n",
            "Train epoch - Accuracy: 0.9244444444444444 Loss: 0.046216452808091134 Corrects: 4576\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.041849005967378616\n",
            "Train step - Step 2590, Loss 0.03650378808379173\n",
            "Train step - Step 2600, Loss 0.03984680771827698\n",
            "Train step - Step 2610, Loss 0.05076181888580322\n",
            "Train epoch - Accuracy: 0.9301010101010101 Loss: 0.04373829779751373 Corrects: 4604\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.05723428726196289\n",
            "Train step - Step 2630, Loss 0.04967765137553215\n",
            "Train step - Step 2640, Loss 0.047086235135793686\n",
            "Train step - Step 2650, Loss 0.047161296010017395\n",
            "Train epoch - Accuracy: 0.9282828282828283 Loss: 0.04488019978458231 Corrects: 4595\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.042705412954092026\n",
            "Train step - Step 2670, Loss 0.046682607382535934\n",
            "Train step - Step 2680, Loss 0.03624289855360985\n",
            "Train step - Step 2690, Loss 0.04015031456947327\n",
            "Train epoch - Accuracy: 0.9286868686868687 Loss: 0.0441076221730974 Corrects: 4597\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.05220557376742363\n",
            "Train step - Step 2710, Loss 0.058807577937841415\n",
            "Train step - Step 2720, Loss 0.04598351940512657\n",
            "Train epoch - Accuracy: 0.9220202020202021 Loss: 0.0464898539176493 Corrects: 4564\n",
            "Training finished in 209.54286432266235 seconds\n",
            "EVALUATION:  0.74 0.17884738743305206\n",
            "TEST GROUP:  0.751\n",
            "TEST ALL:  0.751\n",
            "GROUP:  2\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.42272859811782837\n",
            "Train step - Step 10, Loss 0.17600317299365997\n",
            "Train step - Step 20, Loss 0.1555972397327423\n",
            "Train step - Step 30, Loss 0.13245825469493866\n",
            "Train epoch - Accuracy: 0.30282828282828284 Loss: 0.17426323670028437 Corrects: 1499\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.11472351849079132\n",
            "Train step - Step 50, Loss 0.1064465194940567\n",
            "Train step - Step 60, Loss 0.10313957929611206\n",
            "Train step - Step 70, Loss 0.09223984181880951\n",
            "Train epoch - Accuracy: 0.5476767676767677 Loss: 0.10648916654514544 Corrects: 2711\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.09351053088903427\n",
            "Train step - Step 90, Loss 0.0990033894777298\n",
            "Train step - Step 100, Loss 0.09144433587789536\n",
            "Train step - Step 110, Loss 0.0971401110291481\n",
            "Train epoch - Accuracy: 0.6317171717171717 Loss: 0.09000671314169661 Corrects: 3127\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.08214541524648666\n",
            "Train step - Step 130, Loss 0.08096738904714584\n",
            "Train step - Step 140, Loss 0.08432935178279877\n",
            "Train step - Step 150, Loss 0.07355822622776031\n",
            "Train epoch - Accuracy: 0.6838383838383838 Loss: 0.07953354303884988 Corrects: 3385\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.07038845866918564\n",
            "Train step - Step 170, Loss 0.06948018074035645\n",
            "Train step - Step 180, Loss 0.07261021435260773\n",
            "Train step - Step 190, Loss 0.07469141483306885\n",
            "Train epoch - Accuracy: 0.7062626262626263 Loss: 0.07375080531895763 Corrects: 3496\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.06955333054065704\n",
            "Train step - Step 210, Loss 0.06023451313376427\n",
            "Train step - Step 220, Loss 0.06375910341739655\n",
            "Train step - Step 230, Loss 0.0780850350856781\n",
            "Train epoch - Accuracy: 0.7406060606060606 Loss: 0.06787662272953024 Corrects: 3666\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.07278511673212051\n",
            "Train step - Step 250, Loss 0.07141240686178207\n",
            "Train step - Step 260, Loss 0.05375766381621361\n",
            "Train step - Step 270, Loss 0.06593552976846695\n",
            "Train epoch - Accuracy: 0.7531313131313131 Loss: 0.06393959099295163 Corrects: 3728\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.07128087431192398\n",
            "Train step - Step 290, Loss 0.06628139317035675\n",
            "Train step - Step 300, Loss 0.06137201935052872\n",
            "Train step - Step 310, Loss 0.05795501545071602\n",
            "Train epoch - Accuracy: 0.7630303030303031 Loss: 0.06104947110017141 Corrects: 3777\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.06157146766781807\n",
            "Train step - Step 330, Loss 0.05645650252699852\n",
            "Train step - Step 340, Loss 0.05139952898025513\n",
            "Train step - Step 350, Loss 0.06664836406707764\n",
            "Train epoch - Accuracy: 0.781010101010101 Loss: 0.05702110266444659 Corrects: 3866\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.0634487196803093\n",
            "Train step - Step 370, Loss 0.059833135455846786\n",
            "Train step - Step 380, Loss 0.05829058960080147\n",
            "Train epoch - Accuracy: 0.7935353535353535 Loss: 0.054505899817955616 Corrects: 3928\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.043461330235004425\n",
            "Train step - Step 400, Loss 0.046652041375637054\n",
            "Train step - Step 410, Loss 0.04931797459721565\n",
            "Train step - Step 420, Loss 0.04622682183980942\n",
            "Train epoch - Accuracy: 0.7987878787878788 Loss: 0.05339202960181718 Corrects: 3954\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.04988285154104233\n",
            "Train step - Step 440, Loss 0.06041494756937027\n",
            "Train step - Step 450, Loss 0.054971564561128616\n",
            "Train step - Step 460, Loss 0.0413401834666729\n",
            "Train epoch - Accuracy: 0.8054545454545454 Loss: 0.05178721062613256 Corrects: 3987\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.054722268134355545\n",
            "Train step - Step 480, Loss 0.04880610853433609\n",
            "Train step - Step 490, Loss 0.04628448560833931\n",
            "Train step - Step 500, Loss 0.048169538378715515\n",
            "Train epoch - Accuracy: 0.8222222222222222 Loss: 0.04791420710207236 Corrects: 4070\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.050282031297683716\n",
            "Train step - Step 520, Loss 0.03494596108794212\n",
            "Train step - Step 530, Loss 0.04221661761403084\n",
            "Train step - Step 540, Loss 0.04389793053269386\n",
            "Train epoch - Accuracy: 0.8244444444444444 Loss: 0.04632222418682744 Corrects: 4081\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.04929810389876366\n",
            "Train step - Step 560, Loss 0.04638102278113365\n",
            "Train step - Step 570, Loss 0.048484910279512405\n",
            "Train step - Step 580, Loss 0.06039813160896301\n",
            "Train epoch - Accuracy: 0.8323232323232324 Loss: 0.0445557932766399 Corrects: 4120\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.0425211638212204\n",
            "Train step - Step 600, Loss 0.03888901323080063\n",
            "Train step - Step 610, Loss 0.04322931542992592\n",
            "Train step - Step 620, Loss 0.034250181168317795\n",
            "Train epoch - Accuracy: 0.8414141414141414 Loss: 0.04318053407349972 Corrects: 4165\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.0381946936249733\n",
            "Train step - Step 640, Loss 0.04614660143852234\n",
            "Train step - Step 650, Loss 0.04062681272625923\n",
            "Train step - Step 660, Loss 0.048864252865314484\n",
            "Train epoch - Accuracy: 0.8478787878787879 Loss: 0.04082781466269734 Corrects: 4197\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.03680748865008354\n",
            "Train step - Step 680, Loss 0.0290814395993948\n",
            "Train step - Step 690, Loss 0.04277855157852173\n",
            "Train step - Step 700, Loss 0.0493503101170063\n",
            "Train epoch - Accuracy: 0.8507070707070707 Loss: 0.040738791099401435 Corrects: 4211\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.040796082466840744\n",
            "Train step - Step 720, Loss 0.031437572091817856\n",
            "Train step - Step 730, Loss 0.03972744569182396\n",
            "Train step - Step 740, Loss 0.03753506392240524\n",
            "Train epoch - Accuracy: 0.8551515151515151 Loss: 0.039119230246905126 Corrects: 4233\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.03887832164764404\n",
            "Train step - Step 760, Loss 0.03570182994008064\n",
            "Train step - Step 770, Loss 0.03725017234683037\n",
            "Train epoch - Accuracy: 0.8561616161616161 Loss: 0.039006322020232076 Corrects: 4238\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.03728816658258438\n",
            "Train step - Step 790, Loss 0.03636015206575394\n",
            "Train step - Step 800, Loss 0.03554698824882507\n",
            "Train step - Step 810, Loss 0.03939429670572281\n",
            "Train epoch - Accuracy: 0.8636363636363636 Loss: 0.03639250325434136 Corrects: 4275\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.0327034555375576\n",
            "Train step - Step 830, Loss 0.039961930364370346\n",
            "Train step - Step 840, Loss 0.02612832747399807\n",
            "Train step - Step 850, Loss 0.04066559672355652\n",
            "Train epoch - Accuracy: 0.8701010101010102 Loss: 0.035537131484409776 Corrects: 4307\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.03262854367494583\n",
            "Train step - Step 870, Loss 0.034322287887334824\n",
            "Train step - Step 880, Loss 0.03652643784880638\n",
            "Train step - Step 890, Loss 0.032198552042245865\n",
            "Train epoch - Accuracy: 0.8816161616161616 Loss: 0.033525267524851694 Corrects: 4364\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.031093811616301537\n",
            "Train step - Step 910, Loss 0.03035566583275795\n",
            "Train step - Step 920, Loss 0.04600569233298302\n",
            "Train step - Step 930, Loss 0.042968474328517914\n",
            "Train epoch - Accuracy: 0.8765656565656565 Loss: 0.0331175358074181 Corrects: 4339\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.032595194876194\n",
            "Train step - Step 950, Loss 0.021250922232866287\n",
            "Train step - Step 960, Loss 0.03973647579550743\n",
            "Train step - Step 970, Loss 0.03334878012537956\n",
            "Train epoch - Accuracy: 0.883030303030303 Loss: 0.03286476043137637 Corrects: 4371\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.028976544737815857\n",
            "Train step - Step 990, Loss 0.035048991441726685\n",
            "Train step - Step 1000, Loss 0.03276829794049263\n",
            "Train step - Step 1010, Loss 0.032798197120428085\n",
            "Train epoch - Accuracy: 0.8892929292929292 Loss: 0.03106408756339189 Corrects: 4402\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.026273751631379128\n",
            "Train step - Step 1030, Loss 0.03313445672392845\n",
            "Train step - Step 1040, Loss 0.03621968626976013\n",
            "Train step - Step 1050, Loss 0.02587118186056614\n",
            "Train epoch - Accuracy: 0.8953535353535353 Loss: 0.030501785261763465 Corrects: 4432\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.022076522931456566\n",
            "Train step - Step 1070, Loss 0.024075880646705627\n",
            "Train step - Step 1080, Loss 0.02330426685512066\n",
            "Train step - Step 1090, Loss 0.031005073338747025\n",
            "Train epoch - Accuracy: 0.8935353535353535 Loss: 0.03004176609895446 Corrects: 4423\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.030616050586104393\n",
            "Train step - Step 1110, Loss 0.03023497201502323\n",
            "Train step - Step 1120, Loss 0.023054098710417747\n",
            "Train step - Step 1130, Loss 0.031105389818549156\n",
            "Train epoch - Accuracy: 0.897979797979798 Loss: 0.02915352973748337 Corrects: 4445\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.02731539122760296\n",
            "Train step - Step 1150, Loss 0.029194099828600883\n",
            "Train step - Step 1160, Loss 0.03234310448169708\n",
            "Train epoch - Accuracy: 0.8997979797979798 Loss: 0.027918919231554474 Corrects: 4454\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.024542635306715965\n",
            "Train step - Step 1180, Loss 0.025994187220931053\n",
            "Train step - Step 1190, Loss 0.021290605887770653\n",
            "Train step - Step 1200, Loss 0.026047050952911377\n",
            "Train epoch - Accuracy: 0.8997979797979798 Loss: 0.02737667182178208 Corrects: 4454\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.029464134946465492\n",
            "Train step - Step 1220, Loss 0.019589651376008987\n",
            "Train step - Step 1230, Loss 0.03247293084859848\n",
            "Train step - Step 1240, Loss 0.02254934422671795\n",
            "Train epoch - Accuracy: 0.9050505050505051 Loss: 0.026741100080838107 Corrects: 4480\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.027972077950835228\n",
            "Train step - Step 1260, Loss 0.022808704525232315\n",
            "Train step - Step 1270, Loss 0.03332369029521942\n",
            "Train step - Step 1280, Loss 0.029778027907013893\n",
            "Train epoch - Accuracy: 0.9113131313131313 Loss: 0.02538013132909934 Corrects: 4511\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.021082106977701187\n",
            "Train step - Step 1300, Loss 0.022133374586701393\n",
            "Train step - Step 1310, Loss 0.02248905412852764\n",
            "Train step - Step 1320, Loss 0.032489627599716187\n",
            "Train epoch - Accuracy: 0.9135353535353535 Loss: 0.024879919602112336 Corrects: 4522\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.021613596007227898\n",
            "Train step - Step 1340, Loss 0.024903856217861176\n",
            "Train step - Step 1350, Loss 0.02204894833266735\n",
            "Train step - Step 1360, Loss 0.03423197939991951\n",
            "Train epoch - Accuracy: 0.918989898989899 Loss: 0.023874024679564468 Corrects: 4549\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.018316451460123062\n",
            "Train step - Step 1380, Loss 0.03071191906929016\n",
            "Train step - Step 1390, Loss 0.021612856537103653\n",
            "Train step - Step 1400, Loss 0.0224612969905138\n",
            "Train epoch - Accuracy: 0.9193939393939394 Loss: 0.02368560603170684 Corrects: 4551\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.0241579357534647\n",
            "Train step - Step 1420, Loss 0.02214108407497406\n",
            "Train step - Step 1430, Loss 0.02015531063079834\n",
            "Train step - Step 1440, Loss 0.025047078728675842\n",
            "Train epoch - Accuracy: 0.9246464646464646 Loss: 0.022722508443726433 Corrects: 4577\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.028702950105071068\n",
            "Train step - Step 1460, Loss 0.018192701041698456\n",
            "Train step - Step 1470, Loss 0.019813111051917076\n",
            "Train step - Step 1480, Loss 0.021240150555968285\n",
            "Train epoch - Accuracy: 0.9202020202020202 Loss: 0.02346719710739574 Corrects: 4555\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.01830587349832058\n",
            "Train step - Step 1500, Loss 0.01753915287554264\n",
            "Train step - Step 1510, Loss 0.02207743376493454\n",
            "Train step - Step 1520, Loss 0.02748284302651882\n",
            "Train epoch - Accuracy: 0.9319191919191919 Loss: 0.02137380613296321 Corrects: 4613\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.01635633036494255\n",
            "Train step - Step 1540, Loss 0.026747558265924454\n",
            "Train step - Step 1550, Loss 0.02938236854970455\n",
            "Train epoch - Accuracy: 0.9264646464646464 Loss: 0.021464180021424487 Corrects: 4586\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.013849413953721523\n",
            "Train step - Step 1570, Loss 0.01797735132277012\n",
            "Train step - Step 1580, Loss 0.0311924796551466\n",
            "Train step - Step 1590, Loss 0.023851022124290466\n",
            "Train epoch - Accuracy: 0.9256565656565656 Loss: 0.02264722537512731 Corrects: 4582\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.012393715791404247\n",
            "Train step - Step 1610, Loss 0.021809130907058716\n",
            "Train step - Step 1620, Loss 0.03269912675023079\n",
            "Train step - Step 1630, Loss 0.017969252541661263\n",
            "Train epoch - Accuracy: 0.9286868686868687 Loss: 0.021102153689722822 Corrects: 4597\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.019911503419280052\n",
            "Train step - Step 1650, Loss 0.026384413242340088\n",
            "Train step - Step 1660, Loss 0.02168004959821701\n",
            "Train step - Step 1670, Loss 0.013305315747857094\n",
            "Train epoch - Accuracy: 0.9258585858585858 Loss: 0.021177935237535323 Corrects: 4583\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.0158479493111372\n",
            "Train step - Step 1690, Loss 0.014306142926216125\n",
            "Train step - Step 1700, Loss 0.013619977049529552\n",
            "Train step - Step 1710, Loss 0.02560928277671337\n",
            "Train epoch - Accuracy: 0.9414141414141414 Loss: 0.018269227574570012 Corrects: 4660\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.015752993524074554\n",
            "Train step - Step 1730, Loss 0.019579902291297913\n",
            "Train step - Step 1740, Loss 0.022022150456905365\n",
            "Train step - Step 1750, Loss 0.024162903428077698\n",
            "Train epoch - Accuracy: 0.9367676767676768 Loss: 0.018955502917971274 Corrects: 4637\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.014455275610089302\n",
            "Train step - Step 1770, Loss 0.01919494941830635\n",
            "Train step - Step 1780, Loss 0.017619717866182327\n",
            "Train step - Step 1790, Loss 0.013331124559044838\n",
            "Train epoch - Accuracy: 0.942020202020202 Loss: 0.01755155917896767 Corrects: 4663\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.021748056635260582\n",
            "Train step - Step 1810, Loss 0.014062158763408661\n",
            "Train step - Step 1820, Loss 0.021648330613970757\n",
            "Train step - Step 1830, Loss 0.020509636029601097\n",
            "Train epoch - Accuracy: 0.9476767676767677 Loss: 0.017083456129437746 Corrects: 4691\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.015212162397801876\n",
            "Train step - Step 1850, Loss 0.02287672832608223\n",
            "Train step - Step 1860, Loss 0.016975147649645805\n",
            "Train step - Step 1870, Loss 0.014906497672200203\n",
            "Train epoch - Accuracy: 0.9448484848484848 Loss: 0.017127797161087845 Corrects: 4677\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.014725998044013977\n",
            "Train step - Step 1890, Loss 0.016662707552313805\n",
            "Train step - Step 1900, Loss 0.020237768068909645\n",
            "Train step - Step 1910, Loss 0.024904360994696617\n",
            "Train epoch - Accuracy: 0.9454545454545454 Loss: 0.017035229525933362 Corrects: 4680\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.0184529609978199\n",
            "Train step - Step 1930, Loss 0.014841294847428799\n",
            "Train step - Step 1940, Loss 0.015769148245453835\n",
            "Train epoch - Accuracy: 0.9624242424242424 Loss: 0.013324942686265768 Corrects: 4764\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.008028355427086353\n",
            "Train step - Step 1960, Loss 0.00941139180213213\n",
            "Train step - Step 1970, Loss 0.01359105110168457\n",
            "Train step - Step 1980, Loss 0.011703001335263252\n",
            "Train epoch - Accuracy: 0.971919191919192 Loss: 0.01138722361150113 Corrects: 4811\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.013398817740380764\n",
            "Train step - Step 2000, Loss 0.012126307003200054\n",
            "Train step - Step 2010, Loss 0.010452072136104107\n",
            "Train step - Step 2020, Loss 0.01594306342303753\n",
            "Train epoch - Accuracy: 0.9711111111111111 Loss: 0.010910875842803055 Corrects: 4807\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.011697850190103054\n",
            "Train step - Step 2040, Loss 0.008937194012105465\n",
            "Train step - Step 2050, Loss 0.008711006492376328\n",
            "Train step - Step 2060, Loss 0.005241379141807556\n",
            "Train epoch - Accuracy: 0.973939393939394 Loss: 0.01037456277946029 Corrects: 4821\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.008675643242895603\n",
            "Train step - Step 2080, Loss 0.01160520501434803\n",
            "Train step - Step 2090, Loss 0.008684650994837284\n",
            "Train step - Step 2100, Loss 0.007417893502861261\n",
            "Train epoch - Accuracy: 0.9743434343434343 Loss: 0.010105235141636144 Corrects: 4823\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.008234726265072823\n",
            "Train step - Step 2120, Loss 0.00769994268193841\n",
            "Train step - Step 2130, Loss 0.0073739648796617985\n",
            "Train step - Step 2140, Loss 0.010317348875105381\n",
            "Train epoch - Accuracy: 0.9759595959595959 Loss: 0.009970981717072051 Corrects: 4831\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.014655283652245998\n",
            "Train step - Step 2160, Loss 0.008102647960186005\n",
            "Train step - Step 2170, Loss 0.00884443148970604\n",
            "Train step - Step 2180, Loss 0.011256386525928974\n",
            "Train epoch - Accuracy: 0.9753535353535353 Loss: 0.009801875325390185 Corrects: 4828\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.00847563799470663\n",
            "Train step - Step 2200, Loss 0.011728838086128235\n",
            "Train step - Step 2210, Loss 0.008373107761144638\n",
            "Train step - Step 2220, Loss 0.010014601051807404\n",
            "Train epoch - Accuracy: 0.9759595959595959 Loss: 0.009646617845829689 Corrects: 4831\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.006645673420280218\n",
            "Train step - Step 2240, Loss 0.007594596594572067\n",
            "Train step - Step 2250, Loss 0.011113283224403858\n",
            "Train step - Step 2260, Loss 0.008163361810147762\n",
            "Train epoch - Accuracy: 0.9757575757575757 Loss: 0.00939442841430204 Corrects: 4830\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.009936222806572914\n",
            "Train step - Step 2280, Loss 0.015635645017027855\n",
            "Train step - Step 2290, Loss 0.007167844567447901\n",
            "Train step - Step 2300, Loss 0.01132613979279995\n",
            "Train epoch - Accuracy: 0.9765656565656565 Loss: 0.009440014086136914 Corrects: 4834\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.009688121266663074\n",
            "Train step - Step 2320, Loss 0.011147885583341122\n",
            "Train step - Step 2330, Loss 0.00978462677448988\n",
            "Train epoch - Accuracy: 0.9777777777777777 Loss: 0.00909760949738098 Corrects: 4840\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.015120873227715492\n",
            "Train step - Step 2350, Loss 0.014631576836109161\n",
            "Train step - Step 2360, Loss 0.011578488163650036\n",
            "Train step - Step 2370, Loss 0.010511055588722229\n",
            "Train epoch - Accuracy: 0.977979797979798 Loss: 0.009060536796471686 Corrects: 4841\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.00885313656181097\n",
            "Train step - Step 2390, Loss 0.008361518383026123\n",
            "Train step - Step 2400, Loss 0.007988322526216507\n",
            "Train step - Step 2410, Loss 0.00754808634519577\n",
            "Train epoch - Accuracy: 0.9802020202020202 Loss: 0.008566017714940538 Corrects: 4852\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.009972035884857178\n",
            "Train step - Step 2430, Loss 0.007102419622242451\n",
            "Train step - Step 2440, Loss 0.0124282855540514\n",
            "Train step - Step 2450, Loss 0.009718996472656727\n",
            "Train epoch - Accuracy: 0.9783838383838384 Loss: 0.009153087472148014 Corrects: 4843\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.008337223902344704\n",
            "Train step - Step 2470, Loss 0.01162716280668974\n",
            "Train step - Step 2480, Loss 0.00839321967214346\n",
            "Train step - Step 2490, Loss 0.009586851112544537\n",
            "Train epoch - Accuracy: 0.98 Loss: 0.008235346716687535 Corrects: 4851\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.008591936901211739\n",
            "Train step - Step 2510, Loss 0.008559966459870338\n",
            "Train step - Step 2520, Loss 0.006635384168475866\n",
            "Train step - Step 2530, Loss 0.011490304954349995\n",
            "Train epoch - Accuracy: 0.9842424242424243 Loss: 0.007740205778467534 Corrects: 4872\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.010021751746535301\n",
            "Train step - Step 2550, Loss 0.0059161195531487465\n",
            "Train step - Step 2560, Loss 0.010700101964175701\n",
            "Train step - Step 2570, Loss 0.00710088899359107\n",
            "Train epoch - Accuracy: 0.9812121212121212 Loss: 0.008012310153607167 Corrects: 4857\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.009094694629311562\n",
            "Train step - Step 2590, Loss 0.009460711851716042\n",
            "Train step - Step 2600, Loss 0.006200479809194803\n",
            "Train step - Step 2610, Loss 0.007610691245645285\n",
            "Train epoch - Accuracy: 0.9816161616161616 Loss: 0.007892550880258734 Corrects: 4859\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.009366190992295742\n",
            "Train step - Step 2630, Loss 0.008516362868249416\n",
            "Train step - Step 2640, Loss 0.012693116441369057\n",
            "Train step - Step 2650, Loss 0.010907306335866451\n",
            "Train epoch - Accuracy: 0.9785858585858586 Loss: 0.008391490589187603 Corrects: 4844\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.0053100320510566235\n",
            "Train step - Step 2670, Loss 0.008172954432666302\n",
            "Train step - Step 2680, Loss 0.010275978595018387\n",
            "Train step - Step 2690, Loss 0.010208643041551113\n",
            "Train epoch - Accuracy: 0.9824242424242424 Loss: 0.008084334838345196 Corrects: 4863\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.004534631967544556\n",
            "Train step - Step 2710, Loss 0.005659586284309626\n",
            "Train step - Step 2720, Loss 0.007179623935371637\n",
            "Train epoch - Accuracy: 0.9812121212121212 Loss: 0.007950614594826192 Corrects: 4857\n",
            "Training finished in 210.28864407539368 seconds\n",
            "EVALUATION:  0.88 0.03479646518826485\n",
            "TEST GROUP:  0.855\n",
            "TEST ALL:  0.4275\n",
            "GROUP:  3\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.3564649522304535\n",
            "Train step - Step 10, Loss 0.13027143478393555\n",
            "Train step - Step 20, Loss 0.1083395853638649\n",
            "Train step - Step 30, Loss 0.07861396670341492\n",
            "Train epoch - Accuracy: 0.29878787878787877 Loss: 0.12265811924982553 Corrects: 1479\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.07237157970666885\n",
            "Train step - Step 50, Loss 0.067146435379982\n",
            "Train step - Step 60, Loss 0.06484279036521912\n",
            "Train step - Step 70, Loss 0.06924411654472351\n",
            "Train epoch - Accuracy: 0.5983838383838384 Loss: 0.06492929850714375 Corrects: 2962\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.0565340630710125\n",
            "Train step - Step 90, Loss 0.05527836084365845\n",
            "Train step - Step 100, Loss 0.051738932728767395\n",
            "Train step - Step 110, Loss 0.05650083348155022\n",
            "Train epoch - Accuracy: 0.6561616161616162 Loss: 0.05505526374686848 Corrects: 3248\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.049917008727788925\n",
            "Train step - Step 130, Loss 0.05768173933029175\n",
            "Train step - Step 140, Loss 0.05103237181901932\n",
            "Train step - Step 150, Loss 0.046448152512311935\n",
            "Train epoch - Accuracy: 0.694949494949495 Loss: 0.04983984997326678 Corrects: 3440\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.04411470144987106\n",
            "Train step - Step 170, Loss 0.04627522826194763\n",
            "Train step - Step 180, Loss 0.04755408316850662\n",
            "Train step - Step 190, Loss 0.040934138000011444\n",
            "Train epoch - Accuracy: 0.7202020202020202 Loss: 0.04597830404988443 Corrects: 3565\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.03925715759396553\n",
            "Train step - Step 210, Loss 0.04361804574728012\n",
            "Train step - Step 220, Loss 0.03994081914424896\n",
            "Train step - Step 230, Loss 0.04688810929656029\n",
            "Train epoch - Accuracy: 0.7464646464646465 Loss: 0.04274630580285583 Corrects: 3695\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.04815230518579483\n",
            "Train step - Step 250, Loss 0.039681363850831985\n",
            "Train step - Step 260, Loss 0.03979194536805153\n",
            "Train step - Step 270, Loss 0.040047597140073776\n",
            "Train epoch - Accuracy: 0.7597979797979798 Loss: 0.04068144173634173 Corrects: 3761\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.041773803532123566\n",
            "Train step - Step 290, Loss 0.03311824053525925\n",
            "Train step - Step 300, Loss 0.040335141122341156\n",
            "Train step - Step 310, Loss 0.038243621587753296\n",
            "Train epoch - Accuracy: 0.7806060606060606 Loss: 0.03802764168893448 Corrects: 3864\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.030043991282582283\n",
            "Train step - Step 330, Loss 0.03307505324482918\n",
            "Train step - Step 340, Loss 0.03889001905918121\n",
            "Train step - Step 350, Loss 0.03786305710673332\n",
            "Train epoch - Accuracy: 0.7888888888888889 Loss: 0.036484625449385306 Corrects: 3905\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.040563683956861496\n",
            "Train step - Step 370, Loss 0.039370473474264145\n",
            "Train step - Step 380, Loss 0.036312468349933624\n",
            "Train epoch - Accuracy: 0.7915151515151515 Loss: 0.03539799048894583 Corrects: 3918\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.025535056367516518\n",
            "Train step - Step 400, Loss 0.03521200269460678\n",
            "Train step - Step 410, Loss 0.03331108018755913\n",
            "Train step - Step 420, Loss 0.029945068061351776\n",
            "Train epoch - Accuracy: 0.8141414141414142 Loss: 0.03337882471205008 Corrects: 4030\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.031118938699364662\n",
            "Train step - Step 440, Loss 0.041164178401231766\n",
            "Train step - Step 450, Loss 0.03603815287351608\n",
            "Train step - Step 460, Loss 0.033767491579055786\n",
            "Train epoch - Accuracy: 0.8137373737373738 Loss: 0.03279701866435282 Corrects: 4028\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.03368443250656128\n",
            "Train step - Step 480, Loss 0.031434498727321625\n",
            "Train step - Step 490, Loss 0.03754594177007675\n",
            "Train step - Step 500, Loss 0.031668104231357574\n",
            "Train epoch - Accuracy: 0.8262626262626263 Loss: 0.031321463172483924 Corrects: 4090\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.02916717156767845\n",
            "Train step - Step 520, Loss 0.03644295409321785\n",
            "Train step - Step 530, Loss 0.029391257092356682\n",
            "Train step - Step 540, Loss 0.027573993429541588\n",
            "Train epoch - Accuracy: 0.8282828282828283 Loss: 0.030263205322653357 Corrects: 4100\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.0223239678889513\n",
            "Train step - Step 560, Loss 0.03141847997903824\n",
            "Train step - Step 570, Loss 0.02684837579727173\n",
            "Train step - Step 580, Loss 0.038120120763778687\n",
            "Train epoch - Accuracy: 0.8385858585858585 Loss: 0.028699823913700653 Corrects: 4151\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.025066344067454338\n",
            "Train step - Step 600, Loss 0.037064019590616226\n",
            "Train step - Step 610, Loss 0.023415271192789078\n",
            "Train step - Step 620, Loss 0.030985908582806587\n",
            "Train epoch - Accuracy: 0.8515151515151516 Loss: 0.027495903148494587 Corrects: 4215\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.03153703361749649\n",
            "Train step - Step 640, Loss 0.026822587475180626\n",
            "Train step - Step 650, Loss 0.02859695814549923\n",
            "Train step - Step 660, Loss 0.028007203713059425\n",
            "Train epoch - Accuracy: 0.8523232323232324 Loss: 0.026529807098435634 Corrects: 4219\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.022515103220939636\n",
            "Train step - Step 680, Loss 0.0370146669447422\n",
            "Train step - Step 690, Loss 0.029600173234939575\n",
            "Train step - Step 700, Loss 0.026379773393273354\n",
            "Train epoch - Accuracy: 0.8545454545454545 Loss: 0.02651084885302216 Corrects: 4230\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.023626528680324554\n",
            "Train step - Step 720, Loss 0.029459502547979355\n",
            "Train step - Step 730, Loss 0.028271863237023354\n",
            "Train step - Step 740, Loss 0.022069960832595825\n",
            "Train epoch - Accuracy: 0.8606060606060606 Loss: 0.025276066690984398 Corrects: 4260\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.02720511704683304\n",
            "Train step - Step 760, Loss 0.02519110031425953\n",
            "Train step - Step 770, Loss 0.027751103043556213\n",
            "Train epoch - Accuracy: 0.8642424242424243 Loss: 0.02483563828272651 Corrects: 4278\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.019491668790578842\n",
            "Train step - Step 790, Loss 0.018946047872304916\n",
            "Train step - Step 800, Loss 0.022005639970302582\n",
            "Train step - Step 810, Loss 0.025856712833046913\n",
            "Train epoch - Accuracy: 0.8694949494949495 Loss: 0.023951618883645897 Corrects: 4304\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.02089097537100315\n",
            "Train step - Step 830, Loss 0.024972615763545036\n",
            "Train step - Step 840, Loss 0.0174958948045969\n",
            "Train step - Step 850, Loss 0.022450150921940804\n",
            "Train epoch - Accuracy: 0.8836363636363637 Loss: 0.022214484076307276 Corrects: 4374\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.020428309217095375\n",
            "Train step - Step 870, Loss 0.01841011270880699\n",
            "Train step - Step 880, Loss 0.024174803867936134\n",
            "Train step - Step 890, Loss 0.024119723588228226\n",
            "Train epoch - Accuracy: 0.8836363636363637 Loss: 0.02215704109331574 Corrects: 4374\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.017811331897974014\n",
            "Train step - Step 910, Loss 0.018727708607912064\n",
            "Train step - Step 920, Loss 0.028226114809513092\n",
            "Train step - Step 930, Loss 0.023736311122775078\n",
            "Train epoch - Accuracy: 0.8856565656565657 Loss: 0.021470700055360795 Corrects: 4384\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.018193485215306282\n",
            "Train step - Step 950, Loss 0.01901145838201046\n",
            "Train step - Step 960, Loss 0.020071877166628838\n",
            "Train step - Step 970, Loss 0.024673406034708023\n",
            "Train epoch - Accuracy: 0.8909090909090909 Loss: 0.020727291329942568 Corrects: 4410\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.018197041004896164\n",
            "Train step - Step 990, Loss 0.02121920883655548\n",
            "Train step - Step 1000, Loss 0.019559629261493683\n",
            "Train step - Step 1010, Loss 0.019731445237994194\n",
            "Train epoch - Accuracy: 0.8951515151515151 Loss: 0.01990912231456752 Corrects: 4431\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.016601379960775375\n",
            "Train step - Step 1030, Loss 0.02141437865793705\n",
            "Train step - Step 1040, Loss 0.019821910187602043\n",
            "Train step - Step 1050, Loss 0.02105284109711647\n",
            "Train epoch - Accuracy: 0.8981818181818182 Loss: 0.019158990144579096 Corrects: 4446\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.013790744356811047\n",
            "Train step - Step 1070, Loss 0.012206759303808212\n",
            "Train step - Step 1080, Loss 0.023694677278399467\n",
            "Train step - Step 1090, Loss 0.015111126005649567\n",
            "Train epoch - Accuracy: 0.9068686868686868 Loss: 0.018546187237958714 Corrects: 4489\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.026211699470877647\n",
            "Train step - Step 1110, Loss 0.01704852283000946\n",
            "Train step - Step 1120, Loss 0.023280762135982513\n",
            "Train step - Step 1130, Loss 0.018151400610804558\n",
            "Train epoch - Accuracy: 0.9004040404040404 Loss: 0.01873976499519565 Corrects: 4457\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.019474660977721214\n",
            "Train step - Step 1150, Loss 0.01833217404782772\n",
            "Train step - Step 1160, Loss 0.027409041300415993\n",
            "Train epoch - Accuracy: 0.9068686868686868 Loss: 0.017957638169478887 Corrects: 4489\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.015916580334305763\n",
            "Train step - Step 1180, Loss 0.017516247928142548\n",
            "Train step - Step 1190, Loss 0.020386235788464546\n",
            "Train step - Step 1200, Loss 0.01618989370763302\n",
            "Train epoch - Accuracy: 0.9119191919191919 Loss: 0.017588715348204578 Corrects: 4514\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.015528171323239803\n",
            "Train step - Step 1220, Loss 0.01645403727889061\n",
            "Train step - Step 1230, Loss 0.01335777435451746\n",
            "Train step - Step 1240, Loss 0.01826884225010872\n",
            "Train epoch - Accuracy: 0.9105050505050505 Loss: 0.017344001665560885 Corrects: 4507\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.010784021578729153\n",
            "Train step - Step 1260, Loss 0.013990778475999832\n",
            "Train step - Step 1270, Loss 0.013231529854238033\n",
            "Train step - Step 1280, Loss 0.01418666634708643\n",
            "Train epoch - Accuracy: 0.9183838383838384 Loss: 0.015816609467370343 Corrects: 4546\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.014725121669471264\n",
            "Train step - Step 1300, Loss 0.018594400957226753\n",
            "Train step - Step 1310, Loss 0.01590772159397602\n",
            "Train step - Step 1320, Loss 0.01934293657541275\n",
            "Train epoch - Accuracy: 0.9173737373737374 Loss: 0.016089504867641612 Corrects: 4541\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.012282407842576504\n",
            "Train step - Step 1340, Loss 0.019676074385643005\n",
            "Train step - Step 1350, Loss 0.014760208316147327\n",
            "Train step - Step 1360, Loss 0.012162216939032078\n",
            "Train epoch - Accuracy: 0.9280808080808081 Loss: 0.014949050006089789 Corrects: 4594\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.015280794352293015\n",
            "Train step - Step 1380, Loss 0.009242173284292221\n",
            "Train step - Step 1390, Loss 0.01353794988244772\n",
            "Train step - Step 1400, Loss 0.013698884285986423\n",
            "Train epoch - Accuracy: 0.9252525252525252 Loss: 0.014855828447775407 Corrects: 4580\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.009978723712265491\n",
            "Train step - Step 1420, Loss 0.013242889195680618\n",
            "Train step - Step 1430, Loss 0.0159335657954216\n",
            "Train step - Step 1440, Loss 0.017131751403212547\n",
            "Train epoch - Accuracy: 0.9292929292929293 Loss: 0.014530332619042107 Corrects: 4600\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.011259364895522594\n",
            "Train step - Step 1460, Loss 0.01024807058274746\n",
            "Train step - Step 1470, Loss 0.011877299286425114\n",
            "Train step - Step 1480, Loss 0.015778914093971252\n",
            "Train epoch - Accuracy: 0.9341414141414142 Loss: 0.013574656382805169 Corrects: 4624\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.010095963254570961\n",
            "Train step - Step 1500, Loss 0.015025610104203224\n",
            "Train step - Step 1510, Loss 0.014158022589981556\n",
            "Train step - Step 1520, Loss 0.010205528698861599\n",
            "Train epoch - Accuracy: 0.9353535353535354 Loss: 0.013588315608600776 Corrects: 4630\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.010745507664978504\n",
            "Train step - Step 1540, Loss 0.0143574308604002\n",
            "Train step - Step 1550, Loss 0.012064757756888866\n",
            "Train epoch - Accuracy: 0.9319191919191919 Loss: 0.01343919418676935 Corrects: 4613\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.014684322290122509\n",
            "Train step - Step 1570, Loss 0.013052829541265965\n",
            "Train step - Step 1580, Loss 0.02084815502166748\n",
            "Train step - Step 1590, Loss 0.010630217380821705\n",
            "Train epoch - Accuracy: 0.9335353535353536 Loss: 0.013408806897097766 Corrects: 4621\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.012501745484769344\n",
            "Train step - Step 1610, Loss 0.019035862758755684\n",
            "Train step - Step 1620, Loss 0.012084764428436756\n",
            "Train step - Step 1630, Loss 0.013473572209477425\n",
            "Train epoch - Accuracy: 0.9393939393939394 Loss: 0.012476908977736127 Corrects: 4650\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.011149934493005276\n",
            "Train step - Step 1650, Loss 0.011928306892514229\n",
            "Train step - Step 1660, Loss 0.013351128436625004\n",
            "Train step - Step 1670, Loss 0.014848141930997372\n",
            "Train epoch - Accuracy: 0.943030303030303 Loss: 0.011831889227032662 Corrects: 4668\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.010258877649903297\n",
            "Train step - Step 1690, Loss 0.011858257465064526\n",
            "Train step - Step 1700, Loss 0.010626260191202164\n",
            "Train step - Step 1710, Loss 0.017501872032880783\n",
            "Train epoch - Accuracy: 0.9428282828282828 Loss: 0.011420527050892512 Corrects: 4667\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.00970370788127184\n",
            "Train step - Step 1730, Loss 0.016130365431308746\n",
            "Train step - Step 1740, Loss 0.012848816812038422\n",
            "Train step - Step 1750, Loss 0.01683364436030388\n",
            "Train epoch - Accuracy: 0.9454545454545454 Loss: 0.011539184088734063 Corrects: 4680\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.009590954519808292\n",
            "Train step - Step 1770, Loss 0.013561038300395012\n",
            "Train step - Step 1780, Loss 0.014936190098524094\n",
            "Train step - Step 1790, Loss 0.009084263816475868\n",
            "Train epoch - Accuracy: 0.9470707070707071 Loss: 0.011201219106412898 Corrects: 4688\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.011699086055159569\n",
            "Train step - Step 1810, Loss 0.01237982977181673\n",
            "Train step - Step 1820, Loss 0.008512049913406372\n",
            "Train step - Step 1830, Loss 0.01014968752861023\n",
            "Train epoch - Accuracy: 0.9531313131313132 Loss: 0.010523350726850707 Corrects: 4718\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.008758469484746456\n",
            "Train step - Step 1850, Loss 0.012401754036545753\n",
            "Train step - Step 1860, Loss 0.017858153209090233\n",
            "Train step - Step 1870, Loss 0.007855652831494808\n",
            "Train epoch - Accuracy: 0.9519191919191919 Loss: 0.010399157852687017 Corrects: 4712\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.008862114511430264\n",
            "Train step - Step 1890, Loss 0.010790052823722363\n",
            "Train step - Step 1900, Loss 0.0128763597458601\n",
            "Train step - Step 1910, Loss 0.01100255735218525\n",
            "Train epoch - Accuracy: 0.9567676767676768 Loss: 0.009719943882088468 Corrects: 4736\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.01289371494203806\n",
            "Train step - Step 1930, Loss 0.009544286876916885\n",
            "Train step - Step 1940, Loss 0.013729952275753021\n",
            "Train epoch - Accuracy: 0.9642424242424242 Loss: 0.008570017593000273 Corrects: 4773\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.00982751790434122\n",
            "Train step - Step 1960, Loss 0.008129860274493694\n",
            "Train step - Step 1970, Loss 0.005865038372576237\n",
            "Train step - Step 1980, Loss 0.008819423615932465\n",
            "Train epoch - Accuracy: 0.977979797979798 Loss: 0.007131454195413324 Corrects: 4841\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.007639791816473007\n",
            "Train step - Step 2000, Loss 0.005468670278787613\n",
            "Train step - Step 2010, Loss 0.005666977260261774\n",
            "Train step - Step 2020, Loss 0.008197180926799774\n",
            "Train epoch - Accuracy: 0.9721212121212122 Loss: 0.007349437969275797 Corrects: 4812\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.005781911313533783\n",
            "Train step - Step 2040, Loss 0.010331884026527405\n",
            "Train step - Step 2050, Loss 0.007941304706037045\n",
            "Train step - Step 2060, Loss 0.0038735701236873865\n",
            "Train epoch - Accuracy: 0.9769696969696969 Loss: 0.0066547590563769896 Corrects: 4836\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.007041124161332846\n",
            "Train step - Step 2080, Loss 0.007183182518929243\n",
            "Train step - Step 2090, Loss 0.005109672900289297\n",
            "Train step - Step 2100, Loss 0.006689263507723808\n",
            "Train epoch - Accuracy: 0.9731313131313132 Loss: 0.006887274807827039 Corrects: 4817\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.007127187214791775\n",
            "Train step - Step 2120, Loss 0.006945191882550716\n",
            "Train step - Step 2130, Loss 0.007594957482069731\n",
            "Train step - Step 2140, Loss 0.005243510473519564\n",
            "Train epoch - Accuracy: 0.9804040404040404 Loss: 0.006359743867822067 Corrects: 4853\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.005899142939597368\n",
            "Train step - Step 2160, Loss 0.007127123419195414\n",
            "Train step - Step 2170, Loss 0.005425070878118277\n",
            "Train step - Step 2180, Loss 0.007046284154057503\n",
            "Train epoch - Accuracy: 0.9763636363636363 Loss: 0.006578482513626417 Corrects: 4833\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.006046558264642954\n",
            "Train step - Step 2200, Loss 0.00593577791005373\n",
            "Train step - Step 2210, Loss 0.005891030188649893\n",
            "Train step - Step 2220, Loss 0.007400084752589464\n",
            "Train epoch - Accuracy: 0.9745454545454545 Loss: 0.006882836855571679 Corrects: 4824\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.00727854622527957\n",
            "Train step - Step 2240, Loss 0.0050385864451527596\n",
            "Train step - Step 2250, Loss 0.004953560419380665\n",
            "Train step - Step 2260, Loss 0.0059033967554569244\n",
            "Train epoch - Accuracy: 0.9735353535353536 Loss: 0.006732536345682662 Corrects: 4819\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.005412663333117962\n",
            "Train step - Step 2280, Loss 0.0034654156770557165\n",
            "Train step - Step 2290, Loss 0.005656775087118149\n",
            "Train step - Step 2300, Loss 0.006399216130375862\n",
            "Train epoch - Accuracy: 0.9753535353535353 Loss: 0.006694164061636636 Corrects: 4828\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.0034536835737526417\n",
            "Train step - Step 2320, Loss 0.007457667030394077\n",
            "Train step - Step 2330, Loss 0.006403908599168062\n",
            "Train epoch - Accuracy: 0.9806060606060606 Loss: 0.006218657400835343 Corrects: 4854\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.008112373761832714\n",
            "Train step - Step 2350, Loss 0.004854332655668259\n",
            "Train step - Step 2360, Loss 0.007106448523700237\n",
            "Train step - Step 2370, Loss 0.0057643405161798\n",
            "Train epoch - Accuracy: 0.9791919191919192 Loss: 0.006145354592890451 Corrects: 4847\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.006333517841994762\n",
            "Train step - Step 2390, Loss 0.006168618332594633\n",
            "Train step - Step 2400, Loss 0.0066346968524158\n",
            "Train step - Step 2410, Loss 0.005045030731707811\n",
            "Train epoch - Accuracy: 0.9775757575757575 Loss: 0.006182494049873015 Corrects: 4839\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.005995460320264101\n",
            "Train step - Step 2430, Loss 0.006087617948651314\n",
            "Train step - Step 2440, Loss 0.010290331207215786\n",
            "Train step - Step 2450, Loss 0.005777957383543253\n",
            "Train epoch - Accuracy: 0.9771717171717171 Loss: 0.006323123526091527 Corrects: 4837\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.005820820108056068\n",
            "Train step - Step 2470, Loss 0.006869829259812832\n",
            "Train step - Step 2480, Loss 0.004899540916085243\n",
            "Train step - Step 2490, Loss 0.007700819056481123\n",
            "Train epoch - Accuracy: 0.9804040404040404 Loss: 0.0059882385755954965 Corrects: 4853\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.006892880890518427\n",
            "Train step - Step 2510, Loss 0.0044293515384197235\n",
            "Train step - Step 2520, Loss 0.0044047217816114426\n",
            "Train step - Step 2530, Loss 0.003936437889933586\n",
            "Train epoch - Accuracy: 0.9791919191919192 Loss: 0.005820571462802513 Corrects: 4847\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.006369307171553373\n",
            "Train step - Step 2550, Loss 0.004353023134171963\n",
            "Train step - Step 2560, Loss 0.008054149337112904\n",
            "Train step - Step 2570, Loss 0.005098693072795868\n",
            "Train epoch - Accuracy: 0.9802020202020202 Loss: 0.005689351691328215 Corrects: 4852\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.00589374965056777\n",
            "Train step - Step 2590, Loss 0.004175443667918444\n",
            "Train step - Step 2600, Loss 0.003889413783326745\n",
            "Train step - Step 2610, Loss 0.0055067152716219425\n",
            "Train epoch - Accuracy: 0.981010101010101 Loss: 0.005416401942834409 Corrects: 4856\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.0052356114611029625\n",
            "Train step - Step 2630, Loss 0.0033594160340726376\n",
            "Train step - Step 2640, Loss 0.006329411640763283\n",
            "Train step - Step 2650, Loss 0.007771412841975689\n",
            "Train epoch - Accuracy: 0.9812121212121212 Loss: 0.005391799080176185 Corrects: 4857\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.007290321867913008\n",
            "Train step - Step 2670, Loss 0.006357960868626833\n",
            "Train step - Step 2680, Loss 0.005436035338789225\n",
            "Train step - Step 2690, Loss 0.003735231002792716\n",
            "Train epoch - Accuracy: 0.9761616161616161 Loss: 0.005841586991410816 Corrects: 4832\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.004066651687026024\n",
            "Train step - Step 2710, Loss 0.006962474901229143\n",
            "Train step - Step 2720, Loss 0.003850487759336829\n",
            "Train epoch - Accuracy: 0.981010101010101 Loss: 0.005563451015776155 Corrects: 4856\n",
            "Training finished in 209.57071232795715 seconds\n",
            "EVALUATION:  0.84 0.04931778460741043\n",
            "TEST GROUP:  0.834\n",
            "TEST ALL:  0.278\n",
            "GROUP:  4\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.3093632757663727\n",
            "Train step - Step 10, Loss 0.1085236594080925\n",
            "Train step - Step 20, Loss 0.08126042038202286\n",
            "Train step - Step 30, Loss 0.06769811362028122\n",
            "Train epoch - Accuracy: 0.2832323232323232 Loss: 0.09858315748548267 Corrects: 1402\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.05579589679837227\n",
            "Train step - Step 50, Loss 0.05137814208865166\n",
            "Train step - Step 60, Loss 0.040382277220487595\n",
            "Train step - Step 70, Loss 0.04314994812011719\n",
            "Train epoch - Accuracy: 0.6260606060606061 Loss: 0.04737340397907026 Corrects: 3099\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.03698129579424858\n",
            "Train step - Step 90, Loss 0.03837139531970024\n",
            "Train step - Step 100, Loss 0.03276713937520981\n",
            "Train step - Step 110, Loss 0.03551894426345825\n",
            "Train epoch - Accuracy: 0.7058585858585859 Loss: 0.03796766081843713 Corrects: 3494\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.03947509452700615\n",
            "Train step - Step 130, Loss 0.031530268490314484\n",
            "Train step - Step 140, Loss 0.02757798694074154\n",
            "Train step - Step 150, Loss 0.030581502243876457\n",
            "Train epoch - Accuracy: 0.7494949494949495 Loss: 0.032776963477483906 Corrects: 3710\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.027044707909226418\n",
            "Train step - Step 170, Loss 0.037437792867422104\n",
            "Train step - Step 180, Loss 0.030702916905283928\n",
            "Train step - Step 190, Loss 0.030981410294771194\n",
            "Train epoch - Accuracy: 0.7793939393939394 Loss: 0.02937831871437304 Corrects: 3858\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.024337803944945335\n",
            "Train step - Step 210, Loss 0.032248981297016144\n",
            "Train step - Step 220, Loss 0.022921640425920486\n",
            "Train step - Step 230, Loss 0.02422051504254341\n",
            "Train epoch - Accuracy: 0.796969696969697 Loss: 0.027053286628891723 Corrects: 3945\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.025853848084807396\n",
            "Train step - Step 250, Loss 0.019277313724160194\n",
            "Train step - Step 260, Loss 0.026172911748290062\n",
            "Train step - Step 270, Loss 0.02909606508910656\n",
            "Train epoch - Accuracy: 0.8135353535353536 Loss: 0.02521601893760339 Corrects: 4027\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.02869473397731781\n",
            "Train step - Step 290, Loss 0.02418258599936962\n",
            "Train step - Step 300, Loss 0.02326267771422863\n",
            "Train step - Step 310, Loss 0.024965962395071983\n",
            "Train epoch - Accuracy: 0.8335353535353536 Loss: 0.023361718933841195 Corrects: 4126\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.02463836781680584\n",
            "Train step - Step 330, Loss 0.015275823883712292\n",
            "Train step - Step 340, Loss 0.02322228066623211\n",
            "Train step - Step 350, Loss 0.016083845868706703\n",
            "Train epoch - Accuracy: 0.8331313131313132 Loss: 0.022626867576530485 Corrects: 4124\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.01738874427974224\n",
            "Train step - Step 370, Loss 0.021935326978564262\n",
            "Train step - Step 380, Loss 0.024359995499253273\n",
            "Train epoch - Accuracy: 0.8383838383838383 Loss: 0.021476352071822292 Corrects: 4150\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.017817435786128044\n",
            "Train step - Step 400, Loss 0.022352224215865135\n",
            "Train step - Step 410, Loss 0.018287813290953636\n",
            "Train step - Step 420, Loss 0.01582259312272072\n",
            "Train epoch - Accuracy: 0.8555555555555555 Loss: 0.020061728652529042 Corrects: 4235\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.01843379996716976\n",
            "Train step - Step 440, Loss 0.018911054357886314\n",
            "Train step - Step 450, Loss 0.021438390016555786\n",
            "Train step - Step 460, Loss 0.0219333004206419\n",
            "Train epoch - Accuracy: 0.8539393939393939 Loss: 0.019586969485337086 Corrects: 4227\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.02316907048225403\n",
            "Train step - Step 480, Loss 0.01998724602162838\n",
            "Train step - Step 490, Loss 0.018799124285578728\n",
            "Train step - Step 500, Loss 0.019307905808091164\n",
            "Train epoch - Accuracy: 0.8616161616161616 Loss: 0.01914597671366099 Corrects: 4265\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.015226083807647228\n",
            "Train step - Step 520, Loss 0.02042328752577305\n",
            "Train step - Step 530, Loss 0.018628496676683426\n",
            "Train step - Step 540, Loss 0.021210266277194023\n",
            "Train epoch - Accuracy: 0.8668686868686869 Loss: 0.018280333713299096 Corrects: 4291\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.022422844544053078\n",
            "Train step - Step 560, Loss 0.02225705049932003\n",
            "Train step - Step 570, Loss 0.01468448992818594\n",
            "Train step - Step 580, Loss 0.01756376586854458\n",
            "Train epoch - Accuracy: 0.8834343434343435 Loss: 0.016832655453772254 Corrects: 4373\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.019565070047974586\n",
            "Train step - Step 600, Loss 0.013830582611262798\n",
            "Train step - Step 610, Loss 0.020028775557875633\n",
            "Train step - Step 620, Loss 0.015363459475338459\n",
            "Train epoch - Accuracy: 0.8858585858585859 Loss: 0.016514780419041412 Corrects: 4385\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.017874741926789284\n",
            "Train step - Step 640, Loss 0.012759627774357796\n",
            "Train step - Step 650, Loss 0.016129417344927788\n",
            "Train step - Step 660, Loss 0.012855303473770618\n",
            "Train epoch - Accuracy: 0.8911111111111111 Loss: 0.015905595508338224 Corrects: 4411\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.017270512878894806\n",
            "Train step - Step 680, Loss 0.012989570386707783\n",
            "Train step - Step 690, Loss 0.013790612109005451\n",
            "Train step - Step 700, Loss 0.010405570268630981\n",
            "Train epoch - Accuracy: 0.8945454545454545 Loss: 0.015308867710256817 Corrects: 4428\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.010509462095797062\n",
            "Train step - Step 720, Loss 0.012010477483272552\n",
            "Train step - Step 730, Loss 0.01358860731124878\n",
            "Train step - Step 740, Loss 0.014206774532794952\n",
            "Train epoch - Accuracy: 0.8983838383838384 Loss: 0.014931662019455071 Corrects: 4447\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.010262160561978817\n",
            "Train step - Step 760, Loss 0.010669822804629803\n",
            "Train step - Step 770, Loss 0.014965206384658813\n",
            "Train epoch - Accuracy: 0.902020202020202 Loss: 0.014019364749241358 Corrects: 4465\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.013716188259422779\n",
            "Train step - Step 790, Loss 0.01268040668219328\n",
            "Train step - Step 800, Loss 0.010856875218451023\n",
            "Train step - Step 810, Loss 0.01443155761808157\n",
            "Train epoch - Accuracy: 0.8963636363636364 Loss: 0.014495554058765523 Corrects: 4437\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.0099880900233984\n",
            "Train step - Step 830, Loss 0.013792017474770546\n",
            "Train step - Step 840, Loss 0.012567934580147266\n",
            "Train step - Step 850, Loss 0.015061316080391407\n",
            "Train epoch - Accuracy: 0.9042424242424243 Loss: 0.013560855796165538 Corrects: 4476\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.012035267427563667\n",
            "Train step - Step 870, Loss 0.010773077607154846\n",
            "Train step - Step 880, Loss 0.013635429553687572\n",
            "Train step - Step 890, Loss 0.013371630571782589\n",
            "Train epoch - Accuracy: 0.9125252525252525 Loss: 0.012681439575748613 Corrects: 4517\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.012338792905211449\n",
            "Train step - Step 910, Loss 0.012407888658344746\n",
            "Train step - Step 920, Loss 0.015924988314509392\n",
            "Train step - Step 930, Loss 0.009834419935941696\n",
            "Train epoch - Accuracy: 0.9183838383838384 Loss: 0.012084222685809087 Corrects: 4546\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.014334402047097683\n",
            "Train step - Step 950, Loss 0.012812054716050625\n",
            "Train step - Step 960, Loss 0.015259838663041592\n",
            "Train step - Step 970, Loss 0.010383456014096737\n",
            "Train epoch - Accuracy: 0.9206060606060606 Loss: 0.01192996140456561 Corrects: 4557\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.013020545244216919\n",
            "Train step - Step 990, Loss 0.010459340177476406\n",
            "Train step - Step 1000, Loss 0.01600276492536068\n",
            "Train step - Step 1010, Loss 0.016017038375139236\n",
            "Train epoch - Accuracy: 0.9151515151515152 Loss: 0.012068902912615526 Corrects: 4530\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.014776545576751232\n",
            "Train step - Step 1030, Loss 0.010170036926865578\n",
            "Train step - Step 1040, Loss 0.01745305210351944\n",
            "Train step - Step 1050, Loss 0.008992238901555538\n",
            "Train epoch - Accuracy: 0.92 Loss: 0.011560299138754906 Corrects: 4554\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.008489400148391724\n",
            "Train step - Step 1070, Loss 0.010591277852654457\n",
            "Train step - Step 1080, Loss 0.009177297353744507\n",
            "Train step - Step 1090, Loss 0.01171644777059555\n",
            "Train epoch - Accuracy: 0.9292929292929293 Loss: 0.010806244061977574 Corrects: 4600\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.009175444953143597\n",
            "Train step - Step 1110, Loss 0.010408169589936733\n",
            "Train step - Step 1120, Loss 0.012851784005761147\n",
            "Train step - Step 1130, Loss 0.011399922892451286\n",
            "Train epoch - Accuracy: 0.9331313131313131 Loss: 0.010271960308003907 Corrects: 4619\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.012990817427635193\n",
            "Train step - Step 1150, Loss 0.009663263335824013\n",
            "Train step - Step 1160, Loss 0.008030988276004791\n",
            "Train epoch - Accuracy: 0.9313131313131313 Loss: 0.01047154599142195 Corrects: 4610\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.01176547259092331\n",
            "Train step - Step 1180, Loss 0.01036653108894825\n",
            "Train step - Step 1190, Loss 0.01179888378828764\n",
            "Train step - Step 1200, Loss 0.009861710481345654\n",
            "Train epoch - Accuracy: 0.9319191919191919 Loss: 0.010267008276976118 Corrects: 4613\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.010460356250405312\n",
            "Train step - Step 1220, Loss 0.009297004900872707\n",
            "Train step - Step 1230, Loss 0.008031387813389301\n",
            "Train step - Step 1240, Loss 0.009742221795022488\n",
            "Train epoch - Accuracy: 0.9357575757575758 Loss: 0.009828550110411163 Corrects: 4632\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.008518069051206112\n",
            "Train step - Step 1260, Loss 0.011480542831122875\n",
            "Train step - Step 1270, Loss 0.009466205723583698\n",
            "Train step - Step 1280, Loss 0.008265433833003044\n",
            "Train epoch - Accuracy: 0.9418181818181818 Loss: 0.009256148733048126 Corrects: 4662\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.006107534281909466\n",
            "Train step - Step 1300, Loss 0.013169235549867153\n",
            "Train step - Step 1310, Loss 0.007990601472556591\n",
            "Train step - Step 1320, Loss 0.007933786138892174\n",
            "Train epoch - Accuracy: 0.9446464646464646 Loss: 0.008836527213905796 Corrects: 4676\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.007594564463943243\n",
            "Train step - Step 1340, Loss 0.00515062315389514\n",
            "Train step - Step 1350, Loss 0.009744971059262753\n",
            "Train step - Step 1360, Loss 0.007708430290222168\n",
            "Train epoch - Accuracy: 0.9416161616161616 Loss: 0.008988763708621263 Corrects: 4661\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.006193379405885935\n",
            "Train step - Step 1380, Loss 0.006517469882965088\n",
            "Train step - Step 1390, Loss 0.009198862127959728\n",
            "Train step - Step 1400, Loss 0.008973806165158749\n",
            "Train epoch - Accuracy: 0.9492929292929293 Loss: 0.008412302901046444 Corrects: 4699\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.007938767783343792\n",
            "Train step - Step 1420, Loss 0.006782430689781904\n",
            "Train step - Step 1430, Loss 0.009081020019948483\n",
            "Train step - Step 1440, Loss 0.012851650826632977\n",
            "Train epoch - Accuracy: 0.9454545454545454 Loss: 0.008596560740184904 Corrects: 4680\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.00953729823231697\n",
            "Train step - Step 1460, Loss 0.005566426552832127\n",
            "Train step - Step 1470, Loss 0.0077257417142391205\n",
            "Train step - Step 1480, Loss 0.0063655786216259\n",
            "Train epoch - Accuracy: 0.9480808080808081 Loss: 0.008213937996952522 Corrects: 4693\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.010889694094657898\n",
            "Train step - Step 1500, Loss 0.006123002152889967\n",
            "Train step - Step 1510, Loss 0.0076058851554989815\n",
            "Train step - Step 1520, Loss 0.011552018113434315\n",
            "Train epoch - Accuracy: 0.9501010101010101 Loss: 0.008052862490894217 Corrects: 4703\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.008836428634822369\n",
            "Train step - Step 1540, Loss 0.006795560475438833\n",
            "Train step - Step 1550, Loss 0.006202283781021833\n",
            "Train epoch - Accuracy: 0.9523232323232323 Loss: 0.007631683206994726 Corrects: 4714\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.009168349206447601\n",
            "Train step - Step 1570, Loss 0.008184345439076424\n",
            "Train step - Step 1580, Loss 0.007175071630626917\n",
            "Train step - Step 1590, Loss 0.008373409509658813\n",
            "Train epoch - Accuracy: 0.9545454545454546 Loss: 0.007418369914997708 Corrects: 4725\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.0068527706898748875\n",
            "Train step - Step 1610, Loss 0.0065450421534478664\n",
            "Train step - Step 1620, Loss 0.006270767655223608\n",
            "Train step - Step 1630, Loss 0.010286561213433743\n",
            "Train epoch - Accuracy: 0.9541414141414142 Loss: 0.007371100662558368 Corrects: 4723\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.004469755571335554\n",
            "Train step - Step 1650, Loss 0.00581992557272315\n",
            "Train step - Step 1660, Loss 0.007320587523281574\n",
            "Train step - Step 1670, Loss 0.007871697656810284\n",
            "Train epoch - Accuracy: 0.9563636363636364 Loss: 0.007070829905193261 Corrects: 4734\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.007993915118277073\n",
            "Train step - Step 1690, Loss 0.00525210564956069\n",
            "Train step - Step 1700, Loss 0.005930573679506779\n",
            "Train step - Step 1710, Loss 0.0035058052744716406\n",
            "Train epoch - Accuracy: 0.9569696969696969 Loss: 0.006981838967538241 Corrects: 4737\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.005308243911713362\n",
            "Train step - Step 1730, Loss 0.006777684669941664\n",
            "Train step - Step 1740, Loss 0.008913213387131691\n",
            "Train step - Step 1750, Loss 0.006831110920757055\n",
            "Train epoch - Accuracy: 0.9642424242424242 Loss: 0.006240877447781539 Corrects: 4773\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.006677459459751844\n",
            "Train step - Step 1770, Loss 0.008560107089579105\n",
            "Train step - Step 1780, Loss 0.005653474014252424\n",
            "Train step - Step 1790, Loss 0.005361645016819239\n",
            "Train epoch - Accuracy: 0.9561616161616162 Loss: 0.0070066819558239945 Corrects: 4733\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.0074564008973538876\n",
            "Train step - Step 1810, Loss 0.0034820192959159613\n",
            "Train step - Step 1820, Loss 0.004249125719070435\n",
            "Train step - Step 1830, Loss 0.004776936490088701\n",
            "Train epoch - Accuracy: 0.9658585858585859 Loss: 0.006105897718419631 Corrects: 4781\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.005197232123464346\n",
            "Train step - Step 1850, Loss 0.006262396462261677\n",
            "Train step - Step 1860, Loss 0.005799782928079367\n",
            "Train step - Step 1870, Loss 0.004427454899996519\n",
            "Train epoch - Accuracy: 0.9680808080808081 Loss: 0.005876438886002458 Corrects: 4792\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.005906250327825546\n",
            "Train step - Step 1890, Loss 0.007189321797341108\n",
            "Train step - Step 1900, Loss 0.005251644644886255\n",
            "Train step - Step 1910, Loss 0.006545955780893564\n",
            "Train epoch - Accuracy: 0.9636363636363636 Loss: 0.005862378298747118 Corrects: 4770\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.006104967091232538\n",
            "Train step - Step 1930, Loss 0.0055023496970534325\n",
            "Train step - Step 1940, Loss 0.003716981504112482\n",
            "Train epoch - Accuracy: 0.9743434343434343 Loss: 0.00507142986152133 Corrects: 4823\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.004611973650753498\n",
            "Train step - Step 1960, Loss 0.0038231879007071257\n",
            "Train step - Step 1970, Loss 0.004853565711528063\n",
            "Train step - Step 1980, Loss 0.004005239810794592\n",
            "Train epoch - Accuracy: 0.9771717171717171 Loss: 0.004642981155258086 Corrects: 4837\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.004248177167028189\n",
            "Train step - Step 2000, Loss 0.003805542830377817\n",
            "Train step - Step 2010, Loss 0.003346503246575594\n",
            "Train step - Step 2020, Loss 0.0044076815247535706\n",
            "Train epoch - Accuracy: 0.9785858585858586 Loss: 0.004419904076917605 Corrects: 4844\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.004119925666600466\n",
            "Train step - Step 2040, Loss 0.006220338866114616\n",
            "Train step - Step 2050, Loss 0.005799831822514534\n",
            "Train step - Step 2060, Loss 0.004876916762441397\n",
            "Train epoch - Accuracy: 0.9773737373737373 Loss: 0.004367483707916255 Corrects: 4838\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.003273039823397994\n",
            "Train step - Step 2080, Loss 0.003406175645068288\n",
            "Train step - Step 2090, Loss 0.004321517888456583\n",
            "Train step - Step 2100, Loss 0.004350781440734863\n",
            "Train epoch - Accuracy: 0.9785858585858586 Loss: 0.004176760832826146 Corrects: 4844\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.004505138378590345\n",
            "Train step - Step 2120, Loss 0.003858389100059867\n",
            "Train step - Step 2130, Loss 0.0029220094438642263\n",
            "Train step - Step 2140, Loss 0.003320080926641822\n",
            "Train epoch - Accuracy: 0.9826262626262626 Loss: 0.004072909752904165 Corrects: 4864\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.004909386858344078\n",
            "Train step - Step 2160, Loss 0.0038926394190639257\n",
            "Train step - Step 2170, Loss 0.002996049588546157\n",
            "Train step - Step 2180, Loss 0.005058872047811747\n",
            "Train epoch - Accuracy: 0.98 Loss: 0.004164078757166862 Corrects: 4851\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.004999925848096609\n",
            "Train step - Step 2200, Loss 0.0024210044648498297\n",
            "Train step - Step 2210, Loss 0.005896422546356916\n",
            "Train step - Step 2220, Loss 0.0047083026729524136\n",
            "Train epoch - Accuracy: 0.9795959595959596 Loss: 0.004156676126512313 Corrects: 4849\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.004130450077354908\n",
            "Train step - Step 2240, Loss 0.002054803539067507\n",
            "Train step - Step 2250, Loss 0.0036933310329914093\n",
            "Train step - Step 2260, Loss 0.006092017982155085\n",
            "Train epoch - Accuracy: 0.9816161616161616 Loss: 0.004030601909816867 Corrects: 4859\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.004832815378904343\n",
            "Train step - Step 2280, Loss 0.003485774388536811\n",
            "Train step - Step 2290, Loss 0.006124554201960564\n",
            "Train step - Step 2300, Loss 0.0044766697101294994\n",
            "Train epoch - Accuracy: 0.9804040404040404 Loss: 0.0040963212011212655 Corrects: 4853\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.005113146733492613\n",
            "Train step - Step 2320, Loss 0.003983523230999708\n",
            "Train step - Step 2330, Loss 0.0035986665170639753\n",
            "Train epoch - Accuracy: 0.977979797979798 Loss: 0.004305670273800691 Corrects: 4841\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.003744959831237793\n",
            "Train step - Step 2350, Loss 0.005578523967415094\n",
            "Train step - Step 2360, Loss 0.005039707757532597\n",
            "Train step - Step 2370, Loss 0.0033838064409792423\n",
            "Train epoch - Accuracy: 0.9852525252525253 Loss: 0.0039032555889865063 Corrects: 4877\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.0016743614105507731\n",
            "Train step - Step 2390, Loss 0.0032650381326675415\n",
            "Train step - Step 2400, Loss 0.004603119101375341\n",
            "Train step - Step 2410, Loss 0.0032446037512272596\n",
            "Train epoch - Accuracy: 0.9814141414141414 Loss: 0.0040546097219780535 Corrects: 4858\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.0026522839907556772\n",
            "Train step - Step 2430, Loss 0.003279807511717081\n",
            "Train step - Step 2440, Loss 0.004059364087879658\n",
            "Train step - Step 2450, Loss 0.0026223897002637386\n",
            "Train epoch - Accuracy: 0.9828282828282828 Loss: 0.0039604972664153935 Corrects: 4865\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.005021139979362488\n",
            "Train step - Step 2470, Loss 0.003626531455665827\n",
            "Train step - Step 2480, Loss 0.004169831518083811\n",
            "Train step - Step 2490, Loss 0.005560071207582951\n",
            "Train epoch - Accuracy: 0.9824242424242424 Loss: 0.0037867087669520064 Corrects: 4863\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.0034194202162325382\n",
            "Train step - Step 2510, Loss 0.004092489834874868\n",
            "Train step - Step 2520, Loss 0.003984705079346895\n",
            "Train step - Step 2530, Loss 0.0023194614332169294\n",
            "Train epoch - Accuracy: 0.9844444444444445 Loss: 0.003683165226583228 Corrects: 4873\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.0031751671340316534\n",
            "Train step - Step 2550, Loss 0.0029993471689522266\n",
            "Train step - Step 2560, Loss 0.0038954198826104403\n",
            "Train step - Step 2570, Loss 0.003489268245175481\n",
            "Train epoch - Accuracy: 0.9850505050505051 Loss: 0.003361977943680202 Corrects: 4876\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.004150068387389183\n",
            "Train step - Step 2590, Loss 0.0029697581194341183\n",
            "Train step - Step 2600, Loss 0.0024593009147793055\n",
            "Train step - Step 2610, Loss 0.0022158187348395586\n",
            "Train epoch - Accuracy: 0.9846464646464647 Loss: 0.0036446322802680007 Corrects: 4874\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.004847928415983915\n",
            "Train step - Step 2630, Loss 0.003563827369362116\n",
            "Train step - Step 2640, Loss 0.003756181104108691\n",
            "Train step - Step 2650, Loss 0.0021162766497582197\n",
            "Train epoch - Accuracy: 0.984040404040404 Loss: 0.0036479252319068017 Corrects: 4871\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.002846820978447795\n",
            "Train step - Step 2670, Loss 0.002889395458623767\n",
            "Train step - Step 2680, Loss 0.0023697130382061005\n",
            "Train step - Step 2690, Loss 0.0025409997906535864\n",
            "Train epoch - Accuracy: 0.9828282828282828 Loss: 0.0036682669996199284 Corrects: 4865\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.004980289842933416\n",
            "Train step - Step 2710, Loss 0.004940974060446024\n",
            "Train step - Step 2720, Loss 0.0042978497222065926\n",
            "Train epoch - Accuracy: 0.9852525252525253 Loss: 0.0035553393911833715 Corrects: 4877\n",
            "Training finished in 210.17174124717712 seconds\n",
            "EVALUATION:  0.88 0.020380856469273567\n",
            "TEST GROUP:  0.879\n",
            "TEST ALL:  0.21975\n",
            "GROUP:  5\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.22225669026374817\n",
            "Train step - Step 10, Loss 0.08600392937660217\n",
            "Train step - Step 20, Loss 0.06857982277870178\n",
            "Train step - Step 30, Loss 0.054639823734760284\n",
            "Train epoch - Accuracy: 0.2286868686868687 Loss: 0.0794405663585422 Corrects: 1132\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.043430957943201065\n",
            "Train step - Step 50, Loss 0.042877811938524246\n",
            "Train step - Step 60, Loss 0.04006177932024002\n",
            "Train step - Step 70, Loss 0.0378006175160408\n",
            "Train epoch - Accuracy: 0.5884848484848485 Loss: 0.04096411621781311 Corrects: 2913\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.036943625658750534\n",
            "Train step - Step 90, Loss 0.035135459154844284\n",
            "Train step - Step 100, Loss 0.03135699778795242\n",
            "Train step - Step 110, Loss 0.03770765662193298\n",
            "Train epoch - Accuracy: 0.6753535353535354 Loss: 0.03337982885012723 Corrects: 3343\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.03286050632596016\n",
            "Train step - Step 130, Loss 0.027674516662955284\n",
            "Train step - Step 140, Loss 0.029848327860236168\n",
            "Train step - Step 150, Loss 0.02788265235722065\n",
            "Train epoch - Accuracy: 0.7135353535353536 Loss: 0.02984140510962467 Corrects: 3532\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.02570232003927231\n",
            "Train step - Step 170, Loss 0.0260053351521492\n",
            "Train step - Step 180, Loss 0.028651956468820572\n",
            "Train step - Step 190, Loss 0.025687314569950104\n",
            "Train epoch - Accuracy: 0.7470707070707071 Loss: 0.027244319750203028 Corrects: 3698\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.024479636922478676\n",
            "Train step - Step 210, Loss 0.02550375834107399\n",
            "Train step - Step 220, Loss 0.026145057752728462\n",
            "Train step - Step 230, Loss 0.023986708372831345\n",
            "Train epoch - Accuracy: 0.7602020202020202 Loss: 0.02533028728131092 Corrects: 3763\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.02468079701066017\n",
            "Train step - Step 250, Loss 0.02204153873026371\n",
            "Train step - Step 260, Loss 0.028237713500857353\n",
            "Train step - Step 270, Loss 0.020985722541809082\n",
            "Train epoch - Accuracy: 0.7733333333333333 Loss: 0.023608298658421546 Corrects: 3828\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.022538132965564728\n",
            "Train step - Step 290, Loss 0.017347538843750954\n",
            "Train step - Step 300, Loss 0.02111489698290825\n",
            "Train step - Step 310, Loss 0.023325443267822266\n",
            "Train epoch - Accuracy: 0.7915151515151515 Loss: 0.02235588041143586 Corrects: 3918\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.025448469445109367\n",
            "Train step - Step 330, Loss 0.021595215424895287\n",
            "Train step - Step 340, Loss 0.023695863783359528\n",
            "Train step - Step 350, Loss 0.02361004799604416\n",
            "Train epoch - Accuracy: 0.7896969696969697 Loss: 0.02184550934668743 Corrects: 3909\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.022477567195892334\n",
            "Train step - Step 370, Loss 0.02226480469107628\n",
            "Train step - Step 380, Loss 0.021577592939138412\n",
            "Train epoch - Accuracy: 0.8143434343434344 Loss: 0.02025325703651014 Corrects: 4031\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.017393184825778008\n",
            "Train step - Step 400, Loss 0.01911924034357071\n",
            "Train step - Step 410, Loss 0.015152445994317532\n",
            "Train step - Step 420, Loss 0.021688051521778107\n",
            "Train epoch - Accuracy: 0.8214141414141414 Loss: 0.019539757355897115 Corrects: 4066\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.018452193588018417\n",
            "Train step - Step 440, Loss 0.020829996094107628\n",
            "Train step - Step 450, Loss 0.017818842083215714\n",
            "Train step - Step 460, Loss 0.01885262131690979\n",
            "Train epoch - Accuracy: 0.8191919191919191 Loss: 0.018891137305534246 Corrects: 4055\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.02241150289773941\n",
            "Train step - Step 480, Loss 0.022430023178458214\n",
            "Train step - Step 490, Loss 0.015180501155555248\n",
            "Train step - Step 500, Loss 0.019840484485030174\n",
            "Train epoch - Accuracy: 0.8307070707070707 Loss: 0.018093050737874677 Corrects: 4112\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.015169552527368069\n",
            "Train step - Step 520, Loss 0.014012310653924942\n",
            "Train step - Step 530, Loss 0.015508126467466354\n",
            "Train step - Step 540, Loss 0.015791473910212517\n",
            "Train epoch - Accuracy: 0.8434343434343434 Loss: 0.017232166539237957 Corrects: 4175\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.015855148434638977\n",
            "Train step - Step 560, Loss 0.014654488302767277\n",
            "Train step - Step 570, Loss 0.01704452931880951\n",
            "Train step - Step 580, Loss 0.015384970232844353\n",
            "Train epoch - Accuracy: 0.8460606060606061 Loss: 0.016803803618884446 Corrects: 4188\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.013600866310298443\n",
            "Train step - Step 600, Loss 0.014185233972966671\n",
            "Train step - Step 610, Loss 0.014978344552218914\n",
            "Train step - Step 620, Loss 0.014268425293266773\n",
            "Train epoch - Accuracy: 0.8565656565656565 Loss: 0.015844709408102613 Corrects: 4240\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.018685974180698395\n",
            "Train step - Step 640, Loss 0.017539700493216515\n",
            "Train step - Step 650, Loss 0.019903944805264473\n",
            "Train step - Step 660, Loss 0.014500288292765617\n",
            "Train epoch - Accuracy: 0.8581818181818182 Loss: 0.015529074323448269 Corrects: 4248\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.0131031833589077\n",
            "Train step - Step 680, Loss 0.015812192112207413\n",
            "Train step - Step 690, Loss 0.01506758015602827\n",
            "Train step - Step 700, Loss 0.012904901057481766\n",
            "Train epoch - Accuracy: 0.8648484848484849 Loss: 0.01477073539349467 Corrects: 4281\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.015043648891150951\n",
            "Train step - Step 720, Loss 0.014193126931786537\n",
            "Train step - Step 730, Loss 0.013373137451708317\n",
            "Train step - Step 740, Loss 0.017260853201150894\n",
            "Train epoch - Accuracy: 0.8753535353535353 Loss: 0.014482806781024643 Corrects: 4333\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.010608082637190819\n",
            "Train step - Step 760, Loss 0.013619381934404373\n",
            "Train step - Step 770, Loss 0.014813629910349846\n",
            "Train epoch - Accuracy: 0.8797979797979798 Loss: 0.01386388951178753 Corrects: 4355\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.01163233257830143\n",
            "Train step - Step 790, Loss 0.015020308084785938\n",
            "Train step - Step 800, Loss 0.01329018548130989\n",
            "Train step - Step 810, Loss 0.014766107313334942\n",
            "Train epoch - Accuracy: 0.8743434343434343 Loss: 0.013701045609483814 Corrects: 4328\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.01395539753139019\n",
            "Train step - Step 830, Loss 0.01075985748320818\n",
            "Train step - Step 840, Loss 0.008478140458464622\n",
            "Train step - Step 850, Loss 0.013983280397951603\n",
            "Train epoch - Accuracy: 0.8840404040404041 Loss: 0.01316414688346964 Corrects: 4376\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.012516145594418049\n",
            "Train step - Step 870, Loss 0.010780122131109238\n",
            "Train step - Step 880, Loss 0.009798851795494556\n",
            "Train step - Step 890, Loss 0.01292532216757536\n",
            "Train epoch - Accuracy: 0.8844444444444445 Loss: 0.012972649495785285 Corrects: 4378\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.011611135676503181\n",
            "Train step - Step 910, Loss 0.01059694029390812\n",
            "Train step - Step 920, Loss 0.014308568090200424\n",
            "Train step - Step 930, Loss 0.01618785969913006\n",
            "Train epoch - Accuracy: 0.8929292929292929 Loss: 0.012690469669272201 Corrects: 4420\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.009487089700996876\n",
            "Train step - Step 950, Loss 0.012399290688335896\n",
            "Train step - Step 960, Loss 0.009623638354241848\n",
            "Train step - Step 970, Loss 0.016385912895202637\n",
            "Train epoch - Accuracy: 0.8941414141414141 Loss: 0.012069510867950892 Corrects: 4426\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.010796156711876392\n",
            "Train step - Step 990, Loss 0.019449030980467796\n",
            "Train step - Step 1000, Loss 0.011096343398094177\n",
            "Train step - Step 1010, Loss 0.0111846299842\n",
            "Train epoch - Accuracy: 0.897979797979798 Loss: 0.011782005332937145 Corrects: 4445\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.01502037514001131\n",
            "Train step - Step 1030, Loss 0.011468088254332542\n",
            "Train step - Step 1040, Loss 0.014526748098433018\n",
            "Train step - Step 1050, Loss 0.010713500902056694\n",
            "Train epoch - Accuracy: 0.9034343434343435 Loss: 0.011454567103599659 Corrects: 4472\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.013953802175819874\n",
            "Train step - Step 1070, Loss 0.012975182384252548\n",
            "Train step - Step 1080, Loss 0.007183701265603304\n",
            "Train step - Step 1090, Loss 0.016276497393846512\n",
            "Train epoch - Accuracy: 0.9068686868686868 Loss: 0.01107464161786166 Corrects: 4489\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.008512064814567566\n",
            "Train step - Step 1110, Loss 0.010152271948754787\n",
            "Train step - Step 1120, Loss 0.0071390774101018906\n",
            "Train step - Step 1130, Loss 0.008824395947158337\n",
            "Train epoch - Accuracy: 0.9028282828282829 Loss: 0.010917811966077847 Corrects: 4469\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.010720593854784966\n",
            "Train step - Step 1150, Loss 0.009313631802797318\n",
            "Train step - Step 1160, Loss 0.012079639360308647\n",
            "Train epoch - Accuracy: 0.908080808080808 Loss: 0.01064042924893926 Corrects: 4495\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.00884475838392973\n",
            "Train step - Step 1180, Loss 0.01277864445000887\n",
            "Train step - Step 1190, Loss 0.009156852960586548\n",
            "Train step - Step 1200, Loss 0.008533772081136703\n",
            "Train epoch - Accuracy: 0.9183838383838384 Loss: 0.009996545766339158 Corrects: 4546\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.010271401144564152\n",
            "Train step - Step 1220, Loss 0.009863577783107758\n",
            "Train step - Step 1230, Loss 0.010936797596514225\n",
            "Train step - Step 1240, Loss 0.00917727965861559\n",
            "Train epoch - Accuracy: 0.916969696969697 Loss: 0.009744485709628072 Corrects: 4539\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.008052477613091469\n",
            "Train step - Step 1260, Loss 0.012569512240588665\n",
            "Train step - Step 1270, Loss 0.01104689296334982\n",
            "Train step - Step 1280, Loss 0.008626812137663364\n",
            "Train epoch - Accuracy: 0.916969696969697 Loss: 0.009674879290314034 Corrects: 4539\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.009466856718063354\n",
            "Train step - Step 1300, Loss 0.010829810984432697\n",
            "Train step - Step 1310, Loss 0.008583868853747845\n",
            "Train step - Step 1320, Loss 0.010988321155309677\n",
            "Train epoch - Accuracy: 0.9226262626262626 Loss: 0.009241854493891952 Corrects: 4567\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.008194155059754848\n",
            "Train step - Step 1340, Loss 0.008966297842562199\n",
            "Train step - Step 1350, Loss 0.009890535846352577\n",
            "Train step - Step 1360, Loss 0.010705893859267235\n",
            "Train epoch - Accuracy: 0.9264646464646464 Loss: 0.008983869730974689 Corrects: 4586\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.008862212300300598\n",
            "Train step - Step 1380, Loss 0.008304775692522526\n",
            "Train step - Step 1390, Loss 0.007866250351071358\n",
            "Train step - Step 1400, Loss 0.007199903484433889\n",
            "Train epoch - Accuracy: 0.9222222222222223 Loss: 0.009038940662273554 Corrects: 4565\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.008396551012992859\n",
            "Train step - Step 1420, Loss 0.009255676530301571\n",
            "Train step - Step 1430, Loss 0.005518716294318438\n",
            "Train step - Step 1440, Loss 0.00778720248490572\n",
            "Train epoch - Accuracy: 0.9268686868686868 Loss: 0.00869184732888684 Corrects: 4588\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.006235072389245033\n",
            "Train step - Step 1460, Loss 0.00959372241050005\n",
            "Train step - Step 1470, Loss 0.010150358080863953\n",
            "Train step - Step 1480, Loss 0.008381850086152554\n",
            "Train epoch - Accuracy: 0.9290909090909091 Loss: 0.008555960645805103 Corrects: 4599\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.007444326765835285\n",
            "Train step - Step 1500, Loss 0.008263773284852505\n",
            "Train step - Step 1510, Loss 0.006794043816626072\n",
            "Train step - Step 1520, Loss 0.010639541782438755\n",
            "Train epoch - Accuracy: 0.9333333333333333 Loss: 0.008262968600323104 Corrects: 4620\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.008315582759678364\n",
            "Train step - Step 1540, Loss 0.005330774001777172\n",
            "Train step - Step 1550, Loss 0.006968745030462742\n",
            "Train epoch - Accuracy: 0.9347474747474748 Loss: 0.007917483264222891 Corrects: 4627\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.008483024314045906\n",
            "Train step - Step 1570, Loss 0.007050858810544014\n",
            "Train step - Step 1580, Loss 0.005418733227998018\n",
            "Train step - Step 1590, Loss 0.008696834556758404\n",
            "Train epoch - Accuracy: 0.9408080808080808 Loss: 0.007706233156525125 Corrects: 4657\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.004985822830349207\n",
            "Train step - Step 1610, Loss 0.008888507261872292\n",
            "Train step - Step 1620, Loss 0.009536330588161945\n",
            "Train step - Step 1630, Loss 0.006651050876826048\n",
            "Train epoch - Accuracy: 0.9321212121212121 Loss: 0.008162501393394037 Corrects: 4614\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.009004001505672932\n",
            "Train step - Step 1650, Loss 0.007202531676739454\n",
            "Train step - Step 1660, Loss 0.00607080664485693\n",
            "Train step - Step 1670, Loss 0.006984494626522064\n",
            "Train epoch - Accuracy: 0.9414141414141414 Loss: 0.00754472463006022 Corrects: 4660\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.007369019091129303\n",
            "Train step - Step 1690, Loss 0.007037242874503136\n",
            "Train step - Step 1700, Loss 0.007897637784481049\n",
            "Train step - Step 1710, Loss 0.00828105304390192\n",
            "Train epoch - Accuracy: 0.936969696969697 Loss: 0.007739381576427305 Corrects: 4638\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.006130714435130358\n",
            "Train step - Step 1730, Loss 0.005434089805930853\n",
            "Train step - Step 1740, Loss 0.007466578856110573\n",
            "Train step - Step 1750, Loss 0.005194268189370632\n",
            "Train epoch - Accuracy: 0.941010101010101 Loss: 0.007304814723179196 Corrects: 4658\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.006963738240301609\n",
            "Train step - Step 1770, Loss 0.005550387315452099\n",
            "Train step - Step 1780, Loss 0.00909324549138546\n",
            "Train step - Step 1790, Loss 0.006204439792782068\n",
            "Train epoch - Accuracy: 0.9464646464646465 Loss: 0.007011475181112988 Corrects: 4685\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.005554625764489174\n",
            "Train step - Step 1810, Loss 0.007350779604166746\n",
            "Train step - Step 1820, Loss 0.007081302348524332\n",
            "Train step - Step 1830, Loss 0.007975924760103226\n",
            "Train epoch - Accuracy: 0.9478787878787879 Loss: 0.006753685902992282 Corrects: 4692\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.0069127450697124004\n",
            "Train step - Step 1850, Loss 0.006237892434000969\n",
            "Train step - Step 1860, Loss 0.004934295080602169\n",
            "Train step - Step 1870, Loss 0.0066541265696287155\n",
            "Train epoch - Accuracy: 0.9521212121212121 Loss: 0.0061930369126676306 Corrects: 4713\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.004671949427574873\n",
            "Train step - Step 1890, Loss 0.005960188806056976\n",
            "Train step - Step 1900, Loss 0.008747866377234459\n",
            "Train step - Step 1910, Loss 0.004070925991982222\n",
            "Train epoch - Accuracy: 0.9488888888888889 Loss: 0.006463528536824566 Corrects: 4697\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.005470625124871731\n",
            "Train step - Step 1930, Loss 0.006486175116151571\n",
            "Train step - Step 1940, Loss 0.003880304517224431\n",
            "Train epoch - Accuracy: 0.9597979797979798 Loss: 0.0055666379323887705 Corrects: 4751\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.005272521171718836\n",
            "Train step - Step 1960, Loss 0.005860954988747835\n",
            "Train step - Step 1970, Loss 0.006447895430028439\n",
            "Train step - Step 1980, Loss 0.005080442409962416\n",
            "Train epoch - Accuracy: 0.961010101010101 Loss: 0.005346843306953558 Corrects: 4757\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.004007137846201658\n",
            "Train step - Step 2000, Loss 0.005250190384685993\n",
            "Train step - Step 2010, Loss 0.004946801345795393\n",
            "Train step - Step 2020, Loss 0.004645774140954018\n",
            "Train epoch - Accuracy: 0.9662626262626263 Loss: 0.004867650575446661 Corrects: 4783\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.007561369799077511\n",
            "Train step - Step 2040, Loss 0.003510206937789917\n",
            "Train step - Step 2050, Loss 0.004488535225391388\n",
            "Train step - Step 2060, Loss 0.004508857615292072\n",
            "Train epoch - Accuracy: 0.9642424242424242 Loss: 0.0049882209737493535 Corrects: 4773\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.004170041531324387\n",
            "Train step - Step 2080, Loss 0.004797338508069515\n",
            "Train step - Step 2090, Loss 0.004999865312129259\n",
            "Train step - Step 2100, Loss 0.004395469091832638\n",
            "Train epoch - Accuracy: 0.9666666666666667 Loss: 0.004835080249479624 Corrects: 4785\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.006457769777625799\n",
            "Train step - Step 2120, Loss 0.005479097831994295\n",
            "Train step - Step 2130, Loss 0.004627226386219263\n",
            "Train step - Step 2140, Loss 0.0071201929822564125\n",
            "Train epoch - Accuracy: 0.9694949494949495 Loss: 0.004810284396276028 Corrects: 4799\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.004404988139867783\n",
            "Train step - Step 2160, Loss 0.005630191881209612\n",
            "Train step - Step 2170, Loss 0.00489641260355711\n",
            "Train step - Step 2180, Loss 0.0037544071674346924\n",
            "Train epoch - Accuracy: 0.9644444444444444 Loss: 0.0049814641631838645 Corrects: 4774\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.003877960843965411\n",
            "Train step - Step 2200, Loss 0.004811983555555344\n",
            "Train step - Step 2210, Loss 0.003676873864606023\n",
            "Train step - Step 2220, Loss 0.005461570341140032\n",
            "Train epoch - Accuracy: 0.9694949494949495 Loss: 0.0046920011734420605 Corrects: 4799\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.0030032717622816563\n",
            "Train step - Step 2240, Loss 0.005426045972853899\n",
            "Train step - Step 2250, Loss 0.004565285984426737\n",
            "Train step - Step 2260, Loss 0.0037380948197096586\n",
            "Train epoch - Accuracy: 0.9696969696969697 Loss: 0.004648013659988088 Corrects: 4800\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.0038914280012249947\n",
            "Train step - Step 2280, Loss 0.0038855927996337414\n",
            "Train step - Step 2290, Loss 0.004928594455122948\n",
            "Train step - Step 2300, Loss 0.004080113489180803\n",
            "Train epoch - Accuracy: 0.9701010101010101 Loss: 0.004594339563351388 Corrects: 4802\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.004531616345047951\n",
            "Train step - Step 2320, Loss 0.006747587583959103\n",
            "Train step - Step 2330, Loss 0.0038557471707463264\n",
            "Train epoch - Accuracy: 0.9660606060606061 Loss: 0.004813017007151637 Corrects: 4782\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.004367202054709196\n",
            "Train step - Step 2350, Loss 0.004883916117250919\n",
            "Train step - Step 2360, Loss 0.005136468447744846\n",
            "Train step - Step 2370, Loss 0.0030483317095786333\n",
            "Train epoch - Accuracy: 0.9709090909090909 Loss: 0.004548421945197113 Corrects: 4806\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.004414665512740612\n",
            "Train step - Step 2390, Loss 0.006129622459411621\n",
            "Train step - Step 2400, Loss 0.0035102658439427614\n",
            "Train step - Step 2410, Loss 0.0038578484673053026\n",
            "Train epoch - Accuracy: 0.9713131313131314 Loss: 0.004468359645606592 Corrects: 4808\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.004716940224170685\n",
            "Train step - Step 2430, Loss 0.004366885405033827\n",
            "Train step - Step 2440, Loss 0.00404695188626647\n",
            "Train step - Step 2450, Loss 0.0036578455474227667\n",
            "Train epoch - Accuracy: 0.9698989898989899 Loss: 0.00452969306788285 Corrects: 4801\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.003972396720200777\n",
            "Train step - Step 2470, Loss 0.0049061221070587635\n",
            "Train step - Step 2480, Loss 0.00547222001478076\n",
            "Train step - Step 2490, Loss 0.003583361394703388\n",
            "Train epoch - Accuracy: 0.9733333333333334 Loss: 0.004453754887887925 Corrects: 4818\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.005232134833931923\n",
            "Train step - Step 2510, Loss 0.003911848645657301\n",
            "Train step - Step 2520, Loss 0.00582454539835453\n",
            "Train step - Step 2530, Loss 0.003965654410421848\n",
            "Train epoch - Accuracy: 0.9705050505050505 Loss: 0.004506051403592632 Corrects: 4804\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.0032958807423710823\n",
            "Train step - Step 2550, Loss 0.004602328408509493\n",
            "Train step - Step 2560, Loss 0.0029859894420951605\n",
            "Train step - Step 2570, Loss 0.005138970445841551\n",
            "Train epoch - Accuracy: 0.9743434343434343 Loss: 0.004328198580993245 Corrects: 4823\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.005714588798582554\n",
            "Train step - Step 2590, Loss 0.004122643265873194\n",
            "Train step - Step 2600, Loss 0.0035757666919380426\n",
            "Train step - Step 2610, Loss 0.004580349661409855\n",
            "Train epoch - Accuracy: 0.9717171717171718 Loss: 0.004446111610253351 Corrects: 4810\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.0039372253231704235\n",
            "Train step - Step 2630, Loss 0.002516878303140402\n",
            "Train step - Step 2640, Loss 0.003921139054000378\n",
            "Train step - Step 2650, Loss 0.0038661977741867304\n",
            "Train epoch - Accuracy: 0.9749494949494949 Loss: 0.004061969649234805 Corrects: 4826\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.005369206424802542\n",
            "Train step - Step 2670, Loss 0.002957892371341586\n",
            "Train step - Step 2680, Loss 0.004276114981621504\n",
            "Train step - Step 2690, Loss 0.0035363684874027967\n",
            "Train epoch - Accuracy: 0.9733333333333334 Loss: 0.0044166292981103515 Corrects: 4818\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.0027155440766364336\n",
            "Train step - Step 2710, Loss 0.005367299076169729\n",
            "Train step - Step 2720, Loss 0.0035207432229071856\n",
            "Train epoch - Accuracy: 0.9725252525252526 Loss: 0.004332497344173567 Corrects: 4814\n",
            "Training finished in 211.48021459579468 seconds\n",
            "EVALUATION:  0.88 0.015030574053525925\n",
            "TEST GROUP:  0.863\n",
            "TEST ALL:  0.1726\n",
            "GROUP:  6\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.1584828943014145\n",
            "Train step - Step 10, Loss 0.06400040537118912\n",
            "Train step - Step 20, Loss 0.05353013798594475\n",
            "Train step - Step 30, Loss 0.04596567526459694\n",
            "Train epoch - Accuracy: 0.2296969696969697 Loss: 0.06463379949933351 Corrects: 1137\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.03963698074221611\n",
            "Train step - Step 50, Loss 0.037255868315696716\n",
            "Train step - Step 60, Loss 0.03645559400320053\n",
            "Train step - Step 70, Loss 0.03302926942706108\n",
            "Train epoch - Accuracy: 0.5543434343434344 Loss: 0.035514912838586654 Corrects: 2744\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.03191886097192764\n",
            "Train step - Step 90, Loss 0.029444027692079544\n",
            "Train step - Step 100, Loss 0.03283644840121269\n",
            "Train step - Step 110, Loss 0.027270568534731865\n",
            "Train epoch - Accuracy: 0.6438383838383839 Loss: 0.02999469468840445 Corrects: 3187\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.02756197564303875\n",
            "Train step - Step 130, Loss 0.02624458819627762\n",
            "Train step - Step 140, Loss 0.026523642241954803\n",
            "Train step - Step 150, Loss 0.02826225571334362\n",
            "Train epoch - Accuracy: 0.6814141414141414 Loss: 0.027104382337343812 Corrects: 3373\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.024784177541732788\n",
            "Train step - Step 170, Loss 0.02594730816781521\n",
            "Train step - Step 180, Loss 0.0266000647097826\n",
            "Train step - Step 190, Loss 0.025313112884759903\n",
            "Train epoch - Accuracy: 0.7103030303030303 Loss: 0.02470573528773255 Corrects: 3516\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.02585090883076191\n",
            "Train step - Step 210, Loss 0.022102929651737213\n",
            "Train step - Step 220, Loss 0.02245151251554489\n",
            "Train step - Step 230, Loss 0.02086598426103592\n",
            "Train epoch - Accuracy: 0.7256565656565657 Loss: 0.023153198655657094 Corrects: 3592\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.02525182254612446\n",
            "Train step - Step 250, Loss 0.021242603659629822\n",
            "Train step - Step 260, Loss 0.01937447302043438\n",
            "Train step - Step 270, Loss 0.020920535549521446\n",
            "Train epoch - Accuracy: 0.7517171717171717 Loss: 0.021722421385724135 Corrects: 3721\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.01871468313038349\n",
            "Train step - Step 290, Loss 0.020595312118530273\n",
            "Train step - Step 300, Loss 0.021342191845178604\n",
            "Train step - Step 310, Loss 0.023435214534401894\n",
            "Train epoch - Accuracy: 0.7674747474747474 Loss: 0.020290444920761416 Corrects: 3799\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.019576776772737503\n",
            "Train step - Step 330, Loss 0.019269386306405067\n",
            "Train step - Step 340, Loss 0.020171627402305603\n",
            "Train step - Step 350, Loss 0.018748674541711807\n",
            "Train epoch - Accuracy: 0.7771717171717172 Loss: 0.019678598899732936 Corrects: 3847\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.020470449700951576\n",
            "Train step - Step 370, Loss 0.019995715469121933\n",
            "Train step - Step 380, Loss 0.017732465639710426\n",
            "Train epoch - Accuracy: 0.7903030303030303 Loss: 0.018516410411608338 Corrects: 3912\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.015797898173332214\n",
            "Train step - Step 400, Loss 0.016392339020967484\n",
            "Train step - Step 410, Loss 0.01836804486811161\n",
            "Train step - Step 420, Loss 0.02040484920144081\n",
            "Train epoch - Accuracy: 0.803030303030303 Loss: 0.017762463015287812 Corrects: 3975\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.01715582236647606\n",
            "Train step - Step 440, Loss 0.016154084354639053\n",
            "Train step - Step 450, Loss 0.017950817942619324\n",
            "Train step - Step 460, Loss 0.01839609630405903\n",
            "Train epoch - Accuracy: 0.8107070707070707 Loss: 0.01710115553453715 Corrects: 4013\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.01735210046172142\n",
            "Train step - Step 480, Loss 0.01680595614016056\n",
            "Train step - Step 490, Loss 0.015153245069086552\n",
            "Train step - Step 500, Loss 0.015233021229505539\n",
            "Train epoch - Accuracy: 0.8094949494949495 Loss: 0.016485871132275072 Corrects: 4007\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.014096636325120926\n",
            "Train step - Step 520, Loss 0.02397165074944496\n",
            "Train step - Step 530, Loss 0.01861684024333954\n",
            "Train step - Step 540, Loss 0.018942786380648613\n",
            "Train epoch - Accuracy: 0.8189898989898989 Loss: 0.016195162344310017 Corrects: 4054\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.01418429333716631\n",
            "Train step - Step 560, Loss 0.01589292101562023\n",
            "Train step - Step 570, Loss 0.013358091004192829\n",
            "Train step - Step 580, Loss 0.014889178797602654\n",
            "Train epoch - Accuracy: 0.8335353535353536 Loss: 0.015527387100337731 Corrects: 4126\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.014745770022273064\n",
            "Train step - Step 600, Loss 0.016811827197670937\n",
            "Train step - Step 610, Loss 0.014367879368364811\n",
            "Train step - Step 620, Loss 0.013866694644093513\n",
            "Train epoch - Accuracy: 0.8321212121212122 Loss: 0.014987717946295184 Corrects: 4119\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.011737748049199581\n",
            "Train step - Step 640, Loss 0.021329253911972046\n",
            "Train step - Step 650, Loss 0.012709997594356537\n",
            "Train step - Step 660, Loss 0.012729588896036148\n",
            "Train epoch - Accuracy: 0.8402020202020202 Loss: 0.01472689875072301 Corrects: 4159\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.016584694385528564\n",
            "Train step - Step 680, Loss 0.01152050867676735\n",
            "Train step - Step 690, Loss 0.012463554739952087\n",
            "Train step - Step 700, Loss 0.01579887419939041\n",
            "Train epoch - Accuracy: 0.8464646464646465 Loss: 0.013908664052835619 Corrects: 4190\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.01479143276810646\n",
            "Train step - Step 720, Loss 0.016133753582835197\n",
            "Train step - Step 730, Loss 0.01715194061398506\n",
            "Train step - Step 740, Loss 0.010492868721485138\n",
            "Train epoch - Accuracy: 0.8492929292929293 Loss: 0.01365645036251858 Corrects: 4204\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.012714048847556114\n",
            "Train step - Step 760, Loss 0.014682621695101261\n",
            "Train step - Step 770, Loss 0.016347095370292664\n",
            "Train epoch - Accuracy: 0.852929292929293 Loss: 0.013519087716786548 Corrects: 4222\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.009446641430258751\n",
            "Train step - Step 790, Loss 0.010624206624925137\n",
            "Train step - Step 800, Loss 0.015182171948254108\n",
            "Train step - Step 810, Loss 0.01444163266569376\n",
            "Train epoch - Accuracy: 0.8654545454545455 Loss: 0.012660432265638703 Corrects: 4284\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.011943861842155457\n",
            "Train step - Step 830, Loss 0.014475506730377674\n",
            "Train step - Step 840, Loss 0.00890649575740099\n",
            "Train step - Step 850, Loss 0.008081457577645779\n",
            "Train epoch - Accuracy: 0.8660606060606061 Loss: 0.012479754851924048 Corrects: 4287\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.01049250178039074\n",
            "Train step - Step 870, Loss 0.01509127113968134\n",
            "Train step - Step 880, Loss 0.01411064900457859\n",
            "Train step - Step 890, Loss 0.012984310276806355\n",
            "Train epoch - Accuracy: 0.8707070707070707 Loss: 0.012114531211178713 Corrects: 4310\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.01425397302955389\n",
            "Train step - Step 910, Loss 0.01422665361315012\n",
            "Train step - Step 920, Loss 0.010979634709656239\n",
            "Train step - Step 930, Loss 0.011401849798858166\n",
            "Train epoch - Accuracy: 0.8745454545454545 Loss: 0.01192505256921956 Corrects: 4329\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.011552330106496811\n",
            "Train step - Step 950, Loss 0.009385381825268269\n",
            "Train step - Step 960, Loss 0.013763481751084328\n",
            "Train step - Step 970, Loss 0.00985480286180973\n",
            "Train epoch - Accuracy: 0.8755555555555555 Loss: 0.01156896328384226 Corrects: 4334\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.012175711803138256\n",
            "Train step - Step 990, Loss 0.011441000737249851\n",
            "Train step - Step 1000, Loss 0.011721575632691383\n",
            "Train step - Step 1010, Loss 0.008542200550436974\n",
            "Train epoch - Accuracy: 0.881010101010101 Loss: 0.011320757514072789 Corrects: 4361\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.01010212767869234\n",
            "Train step - Step 1030, Loss 0.008327821269631386\n",
            "Train step - Step 1040, Loss 0.009512909688055515\n",
            "Train step - Step 1050, Loss 0.011574876494705677\n",
            "Train epoch - Accuracy: 0.8856565656565657 Loss: 0.010807896024274706 Corrects: 4384\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.01204625889658928\n",
            "Train step - Step 1070, Loss 0.008722413331270218\n",
            "Train step - Step 1080, Loss 0.012589851394295692\n",
            "Train step - Step 1090, Loss 0.012251073494553566\n",
            "Train epoch - Accuracy: 0.889090909090909 Loss: 0.01074323071674867 Corrects: 4401\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.00998979713767767\n",
            "Train step - Step 1110, Loss 0.011391923762857914\n",
            "Train step - Step 1120, Loss 0.011176922358572483\n",
            "Train step - Step 1130, Loss 0.0124595258384943\n",
            "Train epoch - Accuracy: 0.8872727272727273 Loss: 0.010583370038204722 Corrects: 4392\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.010692947544157505\n",
            "Train step - Step 1150, Loss 0.011164129711687565\n",
            "Train step - Step 1160, Loss 0.010516121983528137\n",
            "Train epoch - Accuracy: 0.8896969696969697 Loss: 0.010586127151518758 Corrects: 4404\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.007635952439159155\n",
            "Train step - Step 1180, Loss 0.010912365280091763\n",
            "Train step - Step 1190, Loss 0.012405754067003727\n",
            "Train step - Step 1200, Loss 0.011508275754749775\n",
            "Train epoch - Accuracy: 0.8961616161616162 Loss: 0.009848568716810809 Corrects: 4436\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.011191197670996189\n",
            "Train step - Step 1220, Loss 0.0070055341348052025\n",
            "Train step - Step 1230, Loss 0.010050575248897076\n",
            "Train step - Step 1240, Loss 0.013006454333662987\n",
            "Train epoch - Accuracy: 0.8939393939393939 Loss: 0.009823096778356667 Corrects: 4425\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.009816416539251804\n",
            "Train step - Step 1260, Loss 0.006857171654701233\n",
            "Train step - Step 1270, Loss 0.008321721106767654\n",
            "Train step - Step 1280, Loss 0.008336804807186127\n",
            "Train epoch - Accuracy: 0.9022222222222223 Loss: 0.00931869929675201 Corrects: 4466\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.010264855809509754\n",
            "Train step - Step 1300, Loss 0.011168703436851501\n",
            "Train step - Step 1310, Loss 0.012850173749029636\n",
            "Train step - Step 1320, Loss 0.009603532031178474\n",
            "Train epoch - Accuracy: 0.9036363636363637 Loss: 0.009388819468028919 Corrects: 4473\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.0070195975713431835\n",
            "Train step - Step 1340, Loss 0.008642241358757019\n",
            "Train step - Step 1350, Loss 0.0073409304022789\n",
            "Train step - Step 1360, Loss 0.008184484206140041\n",
            "Train epoch - Accuracy: 0.9062626262626262 Loss: 0.009139164848384833 Corrects: 4486\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.007161564193665981\n",
            "Train step - Step 1380, Loss 0.009130685590207577\n",
            "Train step - Step 1390, Loss 0.009501088410615921\n",
            "Train step - Step 1400, Loss 0.01035804208368063\n",
            "Train epoch - Accuracy: 0.9109090909090909 Loss: 0.008936288356404714 Corrects: 4509\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.009858418256044388\n",
            "Train step - Step 1420, Loss 0.006428124848753214\n",
            "Train step - Step 1430, Loss 0.008507519029080868\n",
            "Train step - Step 1440, Loss 0.007575072813779116\n",
            "Train epoch - Accuracy: 0.9145454545454546 Loss: 0.008413592338975933 Corrects: 4527\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.007379897870123386\n",
            "Train step - Step 1460, Loss 0.006160305812954903\n",
            "Train step - Step 1470, Loss 0.007936576381325722\n",
            "Train step - Step 1480, Loss 0.00904464814811945\n",
            "Train epoch - Accuracy: 0.9133333333333333 Loss: 0.008635147197755298 Corrects: 4521\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.008445452898740768\n",
            "Train step - Step 1500, Loss 0.008153448812663555\n",
            "Train step - Step 1510, Loss 0.009448534809052944\n",
            "Train step - Step 1520, Loss 0.01053207740187645\n",
            "Train epoch - Accuracy: 0.917979797979798 Loss: 0.008209561321771506 Corrects: 4544\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.00945817306637764\n",
            "Train step - Step 1540, Loss 0.0069949147291481495\n",
            "Train step - Step 1550, Loss 0.006526429671794176\n",
            "Train epoch - Accuracy: 0.9193939393939394 Loss: 0.007863029349933971 Corrects: 4551\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.006725788582116365\n",
            "Train step - Step 1570, Loss 0.007036604918539524\n",
            "Train step - Step 1580, Loss 0.006043580826371908\n",
            "Train step - Step 1590, Loss 0.009176962077617645\n",
            "Train epoch - Accuracy: 0.9222222222222223 Loss: 0.007873793470513339 Corrects: 4565\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.00672035152092576\n",
            "Train step - Step 1610, Loss 0.008033386431634426\n",
            "Train step - Step 1620, Loss 0.00756076630204916\n",
            "Train step - Step 1630, Loss 0.006064045242965221\n",
            "Train epoch - Accuracy: 0.9197979797979798 Loss: 0.00808699771536119 Corrects: 4553\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.007175068370997906\n",
            "Train step - Step 1650, Loss 0.006289192475378513\n",
            "Train step - Step 1660, Loss 0.006851169280707836\n",
            "Train step - Step 1670, Loss 0.00556095689535141\n",
            "Train epoch - Accuracy: 0.9284848484848485 Loss: 0.007448055926296446 Corrects: 4596\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.006342807784676552\n",
            "Train step - Step 1690, Loss 0.0074934628792107105\n",
            "Train step - Step 1700, Loss 0.008062239736318588\n",
            "Train step - Step 1710, Loss 0.008474815636873245\n",
            "Train epoch - Accuracy: 0.9296969696969697 Loss: 0.007263638555740166 Corrects: 4602\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.007400008384138346\n",
            "Train step - Step 1730, Loss 0.0074968221597373486\n",
            "Train step - Step 1740, Loss 0.004966593813151121\n",
            "Train step - Step 1750, Loss 0.009360110387206078\n",
            "Train epoch - Accuracy: 0.9284848484848485 Loss: 0.007262078592873583 Corrects: 4596\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.005190793890506029\n",
            "Train step - Step 1770, Loss 0.006657567340880632\n",
            "Train step - Step 1780, Loss 0.006415489595383406\n",
            "Train step - Step 1790, Loss 0.007688160054385662\n",
            "Train epoch - Accuracy: 0.9337373737373738 Loss: 0.00702372691556435 Corrects: 4622\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.00534803606569767\n",
            "Train step - Step 1810, Loss 0.007117167580872774\n",
            "Train step - Step 1820, Loss 0.00507715530693531\n",
            "Train step - Step 1830, Loss 0.008516507223248482\n",
            "Train epoch - Accuracy: 0.9313131313131313 Loss: 0.006843339838463851 Corrects: 4610\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.005272668786346912\n",
            "Train step - Step 1850, Loss 0.0059029762633144855\n",
            "Train step - Step 1860, Loss 0.0070298584178090096\n",
            "Train step - Step 1870, Loss 0.005514063406735659\n",
            "Train epoch - Accuracy: 0.9367676767676768 Loss: 0.006582309009714259 Corrects: 4637\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.005878726951777935\n",
            "Train step - Step 1890, Loss 0.006209110375493765\n",
            "Train step - Step 1900, Loss 0.005228797439485788\n",
            "Train step - Step 1910, Loss 0.006474427413195372\n",
            "Train epoch - Accuracy: 0.9391919191919192 Loss: 0.006412754788279834 Corrects: 4649\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.009607876650989056\n",
            "Train step - Step 1930, Loss 0.006905387621372938\n",
            "Train step - Step 1940, Loss 0.005398899782449007\n",
            "Train epoch - Accuracy: 0.9426262626262626 Loss: 0.0061704252955663685 Corrects: 4666\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.004806334152817726\n",
            "Train step - Step 1960, Loss 0.0052040391601622105\n",
            "Train step - Step 1970, Loss 0.005600510165095329\n",
            "Train step - Step 1980, Loss 0.0052389707416296005\n",
            "Train epoch - Accuracy: 0.944040404040404 Loss: 0.005799322515688461 Corrects: 4673\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.004999105352908373\n",
            "Train step - Step 2000, Loss 0.006179848685860634\n",
            "Train step - Step 2010, Loss 0.0063648405484855175\n",
            "Train step - Step 2020, Loss 0.003461597952991724\n",
            "Train epoch - Accuracy: 0.9527272727272728 Loss: 0.005330054616612016 Corrects: 4716\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.0038975717034190893\n",
            "Train step - Step 2040, Loss 0.0058250511065125465\n",
            "Train step - Step 2050, Loss 0.005646360106766224\n",
            "Train step - Step 2060, Loss 0.0058092642575502396\n",
            "Train epoch - Accuracy: 0.9515151515151515 Loss: 0.0052868336049670525 Corrects: 4710\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.006213754415512085\n",
            "Train step - Step 2080, Loss 0.008648323826491833\n",
            "Train step - Step 2090, Loss 0.005101102404296398\n",
            "Train step - Step 2100, Loss 0.005315045826137066\n",
            "Train epoch - Accuracy: 0.9488888888888889 Loss: 0.005459281177720939 Corrects: 4697\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.004885596223175526\n",
            "Train step - Step 2120, Loss 0.006415278185158968\n",
            "Train step - Step 2130, Loss 0.005775811616331339\n",
            "Train step - Step 2140, Loss 0.007342774886637926\n",
            "Train epoch - Accuracy: 0.952929292929293 Loss: 0.005376615232101293 Corrects: 4717\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.005068651400506496\n",
            "Train step - Step 2160, Loss 0.0048992447555065155\n",
            "Train step - Step 2170, Loss 0.005061114206910133\n",
            "Train step - Step 2180, Loss 0.005557359661906958\n",
            "Train epoch - Accuracy: 0.9563636363636364 Loss: 0.005288333152279709 Corrects: 4734\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.005709965247660875\n",
            "Train step - Step 2200, Loss 0.005440875887870789\n",
            "Train step - Step 2210, Loss 0.004245205782353878\n",
            "Train step - Step 2220, Loss 0.006865413393825293\n",
            "Train epoch - Accuracy: 0.9537373737373738 Loss: 0.005348672024303614 Corrects: 4721\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.00580579461529851\n",
            "Train step - Step 2240, Loss 0.005592551082372665\n",
            "Train step - Step 2250, Loss 0.00627153692767024\n",
            "Train step - Step 2260, Loss 0.004773955792188644\n",
            "Train epoch - Accuracy: 0.9541414141414142 Loss: 0.005075304235021273 Corrects: 4723\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.004463047720491886\n",
            "Train step - Step 2280, Loss 0.004453933332115412\n",
            "Train step - Step 2290, Loss 0.003828879678621888\n",
            "Train step - Step 2300, Loss 0.004401399753987789\n",
            "Train epoch - Accuracy: 0.9521212121212121 Loss: 0.005194782273411149 Corrects: 4713\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.006941270083189011\n",
            "Train step - Step 2320, Loss 0.005106017459183931\n",
            "Train step - Step 2330, Loss 0.003769639879465103\n",
            "Train epoch - Accuracy: 0.9551515151515152 Loss: 0.005077229857802241 Corrects: 4728\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.007043855264782906\n",
            "Train step - Step 2350, Loss 0.008156483992934227\n",
            "Train step - Step 2360, Loss 0.0054964893497526646\n",
            "Train step - Step 2370, Loss 0.004939290229231119\n",
            "Train epoch - Accuracy: 0.954949494949495 Loss: 0.0051514743590219455 Corrects: 4727\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.005867790896445513\n",
            "Train step - Step 2390, Loss 0.004075175616890192\n",
            "Train step - Step 2400, Loss 0.007650819607079029\n",
            "Train step - Step 2410, Loss 0.004050531890243292\n",
            "Train epoch - Accuracy: 0.9527272727272728 Loss: 0.005217491047964855 Corrects: 4716\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.0038990562316030264\n",
            "Train step - Step 2430, Loss 0.006349128670990467\n",
            "Train step - Step 2440, Loss 0.004671072121709585\n",
            "Train step - Step 2450, Loss 0.004285020288079977\n",
            "Train epoch - Accuracy: 0.9577777777777777 Loss: 0.005037516809134471 Corrects: 4741\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.0047752754762768745\n",
            "Train step - Step 2470, Loss 0.004288257099688053\n",
            "Train step - Step 2480, Loss 0.005410798825323582\n",
            "Train step - Step 2490, Loss 0.0033825826831161976\n",
            "Train epoch - Accuracy: 0.9585858585858585 Loss: 0.004873743159602387 Corrects: 4745\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.006834116764366627\n",
            "Train step - Step 2510, Loss 0.004149808082729578\n",
            "Train step - Step 2520, Loss 0.006434222217649221\n",
            "Train step - Step 2530, Loss 0.0057249958626925945\n",
            "Train epoch - Accuracy: 0.961010101010101 Loss: 0.004806175391857672 Corrects: 4757\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.003651977516710758\n",
            "Train step - Step 2550, Loss 0.004524792544543743\n",
            "Train step - Step 2560, Loss 0.004309701733291149\n",
            "Train step - Step 2570, Loss 0.004250415600836277\n",
            "Train epoch - Accuracy: 0.9585858585858585 Loss: 0.004809687840705267 Corrects: 4745\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.003610607935115695\n",
            "Train step - Step 2590, Loss 0.005853564012795687\n",
            "Train step - Step 2600, Loss 0.004577940795570612\n",
            "Train step - Step 2610, Loss 0.0035493255127221346\n",
            "Train epoch - Accuracy: 0.9606060606060606 Loss: 0.004718312165050796 Corrects: 4755\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.004095761571079493\n",
            "Train step - Step 2630, Loss 0.0024307514540851116\n",
            "Train step - Step 2640, Loss 0.003717329353094101\n",
            "Train step - Step 2650, Loss 0.006549215875566006\n",
            "Train epoch - Accuracy: 0.9612121212121212 Loss: 0.004679146841582325 Corrects: 4758\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.005409243982285261\n",
            "Train step - Step 2670, Loss 0.0039700912311673164\n",
            "Train step - Step 2680, Loss 0.004872990772128105\n",
            "Train step - Step 2690, Loss 0.007008969318121672\n",
            "Train epoch - Accuracy: 0.9612121212121212 Loss: 0.004622860344108006 Corrects: 4758\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.004117287695407867\n",
            "Train step - Step 2710, Loss 0.005170024000108242\n",
            "Train step - Step 2720, Loss 0.005872885696589947\n",
            "Train epoch - Accuracy: 0.9587878787878787 Loss: 0.004883939529477496 Corrects: 4746\n",
            "Training finished in 210.93584561347961 seconds\n",
            "EVALUATION:  0.84 0.014301599934697151\n",
            "TEST GROUP:  0.849\n",
            "TEST ALL:  0.1415\n",
            "GROUP:  7\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.19079843163490295\n",
            "Train step - Step 10, Loss 0.060884326696395874\n",
            "Train step - Step 20, Loss 0.05332502722740173\n",
            "Train step - Step 30, Loss 0.041622310876846313\n",
            "Train epoch - Accuracy: 0.2381818181818182 Loss: 0.060553831390359185 Corrects: 1179\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.033040259033441544\n",
            "Train step - Step 50, Loss 0.03067207708954811\n",
            "Train step - Step 60, Loss 0.030422452837228775\n",
            "Train step - Step 70, Loss 0.029296480119228363\n",
            "Train epoch - Accuracy: 0.5753535353535354 Loss: 0.02964363949015887 Corrects: 2848\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.025396941229701042\n",
            "Train step - Step 90, Loss 0.02267075888812542\n",
            "Train step - Step 100, Loss 0.026401054114103317\n",
            "Train step - Step 110, Loss 0.0223401952534914\n",
            "Train epoch - Accuracy: 0.6729292929292929 Loss: 0.024017490195204515 Corrects: 3331\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.022142237052321434\n",
            "Train step - Step 130, Loss 0.019339660182595253\n",
            "Train step - Step 140, Loss 0.021643737331032753\n",
            "Train step - Step 150, Loss 0.018536999821662903\n",
            "Train epoch - Accuracy: 0.7137373737373738 Loss: 0.020985264997891707 Corrects: 3533\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.019095951691269875\n",
            "Train step - Step 170, Loss 0.019812345504760742\n",
            "Train step - Step 180, Loss 0.018033381551504135\n",
            "Train step - Step 190, Loss 0.018002456054091454\n",
            "Train epoch - Accuracy: 0.7426262626262626 Loss: 0.01921584193604161 Corrects: 3676\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.0198674276471138\n",
            "Train step - Step 210, Loss 0.01565270684659481\n",
            "Train step - Step 220, Loss 0.018267782405018806\n",
            "Train step - Step 230, Loss 0.02060004696249962\n",
            "Train epoch - Accuracy: 0.776969696969697 Loss: 0.017450273806473824 Corrects: 3846\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.014795017428696156\n",
            "Train step - Step 250, Loss 0.016696225851774216\n",
            "Train step - Step 260, Loss 0.017523786053061485\n",
            "Train step - Step 270, Loss 0.017398275434970856\n",
            "Train epoch - Accuracy: 0.7822222222222223 Loss: 0.016432950185856433 Corrects: 3872\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.018550226464867592\n",
            "Train step - Step 290, Loss 0.016871077939867973\n",
            "Train step - Step 300, Loss 0.01571868173778057\n",
            "Train step - Step 310, Loss 0.014450517483055592\n",
            "Train epoch - Accuracy: 0.7913131313131313 Loss: 0.015724184519112713 Corrects: 3917\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.014026383869349957\n",
            "Train step - Step 330, Loss 0.014287447556853294\n",
            "Train step - Step 340, Loss 0.01744494028389454\n",
            "Train step - Step 350, Loss 0.013386484235525131\n",
            "Train epoch - Accuracy: 0.8062626262626262 Loss: 0.014913006053127424 Corrects: 3991\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.013973300345242023\n",
            "Train step - Step 370, Loss 0.011575561948120594\n",
            "Train step - Step 380, Loss 0.012695828452706337\n",
            "Train epoch - Accuracy: 0.8195959595959595 Loss: 0.013827380101864387 Corrects: 4057\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.011474126018583775\n",
            "Train step - Step 400, Loss 0.014189514331519604\n",
            "Train step - Step 410, Loss 0.011579406447708607\n",
            "Train step - Step 420, Loss 0.01571561023592949\n",
            "Train epoch - Accuracy: 0.8280808080808081 Loss: 0.013380636678199576 Corrects: 4099\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.012857699766755104\n",
            "Train step - Step 440, Loss 0.01196656096726656\n",
            "Train step - Step 450, Loss 0.012373341247439384\n",
            "Train step - Step 460, Loss 0.010896975174546242\n",
            "Train epoch - Accuracy: 0.8355555555555556 Loss: 0.01268685811208655 Corrects: 4136\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.012401429936289787\n",
            "Train step - Step 480, Loss 0.011993811465799809\n",
            "Train step - Step 490, Loss 0.011053993366658688\n",
            "Train step - Step 500, Loss 0.012078436091542244\n",
            "Train epoch - Accuracy: 0.8501010101010101 Loss: 0.011933177863859166 Corrects: 4208\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.013518113642930984\n",
            "Train step - Step 520, Loss 0.01003650575876236\n",
            "Train step - Step 530, Loss 0.013217694126069546\n",
            "Train step - Step 540, Loss 0.009885883890092373\n",
            "Train epoch - Accuracy: 0.8567676767676767 Loss: 0.011447163340494488 Corrects: 4241\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.011014151386916637\n",
            "Train step - Step 560, Loss 0.013033276423811913\n",
            "Train step - Step 570, Loss 0.011688890866935253\n",
            "Train step - Step 580, Loss 0.007956043817102909\n",
            "Train epoch - Accuracy: 0.86 Loss: 0.011145647546709186 Corrects: 4257\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.011058277450501919\n",
            "Train step - Step 600, Loss 0.009950775653123856\n",
            "Train step - Step 610, Loss 0.013367917388677597\n",
            "Train step - Step 620, Loss 0.010029514320194721\n",
            "Train epoch - Accuracy: 0.8606060606060606 Loss: 0.011141871699329578 Corrects: 4260\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.010293867439031601\n",
            "Train step - Step 640, Loss 0.011735683307051659\n",
            "Train step - Step 650, Loss 0.009594185277819633\n",
            "Train step - Step 660, Loss 0.009350663982331753\n",
            "Train epoch - Accuracy: 0.8674747474747475 Loss: 0.010415653885163441 Corrects: 4294\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.009574511088430882\n",
            "Train step - Step 680, Loss 0.010083368979394436\n",
            "Train step - Step 690, Loss 0.010883299633860588\n",
            "Train step - Step 700, Loss 0.011711716651916504\n",
            "Train epoch - Accuracy: 0.8759595959595959 Loss: 0.009859659137448878 Corrects: 4336\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.008727706968784332\n",
            "Train step - Step 720, Loss 0.009214125573635101\n",
            "Train step - Step 730, Loss 0.010808713734149933\n",
            "Train step - Step 740, Loss 0.009420836344361305\n",
            "Train epoch - Accuracy: 0.8797979797979798 Loss: 0.00966795671994638 Corrects: 4355\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.009323201142251492\n",
            "Train step - Step 760, Loss 0.009071743115782738\n",
            "Train step - Step 770, Loss 0.007934128865599632\n",
            "Train epoch - Accuracy: 0.88 Loss: 0.009584204014578853 Corrects: 4356\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.008469169959425926\n",
            "Train step - Step 790, Loss 0.00621027871966362\n",
            "Train step - Step 800, Loss 0.008449680171906948\n",
            "Train step - Step 810, Loss 0.00859369058161974\n",
            "Train epoch - Accuracy: 0.8919191919191919 Loss: 0.008923224942815123 Corrects: 4415\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.009002048522233963\n",
            "Train step - Step 830, Loss 0.007114167790859938\n",
            "Train step - Step 840, Loss 0.010785211808979511\n",
            "Train step - Step 850, Loss 0.0061206151731312275\n",
            "Train epoch - Accuracy: 0.8955555555555555 Loss: 0.008817686031036305 Corrects: 4433\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.00710247503593564\n",
            "Train step - Step 870, Loss 0.008879289962351322\n",
            "Train step - Step 880, Loss 0.007961973547935486\n",
            "Train step - Step 890, Loss 0.010653822682797909\n",
            "Train epoch - Accuracy: 0.8993939393939394 Loss: 0.008536450626949469 Corrects: 4452\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.007573694922029972\n",
            "Train step - Step 910, Loss 0.006876766216009855\n",
            "Train step - Step 920, Loss 0.008418658748269081\n",
            "Train step - Step 930, Loss 0.007873762398958206\n",
            "Train epoch - Accuracy: 0.8997979797979798 Loss: 0.008317291986084345 Corrects: 4454\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.005293873604387045\n",
            "Train step - Step 950, Loss 0.011573072522878647\n",
            "Train step - Step 960, Loss 0.007653745822608471\n",
            "Train step - Step 970, Loss 0.006650385446846485\n",
            "Train epoch - Accuracy: 0.9048484848484849 Loss: 0.007968694451029854 Corrects: 4479\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.00754470843821764\n",
            "Train step - Step 990, Loss 0.009101251140236855\n",
            "Train step - Step 1000, Loss 0.009575499221682549\n",
            "Train step - Step 1010, Loss 0.006368571892380714\n",
            "Train epoch - Accuracy: 0.9028282828282829 Loss: 0.00804439385355723 Corrects: 4469\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.006197584792971611\n",
            "Train step - Step 1030, Loss 0.008422580547630787\n",
            "Train step - Step 1040, Loss 0.0077417101711034775\n",
            "Train step - Step 1050, Loss 0.00986081175506115\n",
            "Train epoch - Accuracy: 0.9026262626262627 Loss: 0.007850569474200408 Corrects: 4468\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.007959256879985332\n",
            "Train step - Step 1070, Loss 0.008026002906262875\n",
            "Train step - Step 1080, Loss 0.007046084851026535\n",
            "Train step - Step 1090, Loss 0.008063254877924919\n",
            "Train epoch - Accuracy: 0.9151515151515152 Loss: 0.007374051150988148 Corrects: 4530\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.007751693017780781\n",
            "Train step - Step 1110, Loss 0.007347544655203819\n",
            "Train step - Step 1120, Loss 0.0051454808562994\n",
            "Train step - Step 1130, Loss 0.005794467870146036\n",
            "Train epoch - Accuracy: 0.9161616161616162 Loss: 0.007159475790844722 Corrects: 4535\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.008196466602385044\n",
            "Train step - Step 1150, Loss 0.006705156061798334\n",
            "Train step - Step 1160, Loss 0.006732799578458071\n",
            "Train epoch - Accuracy: 0.9191919191919192 Loss: 0.00688276110273419 Corrects: 4550\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.008618192747235298\n",
            "Train step - Step 1180, Loss 0.0054862890392541885\n",
            "Train step - Step 1190, Loss 0.006814735941588879\n",
            "Train step - Step 1200, Loss 0.007137235254049301\n",
            "Train epoch - Accuracy: 0.9163636363636364 Loss: 0.0070617844962110425 Corrects: 4536\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.008172571659088135\n",
            "Train step - Step 1220, Loss 0.005567610263824463\n",
            "Train step - Step 1230, Loss 0.005289472173899412\n",
            "Train step - Step 1240, Loss 0.006102296523749828\n",
            "Train epoch - Accuracy: 0.9246464646464646 Loss: 0.006401869490592167 Corrects: 4577\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.0038638676051050425\n",
            "Train step - Step 1260, Loss 0.005261593032628298\n",
            "Train step - Step 1270, Loss 0.007108398247510195\n",
            "Train step - Step 1280, Loss 0.006924037355929613\n",
            "Train epoch - Accuracy: 0.9264646464646464 Loss: 0.006276781160121012 Corrects: 4586\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.006706259213387966\n",
            "Train step - Step 1300, Loss 0.005909618455916643\n",
            "Train step - Step 1310, Loss 0.006043482571840286\n",
            "Train step - Step 1320, Loss 0.008012762293219566\n",
            "Train epoch - Accuracy: 0.9238383838383838 Loss: 0.006397047085743962 Corrects: 4573\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.005985020659863949\n",
            "Train step - Step 1340, Loss 0.007341977674514055\n",
            "Train step - Step 1350, Loss 0.005050090607255697\n",
            "Train step - Step 1360, Loss 0.005340321455150843\n",
            "Train epoch - Accuracy: 0.9266666666666666 Loss: 0.0062679395753175325 Corrects: 4587\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.007076187990605831\n",
            "Train step - Step 1380, Loss 0.005463292356580496\n",
            "Train step - Step 1390, Loss 0.004155727103352547\n",
            "Train step - Step 1400, Loss 0.006677187047898769\n",
            "Train epoch - Accuracy: 0.9290909090909091 Loss: 0.006019324265194662 Corrects: 4599\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.004758956842124462\n",
            "Train step - Step 1420, Loss 0.006266721058636904\n",
            "Train step - Step 1430, Loss 0.0038224621675908566\n",
            "Train step - Step 1440, Loss 0.006003811489790678\n",
            "Train epoch - Accuracy: 0.9325252525252525 Loss: 0.005795167325384388 Corrects: 4616\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.004522666800767183\n",
            "Train step - Step 1460, Loss 0.006604446563869715\n",
            "Train step - Step 1470, Loss 0.005691904108971357\n",
            "Train step - Step 1480, Loss 0.009460213594138622\n",
            "Train epoch - Accuracy: 0.9353535353535354 Loss: 0.005681840105716026 Corrects: 4630\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.004638282582163811\n",
            "Train step - Step 1500, Loss 0.005354787688702345\n",
            "Train step - Step 1510, Loss 0.005477388389408588\n",
            "Train step - Step 1520, Loss 0.004587572533637285\n",
            "Train epoch - Accuracy: 0.935959595959596 Loss: 0.00554535350771715 Corrects: 4633\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.004841449670493603\n",
            "Train step - Step 1540, Loss 0.004735431168228388\n",
            "Train step - Step 1550, Loss 0.005153799895197153\n",
            "Train epoch - Accuracy: 0.9416161616161616 Loss: 0.005196280816212447 Corrects: 4661\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.005583529826253653\n",
            "Train step - Step 1570, Loss 0.005644586402922869\n",
            "Train step - Step 1580, Loss 0.003679918358102441\n",
            "Train step - Step 1590, Loss 0.00412556529045105\n",
            "Train epoch - Accuracy: 0.944040404040404 Loss: 0.005241598358772921 Corrects: 4673\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.005515462253242731\n",
            "Train step - Step 1610, Loss 0.0060995654202997684\n",
            "Train step - Step 1620, Loss 0.006203799042850733\n",
            "Train step - Step 1630, Loss 0.008998271077871323\n",
            "Train epoch - Accuracy: 0.9464646464646465 Loss: 0.0050610325290736825 Corrects: 4685\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.00435865530744195\n",
            "Train step - Step 1650, Loss 0.0051618916913867\n",
            "Train step - Step 1660, Loss 0.005545985884964466\n",
            "Train step - Step 1670, Loss 0.004101496655493975\n",
            "Train epoch - Accuracy: 0.9464646464646465 Loss: 0.004924641897281011 Corrects: 4685\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.005167735740542412\n",
            "Train step - Step 1690, Loss 0.005686809774488211\n",
            "Train step - Step 1700, Loss 0.0050706868059933186\n",
            "Train step - Step 1710, Loss 0.004476890433579683\n",
            "Train epoch - Accuracy: 0.944040404040404 Loss: 0.005072335008569438 Corrects: 4673\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.005252528004348278\n",
            "Train step - Step 1730, Loss 0.0047863624058663845\n",
            "Train step - Step 1740, Loss 0.004460426978766918\n",
            "Train step - Step 1750, Loss 0.003994452767074108\n",
            "Train epoch - Accuracy: 0.9478787878787879 Loss: 0.004573831762930359 Corrects: 4692\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.0042190952226519585\n",
            "Train step - Step 1770, Loss 0.004837359767407179\n",
            "Train step - Step 1780, Loss 0.0048295422457158566\n",
            "Train step - Step 1790, Loss 0.004961918108165264\n",
            "Train epoch - Accuracy: 0.9480808080808081 Loss: 0.00463395848700946 Corrects: 4693\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.0038249774370342493\n",
            "Train step - Step 1810, Loss 0.0037495314609259367\n",
            "Train step - Step 1820, Loss 0.0029881135560572147\n",
            "Train step - Step 1830, Loss 0.005330095998942852\n",
            "Train epoch - Accuracy: 0.9505050505050505 Loss: 0.004493123390382588 Corrects: 4705\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.00440542958676815\n",
            "Train step - Step 1850, Loss 0.004723619669675827\n",
            "Train step - Step 1860, Loss 0.0037375851534307003\n",
            "Train step - Step 1870, Loss 0.005175212863832712\n",
            "Train epoch - Accuracy: 0.952929292929293 Loss: 0.004420386794578246 Corrects: 4717\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.003337625414133072\n",
            "Train step - Step 1890, Loss 0.005498762708157301\n",
            "Train step - Step 1900, Loss 0.005156095139682293\n",
            "Train step - Step 1910, Loss 0.0034618990030139685\n",
            "Train epoch - Accuracy: 0.9567676767676768 Loss: 0.0040929927230069435 Corrects: 4736\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.004504772834479809\n",
            "Train step - Step 1930, Loss 0.003476354293525219\n",
            "Train step - Step 1940, Loss 0.005199066828936338\n",
            "Train epoch - Accuracy: 0.9642424242424242 Loss: 0.0037635994083577333 Corrects: 4773\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.005130924750119448\n",
            "Train step - Step 1960, Loss 0.0030117291025817394\n",
            "Train step - Step 1970, Loss 0.003404292045161128\n",
            "Train step - Step 1980, Loss 0.004405664745718241\n",
            "Train epoch - Accuracy: 0.9670707070707071 Loss: 0.0034551680381550934 Corrects: 4787\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.002618059515953064\n",
            "Train step - Step 2000, Loss 0.003275642404332757\n",
            "Train step - Step 2010, Loss 0.003422373440116644\n",
            "Train step - Step 2020, Loss 0.003576677991077304\n",
            "Train epoch - Accuracy: 0.9664646464646465 Loss: 0.003443166046036464 Corrects: 4784\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.0032236857805401087\n",
            "Train step - Step 2040, Loss 0.005774229299277067\n",
            "Train step - Step 2050, Loss 0.003605026053264737\n",
            "Train step - Step 2060, Loss 0.004520744550973177\n",
            "Train epoch - Accuracy: 0.962020202020202 Loss: 0.0036205539307698157 Corrects: 4762\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.0029608551412820816\n",
            "Train step - Step 2080, Loss 0.003912429325282574\n",
            "Train step - Step 2090, Loss 0.002497491892427206\n",
            "Train step - Step 2100, Loss 0.0034504844807088375\n",
            "Train epoch - Accuracy: 0.9642424242424242 Loss: 0.0034674031379623243 Corrects: 4773\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.003965786658227444\n",
            "Train step - Step 2120, Loss 0.002917641308158636\n",
            "Train step - Step 2130, Loss 0.00388543913140893\n",
            "Train step - Step 2140, Loss 0.003417728701606393\n",
            "Train epoch - Accuracy: 0.9660606060606061 Loss: 0.003543746022488734 Corrects: 4782\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.0032743087504059076\n",
            "Train step - Step 2160, Loss 0.0026776501908898354\n",
            "Train step - Step 2170, Loss 0.004768445622175932\n",
            "Train step - Step 2180, Loss 0.002277740277349949\n",
            "Train epoch - Accuracy: 0.9703030303030303 Loss: 0.003352441688566798 Corrects: 4803\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.00406469264999032\n",
            "Train step - Step 2200, Loss 0.0026271287351846695\n",
            "Train step - Step 2210, Loss 0.002546403557062149\n",
            "Train step - Step 2220, Loss 0.004115949384868145\n",
            "Train epoch - Accuracy: 0.9672727272727273 Loss: 0.003418505965929591 Corrects: 4788\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.004283690359443426\n",
            "Train step - Step 2240, Loss 0.004497676156461239\n",
            "Train step - Step 2250, Loss 0.0031133447773754597\n",
            "Train step - Step 2260, Loss 0.003795985598117113\n",
            "Train epoch - Accuracy: 0.9670707070707071 Loss: 0.0033523138245624123 Corrects: 4787\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.0032203567679971457\n",
            "Train step - Step 2280, Loss 0.005262003745883703\n",
            "Train step - Step 2290, Loss 0.00251050665974617\n",
            "Train step - Step 2300, Loss 0.0072740293107926846\n",
            "Train epoch - Accuracy: 0.9688888888888889 Loss: 0.003333070586915269 Corrects: 4796\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.0028535188175737858\n",
            "Train step - Step 2320, Loss 0.0025748624466359615\n",
            "Train step - Step 2330, Loss 0.005159775260835886\n",
            "Train epoch - Accuracy: 0.9694949494949495 Loss: 0.0032141902591242937 Corrects: 4799\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.0033005180303007364\n",
            "Train step - Step 2350, Loss 0.0040838224813342094\n",
            "Train step - Step 2360, Loss 0.0028416721615940332\n",
            "Train step - Step 2370, Loss 0.00458470918238163\n",
            "Train epoch - Accuracy: 0.9688888888888889 Loss: 0.003443696724081581 Corrects: 4796\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.0021549512166529894\n",
            "Train step - Step 2390, Loss 0.0031707328744232655\n",
            "Train step - Step 2400, Loss 0.0028735282830893993\n",
            "Train step - Step 2410, Loss 0.001527759712189436\n",
            "Train epoch - Accuracy: 0.9705050505050505 Loss: 0.00324118924966891 Corrects: 4804\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.0039009815081954002\n",
            "Train step - Step 2430, Loss 0.0023433780297636986\n",
            "Train step - Step 2440, Loss 0.002992679364979267\n",
            "Train step - Step 2450, Loss 0.0023669791407883167\n",
            "Train epoch - Accuracy: 0.9725252525252526 Loss: 0.003089219168430627 Corrects: 4814\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.003665875643491745\n",
            "Train step - Step 2470, Loss 0.00313539057970047\n",
            "Train step - Step 2480, Loss 0.0029208410996943712\n",
            "Train step - Step 2490, Loss 0.0028612776659429073\n",
            "Train epoch - Accuracy: 0.9674747474747475 Loss: 0.0032197325716190267 Corrects: 4789\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.0020743990316987038\n",
            "Train step - Step 2510, Loss 0.0026651867665350437\n",
            "Train step - Step 2520, Loss 0.003309676656499505\n",
            "Train step - Step 2530, Loss 0.00420975498855114\n",
            "Train epoch - Accuracy: 0.9701010101010101 Loss: 0.0032537919487521955 Corrects: 4802\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.0033426505979150534\n",
            "Train step - Step 2550, Loss 0.003626673948019743\n",
            "Train step - Step 2560, Loss 0.0033291447907686234\n",
            "Train step - Step 2570, Loss 0.0041178991086781025\n",
            "Train epoch - Accuracy: 0.9696969696969697 Loss: 0.003192282785576853 Corrects: 4800\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.0026524863205850124\n",
            "Train step - Step 2590, Loss 0.004472967702895403\n",
            "Train step - Step 2600, Loss 0.003373060841113329\n",
            "Train step - Step 2610, Loss 0.0021076411940157413\n",
            "Train epoch - Accuracy: 0.9705050505050505 Loss: 0.0031154269939570717 Corrects: 4804\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.0025941904168576\n",
            "Train step - Step 2630, Loss 0.003935426473617554\n",
            "Train step - Step 2640, Loss 0.00293001183308661\n",
            "Train step - Step 2650, Loss 0.004118301440030336\n",
            "Train epoch - Accuracy: 0.9709090909090909 Loss: 0.003190110203236191 Corrects: 4806\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.003910413943231106\n",
            "Train step - Step 2670, Loss 0.0029753935523331165\n",
            "Train step - Step 2680, Loss 0.0030466674361377954\n",
            "Train step - Step 2690, Loss 0.003360119881108403\n",
            "Train epoch - Accuracy: 0.9690909090909091 Loss: 0.003125233536473278 Corrects: 4797\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.004732255823910236\n",
            "Train step - Step 2710, Loss 0.0029207998886704445\n",
            "Train step - Step 2720, Loss 0.0031162381637841463\n",
            "Train epoch - Accuracy: 0.9696969696969697 Loss: 0.0031334476090139813 Corrects: 4800\n",
            "Training finished in 211.04421973228455 seconds\n",
            "EVALUATION:  0.86 0.0085431644693017\n",
            "TEST GROUP:  0.861\n",
            "TEST ALL:  0.123\n",
            "GROUP:  8\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.14977528154850006\n",
            "Train step - Step 10, Loss 0.05111587047576904\n",
            "Train step - Step 20, Loss 0.045791205018758774\n",
            "Train step - Step 30, Loss 0.0332384817302227\n",
            "Train epoch - Accuracy: 0.2476767676767677 Loss: 0.05188750501232918 Corrects: 1226\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.030633483082056046\n",
            "Train step - Step 50, Loss 0.025103097781538963\n",
            "Train step - Step 60, Loss 0.02488791011273861\n",
            "Train step - Step 70, Loss 0.022631799802184105\n",
            "Train epoch - Accuracy: 0.5666666666666667 Loss: 0.02601990997113965 Corrects: 2805\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.02306203544139862\n",
            "Train step - Step 90, Loss 0.024887917563319206\n",
            "Train step - Step 100, Loss 0.02186690829694271\n",
            "Train step - Step 110, Loss 0.021816646680235863\n",
            "Train epoch - Accuracy: 0.6450505050505051 Loss: 0.021829778009442368 Corrects: 3193\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.021638456732034683\n",
            "Train step - Step 130, Loss 0.021112581714987755\n",
            "Train step - Step 140, Loss 0.02031072974205017\n",
            "Train step - Step 150, Loss 0.014433080330491066\n",
            "Train epoch - Accuracy: 0.675959595959596 Loss: 0.01984912398713406 Corrects: 3346\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.01925959251821041\n",
            "Train step - Step 170, Loss 0.020362025126814842\n",
            "Train step - Step 180, Loss 0.018827883526682854\n",
            "Train step - Step 190, Loss 0.017912479117512703\n",
            "Train epoch - Accuracy: 0.692929292929293 Loss: 0.018757728617450203 Corrects: 3430\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.016191978007555008\n",
            "Train step - Step 210, Loss 0.020359953865408897\n",
            "Train step - Step 220, Loss 0.01648254133760929\n",
            "Train step - Step 230, Loss 0.01701902411878109\n",
            "Train epoch - Accuracy: 0.7145454545454546 Loss: 0.01770792771845755 Corrects: 3537\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.018905989825725555\n",
            "Train step - Step 250, Loss 0.01702372543513775\n",
            "Train step - Step 260, Loss 0.018722666427493095\n",
            "Train step - Step 270, Loss 0.015780383720993996\n",
            "Train epoch - Accuracy: 0.7224242424242424 Loss: 0.01706111334979233 Corrects: 3576\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.014848428778350353\n",
            "Train step - Step 290, Loss 0.016268117353320122\n",
            "Train step - Step 300, Loss 0.016319889575242996\n",
            "Train step - Step 310, Loss 0.0167363490909338\n",
            "Train epoch - Accuracy: 0.7317171717171718 Loss: 0.01655318798967684 Corrects: 3622\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.015390758402645588\n",
            "Train step - Step 330, Loss 0.017712164670228958\n",
            "Train step - Step 340, Loss 0.014063569717109203\n",
            "Train step - Step 350, Loss 0.016746897250413895\n",
            "Train epoch - Accuracy: 0.7436363636363637 Loss: 0.01589731990538462 Corrects: 3681\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.016701901331543922\n",
            "Train step - Step 370, Loss 0.014555600471794605\n",
            "Train step - Step 380, Loss 0.01618792675435543\n",
            "Train epoch - Accuracy: 0.7511111111111111 Loss: 0.015198832767253572 Corrects: 3718\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.015097987838089466\n",
            "Train step - Step 400, Loss 0.01540311612188816\n",
            "Train step - Step 410, Loss 0.014277237467467785\n",
            "Train step - Step 420, Loss 0.012889678589999676\n",
            "Train epoch - Accuracy: 0.762020202020202 Loss: 0.01483844867860428 Corrects: 3772\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.014207619242370129\n",
            "Train step - Step 440, Loss 0.014123143628239632\n",
            "Train step - Step 450, Loss 0.012150677852332592\n",
            "Train step - Step 460, Loss 0.015709055587649345\n",
            "Train epoch - Accuracy: 0.7711111111111111 Loss: 0.014445158867071373 Corrects: 3817\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.014176092110574245\n",
            "Train step - Step 480, Loss 0.016507064923644066\n",
            "Train step - Step 490, Loss 0.013345861807465553\n",
            "Train step - Step 500, Loss 0.012414216063916683\n",
            "Train epoch - Accuracy: 0.7751515151515151 Loss: 0.01404235433523703 Corrects: 3837\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.014965695329010487\n",
            "Train step - Step 520, Loss 0.0143350875005126\n",
            "Train step - Step 530, Loss 0.013254992663860321\n",
            "Train step - Step 540, Loss 0.013881226070225239\n",
            "Train epoch - Accuracy: 0.7852525252525252 Loss: 0.01376104126975994 Corrects: 3887\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.012277917936444283\n",
            "Train step - Step 560, Loss 0.015578383579850197\n",
            "Train step - Step 570, Loss 0.013346651569008827\n",
            "Train step - Step 580, Loss 0.014188572764396667\n",
            "Train epoch - Accuracy: 0.7903030303030303 Loss: 0.013352765440940858 Corrects: 3912\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.01288992166519165\n",
            "Train step - Step 600, Loss 0.01226600632071495\n",
            "Train step - Step 610, Loss 0.013553199358284473\n",
            "Train step - Step 620, Loss 0.013519403524696827\n",
            "Train epoch - Accuracy: 0.795959595959596 Loss: 0.013032643493979867 Corrects: 3940\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.011324817314743996\n",
            "Train step - Step 640, Loss 0.0165563952177763\n",
            "Train step - Step 650, Loss 0.012373062781989574\n",
            "Train step - Step 660, Loss 0.012550264596939087\n",
            "Train epoch - Accuracy: 0.8042424242424242 Loss: 0.012680152524450813 Corrects: 3981\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.011736738495528698\n",
            "Train step - Step 680, Loss 0.011401700787246227\n",
            "Train step - Step 690, Loss 0.011706686578691006\n",
            "Train step - Step 700, Loss 0.013503536581993103\n",
            "Train epoch - Accuracy: 0.8117171717171717 Loss: 0.012254887868584407 Corrects: 4018\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.010825708508491516\n",
            "Train step - Step 720, Loss 0.010524607263505459\n",
            "Train step - Step 730, Loss 0.011948495171964169\n",
            "Train step - Step 740, Loss 0.01425954606384039\n",
            "Train epoch - Accuracy: 0.8212121212121212 Loss: 0.01194611476715466 Corrects: 4065\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.013841596432030201\n",
            "Train step - Step 760, Loss 0.011023317463696003\n",
            "Train step - Step 770, Loss 0.012450489215552807\n",
            "Train epoch - Accuracy: 0.8202020202020202 Loss: 0.0116495982659134 Corrects: 4060\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.010617710649967194\n",
            "Train step - Step 790, Loss 0.012637889944016933\n",
            "Train step - Step 800, Loss 0.012174784205853939\n",
            "Train step - Step 810, Loss 0.012413769960403442\n",
            "Train epoch - Accuracy: 0.8193939393939393 Loss: 0.011590021616130165 Corrects: 4056\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.00993099994957447\n",
            "Train step - Step 830, Loss 0.008060583844780922\n",
            "Train step - Step 840, Loss 0.013133448548614979\n",
            "Train step - Step 850, Loss 0.014299944043159485\n",
            "Train epoch - Accuracy: 0.8248484848484848 Loss: 0.01139733091147259 Corrects: 4083\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.010937179438769817\n",
            "Train step - Step 870, Loss 0.009668606333434582\n",
            "Train step - Step 880, Loss 0.010251910425722599\n",
            "Train step - Step 890, Loss 0.00986567884683609\n",
            "Train epoch - Accuracy: 0.832929292929293 Loss: 0.010905524981443327 Corrects: 4123\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.011925323866307735\n",
            "Train step - Step 910, Loss 0.01104414276778698\n",
            "Train step - Step 920, Loss 0.011322352103888988\n",
            "Train step - Step 930, Loss 0.010675573721528053\n",
            "Train epoch - Accuracy: 0.8345454545454546 Loss: 0.010907630061711927 Corrects: 4131\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.010326534509658813\n",
            "Train step - Step 950, Loss 0.012904827482998371\n",
            "Train step - Step 960, Loss 0.01015561819076538\n",
            "Train step - Step 970, Loss 0.010765988379716873\n",
            "Train epoch - Accuracy: 0.8387878787878787 Loss: 0.010578844998822067 Corrects: 4152\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.011533734388649464\n",
            "Train step - Step 990, Loss 0.00829747412353754\n",
            "Train step - Step 1000, Loss 0.01089868601411581\n",
            "Train step - Step 1010, Loss 0.013513343408703804\n",
            "Train epoch - Accuracy: 0.8404040404040404 Loss: 0.010387137990181494 Corrects: 4160\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.006260988302528858\n",
            "Train step - Step 1030, Loss 0.010045492090284824\n",
            "Train step - Step 1040, Loss 0.009377642534673214\n",
            "Train step - Step 1050, Loss 0.008167237974703312\n",
            "Train epoch - Accuracy: 0.8505050505050505 Loss: 0.009894883154392844 Corrects: 4210\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.012408707290887833\n",
            "Train step - Step 1070, Loss 0.00824201013892889\n",
            "Train step - Step 1080, Loss 0.009299710392951965\n",
            "Train step - Step 1090, Loss 0.010609951801598072\n",
            "Train epoch - Accuracy: 0.8537373737373737 Loss: 0.009791438327443721 Corrects: 4226\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.009840643964707851\n",
            "Train step - Step 1110, Loss 0.00893685407936573\n",
            "Train step - Step 1120, Loss 0.007824430242180824\n",
            "Train step - Step 1130, Loss 0.01023987028747797\n",
            "Train epoch - Accuracy: 0.8513131313131314 Loss: 0.009831535049685926 Corrects: 4214\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.009345734491944313\n",
            "Train step - Step 1150, Loss 0.008815457113087177\n",
            "Train step - Step 1160, Loss 0.008515658788383007\n",
            "Train epoch - Accuracy: 0.8608080808080808 Loss: 0.009356751032173634 Corrects: 4261\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.006949249189347029\n",
            "Train step - Step 1180, Loss 0.010030812583863735\n",
            "Train step - Step 1190, Loss 0.008211592212319374\n",
            "Train step - Step 1200, Loss 0.011893908493220806\n",
            "Train epoch - Accuracy: 0.8567676767676767 Loss: 0.009467690468511798 Corrects: 4241\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.009611445479094982\n",
            "Train step - Step 1220, Loss 0.008321569301187992\n",
            "Train step - Step 1230, Loss 0.010621623136103153\n",
            "Train step - Step 1240, Loss 0.00849101971834898\n",
            "Train epoch - Accuracy: 0.8557575757575757 Loss: 0.00938058681710802 Corrects: 4236\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.010837950743734837\n",
            "Train step - Step 1260, Loss 0.010891246609389782\n",
            "Train step - Step 1270, Loss 0.005400634370744228\n",
            "Train step - Step 1280, Loss 0.009521874599158764\n",
            "Train epoch - Accuracy: 0.863030303030303 Loss: 0.009077659023154263 Corrects: 4272\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.006360339466482401\n",
            "Train step - Step 1300, Loss 0.007486185524612665\n",
            "Train step - Step 1310, Loss 0.00982487853616476\n",
            "Train step - Step 1320, Loss 0.009768830612301826\n",
            "Train epoch - Accuracy: 0.8727272727272727 Loss: 0.008693120158982999 Corrects: 4320\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.00616909796372056\n",
            "Train step - Step 1340, Loss 0.0068116458132863045\n",
            "Train step - Step 1350, Loss 0.010492321103811264\n",
            "Train step - Step 1360, Loss 0.008565882220864296\n",
            "Train epoch - Accuracy: 0.8727272727272727 Loss: 0.008609939583699511 Corrects: 4320\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.008870272897183895\n",
            "Train step - Step 1380, Loss 0.007757175713777542\n",
            "Train step - Step 1390, Loss 0.008392440155148506\n",
            "Train step - Step 1400, Loss 0.010281044989824295\n",
            "Train epoch - Accuracy: 0.8759595959595959 Loss: 0.008573947294492915 Corrects: 4336\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.006014796439558268\n",
            "Train step - Step 1420, Loss 0.005538922734558582\n",
            "Train step - Step 1430, Loss 0.009784610942006111\n",
            "Train step - Step 1440, Loss 0.007794139441102743\n",
            "Train epoch - Accuracy: 0.8777777777777778 Loss: 0.00831279498496742 Corrects: 4345\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.008613241836428642\n",
            "Train step - Step 1460, Loss 0.005830723326653242\n",
            "Train step - Step 1470, Loss 0.009830890223383904\n",
            "Train step - Step 1480, Loss 0.008171197026968002\n",
            "Train epoch - Accuracy: 0.8759595959595959 Loss: 0.008129828406704797 Corrects: 4336\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.008305223658680916\n",
            "Train step - Step 1500, Loss 0.010948165319859982\n",
            "Train step - Step 1510, Loss 0.00904283206909895\n",
            "Train step - Step 1520, Loss 0.005081778857856989\n",
            "Train epoch - Accuracy: 0.8816161616161616 Loss: 0.008159168931382774 Corrects: 4364\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.007687011267989874\n",
            "Train step - Step 1540, Loss 0.007454045116901398\n",
            "Train step - Step 1550, Loss 0.007836698554456234\n",
            "Train epoch - Accuracy: 0.8822222222222222 Loss: 0.008000741423944933 Corrects: 4367\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.006311470177024603\n",
            "Train step - Step 1570, Loss 0.007119731046259403\n",
            "Train step - Step 1580, Loss 0.009354243986308575\n",
            "Train step - Step 1590, Loss 0.007452857680618763\n",
            "Train epoch - Accuracy: 0.8852525252525253 Loss: 0.00786704663112007 Corrects: 4382\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.0071769775822758675\n",
            "Train step - Step 1610, Loss 0.007288965862244368\n",
            "Train step - Step 1620, Loss 0.006611769087612629\n",
            "Train step - Step 1630, Loss 0.008237308822572231\n",
            "Train epoch - Accuracy: 0.8826262626262626 Loss: 0.007848558331077749 Corrects: 4369\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.009747181087732315\n",
            "Train step - Step 1650, Loss 0.0074061318300664425\n",
            "Train step - Step 1660, Loss 0.007852069102227688\n",
            "Train step - Step 1670, Loss 0.006147785112261772\n",
            "Train epoch - Accuracy: 0.8874747474747475 Loss: 0.007539797222825012 Corrects: 4393\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.010031393729150295\n",
            "Train step - Step 1690, Loss 0.006659732665866613\n",
            "Train step - Step 1700, Loss 0.00704271299764514\n",
            "Train step - Step 1710, Loss 0.006926531437784433\n",
            "Train epoch - Accuracy: 0.8921212121212121 Loss: 0.00749541459548654 Corrects: 4416\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.005465400870889425\n",
            "Train step - Step 1730, Loss 0.00846041925251484\n",
            "Train step - Step 1740, Loss 0.005885863211005926\n",
            "Train step - Step 1750, Loss 0.007389497943222523\n",
            "Train epoch - Accuracy: 0.901010101010101 Loss: 0.007212579553627005 Corrects: 4460\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.0098424619063735\n",
            "Train step - Step 1770, Loss 0.008777412585914135\n",
            "Train step - Step 1780, Loss 0.006324338261038065\n",
            "Train step - Step 1790, Loss 0.0054058292880654335\n",
            "Train epoch - Accuracy: 0.8913131313131313 Loss: 0.007186653247347685 Corrects: 4412\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.007180510554462671\n",
            "Train step - Step 1810, Loss 0.005748494062572718\n",
            "Train step - Step 1820, Loss 0.006077159196138382\n",
            "Train step - Step 1830, Loss 0.008689092472195625\n",
            "Train epoch - Accuracy: 0.9 Loss: 0.006908322351824756 Corrects: 4455\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.006645694375038147\n",
            "Train step - Step 1850, Loss 0.005843642633408308\n",
            "Train step - Step 1860, Loss 0.007570108864456415\n",
            "Train step - Step 1870, Loss 0.007805895060300827\n",
            "Train epoch - Accuracy: 0.8981818181818182 Loss: 0.007102848049441371 Corrects: 4446\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.007072811014950275\n",
            "Train step - Step 1890, Loss 0.006262715440243483\n",
            "Train step - Step 1900, Loss 0.005084572825580835\n",
            "Train step - Step 1910, Loss 0.006996620446443558\n",
            "Train epoch - Accuracy: 0.9002020202020202 Loss: 0.006845166043500708 Corrects: 4456\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.007680719252675772\n",
            "Train step - Step 1930, Loss 0.00540152657777071\n",
            "Train step - Step 1940, Loss 0.006111838389188051\n",
            "Train epoch - Accuracy: 0.9078787878787878 Loss: 0.006403638478368521 Corrects: 4494\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.0070473672822117805\n",
            "Train step - Step 1960, Loss 0.004946622531861067\n",
            "Train step - Step 1970, Loss 0.005524975713342428\n",
            "Train step - Step 1980, Loss 0.005975102074444294\n",
            "Train epoch - Accuracy: 0.9177777777777778 Loss: 0.005938060152334998 Corrects: 4543\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.004990852903574705\n",
            "Train step - Step 2000, Loss 0.006629329174757004\n",
            "Train step - Step 2010, Loss 0.005415137391537428\n",
            "Train step - Step 2020, Loss 0.005025432910770178\n",
            "Train epoch - Accuracy: 0.9197979797979798 Loss: 0.005971817647417386 Corrects: 4553\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.007620935793966055\n",
            "Train step - Step 2040, Loss 0.007187384646385908\n",
            "Train step - Step 2050, Loss 0.004980430006980896\n",
            "Train step - Step 2060, Loss 0.006861725356429815\n",
            "Train epoch - Accuracy: 0.9210101010101011 Loss: 0.005798965857336015 Corrects: 4559\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.0050409696996212006\n",
            "Train step - Step 2080, Loss 0.006134627852588892\n",
            "Train step - Step 2090, Loss 0.006147214211523533\n",
            "Train step - Step 2100, Loss 0.0046642073430120945\n",
            "Train epoch - Accuracy: 0.9248484848484848 Loss: 0.005695307488656706 Corrects: 4578\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.005843368358910084\n",
            "Train step - Step 2120, Loss 0.006905661430209875\n",
            "Train step - Step 2130, Loss 0.0068956720642745495\n",
            "Train step - Step 2140, Loss 0.006552636157721281\n",
            "Train epoch - Accuracy: 0.9208080808080809 Loss: 0.005939426449588453 Corrects: 4558\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.009322774596512318\n",
            "Train step - Step 2160, Loss 0.005050046369433403\n",
            "Train step - Step 2170, Loss 0.005780477542430162\n",
            "Train step - Step 2180, Loss 0.005367015488445759\n",
            "Train epoch - Accuracy: 0.9234343434343434 Loss: 0.005773584542589055 Corrects: 4571\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.005771947093307972\n",
            "Train step - Step 2200, Loss 0.0061673871241509914\n",
            "Train step - Step 2210, Loss 0.0074424417689442635\n",
            "Train step - Step 2220, Loss 0.006605581846088171\n",
            "Train epoch - Accuracy: 0.9238383838383838 Loss: 0.0056682515808503435 Corrects: 4573\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.004798689857125282\n",
            "Train step - Step 2240, Loss 0.004287208896130323\n",
            "Train step - Step 2250, Loss 0.00567166181281209\n",
            "Train step - Step 2260, Loss 0.0046309600584208965\n",
            "Train epoch - Accuracy: 0.9248484848484848 Loss: 0.005808970392728695 Corrects: 4578\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.004777695517987013\n",
            "Train step - Step 2280, Loss 0.006860229652374983\n",
            "Train step - Step 2290, Loss 0.006455232854932547\n",
            "Train step - Step 2300, Loss 0.004506138619035482\n",
            "Train epoch - Accuracy: 0.9222222222222223 Loss: 0.005715698113834316 Corrects: 4565\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.005361267365515232\n",
            "Train step - Step 2320, Loss 0.006761276628822088\n",
            "Train step - Step 2330, Loss 0.0048551540821790695\n",
            "Train epoch - Accuracy: 0.9238383838383838 Loss: 0.00559636631528988 Corrects: 4573\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.005704327952116728\n",
            "Train step - Step 2350, Loss 0.006551672704517841\n",
            "Train step - Step 2360, Loss 0.005791669245809317\n",
            "Train step - Step 2370, Loss 0.004496175795793533\n",
            "Train epoch - Accuracy: 0.9226262626262626 Loss: 0.00576788542096061 Corrects: 4567\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.0049733445048332214\n",
            "Train step - Step 2390, Loss 0.00468825176358223\n",
            "Train step - Step 2400, Loss 0.005362676922231913\n",
            "Train step - Step 2410, Loss 0.005242803134024143\n",
            "Train epoch - Accuracy: 0.925050505050505 Loss: 0.005641414324630691 Corrects: 4579\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.0066387467086315155\n",
            "Train step - Step 2430, Loss 0.0042144483886659145\n",
            "Train step - Step 2440, Loss 0.004202804993838072\n",
            "Train step - Step 2450, Loss 0.006109962705522776\n",
            "Train epoch - Accuracy: 0.9246464646464646 Loss: 0.0055764257153402075 Corrects: 4577\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.005113409366458654\n",
            "Train step - Step 2470, Loss 0.005387549754232168\n",
            "Train step - Step 2480, Loss 0.005569183733314276\n",
            "Train step - Step 2490, Loss 0.005475577898323536\n",
            "Train epoch - Accuracy: 0.9278787878787879 Loss: 0.005481234220881956 Corrects: 4593\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.004871433135122061\n",
            "Train step - Step 2510, Loss 0.004572515841573477\n",
            "Train step - Step 2520, Loss 0.005295685958117247\n",
            "Train step - Step 2530, Loss 0.004593224730342627\n",
            "Train epoch - Accuracy: 0.9278787878787879 Loss: 0.005372160740968104 Corrects: 4593\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.006027620285749435\n",
            "Train step - Step 2550, Loss 0.004609689116477966\n",
            "Train step - Step 2560, Loss 0.0047343457117676735\n",
            "Train step - Step 2570, Loss 0.005042920354753733\n",
            "Train epoch - Accuracy: 0.9216161616161617 Loss: 0.0055341161452610085 Corrects: 4562\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.004795074462890625\n",
            "Train step - Step 2590, Loss 0.005856806878000498\n",
            "Train step - Step 2600, Loss 0.004130444955080748\n",
            "Train step - Step 2610, Loss 0.008634793572127819\n",
            "Train epoch - Accuracy: 0.9280808080808081 Loss: 0.005498298575932329 Corrects: 4594\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.005579461809247732\n",
            "Train step - Step 2630, Loss 0.004112716298550367\n",
            "Train step - Step 2640, Loss 0.004433218855410814\n",
            "Train step - Step 2650, Loss 0.006831268314272165\n",
            "Train epoch - Accuracy: 0.9288888888888889 Loss: 0.005429158335495176 Corrects: 4598\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.00573822483420372\n",
            "Train step - Step 2670, Loss 0.006031181663274765\n",
            "Train step - Step 2680, Loss 0.006212508771568537\n",
            "Train step - Step 2690, Loss 0.003974271006882191\n",
            "Train epoch - Accuracy: 0.9258585858585858 Loss: 0.005530874578263422 Corrects: 4583\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.005851526744663715\n",
            "Train step - Step 2710, Loss 0.006209793500602245\n",
            "Train step - Step 2720, Loss 0.005026474595069885\n",
            "Train epoch - Accuracy: 0.9319191919191919 Loss: 0.005378803462166377 Corrects: 4613\n",
            "Training finished in 212.28017330169678 seconds\n",
            "EVALUATION:  0.9 0.008731821551918983\n",
            "TEST GROUP:  0.783\n",
            "TEST ALL:  0.097875\n",
            "GROUP:  9\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.11222080886363983\n",
            "Train step - Step 10, Loss 0.042865537106990814\n",
            "Train step - Step 20, Loss 0.03541687875986099\n",
            "Train step - Step 30, Loss 0.02883632853627205\n",
            "Train epoch - Accuracy: 0.27575757575757576 Loss: 0.041732885944421844 Corrects: 1365\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.025477547198534012\n",
            "Train step - Step 50, Loss 0.02179754339158535\n",
            "Train step - Step 60, Loss 0.021684307605028152\n",
            "Train step - Step 70, Loss 0.02075846865773201\n",
            "Train epoch - Accuracy: 0.5905050505050505 Loss: 0.022179453737958512 Corrects: 2923\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.019573790952563286\n",
            "Train step - Step 90, Loss 0.017647258937358856\n",
            "Train step - Step 100, Loss 0.018210232257843018\n",
            "Train step - Step 110, Loss 0.017587240785360336\n",
            "Train epoch - Accuracy: 0.6612121212121213 Loss: 0.01851311245199406 Corrects: 3273\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.014891683124005795\n",
            "Train step - Step 130, Loss 0.01488387119024992\n",
            "Train step - Step 140, Loss 0.015692874789237976\n",
            "Train step - Step 150, Loss 0.015322395600378513\n",
            "Train epoch - Accuracy: 0.6989898989898989 Loss: 0.0164556577914592 Corrects: 3460\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.013974539935588837\n",
            "Train step - Step 170, Loss 0.014755851589143276\n",
            "Train step - Step 180, Loss 0.013581967912614346\n",
            "Train step - Step 190, Loss 0.014529453590512276\n",
            "Train epoch - Accuracy: 0.7365656565656565 Loss: 0.01504889439227003 Corrects: 3646\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.014737105928361416\n",
            "Train step - Step 210, Loss 0.014879792928695679\n",
            "Train step - Step 220, Loss 0.012208109721541405\n",
            "Train step - Step 230, Loss 0.014743401668965816\n",
            "Train epoch - Accuracy: 0.743030303030303 Loss: 0.014320193264595788 Corrects: 3678\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.015546233393251896\n",
            "Train step - Step 250, Loss 0.014272277243435383\n",
            "Train step - Step 260, Loss 0.01270483061671257\n",
            "Train step - Step 270, Loss 0.013738272711634636\n",
            "Train epoch - Accuracy: 0.7587878787878788 Loss: 0.013506521586742665 Corrects: 3756\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.014644909650087357\n",
            "Train step - Step 290, Loss 0.013157705776393414\n",
            "Train step - Step 300, Loss 0.015023894608020782\n",
            "Train step - Step 310, Loss 0.011463126167654991\n",
            "Train epoch - Accuracy: 0.776969696969697 Loss: 0.012870974066958885 Corrects: 3846\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.0120912566781044\n",
            "Train step - Step 330, Loss 0.00986306369304657\n",
            "Train step - Step 340, Loss 0.012256747111678123\n",
            "Train step - Step 350, Loss 0.0133333345875144\n",
            "Train epoch - Accuracy: 0.7890909090909091 Loss: 0.012188220917049682 Corrects: 3906\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.012837806716561317\n",
            "Train step - Step 370, Loss 0.01246688887476921\n",
            "Train step - Step 380, Loss 0.01296737976372242\n",
            "Train epoch - Accuracy: 0.788080808080808 Loss: 0.011961292631246827 Corrects: 3901\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.012434578500688076\n",
            "Train step - Step 400, Loss 0.011820604093372822\n",
            "Train step - Step 410, Loss 0.009661253541707993\n",
            "Train step - Step 420, Loss 0.01192181371152401\n",
            "Train epoch - Accuracy: 0.7975757575757576 Loss: 0.011346478468239909 Corrects: 3948\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.0093653229996562\n",
            "Train step - Step 440, Loss 0.01093206275254488\n",
            "Train step - Step 450, Loss 0.009947205893695354\n",
            "Train step - Step 460, Loss 0.011092958971858025\n",
            "Train epoch - Accuracy: 0.8117171717171717 Loss: 0.011036811483779338 Corrects: 4018\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.009343266487121582\n",
            "Train step - Step 480, Loss 0.010513774119317532\n",
            "Train step - Step 490, Loss 0.011019091121852398\n",
            "Train step - Step 500, Loss 0.012788389809429646\n",
            "Train epoch - Accuracy: 0.812929292929293 Loss: 0.010546289043820867 Corrects: 4024\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.011242774315178394\n",
            "Train step - Step 520, Loss 0.011762706562876701\n",
            "Train step - Step 530, Loss 0.0083710802718997\n",
            "Train step - Step 540, Loss 0.010143332183361053\n",
            "Train epoch - Accuracy: 0.8246464646464646 Loss: 0.01016822409336314 Corrects: 4082\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.010653715580701828\n",
            "Train step - Step 560, Loss 0.008465060964226723\n",
            "Train step - Step 570, Loss 0.00991352740675211\n",
            "Train step - Step 580, Loss 0.010034594684839249\n",
            "Train epoch - Accuracy: 0.8195959595959595 Loss: 0.010087450654669242 Corrects: 4057\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.010599901899695396\n",
            "Train step - Step 600, Loss 0.01053185947239399\n",
            "Train step - Step 610, Loss 0.009601191617548466\n",
            "Train step - Step 620, Loss 0.010184109210968018\n",
            "Train epoch - Accuracy: 0.8361616161616162 Loss: 0.009566735077762243 Corrects: 4139\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.0077577936463057995\n",
            "Train step - Step 640, Loss 0.009160797111690044\n",
            "Train step - Step 650, Loss 0.009190456010401249\n",
            "Train step - Step 660, Loss 0.010723928920924664\n",
            "Train epoch - Accuracy: 0.8377777777777777 Loss: 0.009295037467822884 Corrects: 4147\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.007292558439075947\n",
            "Train step - Step 680, Loss 0.009745842777192593\n",
            "Train step - Step 690, Loss 0.01077164988964796\n",
            "Train step - Step 700, Loss 0.00868807453662157\n",
            "Train epoch - Accuracy: 0.8414141414141414 Loss: 0.00908541041003032 Corrects: 4165\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.00749710900709033\n",
            "Train step - Step 720, Loss 0.008142776787281036\n",
            "Train step - Step 730, Loss 0.011072018183767796\n",
            "Train step - Step 740, Loss 0.008582224138081074\n",
            "Train epoch - Accuracy: 0.8466666666666667 Loss: 0.008931820178498523 Corrects: 4191\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.00774642126634717\n",
            "Train step - Step 760, Loss 0.01092991977930069\n",
            "Train step - Step 770, Loss 0.011238212697207928\n",
            "Train epoch - Accuracy: 0.8490909090909091 Loss: 0.008756390438612663 Corrects: 4203\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.007926128804683685\n",
            "Train step - Step 790, Loss 0.009428945370018482\n",
            "Train step - Step 800, Loss 0.009505362249910831\n",
            "Train step - Step 810, Loss 0.009270673617720604\n",
            "Train epoch - Accuracy: 0.8533333333333334 Loss: 0.008423595646339835 Corrects: 4224\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.010944257490336895\n",
            "Train step - Step 830, Loss 0.008500675670802593\n",
            "Train step - Step 840, Loss 0.007488583214581013\n",
            "Train step - Step 850, Loss 0.009201651439070702\n",
            "Train epoch - Accuracy: 0.8597979797979798 Loss: 0.008311899815770712 Corrects: 4256\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.006756998598575592\n",
            "Train step - Step 870, Loss 0.007229424547404051\n",
            "Train step - Step 880, Loss 0.00757251214236021\n",
            "Train step - Step 890, Loss 0.0089802797883749\n",
            "Train epoch - Accuracy: 0.8597979797979798 Loss: 0.008144574712849025 Corrects: 4256\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.007984674535691738\n",
            "Train step - Step 910, Loss 0.006731772795319557\n",
            "Train step - Step 920, Loss 0.008816483430564404\n",
            "Train step - Step 930, Loss 0.008472359739243984\n",
            "Train epoch - Accuracy: 0.8648484848484849 Loss: 0.007910763676645179 Corrects: 4281\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.009238135069608688\n",
            "Train step - Step 950, Loss 0.005166064016520977\n",
            "Train step - Step 960, Loss 0.006649032700806856\n",
            "Train step - Step 970, Loss 0.00684393709525466\n",
            "Train epoch - Accuracy: 0.8696969696969697 Loss: 0.007664239358044032 Corrects: 4305\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.006326988339424133\n",
            "Train step - Step 990, Loss 0.006414869800209999\n",
            "Train step - Step 1000, Loss 0.008338699117302895\n",
            "Train step - Step 1010, Loss 0.006466102786362171\n",
            "Train epoch - Accuracy: 0.8785858585858586 Loss: 0.007530868322861315 Corrects: 4349\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.00877698790282011\n",
            "Train step - Step 1030, Loss 0.007571483496576548\n",
            "Train step - Step 1040, Loss 0.009059472009539604\n",
            "Train step - Step 1050, Loss 0.00774298794567585\n",
            "Train epoch - Accuracy: 0.8696969696969697 Loss: 0.007594422523609617 Corrects: 4305\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.007351269945502281\n",
            "Train step - Step 1070, Loss 0.005688989069312811\n",
            "Train step - Step 1080, Loss 0.005793978460133076\n",
            "Train step - Step 1090, Loss 0.00891518872231245\n",
            "Train epoch - Accuracy: 0.8753535353535353 Loss: 0.007368033434631246 Corrects: 4333\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.006323348265141249\n",
            "Train step - Step 1110, Loss 0.006110569462180138\n",
            "Train step - Step 1120, Loss 0.0064368764869868755\n",
            "Train step - Step 1130, Loss 0.0069793397560715675\n",
            "Train epoch - Accuracy: 0.8812121212121212 Loss: 0.007211020276928791 Corrects: 4362\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.006861938629299402\n",
            "Train step - Step 1150, Loss 0.007834908552467823\n",
            "Train step - Step 1160, Loss 0.007053722627460957\n",
            "Train epoch - Accuracy: 0.8836363636363637 Loss: 0.007018199390684716 Corrects: 4374\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.005518702324479818\n",
            "Train step - Step 1180, Loss 0.006062577944248915\n",
            "Train step - Step 1190, Loss 0.005677042528986931\n",
            "Train step - Step 1200, Loss 0.006800854578614235\n",
            "Train epoch - Accuracy: 0.8905050505050505 Loss: 0.006720063979690424 Corrects: 4408\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.004951694514602423\n",
            "Train step - Step 1220, Loss 0.005539977457374334\n",
            "Train step - Step 1230, Loss 0.006057159975171089\n",
            "Train step - Step 1240, Loss 0.004284949973225594\n",
            "Train epoch - Accuracy: 0.8907070707070707 Loss: 0.006609413586556911 Corrects: 4409\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.00481247715651989\n",
            "Train step - Step 1260, Loss 0.0062822336331009865\n",
            "Train step - Step 1270, Loss 0.006720575038343668\n",
            "Train step - Step 1280, Loss 0.00760744558647275\n",
            "Train epoch - Accuracy: 0.888080808080808 Loss: 0.006627242491589953 Corrects: 4396\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.005987092852592468\n",
            "Train step - Step 1300, Loss 0.00559809897094965\n",
            "Train step - Step 1310, Loss 0.008577324450016022\n",
            "Train step - Step 1320, Loss 0.007340374868363142\n",
            "Train epoch - Accuracy: 0.8955555555555555 Loss: 0.006488010123974145 Corrects: 4433\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.004707458429038525\n",
            "Train step - Step 1340, Loss 0.006939457729458809\n",
            "Train step - Step 1350, Loss 0.00784515030682087\n",
            "Train step - Step 1360, Loss 0.006191125139594078\n",
            "Train epoch - Accuracy: 0.8949494949494949 Loss: 0.006347660173031718 Corrects: 4430\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.0052030994556844234\n",
            "Train step - Step 1380, Loss 0.004804880823940039\n",
            "Train step - Step 1390, Loss 0.00588421244174242\n",
            "Train step - Step 1400, Loss 0.005020276643335819\n",
            "Train epoch - Accuracy: 0.9030303030303031 Loss: 0.00604027028261411 Corrects: 4470\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.008622759021818638\n",
            "Train step - Step 1420, Loss 0.004213086795061827\n",
            "Train step - Step 1430, Loss 0.005942902993410826\n",
            "Train step - Step 1440, Loss 0.007268120534718037\n",
            "Train epoch - Accuracy: 0.8993939393939394 Loss: 0.006002487853884396 Corrects: 4452\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.007540928199887276\n",
            "Train step - Step 1460, Loss 0.006639543455094099\n",
            "Train step - Step 1470, Loss 0.005335091147571802\n",
            "Train step - Step 1480, Loss 0.007163927424699068\n",
            "Train epoch - Accuracy: 0.9042424242424243 Loss: 0.005966347355696589 Corrects: 4476\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.005534587427973747\n",
            "Train step - Step 1500, Loss 0.007856203243136406\n",
            "Train step - Step 1510, Loss 0.004907812457531691\n",
            "Train step - Step 1520, Loss 0.005941907409578562\n",
            "Train epoch - Accuracy: 0.9086868686868687 Loss: 0.0057728745613360045 Corrects: 4498\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.0055126831866800785\n",
            "Train step - Step 1540, Loss 0.004091888666152954\n",
            "Train step - Step 1550, Loss 0.0053421808406710625\n",
            "Train epoch - Accuracy: 0.9171717171717172 Loss: 0.005434269383925982 Corrects: 4540\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.0071902647614479065\n",
            "Train step - Step 1570, Loss 0.005292301997542381\n",
            "Train step - Step 1580, Loss 0.005428882781416178\n",
            "Train step - Step 1590, Loss 0.005834516137838364\n",
            "Train epoch - Accuracy: 0.9153535353535354 Loss: 0.005436968648185333 Corrects: 4531\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.00555938296020031\n",
            "Train step - Step 1610, Loss 0.005274712573736906\n",
            "Train step - Step 1620, Loss 0.004347146488726139\n",
            "Train step - Step 1630, Loss 0.004147061612457037\n",
            "Train epoch - Accuracy: 0.9127272727272727 Loss: 0.0054445748535370585 Corrects: 4518\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.006953750737011433\n",
            "Train step - Step 1650, Loss 0.004426016006618738\n",
            "Train step - Step 1660, Loss 0.0064092460088431835\n",
            "Train step - Step 1670, Loss 0.005874338559806347\n",
            "Train epoch - Accuracy: 0.9082828282828282 Loss: 0.005525951507981077 Corrects: 4496\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.005267367698252201\n",
            "Train step - Step 1690, Loss 0.005250831134617329\n",
            "Train step - Step 1700, Loss 0.00488406652584672\n",
            "Train step - Step 1710, Loss 0.006695161573588848\n",
            "Train epoch - Accuracy: 0.924040404040404 Loss: 0.004908336066556248 Corrects: 4574\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.004131476394832134\n",
            "Train step - Step 1730, Loss 0.006068161688745022\n",
            "Train step - Step 1740, Loss 0.004808853846043348\n",
            "Train step - Step 1750, Loss 0.004061736632138491\n",
            "Train epoch - Accuracy: 0.9187878787878788 Loss: 0.005135166969978147 Corrects: 4548\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.007213766220957041\n",
            "Train step - Step 1770, Loss 0.004644838627427816\n",
            "Train step - Step 1780, Loss 0.004563446156680584\n",
            "Train step - Step 1790, Loss 0.004020251799374819\n",
            "Train epoch - Accuracy: 0.9236363636363636 Loss: 0.004942754376867805 Corrects: 4572\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.004255850333720446\n",
            "Train step - Step 1810, Loss 0.004979016724973917\n",
            "Train step - Step 1820, Loss 0.004468664992600679\n",
            "Train step - Step 1830, Loss 0.005474633537232876\n",
            "Train epoch - Accuracy: 0.9193939393939394 Loss: 0.004857122511838121 Corrects: 4551\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.004380500875413418\n",
            "Train step - Step 1850, Loss 0.005186040885746479\n",
            "Train step - Step 1860, Loss 0.0041342927142977715\n",
            "Train step - Step 1870, Loss 0.006618259474635124\n",
            "Train epoch - Accuracy: 0.926060606060606 Loss: 0.004753619187389209 Corrects: 4584\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.005048980470746756\n",
            "Train step - Step 1890, Loss 0.005437202285975218\n",
            "Train step - Step 1900, Loss 0.0038443838711827993\n",
            "Train step - Step 1910, Loss 0.00517078535631299\n",
            "Train epoch - Accuracy: 0.9298989898989899 Loss: 0.004598808070889326 Corrects: 4603\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.004986937623471022\n",
            "Train step - Step 1930, Loss 0.004200502298772335\n",
            "Train step - Step 1940, Loss 0.003917894326150417\n",
            "Train epoch - Accuracy: 0.9355555555555556 Loss: 0.004482251433937838 Corrects: 4631\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.0028524836525321007\n",
            "Train step - Step 1960, Loss 0.0037062440533190966\n",
            "Train step - Step 1970, Loss 0.00575602613389492\n",
            "Train step - Step 1980, Loss 0.004416139330714941\n",
            "Train epoch - Accuracy: 0.9418181818181818 Loss: 0.004017775187438184 Corrects: 4662\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.003477205988019705\n",
            "Train step - Step 2000, Loss 0.003358082612976432\n",
            "Train step - Step 2010, Loss 0.0034589231945574284\n",
            "Train step - Step 2020, Loss 0.0028611693996936083\n",
            "Train epoch - Accuracy: 0.9397979797979797 Loss: 0.00420601426256877 Corrects: 4652\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.0029675974510610104\n",
            "Train step - Step 2040, Loss 0.004008803982287645\n",
            "Train step - Step 2050, Loss 0.0030760089866816998\n",
            "Train step - Step 2060, Loss 0.004994133021682501\n",
            "Train epoch - Accuracy: 0.9442424242424242 Loss: 0.003877930797899913 Corrects: 4674\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.0034002582542598248\n",
            "Train step - Step 2080, Loss 0.0048079220578074455\n",
            "Train step - Step 2090, Loss 0.00551120238378644\n",
            "Train step - Step 2100, Loss 0.0034257161896675825\n",
            "Train epoch - Accuracy: 0.9424242424242424 Loss: 0.0040234866711978965 Corrects: 4665\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.0034672687761485577\n",
            "Train step - Step 2120, Loss 0.003316098591312766\n",
            "Train step - Step 2130, Loss 0.004115457646548748\n",
            "Train step - Step 2140, Loss 0.0034219210501760244\n",
            "Train epoch - Accuracy: 0.9462626262626262 Loss: 0.0038246931044403653 Corrects: 4684\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.004227486439049244\n",
            "Train step - Step 2160, Loss 0.004641773644834757\n",
            "Train step - Step 2170, Loss 0.004483948461711407\n",
            "Train step - Step 2180, Loss 0.005322248674929142\n",
            "Train epoch - Accuracy: 0.9474747474747475 Loss: 0.003924153172363988 Corrects: 4690\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.005108596757054329\n",
            "Train step - Step 2200, Loss 0.00529147544875741\n",
            "Train step - Step 2210, Loss 0.004852845333516598\n",
            "Train step - Step 2220, Loss 0.003078463487327099\n",
            "Train epoch - Accuracy: 0.9488888888888889 Loss: 0.003920821124556089 Corrects: 4697\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.0032724328339099884\n",
            "Train step - Step 2240, Loss 0.002992109628394246\n",
            "Train step - Step 2250, Loss 0.003282492747530341\n",
            "Train step - Step 2260, Loss 0.004333512857556343\n",
            "Train epoch - Accuracy: 0.945050505050505 Loss: 0.0037818537400376917 Corrects: 4678\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.0041059148497879505\n",
            "Train step - Step 2280, Loss 0.0027752730529755354\n",
            "Train step - Step 2290, Loss 0.003060359274968505\n",
            "Train step - Step 2300, Loss 0.003447094699367881\n",
            "Train epoch - Accuracy: 0.9490909090909091 Loss: 0.003672179924136009 Corrects: 4698\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.0031093100551515818\n",
            "Train step - Step 2320, Loss 0.003911228850483894\n",
            "Train step - Step 2330, Loss 0.0034157331101596355\n",
            "Train epoch - Accuracy: 0.9486868686868687 Loss: 0.0037227502632698026 Corrects: 4696\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.0038675940595567226\n",
            "Train step - Step 2350, Loss 0.004343433305621147\n",
            "Train step - Step 2360, Loss 0.0034196185879409313\n",
            "Train step - Step 2370, Loss 0.0035610939376056194\n",
            "Train epoch - Accuracy: 0.9456565656565656 Loss: 0.0038720707922722353 Corrects: 4681\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.0028944145888090134\n",
            "Train step - Step 2390, Loss 0.0032693990506231785\n",
            "Train step - Step 2400, Loss 0.0037942894268780947\n",
            "Train step - Step 2410, Loss 0.002791103208437562\n",
            "Train epoch - Accuracy: 0.9484848484848485 Loss: 0.0036232921659871183 Corrects: 4695\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.0026374582666903734\n",
            "Train step - Step 2430, Loss 0.0028238908853381872\n",
            "Train step - Step 2440, Loss 0.003964362200349569\n",
            "Train step - Step 2450, Loss 0.004567089956253767\n",
            "Train epoch - Accuracy: 0.9470707070707071 Loss: 0.0036868253886473902 Corrects: 4688\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.0038058848585933447\n",
            "Train step - Step 2470, Loss 0.003616736503317952\n",
            "Train step - Step 2480, Loss 0.004389220383018255\n",
            "Train step - Step 2490, Loss 0.003754268167540431\n",
            "Train epoch - Accuracy: 0.9503030303030303 Loss: 0.0036595256248432577 Corrects: 4704\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.00404273672029376\n",
            "Train step - Step 2510, Loss 0.00360566726885736\n",
            "Train step - Step 2520, Loss 0.004305099602788687\n",
            "Train step - Step 2530, Loss 0.0026557811070233583\n",
            "Train epoch - Accuracy: 0.9555555555555556 Loss: 0.003372095427240687 Corrects: 4730\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.0025953177828341722\n",
            "Train step - Step 2550, Loss 0.0026817272882908583\n",
            "Train step - Step 2560, Loss 0.003274648217484355\n",
            "Train step - Step 2570, Loss 0.003894492518156767\n",
            "Train epoch - Accuracy: 0.9527272727272728 Loss: 0.0036506980424276508 Corrects: 4716\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.0026180667337030172\n",
            "Train step - Step 2590, Loss 0.0029458156786859035\n",
            "Train step - Step 2600, Loss 0.0022775703109800816\n",
            "Train step - Step 2610, Loss 0.003950104117393494\n",
            "Train epoch - Accuracy: 0.9507070707070707 Loss: 0.003634167853366546 Corrects: 4706\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.003368217498064041\n",
            "Train step - Step 2630, Loss 0.00507136806845665\n",
            "Train step - Step 2640, Loss 0.003452902426943183\n",
            "Train step - Step 2650, Loss 0.003451819298788905\n",
            "Train epoch - Accuracy: 0.945050505050505 Loss: 0.0037789698362802014 Corrects: 4678\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.003659416688606143\n",
            "Train step - Step 2670, Loss 0.002952708164229989\n",
            "Train step - Step 2680, Loss 0.004333981778472662\n",
            "Train step - Step 2690, Loss 0.0022398647852241993\n",
            "Train epoch - Accuracy: 0.9525252525252526 Loss: 0.003528521034415021 Corrects: 4715\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.004280120134353638\n",
            "Train step - Step 2710, Loss 0.00301473680883646\n",
            "Train step - Step 2720, Loss 0.003233964554965496\n",
            "Train epoch - Accuracy: 0.9531313131313132 Loss: 0.003622811957253049 Corrects: 4718\n",
            "Training finished in 212.81819653511047 seconds\n",
            "EVALUATION:  0.86 0.008454719558358192\n",
            "TEST GROUP:  0.838\n",
            "TEST ALL:  0.09311111111111112\n",
            "GROUP:  10\n",
            "Starting epoch 1/70, LR = [0.1]\n",
            "Train step - Step 0, Loss 0.11129335314035416\n",
            "Train step - Step 10, Loss 0.03999054431915283\n",
            "Train step - Step 20, Loss 0.03532654419541359\n",
            "Train step - Step 30, Loss 0.022739488631486893\n",
            "Train epoch - Accuracy: 0.2703030303030303 Loss: 0.039027153325050766 Corrects: 1338\n",
            "Starting epoch 2/70, LR = [0.1]\n",
            "Train step - Step 40, Loss 0.02494090236723423\n",
            "Train step - Step 50, Loss 0.02172539383172989\n",
            "Train step - Step 60, Loss 0.019833972677588463\n",
            "Train step - Step 70, Loss 0.02162400260567665\n",
            "Train epoch - Accuracy: 0.5444444444444444 Loss: 0.021343925912873915 Corrects: 2695\n",
            "Starting epoch 3/70, LR = [0.1]\n",
            "Train step - Step 80, Loss 0.020448045805096626\n",
            "Train step - Step 90, Loss 0.018156426027417183\n",
            "Train step - Step 100, Loss 0.018100349232554436\n",
            "Train step - Step 110, Loss 0.017665375024080276\n",
            "Train epoch - Accuracy: 0.6244444444444445 Loss: 0.01853272844896172 Corrects: 3091\n",
            "Starting epoch 4/70, LR = [0.1]\n",
            "Train step - Step 120, Loss 0.016422459855675697\n",
            "Train step - Step 130, Loss 0.017023971304297447\n",
            "Train step - Step 140, Loss 0.01650506630539894\n",
            "Train step - Step 150, Loss 0.015707436949014664\n",
            "Train epoch - Accuracy: 0.655959595959596 Loss: 0.016911182517204623 Corrects: 3247\n",
            "Starting epoch 5/70, LR = [0.1]\n",
            "Train step - Step 160, Loss 0.014450564980506897\n",
            "Train step - Step 170, Loss 0.0147439856082201\n",
            "Train step - Step 180, Loss 0.015564792789518833\n",
            "Train step - Step 190, Loss 0.015127490274608135\n",
            "Train epoch - Accuracy: 0.6834343434343434 Loss: 0.01566629604332977 Corrects: 3383\n",
            "Starting epoch 6/70, LR = [0.1]\n",
            "Train step - Step 200, Loss 0.016340335831046104\n",
            "Train step - Step 210, Loss 0.012997197918593884\n",
            "Train step - Step 220, Loss 0.013723284006118774\n",
            "Train step - Step 230, Loss 0.014630954712629318\n",
            "Train epoch - Accuracy: 0.7002020202020202 Loss: 0.01472586635011013 Corrects: 3466\n",
            "Starting epoch 7/70, LR = [0.1]\n",
            "Train step - Step 240, Loss 0.015148179605603218\n",
            "Train step - Step 250, Loss 0.014665558934211731\n",
            "Train step - Step 260, Loss 0.0139934616163373\n",
            "Train step - Step 270, Loss 0.012609032914042473\n",
            "Train epoch - Accuracy: 0.723030303030303 Loss: 0.013992207029627429 Corrects: 3579\n",
            "Starting epoch 8/70, LR = [0.1]\n",
            "Train step - Step 280, Loss 0.014408091083168983\n",
            "Train step - Step 290, Loss 0.014253911562263966\n",
            "Train step - Step 300, Loss 0.01208092737942934\n",
            "Train step - Step 310, Loss 0.012801408767700195\n",
            "Train epoch - Accuracy: 0.7321212121212122 Loss: 0.013214597484529621 Corrects: 3624\n",
            "Starting epoch 9/70, LR = [0.1]\n",
            "Train step - Step 320, Loss 0.01371885184198618\n",
            "Train step - Step 330, Loss 0.012865470722317696\n",
            "Train step - Step 340, Loss 0.012993942946195602\n",
            "Train step - Step 350, Loss 0.01338485348969698\n",
            "Train epoch - Accuracy: 0.7450505050505051 Loss: 0.012815210743609702 Corrects: 3688\n",
            "Starting epoch 10/70, LR = [0.1]\n",
            "Train step - Step 360, Loss 0.013974030502140522\n",
            "Train step - Step 370, Loss 0.012840449810028076\n",
            "Train step - Step 380, Loss 0.011982080526649952\n",
            "Train epoch - Accuracy: 0.7614141414141414 Loss: 0.012316284793043378 Corrects: 3769\n",
            "Starting epoch 11/70, LR = [0.1]\n",
            "Train step - Step 390, Loss 0.011398575268685818\n",
            "Train step - Step 400, Loss 0.010271157138049603\n",
            "Train step - Step 410, Loss 0.012235701084136963\n",
            "Train step - Step 420, Loss 0.014414744451642036\n",
            "Train epoch - Accuracy: 0.7634343434343435 Loss: 0.012014702235282672 Corrects: 3779\n",
            "Starting epoch 12/70, LR = [0.1]\n",
            "Train step - Step 430, Loss 0.010710593312978745\n",
            "Train step - Step 440, Loss 0.012182138860225677\n",
            "Train step - Step 450, Loss 0.01102671679109335\n",
            "Train step - Step 460, Loss 0.011011474765837193\n",
            "Train epoch - Accuracy: 0.7739393939393939 Loss: 0.011576339401530498 Corrects: 3831\n",
            "Starting epoch 13/70, LR = [0.1]\n",
            "Train step - Step 470, Loss 0.011798794381320477\n",
            "Train step - Step 480, Loss 0.009647848084568977\n",
            "Train step - Step 490, Loss 0.011834966950118542\n",
            "Train step - Step 500, Loss 0.011202891357243061\n",
            "Train epoch - Accuracy: 0.7826262626262627 Loss: 0.011259311206187263 Corrects: 3874\n",
            "Starting epoch 14/70, LR = [0.1]\n",
            "Train step - Step 510, Loss 0.012437782250344753\n",
            "Train step - Step 520, Loss 0.011685366742312908\n",
            "Train step - Step 530, Loss 0.010781986638903618\n",
            "Train step - Step 540, Loss 0.009161108173429966\n",
            "Train epoch - Accuracy: 0.7848484848484848 Loss: 0.010936512224601977 Corrects: 3885\n",
            "Starting epoch 15/70, LR = [0.1]\n",
            "Train step - Step 550, Loss 0.009974673390388489\n",
            "Train step - Step 560, Loss 0.009841284714639187\n",
            "Train step - Step 570, Loss 0.011524533852934837\n",
            "Train step - Step 580, Loss 0.012529894709587097\n",
            "Train epoch - Accuracy: 0.7993939393939394 Loss: 0.010547562234028422 Corrects: 3957\n",
            "Starting epoch 16/70, LR = [0.1]\n",
            "Train step - Step 590, Loss 0.009314265102148056\n",
            "Train step - Step 600, Loss 0.011513907462358475\n",
            "Train step - Step 610, Loss 0.0109218405559659\n",
            "Train step - Step 620, Loss 0.010826129466295242\n",
            "Train epoch - Accuracy: 0.8010101010101011 Loss: 0.010266053072656645 Corrects: 3965\n",
            "Starting epoch 17/70, LR = [0.1]\n",
            "Train step - Step 630, Loss 0.010194316506385803\n",
            "Train step - Step 640, Loss 0.008904588408768177\n",
            "Train step - Step 650, Loss 0.011943461373448372\n",
            "Train step - Step 660, Loss 0.01269244309514761\n",
            "Train epoch - Accuracy: 0.8044444444444444 Loss: 0.010031209826619938 Corrects: 3982\n",
            "Starting epoch 18/70, LR = [0.1]\n",
            "Train step - Step 670, Loss 0.011701371520757675\n",
            "Train step - Step 680, Loss 0.010934431105852127\n",
            "Train step - Step 690, Loss 0.009290769696235657\n",
            "Train step - Step 700, Loss 0.009669833816587925\n",
            "Train epoch - Accuracy: 0.8107070707070707 Loss: 0.00985381936635634 Corrects: 4013\n",
            "Starting epoch 19/70, LR = [0.1]\n",
            "Train step - Step 710, Loss 0.011473162099719048\n",
            "Train step - Step 720, Loss 0.011151335202157497\n",
            "Train step - Step 730, Loss 0.0072461641393601894\n",
            "Train step - Step 740, Loss 0.008553328923881054\n",
            "Train epoch - Accuracy: 0.8157575757575758 Loss: 0.009710526237960416 Corrects: 4038\n",
            "Starting epoch 20/70, LR = [0.1]\n",
            "Train step - Step 750, Loss 0.010115568526089191\n",
            "Train step - Step 760, Loss 0.009937397204339504\n",
            "Train step - Step 770, Loss 0.008185393176972866\n",
            "Train epoch - Accuracy: 0.825050505050505 Loss: 0.009386720360077993 Corrects: 4084\n",
            "Starting epoch 21/70, LR = [0.1]\n",
            "Train step - Step 780, Loss 0.011630852706730366\n",
            "Train step - Step 790, Loss 0.007226313930004835\n",
            "Train step - Step 800, Loss 0.010188610292971134\n",
            "Train step - Step 810, Loss 0.009480921551585197\n",
            "Train epoch - Accuracy: 0.8309090909090909 Loss: 0.0089958150756329 Corrects: 4113\n",
            "Starting epoch 22/70, LR = [0.1]\n",
            "Train step - Step 820, Loss 0.008601728826761246\n",
            "Train step - Step 830, Loss 0.009338432922959328\n",
            "Train step - Step 840, Loss 0.009524423629045486\n",
            "Train step - Step 850, Loss 0.007645369507372379\n",
            "Train epoch - Accuracy: 0.8321212121212122 Loss: 0.008962172307826654 Corrects: 4119\n",
            "Starting epoch 23/70, LR = [0.1]\n",
            "Train step - Step 860, Loss 0.009416848421096802\n",
            "Train step - Step 870, Loss 0.0077450997196137905\n",
            "Train step - Step 880, Loss 0.010161914862692356\n",
            "Train step - Step 890, Loss 0.008614416234195232\n",
            "Train epoch - Accuracy: 0.8377777777777777 Loss: 0.008646472773392392 Corrects: 4147\n",
            "Starting epoch 24/70, LR = [0.1]\n",
            "Train step - Step 900, Loss 0.008801017887890339\n",
            "Train step - Step 910, Loss 0.007620550226420164\n",
            "Train step - Step 920, Loss 0.007583930157124996\n",
            "Train step - Step 930, Loss 0.00760823767632246\n",
            "Train epoch - Accuracy: 0.8402020202020202 Loss: 0.008579532045758132 Corrects: 4159\n",
            "Starting epoch 25/70, LR = [0.1]\n",
            "Train step - Step 940, Loss 0.009143179282546043\n",
            "Train step - Step 950, Loss 0.007335289381444454\n",
            "Train step - Step 960, Loss 0.008775239810347557\n",
            "Train step - Step 970, Loss 0.009801487438380718\n",
            "Train epoch - Accuracy: 0.8482828282828283 Loss: 0.008290994580496442 Corrects: 4199\n",
            "Starting epoch 26/70, LR = [0.1]\n",
            "Train step - Step 980, Loss 0.0074625383131206036\n",
            "Train step - Step 990, Loss 0.00952085480093956\n",
            "Train step - Step 1000, Loss 0.008383734151721\n",
            "Train step - Step 1010, Loss 0.007223542779684067\n",
            "Train epoch - Accuracy: 0.8486868686868687 Loss: 0.008211017892968775 Corrects: 4201\n",
            "Starting epoch 27/70, LR = [0.1]\n",
            "Train step - Step 1020, Loss 0.009613950736820698\n",
            "Train step - Step 1030, Loss 0.008442177437245846\n",
            "Train step - Step 1040, Loss 0.008454427123069763\n",
            "Train step - Step 1050, Loss 0.00783136673271656\n",
            "Train epoch - Accuracy: 0.8521212121212122 Loss: 0.008017830571366681 Corrects: 4218\n",
            "Starting epoch 28/70, LR = [0.1]\n",
            "Train step - Step 1060, Loss 0.008484949357807636\n",
            "Train step - Step 1070, Loss 0.007169146556407213\n",
            "Train step - Step 1080, Loss 0.008168298751115799\n",
            "Train step - Step 1090, Loss 0.007432123180478811\n",
            "Train epoch - Accuracy: 0.8555555555555555 Loss: 0.007905322542470512 Corrects: 4235\n",
            "Starting epoch 29/70, LR = [0.1]\n",
            "Train step - Step 1100, Loss 0.00761367054656148\n",
            "Train step - Step 1110, Loss 0.009025875478982925\n",
            "Train step - Step 1120, Loss 0.00798774417489767\n",
            "Train step - Step 1130, Loss 0.009411059319972992\n",
            "Train epoch - Accuracy: 0.8537373737373737 Loss: 0.007757441314181896 Corrects: 4226\n",
            "Starting epoch 30/70, LR = [0.1]\n",
            "Train step - Step 1140, Loss 0.0064313956536352634\n",
            "Train step - Step 1150, Loss 0.009545743465423584\n",
            "Train step - Step 1160, Loss 0.006590594071894884\n",
            "Train epoch - Accuracy: 0.8597979797979798 Loss: 0.0074723506854339075 Corrects: 4256\n",
            "Starting epoch 31/70, LR = [0.1]\n",
            "Train step - Step 1170, Loss 0.008282612077891827\n",
            "Train step - Step 1180, Loss 0.006563742179423571\n",
            "Train step - Step 1190, Loss 0.007610770408064127\n",
            "Train step - Step 1200, Loss 0.009450659155845642\n",
            "Train epoch - Accuracy: 0.8682828282828283 Loss: 0.007334106222660553 Corrects: 4298\n",
            "Starting epoch 32/70, LR = [0.1]\n",
            "Train step - Step 1210, Loss 0.007984586991369724\n",
            "Train step - Step 1220, Loss 0.00680041778832674\n",
            "Train step - Step 1230, Loss 0.006161509547382593\n",
            "Train step - Step 1240, Loss 0.007368406280875206\n",
            "Train epoch - Accuracy: 0.86989898989899 Loss: 0.007275470257783779 Corrects: 4306\n",
            "Starting epoch 33/70, LR = [0.1]\n",
            "Train step - Step 1250, Loss 0.007958772592246532\n",
            "Train step - Step 1260, Loss 0.005804856773465872\n",
            "Train step - Step 1270, Loss 0.005244115367531776\n",
            "Train step - Step 1280, Loss 0.007563351187855005\n",
            "Train epoch - Accuracy: 0.8747474747474747 Loss: 0.007024602444296834 Corrects: 4330\n",
            "Starting epoch 34/70, LR = [0.1]\n",
            "Train step - Step 1290, Loss 0.007582077290862799\n",
            "Train step - Step 1300, Loss 0.005597604438662529\n",
            "Train step - Step 1310, Loss 0.005981873255223036\n",
            "Train step - Step 1320, Loss 0.0057397279888391495\n",
            "Train epoch - Accuracy: 0.8674747474747475 Loss: 0.007071887322447516 Corrects: 4294\n",
            "Starting epoch 35/70, LR = [0.1]\n",
            "Train step - Step 1330, Loss 0.005272542126476765\n",
            "Train step - Step 1340, Loss 0.005957882851362228\n",
            "Train step - Step 1350, Loss 0.007015475071966648\n",
            "Train step - Step 1360, Loss 0.006453968118876219\n",
            "Train epoch - Accuracy: 0.8705050505050506 Loss: 0.006877698682323851 Corrects: 4309\n",
            "Starting epoch 36/70, LR = [0.1]\n",
            "Train step - Step 1370, Loss 0.009342104196548462\n",
            "Train step - Step 1380, Loss 0.005318630952388048\n",
            "Train step - Step 1390, Loss 0.006703509017825127\n",
            "Train step - Step 1400, Loss 0.009420397691428661\n",
            "Train epoch - Accuracy: 0.8757575757575757 Loss: 0.006762884796380696 Corrects: 4335\n",
            "Starting epoch 37/70, LR = [0.1]\n",
            "Train step - Step 1410, Loss 0.00537806935608387\n",
            "Train step - Step 1420, Loss 0.006098513957113028\n",
            "Train step - Step 1430, Loss 0.007220726925879717\n",
            "Train step - Step 1440, Loss 0.006111633498221636\n",
            "Train epoch - Accuracy: 0.8787878787878788 Loss: 0.006779265559720572 Corrects: 4350\n",
            "Starting epoch 38/70, LR = [0.1]\n",
            "Train step - Step 1450, Loss 0.00681263068690896\n",
            "Train step - Step 1460, Loss 0.006073043216019869\n",
            "Train step - Step 1470, Loss 0.005562427919358015\n",
            "Train step - Step 1480, Loss 0.0069807106629014015\n",
            "Train epoch - Accuracy: 0.8874747474747475 Loss: 0.006426646179058636 Corrects: 4393\n",
            "Starting epoch 39/70, LR = [0.1]\n",
            "Train step - Step 1490, Loss 0.006593726575374603\n",
            "Train step - Step 1500, Loss 0.004585220478475094\n",
            "Train step - Step 1510, Loss 0.006583793088793755\n",
            "Train step - Step 1520, Loss 0.006533154286444187\n",
            "Train epoch - Accuracy: 0.8836363636363637 Loss: 0.006350900802874204 Corrects: 4374\n",
            "Starting epoch 40/70, LR = [0.1]\n",
            "Train step - Step 1530, Loss 0.006593032740056515\n",
            "Train step - Step 1540, Loss 0.00575727503746748\n",
            "Train step - Step 1550, Loss 0.006147155072540045\n",
            "Train epoch - Accuracy: 0.8913131313131313 Loss: 0.006131966619404278 Corrects: 4412\n",
            "Starting epoch 41/70, LR = [0.1]\n",
            "Train step - Step 1560, Loss 0.005713164806365967\n",
            "Train step - Step 1570, Loss 0.005400409456342459\n",
            "Train step - Step 1580, Loss 0.005842022132128477\n",
            "Train step - Step 1590, Loss 0.006075527518987656\n",
            "Train epoch - Accuracy: 0.898989898989899 Loss: 0.005823419971598519 Corrects: 4450\n",
            "Starting epoch 42/70, LR = [0.1]\n",
            "Train step - Step 1600, Loss 0.0052101644687354565\n",
            "Train step - Step 1610, Loss 0.006584829650819302\n",
            "Train step - Step 1620, Loss 0.005717427469789982\n",
            "Train step - Step 1630, Loss 0.006464420352131128\n",
            "Train epoch - Accuracy: 0.9008080808080808 Loss: 0.005766618640133829 Corrects: 4459\n",
            "Starting epoch 43/70, LR = [0.1]\n",
            "Train step - Step 1640, Loss 0.005403319373726845\n",
            "Train step - Step 1650, Loss 0.006288682110607624\n",
            "Train step - Step 1660, Loss 0.004614683799445629\n",
            "Train step - Step 1670, Loss 0.004445123951882124\n",
            "Train epoch - Accuracy: 0.9008080808080808 Loss: 0.005682626665014811 Corrects: 4459\n",
            "Starting epoch 44/70, LR = [0.1]\n",
            "Train step - Step 1680, Loss 0.004472319968044758\n",
            "Train step - Step 1690, Loss 0.005289035849273205\n",
            "Train step - Step 1700, Loss 0.005495370365679264\n",
            "Train step - Step 1710, Loss 0.007883581332862377\n",
            "Train epoch - Accuracy: 0.902020202020202 Loss: 0.005661388536670593 Corrects: 4465\n",
            "Starting epoch 45/70, LR = [0.1]\n",
            "Train step - Step 1720, Loss 0.006021890323609114\n",
            "Train step - Step 1730, Loss 0.004738425835967064\n",
            "Train step - Step 1740, Loss 0.004462896380573511\n",
            "Train step - Step 1750, Loss 0.0058204480446875095\n",
            "Train epoch - Accuracy: 0.9050505050505051 Loss: 0.0055007649976948295 Corrects: 4480\n",
            "Starting epoch 46/70, LR = [0.1]\n",
            "Train step - Step 1760, Loss 0.005258732941001654\n",
            "Train step - Step 1770, Loss 0.003325904021039605\n",
            "Train step - Step 1780, Loss 0.005031439010053873\n",
            "Train step - Step 1790, Loss 0.0059407735243439674\n",
            "Train epoch - Accuracy: 0.9064646464646464 Loss: 0.0053909181193871935 Corrects: 4487\n",
            "Starting epoch 47/70, LR = [0.1]\n",
            "Train step - Step 1800, Loss 0.0038211860228329897\n",
            "Train step - Step 1810, Loss 0.004381721839308739\n",
            "Train step - Step 1820, Loss 0.004377342294901609\n",
            "Train step - Step 1830, Loss 0.006838185712695122\n",
            "Train epoch - Accuracy: 0.9115151515151515 Loss: 0.005357451430249093 Corrects: 4512\n",
            "Starting epoch 48/70, LR = [0.1]\n",
            "Train step - Step 1840, Loss 0.007405109703540802\n",
            "Train step - Step 1850, Loss 0.005456935148686171\n",
            "Train step - Step 1860, Loss 0.005315595772117376\n",
            "Train step - Step 1870, Loss 0.005831162910908461\n",
            "Train epoch - Accuracy: 0.9028282828282829 Loss: 0.005412701228915742 Corrects: 4469\n",
            "Starting epoch 49/70, LR = [0.1]\n",
            "Train step - Step 1880, Loss 0.0049868980422616005\n",
            "Train step - Step 1890, Loss 0.0070423162542283535\n",
            "Train step - Step 1900, Loss 0.004606712609529495\n",
            "Train step - Step 1910, Loss 0.005862293764948845\n",
            "Train epoch - Accuracy: 0.9119191919191919 Loss: 0.005064790348964508 Corrects: 4514\n",
            "Starting epoch 50/70, LR = [0.004000000000000001]\n",
            "Train step - Step 1920, Loss 0.004090317990630865\n",
            "Train step - Step 1930, Loss 0.005443213973194361\n",
            "Train step - Step 1940, Loss 0.005148792173713446\n",
            "Train epoch - Accuracy: 0.9145454545454546 Loss: 0.0049204541554655696 Corrects: 4527\n",
            "Starting epoch 51/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1950, Loss 0.005313086323440075\n",
            "Train step - Step 1960, Loss 0.003586231032386422\n",
            "Train step - Step 1970, Loss 0.004409166052937508\n",
            "Train step - Step 1980, Loss 0.004976905416697264\n",
            "Train epoch - Accuracy: 0.9202020202020202 Loss: 0.004668712745223081 Corrects: 4555\n",
            "Starting epoch 52/70, LR = [0.020000000000000004]\n",
            "Train step - Step 1990, Loss 0.0037932752165943384\n",
            "Train step - Step 2000, Loss 0.004748836159706116\n",
            "Train step - Step 2010, Loss 0.004301862791180611\n",
            "Train step - Step 2020, Loss 0.00476475665345788\n",
            "Train epoch - Accuracy: 0.9262626262626262 Loss: 0.00452847553398272 Corrects: 4585\n",
            "Starting epoch 53/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2030, Loss 0.006009778939187527\n",
            "Train step - Step 2040, Loss 0.006635712459683418\n",
            "Train step - Step 2050, Loss 0.0048464564606547356\n",
            "Train step - Step 2060, Loss 0.0036533616948872805\n",
            "Train epoch - Accuracy: 0.9224242424242424 Loss: 0.004591972278148839 Corrects: 4566\n",
            "Starting epoch 54/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2070, Loss 0.003705509239807725\n",
            "Train step - Step 2080, Loss 0.0036619650200009346\n",
            "Train step - Step 2090, Loss 0.004030011594295502\n",
            "Train step - Step 2100, Loss 0.004493923857808113\n",
            "Train epoch - Accuracy: 0.9258585858585858 Loss: 0.004520436735127611 Corrects: 4583\n",
            "Starting epoch 55/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2110, Loss 0.0033946228213608265\n",
            "Train step - Step 2120, Loss 0.005373331252485514\n",
            "Train step - Step 2130, Loss 0.004556280095130205\n",
            "Train step - Step 2140, Loss 0.006242847535759211\n",
            "Train epoch - Accuracy: 0.9216161616161617 Loss: 0.0046302132895498565 Corrects: 4562\n",
            "Starting epoch 56/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2150, Loss 0.00475960923358798\n",
            "Train step - Step 2160, Loss 0.00373017112724483\n",
            "Train step - Step 2170, Loss 0.0037226658314466476\n",
            "Train step - Step 2180, Loss 0.003983506932854652\n",
            "Train epoch - Accuracy: 0.9248484848484848 Loss: 0.004534863864607884 Corrects: 4578\n",
            "Starting epoch 57/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2190, Loss 0.00441729836165905\n",
            "Train step - Step 2200, Loss 0.004990765359252691\n",
            "Train step - Step 2210, Loss 0.0032791702542454004\n",
            "Train step - Step 2220, Loss 0.003890544641762972\n",
            "Train epoch - Accuracy: 0.9268686868686868 Loss: 0.004399157369866817 Corrects: 4588\n",
            "Starting epoch 58/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2230, Loss 0.004573128651827574\n",
            "Train step - Step 2240, Loss 0.0038951991591602564\n",
            "Train step - Step 2250, Loss 0.004038406535983086\n",
            "Train step - Step 2260, Loss 0.005048207938671112\n",
            "Train epoch - Accuracy: 0.9284848484848485 Loss: 0.004262187026836204 Corrects: 4596\n",
            "Starting epoch 59/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2270, Loss 0.005059263668954372\n",
            "Train step - Step 2280, Loss 0.003542122198268771\n",
            "Train step - Step 2290, Loss 0.004133015405386686\n",
            "Train step - Step 2300, Loss 0.0035523397382348776\n",
            "Train epoch - Accuracy: 0.926060606060606 Loss: 0.004387677696185431 Corrects: 4584\n",
            "Starting epoch 60/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2310, Loss 0.0051589845679700375\n",
            "Train step - Step 2320, Loss 0.007095935754477978\n",
            "Train step - Step 2330, Loss 0.003631671890616417\n",
            "Train epoch - Accuracy: 0.9321212121212121 Loss: 0.004339321219371726 Corrects: 4614\n",
            "Starting epoch 61/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2340, Loss 0.005040641408413649\n",
            "Train step - Step 2350, Loss 0.00417567603290081\n",
            "Train step - Step 2360, Loss 0.004375175107270479\n",
            "Train step - Step 2370, Loss 0.004044742789119482\n",
            "Train epoch - Accuracy: 0.9319191919191919 Loss: 0.004250794348278732 Corrects: 4613\n",
            "Starting epoch 62/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2380, Loss 0.00448167510330677\n",
            "Train step - Step 2390, Loss 0.0027137345168739557\n",
            "Train step - Step 2400, Loss 0.002878939500078559\n",
            "Train step - Step 2410, Loss 0.0036038001999258995\n",
            "Train epoch - Accuracy: 0.9272727272727272 Loss: 0.004306636057192027 Corrects: 4590\n",
            "Starting epoch 63/70, LR = [0.020000000000000004]\n",
            "Train step - Step 2420, Loss 0.004295858554542065\n",
            "Train step - Step 2430, Loss 0.0035296520218253136\n",
            "Train step - Step 2440, Loss 0.0039214156568050385\n",
            "Train step - Step 2450, Loss 0.0032654141541570425\n",
            "Train epoch - Accuracy: 0.9347474747474748 Loss: 0.004120420581764645 Corrects: 4627\n",
            "Starting epoch 64/70, LR = [0.0008000000000000003]\n",
            "Train step - Step 2460, Loss 0.004396438132971525\n",
            "Train step - Step 2470, Loss 0.004842162132263184\n",
            "Train step - Step 2480, Loss 0.004257336724549532\n",
            "Train step - Step 2490, Loss 0.004340786021202803\n",
            "Train epoch - Accuracy: 0.9313131313131313 Loss: 0.004166654949311656 Corrects: 4610\n",
            "Starting epoch 65/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2500, Loss 0.004089981317520142\n",
            "Train step - Step 2510, Loss 0.0044051664881408215\n",
            "Train step - Step 2520, Loss 0.004493216518312693\n",
            "Train step - Step 2530, Loss 0.002809578087180853\n",
            "Train epoch - Accuracy: 0.9371717171717172 Loss: 0.004103783451095976 Corrects: 4639\n",
            "Starting epoch 66/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2540, Loss 0.003947510849684477\n",
            "Train step - Step 2550, Loss 0.004565728362649679\n",
            "Train step - Step 2560, Loss 0.0038140832912176847\n",
            "Train step - Step 2570, Loss 0.0038884186651557684\n",
            "Train epoch - Accuracy: 0.9288888888888889 Loss: 0.0043068187175826595 Corrects: 4598\n",
            "Starting epoch 67/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2580, Loss 0.004528694786131382\n",
            "Train step - Step 2590, Loss 0.0039076292887330055\n",
            "Train step - Step 2600, Loss 0.0035828035324811935\n",
            "Train step - Step 2610, Loss 0.0034335115924477577\n",
            "Train epoch - Accuracy: 0.9298989898989899 Loss: 0.004248753175177056 Corrects: 4603\n",
            "Starting epoch 68/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2620, Loss 0.004985951818525791\n",
            "Train step - Step 2630, Loss 0.0034429156221449375\n",
            "Train step - Step 2640, Loss 0.003687289310619235\n",
            "Train step - Step 2650, Loss 0.004126065876334906\n",
            "Train epoch - Accuracy: 0.9317171717171717 Loss: 0.0042113339737283465 Corrects: 4612\n",
            "Starting epoch 69/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2660, Loss 0.005184090696275234\n",
            "Train step - Step 2670, Loss 0.004595174919813871\n",
            "Train step - Step 2680, Loss 0.003756097285076976\n",
            "Train step - Step 2690, Loss 0.0032996328081935644\n",
            "Train epoch - Accuracy: 0.9351515151515152 Loss: 0.004110359412269911 Corrects: 4629\n",
            "Starting epoch 70/70, LR = [0.004000000000000001]\n",
            "Train step - Step 2700, Loss 0.004621874541044235\n",
            "Train step - Step 2710, Loss 0.005096188746392727\n",
            "Train step - Step 2720, Loss 0.004963366314768791\n",
            "Train epoch - Accuracy: 0.9333333333333333 Loss: 0.0041872235634034935 Corrects: 4620\n",
            "Training finished in 215.43994688987732 seconds\n",
            "EVALUATION:  0.78 0.013076909817755222\n",
            "TEST GROUP:  0.829\n",
            "TEST ALL:  0.0829\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aakVopSdyeGr"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FZ0VbtGQygRH",
        "outputId": "b0badffc-f538-4935-c5da-49085a14c7bf"
      },
      "source": [
        "method = \"finetuning\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics FINETUNING for seed 1993\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5732534c88d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# write down json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mwriteMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusionMatrixData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracies' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ-J8aC_xx-c"
      },
      "source": [
        "## LWF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZBNivOix1FB"
      },
      "source": [
        "### train, validate, test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdfIllqBx59v"
      },
      "source": [
        "import copy\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, old_net, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkzFiDVDyAm_"
      },
      "source": [
        "### sequential learning LWF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf7REFkIyEiQ"
      },
      "source": [
        "### LWF\n",
        "def sequentialLearningLWF(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        old_net = copy.deepcopy(net)\n",
        "        old_net.to(DEVICE)\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZTwTS_4TcES"
      },
      "source": [
        "### execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkrMQy2TuUAb"
      },
      "source": [
        "# train\n",
        "# net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningLWF(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2qUxVyMTfB4"
      },
      "source": [
        "### plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_4xLfwcpDz"
      },
      "source": [
        "method = \"Learning Without Forgetting\"\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "data_plot_bar=[]\n",
        "data_plot_line=[]\n",
        "for id in range(0,10):\n",
        "    data_plot_bar.append((id+1,old_accuracies[id]))\n",
        "    data_plot_line.append(((id+1)*10,new_accuracies[id]))\n",
        "\n",
        "plt.figure(figsize=(20,7))\n",
        "accuracyDF=pd.DataFrame(data_plot_bar, columns = ['Group','Accuracy'])\n",
        "ax = sns.barplot(x=\"Group\", y=\"Accuracy\",data=accuracyDF)\n",
        "plt.title(\"Single Group Sequential Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy trend\n",
        "plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write down json\n",
        "writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGHBWaLNXGeI"
      },
      "source": [
        "\"\"\"num_classes_seen = 100\n",
        "dif_accuracies=printAccuracyDifference(net,old_accuracies, num_classes_seen)\n",
        "dif_accuracies\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVjGRIqDtNQP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nX10znUi6qd"
      },
      "source": [
        "## iCaRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5HRagcujHgM"
      },
      "source": [
        "### construct exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blwBqG6BjNsM"
      },
      "source": [
        "import sys\n",
        "\n",
        "def constructExemplarSet(net, Xclass, m):\n",
        "#Xclass contiene immagini e label della classe X\n",
        "  exemplars_set = []\n",
        "  feature_exemplars = []\n",
        "  indexes = []\n",
        "  features = [] \n",
        "\n",
        "  with torch.no_grad():\n",
        "    XtrainLoader = torch.utils.data.DataLoader(Xclass, shuffle = True, batch_size=1, num_workers=2)\n",
        "    for _, image, labels in XtrainLoader:\n",
        "      image = image.to(DEVICE)\n",
        "      # per ogni immagine della classe x, prendiamo le rispettive feature e le uniamo nel vettore features, che contiene tutte quelle delle immagini della classe x\n",
        "      featuretmp = net.forward(image)\n",
        "      feature = net.predict(featuretmp)\n",
        "      #print(feature)\n",
        "      feature.data = feature.data/feature.data.norm()\n",
        "      features.append(feature)\n",
        "\n",
        "    features = torch.cat(features, dim=0) #cat solve the problem of inequal size of tensors \n",
        "    current_class_mean = features.mean(0) # mu = media delle features delle immagini della classe\n",
        "\n",
        "    for k in range(1, m+1):\n",
        "      min = 100000\n",
        "      sum = 0\n",
        "      for j in range(k-1):\n",
        "        sum += feature_exemplars[j]\n",
        "      for x in range(len(Xclass)): \n",
        "        if (x not in indexes):\n",
        "          phiX = features[x]\n",
        "          val = current_class_mean - ((phiX + sum)/k)\n",
        "          val = np.linalg.norm(val.cpu().numpy()) ## NORMA \n",
        "          #res = torch.lt(val, min)\n",
        "        #res.sum() ritorna il numero di True \n",
        "          #sumTrue = res.sum() #sumTrue restituisce il numero di True all'interno del tensore Res \n",
        "          #lenghtTmp = res.shape # dimensione del tensore\n",
        "          #lenght = list(lenghtTmp)[0]\n",
        "          #res = (lenght == sumTrue)\n",
        "          #trueLabel = torch.tensor(True).to(device)\n",
        "          #if (torch.equal(trueLabel, res)):\n",
        "          if (val < min):\n",
        "            min = val\n",
        "            feature_min = phiX\n",
        "            index_min = x\n",
        "      indexes.append(index_min)\n",
        "      feature_exemplars.append(feature_min)\n",
        "      exemplars_set.append(Xclass[index_min])\n",
        "\n",
        "    print(\"lunghezza exemplar set: \", len(exemplars_set))\n",
        "    return exemplars_set"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy_HVL-tjZcb"
      },
      "source": [
        "### reduce exemplar set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7kuQ5DnjcCP"
      },
      "source": [
        "# DA CHIAMARE PER OGNI CLASSE k: riduce il numero di exemplars all'interno dell'exemplar_set della classe k \n",
        "def reduceExemplarSet(m, exemplars_set):\n",
        "  exemplars_new = []\n",
        "  for i in range(m):\n",
        "    exemplars_new.append(exemplars_set[i])\n",
        "\n",
        "  return exemplars_new"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMMlEpLsjfaj"
      },
      "source": [
        "### update representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRdA9RtDjhak"
      },
      "source": [
        "def updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot):\n",
        "  #exemplars_set_tot contiene tutti gli exemplars set ottenuti fino ad ora\n",
        "  #train_iter contiene tutti i dati (immagini + labels) delle classi nuove (s, .., t)\n",
        "  \n",
        "  for k, v in exemplars_set_tot.items():\n",
        "    train_subset += v \n",
        "  train_loader = torch.utils.data.DataLoader(train_subset, shuffle = True, batch_size=BATCH_SIZE, num_workers=2)\n",
        "  # train_loader è la concatenazione delle nuove classi con gli exemplar_sets calcolati fino a questo punto \n",
        "  print(\"training\")\n",
        "  train(net, train_loader, criterion, optimizer, scheduler, num_classes, group_id)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac65-6QGjm9O"
      },
      "source": [
        "### incremental train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPU_32ArjpPH"
      },
      "source": [
        "def incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot):\n",
        "  print(\"Starting the update reprensentation\")\n",
        "  num_classes = 10\n",
        "  classes_until_now = []\n",
        "  new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        " \n",
        "\n",
        "  updateRepresentation(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot)\n",
        "                \n",
        "  iteration = group_id - 1\n",
        "  t = (num_classes * iteration) + num_classes # num_classes ricevute fino a questo momento \n",
        "  m = int(K/t) #casto ad intero ? per difetto o eccesso?\n",
        "  s = num_classes * iteration\n",
        "\n",
        "  print(\"reducing examplar for each class\")\n",
        "  for y in classes_until_now: #ci serve un set con tutte le classi fino ad ora viste\n",
        "    exemplar_y_new = reduceExemplarSet(m, exemplars_set_tot[y]) # valore associato alla chiave y che rappresenta il label della classe \n",
        "    exemplars_set_tot[y] = exemplar_y_new\n",
        "  \n",
        "  classes_until_now.append(new_classes_examined)\n",
        "  \n",
        "\n",
        "  for y in new_classes_examined: # nuovi classi in arrivo di cui vogliamo costruire il set rappresentativo\n",
        "    images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data']\n",
        "    imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "    class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "    print(\"class train: \", class_train_subset)\n",
        "    print(\"Constructing exemplars of class\", y)\n",
        "    exemplars_set = constructExemplarSet(net,class_train_subset, m)\n",
        "    exemplars_set_tot[y] = exemplars_set"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDxmsQlWjp5D"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dQznGoEju9J"
      },
      "source": [
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, num_epochs=NUM_EPOCHS):    \n",
        "    num_classes_till_previous_step = group_id * 10 - 10\n",
        "\n",
        "    # network to GPU\n",
        "    net = net.to(DEVICE) \n",
        "\n",
        "    cudnn.benchmark\n",
        "\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for _, images, labels in train_dataloader:\n",
        "\n",
        "            # Bring images and labels to GPU\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Labels encoding \n",
        "            labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "            labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            features = net.forward(images)\n",
        "            outputs = net.predict(features)\n",
        "\n",
        "            # if iteration > 0, loss is the combination between the classification loss on new classes and the distillation loss on old classes\n",
        "            if (group_id > 1):\n",
        "              old_features = old_net.forward(images)\n",
        "              old_outputs = old_net.predict(old_features)\n",
        "              labels_enc[:,0:num_classes_till_previous_step] = torch.sigmoid(old_outputs)\n",
        "\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHpi410ljzWD"
      },
      "source": [
        "### test and validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQYB0-cLj0sP"
      },
      "source": [
        "def validate(net, val_dataloader, criterion, num_classes):\n",
        "    net.eval()\n",
        "\n",
        "    getLossCriterion()\n",
        "\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for _, images, labels in val_dataloader:\n",
        "\n",
        "        # Bring images and labels to GPU\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "       \n",
        "        # Labels encoding \n",
        "        labels_enc = _one_hot_encode(labels, num_classes, outputs_labels_mapping)\n",
        "        labels = outputs_labels_mapping.getNodes(labels)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        features = net.forward(images)\n",
        "        outputs = net.predict(features)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = computeLoss(criterion, outputs, labels_enc)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        all_preds_cm.extend(preds.tolist())\n",
        "        all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "    #Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss, all_preds_cm, all_labels_cm\n",
        "\n",
        "def test(net, test_dataloader, num_classes):\n",
        "    acc, _, all_preds_cm, all_labels_cm = validate(net, test_dataloader, None, num_classes)\n",
        "    return acc, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTv6zBetj3Cb"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fl5mJrUj4fU"
      },
      "source": [
        "# # MAIN quello che facevamo prima\n",
        "# K = 2000\n",
        "# iterations = 10\n",
        "# num_classes = 10\n",
        "# test_set = [] #initialized here because we test over all the classes not only those one in which I train\n",
        "# exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "# for i in range(iterations):\n",
        "#   # train_subsets, val_subsets, test_subsets\n",
        "\n",
        "#   valid_loader = torch.utils.data.DataLoader(val_subsets, shuffle = True, batch_size = batch_size, num_workers=2) \n",
        "#   print(\"Train the network, iteration: \", i, \" on classes: \", classes_current_iter)\n",
        "#   incrementalTrain(train_subsets, i, net, device, epochs, num_classes, K, exemplars_set_tot) # Train the network with 10 classes at a time\n",
        "#   test(test_subsets, i, net) # Test the network with all classes seen until this iteration"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_cnELYdkap4"
      },
      "source": [
        "def sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet32()\n",
        "    test_set = None\n",
        "    groups_accuracies=[]\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "\n",
        "    K = 2000\n",
        "    iterations = 10\n",
        "    num_classes = 10\n",
        "    exemplars_set_tot = {new_list: [] for new_list in range(100)}\n",
        "    \n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "        addOutputs(net,10)\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "        addOutputs(net,10)\n",
        "      \n",
        "      num_classes_per_group = 10\n",
        "      num_classes_seen = group_id*10\n",
        "\n",
        "      print(\"GROUP: \",group_id)\n",
        "      # Train on current group\n",
        "      optimizer, scheduler = getSchedulerOptimizer(net) # reset learning rate and step_size\n",
        "      # train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      # train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\n",
        "      incrementalTrain(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot) # Train the network with 10 classes at a time\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc, loss, _, _ = validate(net, val_loader, criterion, num_classes_seen)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Test on current group\n",
        "      test_group_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_group, _, _ = test(net, test_group_loader, num_classes_seen)\n",
        "      groups_accuracies.append(acc_group)\n",
        "\n",
        "      test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "      acc_all, all_preds_cm, all_labels_cm = test(net, test_loader, num_classes_seen)\n",
        "      all_accuracies.append(acc_all)\n",
        "      \n",
        "      print(\"TEST GROUP: \",acc_group)\n",
        "      print(\"TEST ALL: \",acc_all)\n",
        "      group_id+=1\n",
        "\n",
        "    return net, groups_accuracies, all_accuracies, all_preds_cm, all_labels_cm\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "        id_group+=1\n",
        "    return dif_accuracies"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTV1Gxq6yF5-",
        "outputId": "3f2e1540-12cd-4741-ce9a-afd8c46e4a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "# train\n",
        "net, old_accuracies, new_accuracies, all_preds_cm, all_labels_cm = sequentialLearningiCaRL(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n",
            "Starting the update reprensentation\n",
            "training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-9467244d667a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequentialLearningiCaRL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_subsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-945ab1be12d0>\u001b[0m in \u001b[0;36msequentialLearningiCaRL\u001b[0;34m(train_subsets, val_subsets, test_subsets)\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;31m# train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;31m# train(net, train_loader, criterion, optimizer, scheduler, num_classes_seen, group_id, old_net)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mincrementalTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes_seen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexemplars_set_tot\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Train the network with 10 classes at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;31m# Validate on current group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-7453e923c59c>\u001b[0m in \u001b[0;36mincrementalTrain\u001b[0;34m(net, train_subset, criterion, optimizer, scheduler, num_classes_seen, group_id, K, exemplars_set_tot)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mupdateRepresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexemplars_set_tot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_id\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-fa954e847d37>\u001b[0m in \u001b[0;36mupdateRepresentation\u001b[0;34m(net, train_subset, criterion, optimizer, scheduler, num_classes, group_id, K, exemplars_set_tot)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# train_loader è la concatenazione delle nuove classi con gli exemplar_sets calcolati fino a questo punto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-ba1cd88d55da>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_dataloader, criterion, optimizer, scheduler, num_classes, group_id, num_epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# network to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    }
  ]
}