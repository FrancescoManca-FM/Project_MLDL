{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "incrementalLearningWithoutForgetting.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Gzrd6SoDNp6d",
        "QObN0rGNN2Be"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoManca-FM/Project_MLDL/blob/main/incrementalLearningWithoutForgetting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUHNE0YdXMlR"
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms \n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from torch.nn.init import xavier_uniform_ "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzrd6SoDNp6d"
      },
      "source": [
        "### DATA LOADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qgo5-aj4ScF",
        "outputId": "d731bbd5-1dfe-4265-c589-411e0c0b0e42"
      },
      "source": [
        "# we build a transform to normalize images: Data normalization is an important step which ensures \n",
        "# each input parameter (pixel, in this case) has a similar data distribution. This makes convergence \n",
        "# faster while training the network.\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, \n",
        "                                         download=True, transform=transform)\n",
        "# DataLoader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
        "# batch_size = how many samples per batch to load\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXdZrK0zNwvw"
      },
      "source": [
        "### NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kAHBaHJ4Td1"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        \n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        self.inplanes = 16\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def addOutputNodes(self, num_new_outputs):\n",
        "        in_features = self.fc.in_features\n",
        "        out_features = self.fc.out_features\n",
        "        weight = self.fc.weight.data\n",
        "\n",
        "        self.fc = nn.Linear(in_features, out_features + num_new_outputs)\n",
        "        xavier_uniform_(self.fc.weight)\n",
        "        self.fc.weight.data[:out_features] = weight\n",
        "        \n",
        "\n",
        "\n",
        "def resnet20(pretrained=False, **kwargs):\n",
        "    n = 3\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet32(pretrained=False, **kwargs):\n",
        "    n = 5\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet56(pretrained=False, **kwargs):\n",
        "    n = 9\n",
        "    model = ResNet(Bottleneck, [n, n, n], **kwargs)\n",
        "    return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1JeZ9NB4ZxS",
        "outputId": "3cb6556e-fdfe-4020-fcd0-3120cdc5b89b"
      },
      "source": [
        "net = resnet32()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
        "net.to(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QObN0rGNN2Be"
      },
      "source": [
        "### LOSS & PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUMJGDLO4cR-"
      },
      "source": [
        "lr = 0.01\n",
        "decay = 0.0001\n",
        "epochs = 30\n",
        "momentum = 0.9\n",
        "factor = 5"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLf5BCR-4c3X"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.SGD(net.parameters(), lr = lr, weight_decay=decay,momentum= momentum)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te6KvRNKN5k_"
      },
      "source": [
        "### TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IbM6obCwqXH"
      },
      "source": [
        "def training(trainloader, iteration, network, device, epochs, num_classes):\n",
        "  distillation_loss = 0\n",
        "  if (iteration != 0):\n",
        "    # add 10 output nodes to the network\n",
        "    network.addOutputNodes(num_classes)\n",
        "    network.to(device)\n",
        "\n",
        "  old_net = copy.deepcopy(network)\n",
        "  \n",
        "  #train the network\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "      inputs = data[0].to(device)\n",
        "      labels = data[1].to(device)\n",
        "      \n",
        "      # Sets the gradients of all optimized torch.Tensor to zero.\n",
        "      optimizer.zero_grad() \n",
        "\n",
        "      # forward: assign weights to each edge in each layer\n",
        "      outputs = network.forward(inputs) \n",
        "\n",
        "       # calculate the classification loss \n",
        "      classification_loss = criterion(outputs,labels) \n",
        "\n",
        "      if (iteration > 0):\n",
        "      # calculate the distillation loss\n",
        "        distillation_loss = dist_loss(outputs, old_net, inputs) \n",
        "      \n",
        "      loss = classification_loss + distillation_loss\n",
        "\n",
        "      # redesign the weights evaluating the performance of the network\n",
        "      loss.backward() \n",
        "\n",
        "      # update parameters\n",
        "      optimizer.step()  \n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # print every 20 mini-batches the average value of the loss accumulated in each batch\n",
        "      if i % 20 == 19:    \n",
        "        print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 20))\n",
        "        running_loss = 0.0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agoNfyi00-NF"
      },
      "source": [
        " def dist_loss(outputs, old_net, inputs):\n",
        "    out_old = torch.sigmoid(old_net.forward(inputs))\n",
        "    out_old_new = torch.argmax(out_old, dim=1)\n",
        "    distillation_loss = criterion(outputs, out_old_new)\n",
        "    return distillation_loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHr8sBA_N-zJ"
      },
      "source": [
        "### TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cefaruFe3_DP"
      },
      "source": [
        "def test(testloader, iteration, network, acc):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  print(\"ITERATION: \", iteration)\n",
        "  \n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "          # calculate outputs by running images through the network\n",
        "          outputs = network.forward(images)\n",
        "\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "         \n",
        "        \n",
        "  acc.append(100*correct/total)\n",
        "  print(f'Accuracy of the network on the {iteration} iteration: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LxjrgyTOBWN"
      },
      "source": [
        "### EXECUTION "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bLZetGa4F_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfab98fe-0baf-4bcc-c50f-d30f47dde78c"
      },
      "source": [
        "# divided our dataset into sample of 10 classes each\n",
        "# train the network on the first 10 classes\n",
        "# evaluate the network on the first 10 classes\n",
        "# train the network on the second 10 classes (adding 10 output layers)\n",
        "# evaluate the network on the first 20 classes\n",
        "iterations= 10 \n",
        "num_classes = 10\n",
        "test_set = [] #initialized here because we test over all the classes not only those one in which I train\n",
        "acc = []\n",
        "import random\n",
        "#indices = list(range(0,100))\n",
        "#random.shuffle(indices)\n",
        "for i in range(iterations):\n",
        "  classes_current_iter = range(i*num_classes, i*num_classes+num_classes)\n",
        "  train_iter = [] \n",
        "  for j in range(len(trainset)):\n",
        "    if(trainset[j][-1] in classes_current_iter):\n",
        "      test_set.append(trainset[j]) \n",
        "      train_iter.append(trainset[j])\n",
        "\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_iter, shuffle = True, batch_size=batch_size, num_workers=2)\n",
        "  valid_loader = torch.utils.data.DataLoader(test_set, shuffle = True, batch_size = batch_size, num_workers=2) \n",
        "  training(train_loader, i, net, device, epochs, num_classes) # Train the network with 10 classes at a time\n",
        "\n",
        "  test(valid_loader, i, net, acc) # Test the network with all classes seen until this iteration"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,    20] loss: 2.223\n",
            "[1,    40] loss: 1.899\n",
            "[2,    20] loss: 1.718\n",
            "[2,    40] loss: 1.510\n",
            "[3,    20] loss: 1.360\n",
            "[3,    40] loss: 1.280\n",
            "[4,    20] loss: 1.198\n",
            "[4,    40] loss: 1.165\n",
            "[5,    20] loss: 1.144\n",
            "[5,    40] loss: 1.092\n",
            "[6,    20] loss: 1.029\n",
            "[6,    40] loss: 0.933\n",
            "[7,    20] loss: 0.884\n",
            "[7,    40] loss: 0.814\n",
            "[8,    20] loss: 0.759\n",
            "[8,    40] loss: 0.756\n",
            "[9,    20] loss: 0.754\n",
            "[9,    40] loss: 0.709\n",
            "[10,    20] loss: 0.718\n",
            "[10,    40] loss: 0.649\n",
            "ITERATION:  0\n",
            "Accuracy of the network on the 0 iteration: 71 %\n",
            "[1,    20] loss: 4.656\n",
            "[1,    40] loss: 3.845\n",
            "[2,    20] loss: 3.631\n",
            "[2,    40] loss: 3.483\n",
            "[3,    20] loss: 3.408\n",
            "[3,    40] loss: 3.311\n",
            "[4,    20] loss: 3.272\n",
            "[4,    40] loss: 3.201\n",
            "[5,    20] loss: 3.217\n",
            "[5,    40] loss: 3.201\n",
            "[6,    20] loss: 3.154\n",
            "[6,    40] loss: 3.097\n",
            "[7,    20] loss: 3.062\n",
            "[7,    40] loss: 3.012\n",
            "[8,    20] loss: 2.891\n",
            "[8,    40] loss: 2.927\n",
            "[9,    20] loss: 2.915\n",
            "[9,    40] loss: 2.980\n",
            "[10,    20] loss: 2.967\n",
            "[10,    40] loss: 2.883\n",
            "ITERATION:  1\n",
            "Accuracy of the network on the 1 iteration: 46 %\n",
            "[1,    20] loss: 6.201\n",
            "[1,    40] loss: 5.087\n",
            "[2,    20] loss: 4.565\n",
            "[2,    40] loss: 4.357\n",
            "[3,    20] loss: 4.142\n",
            "[3,    40] loss: 4.081\n",
            "[4,    20] loss: 3.993\n",
            "[4,    40] loss: 3.937\n",
            "[5,    20] loss: 3.848\n",
            "[5,    40] loss: 3.744\n",
            "[6,    20] loss: 3.678\n",
            "[6,    40] loss: 3.645\n",
            "[7,    20] loss: 3.639\n",
            "[7,    40] loss: 3.572\n",
            "[8,    20] loss: 3.451\n",
            "[8,    40] loss: 3.400\n",
            "[9,    20] loss: 3.302\n",
            "[9,    40] loss: 3.302\n",
            "[10,    20] loss: 3.365\n",
            "[10,    40] loss: 3.242\n",
            "ITERATION:  2\n",
            "Accuracy of the network on the 2 iteration: 29 %\n",
            "[1,    20] loss: 6.298\n",
            "[1,    40] loss: 4.896\n",
            "[2,    20] loss: 4.370\n",
            "[2,    40] loss: 4.089\n",
            "[3,    20] loss: 3.932\n",
            "[3,    40] loss: 3.829\n",
            "[4,    20] loss: 3.754\n",
            "[4,    40] loss: 3.635\n",
            "[5,    20] loss: 3.512\n",
            "[5,    40] loss: 3.525\n",
            "[6,    20] loss: 3.404\n",
            "[6,    40] loss: 3.397\n",
            "[7,    20] loss: 3.297\n",
            "[7,    40] loss: 3.420\n",
            "[8,    20] loss: 3.546\n",
            "[8,    40] loss: 3.236\n",
            "[9,    20] loss: 3.131\n",
            "[9,    40] loss: 3.043\n",
            "[10,    20] loss: 3.007\n",
            "[10,    40] loss: 2.957\n",
            "ITERATION:  3\n",
            "Accuracy of the network on the 3 iteration: 17 %\n",
            "[1,    20] loss: 7.175\n",
            "[1,    40] loss: 5.586\n",
            "[2,    20] loss: 4.860\n",
            "[2,    40] loss: 4.588\n",
            "[3,    20] loss: 4.233\n",
            "[3,    40] loss: 4.243\n",
            "[4,    20] loss: 4.115\n",
            "[4,    40] loss: 3.937\n",
            "[5,    20] loss: 3.792\n",
            "[5,    40] loss: 3.854\n",
            "[6,    20] loss: 3.733\n",
            "[6,    40] loss: 3.593\n",
            "[7,    20] loss: 3.583\n",
            "[7,    40] loss: 3.522\n",
            "[8,    20] loss: 3.451\n",
            "[8,    40] loss: 3.459\n",
            "[9,    20] loss: 3.483\n",
            "[9,    40] loss: 3.330\n",
            "[10,    20] loss: 3.275\n",
            "[10,    40] loss: 3.180\n",
            "ITERATION:  4\n",
            "Accuracy of the network on the 4 iteration: 13 %\n",
            "[1,    20] loss: 6.239\n",
            "[1,    40] loss: 4.987\n",
            "[2,    20] loss: 4.239\n",
            "[2,    40] loss: 4.095\n",
            "[3,    20] loss: 3.844\n",
            "[3,    40] loss: 3.731\n",
            "[4,    20] loss: 3.510\n",
            "[4,    40] loss: 3.484\n",
            "[5,    20] loss: 3.258\n",
            "[5,    40] loss: 3.247\n",
            "[6,    20] loss: 3.171\n",
            "[6,    40] loss: 3.038\n",
            "[7,    20] loss: 2.984\n",
            "[7,    40] loss: 2.986\n",
            "[8,    20] loss: 2.875\n",
            "[8,    40] loss: 2.802\n",
            "[9,    20] loss: 2.837\n",
            "[9,    40] loss: 2.741\n",
            "[10,    20] loss: 2.765\n",
            "[10,    40] loss: 2.678\n",
            "ITERATION:  5\n",
            "Accuracy of the network on the 5 iteration: 12 %\n",
            "[1,    20] loss: 6.949\n",
            "[1,    40] loss: 5.129\n",
            "[2,    20] loss: 4.493\n",
            "[2,    40] loss: 4.233\n",
            "[3,    20] loss: 3.962\n",
            "[3,    40] loss: 3.839\n",
            "[4,    20] loss: 3.664\n",
            "[4,    40] loss: 3.547\n",
            "[5,    20] loss: 3.470\n",
            "[5,    40] loss: 3.531\n",
            "[6,    20] loss: 3.333\n",
            "[6,    40] loss: 3.287\n",
            "[7,    20] loss: 3.161\n",
            "[7,    40] loss: 3.130\n",
            "[8,    20] loss: 3.051\n",
            "[8,    40] loss: 3.020\n",
            "[9,    20] loss: 2.963\n",
            "[9,    40] loss: 2.847\n",
            "[10,    20] loss: 2.746\n",
            "[10,    40] loss: 2.774\n",
            "ITERATION:  6\n",
            "Accuracy of the network on the 6 iteration: 11 %\n",
            "[1,    20] loss: 6.954\n",
            "[1,    40] loss: 5.651\n",
            "[2,    20] loss: 4.946\n",
            "[2,    40] loss: 4.684\n",
            "[3,    20] loss: 4.417\n",
            "[3,    40] loss: 4.187\n",
            "[4,    20] loss: 4.068\n",
            "[4,    40] loss: 3.907\n",
            "[5,    20] loss: 3.801\n",
            "[5,    40] loss: 3.863\n",
            "[6,    20] loss: 3.595\n",
            "[6,    40] loss: 3.524\n",
            "[7,    20] loss: 3.293\n",
            "[7,    40] loss: 3.423\n",
            "[8,    20] loss: 3.259\n",
            "[8,    40] loss: 3.290\n",
            "[9,    20] loss: 3.329\n",
            "[9,    40] loss: 3.233\n",
            "[10,    20] loss: 3.231\n",
            "[10,    40] loss: 3.162\n",
            "ITERATION:  7\n",
            "Accuracy of the network on the 7 iteration: 10 %\n",
            "[1,    20] loss: 8.218\n",
            "[1,    40] loss: 6.646\n",
            "[2,    20] loss: 5.657\n",
            "[2,    40] loss: 5.255\n",
            "[3,    20] loss: 4.893\n",
            "[3,    40] loss: 4.693\n",
            "[4,    20] loss: 4.500\n",
            "[4,    40] loss: 4.415\n",
            "[5,    20] loss: 4.110\n",
            "[5,    40] loss: 4.221\n",
            "[6,    20] loss: 4.009\n",
            "[6,    40] loss: 3.987\n",
            "[7,    20] loss: 3.793\n",
            "[7,    40] loss: 3.859\n",
            "[8,    20] loss: 3.671\n",
            "[8,    40] loss: 3.665\n",
            "[9,    20] loss: 3.531\n",
            "[9,    40] loss: 3.420\n",
            "[10,    20] loss: 3.408\n",
            "[10,    40] loss: 3.360\n",
            "ITERATION:  8\n",
            "Accuracy of the network on the 8 iteration: 10 %\n",
            "[1,    20] loss: 7.780\n",
            "[1,    40] loss: 6.092\n",
            "[2,    20] loss: 5.348\n",
            "[2,    40] loss: 4.997\n",
            "[3,    20] loss: 4.602\n",
            "[3,    40] loss: 4.622\n",
            "[4,    20] loss: 4.324\n",
            "[4,    40] loss: 4.245\n",
            "[5,    20] loss: 3.990\n",
            "[5,    40] loss: 3.835\n",
            "[6,    20] loss: 3.842\n",
            "[6,    40] loss: 3.749\n",
            "[7,    20] loss: 3.573\n",
            "[7,    40] loss: 3.523\n",
            "[8,    20] loss: 3.565\n",
            "[8,    40] loss: 3.511\n",
            "[9,    20] loss: 3.335\n",
            "[9,    40] loss: 3.351\n",
            "[10,    20] loss: 3.207\n",
            "[10,    40] loss: 3.268\n",
            "ITERATION:  9\n",
            "Accuracy of the network on the 9 iteration: 8 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjQZSybAOH9C"
      },
      "source": [
        "### CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXP0RxiQ50mm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "ff012655-c2ae-44c3-b23b-fcf3503fd8c8"
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "confusion_matrix = torch.zeros(100,100)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in valid_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net.forward(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "          confusion_matrix[t.long(),p.long()] += 1\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 9 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kxkDFX6DXo6"
      },
      "source": [
        "### RANDOM CLASSES\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKplU4ByDZRv"
      },
      "source": [
        "DATA LOADER\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QleCp90XDa2a",
        "outputId": "cc3ca4d1-25a7-476d-8d0f-36f26cdd5fce"
      },
      "source": [
        "# we build a transform to normalize images: Data normalization is an important step which ensures \n",
        "# each input parameter (pixel, in this case) has a similar data distribution. This makes convergence \n",
        "# faster while training the network.\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "batch_size = 128\n",
        "\n",
        "trainset_raw = torchvision.datasets.CIFAR100(root='./data', train=True, \n",
        "                                         download=True, transform=transform)\n",
        "\n",
        "for i in range(len(trainset_raw)):\n",
        "  if(i==0):\n",
        "    trainset = [[trainset_raw[i][0], trainset_raw[i][1]]]\n",
        "  else:\n",
        "    trainset.append([trainset_raw[i][0], trainset_raw[i][1]])\n",
        "\n",
        "\n",
        "# DataLoader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
        "# batch_size = how many samples per batch to load\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jU3qySqDivk"
      },
      "source": [
        "EXECUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKHoN8W9EJBw"
      },
      "source": [
        "import random\n",
        "indices = list(range(0,100))\n",
        "random.shuffle(indices)\n",
        "dict_classes = dict(zip(indices,range(100)))\n",
        "for i in range(len(trainset)):\n",
        "  trainset[i][1] = dict_classes[trainset[i][1]]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INaUfRGBDkPP",
        "outputId": "c3dd0588-7f50-412d-da76-b69395209db4"
      },
      "source": [
        "#TRYING TO RANDOMIZE CLASSES\n",
        "\n",
        "\n",
        "# divided our dataset into sample of 10 classes each\n",
        "# train the network on the first 10 classes\n",
        "# evaluate the network on the first 10 classes\n",
        "# train the network on the second 10 classes (adding 10 output layers)\n",
        "# evaluate the network on the first 20 classes\n",
        "iterations = 10\n",
        "num_classes = 10\n",
        "test_set = [] #initialized here because we test over all the classes not only those one in which I train\n",
        "acc = []\n",
        "#import random\n",
        "#indices = list(range(0,100))\n",
        "#random.shuffle(indices)\n",
        "for i in range(iterations):\n",
        "  #classes_current_iter = dict(zip(indices[i*num_classes : i*num_classes+num_classes],range(i*num_classes,i*num_classes+num_classes)))\n",
        "  # print(classes_current_iter)\n",
        "  classes_current_iter = range(i*num_classes, i*num_classes+num_classes)\n",
        "  train_iter = []\n",
        "  for j in range(len(trainset)):\n",
        "    #if(trainset[j][-1] in classes_current_iter.keys()):\n",
        "      #trainset[j][-1] = classes_current_iter[trainset[j][-1]]\n",
        "    if(trainset[j][-1] in classes_current_iter):\n",
        "      test_set.append(trainset[j]) \n",
        "      train_iter.append(trainset[j])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_iter, shuffle = True, batch_size=batch_size, num_workers=2)\n",
        "  valid_loader = torch.utils.data.DataLoader(test_set, shuffle = True, batch_size = batch_size, num_workers=2) \n",
        "  print(\"Train the network, iteration: \", i, \" on classes: \", classes_current_iter)\n",
        "  training(train_loader, i, net, device, epochs, num_classes) # Train the network with 10 classes at a time\n",
        "  #print(\"Train_loader length: \",len(train_loader))\n",
        "  #print(\"valid_loader length: \",len(valid_loader))\n",
        "  test(valid_loader, i, net, acc) # Test the network with all classes seen until this iteration"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train the network, iteration:  0  on classes:  range(0, 10)\n",
            "[1,    20] loss: 2.205\n",
            "[1,    40] loss: 1.993\n",
            "[2,    20] loss: 1.893\n",
            "[2,    40] loss: 1.810\n",
            "[3,    20] loss: 1.758\n",
            "[3,    40] loss: 1.643\n",
            "[4,    20] loss: 1.622\n",
            "[4,    40] loss: 1.452\n",
            "[5,    20] loss: 1.475\n",
            "[5,    40] loss: 1.344\n",
            "[6,    20] loss: 1.302\n",
            "[6,    40] loss: 1.268\n",
            "[7,    20] loss: 1.291\n",
            "[7,    40] loss: 1.203\n",
            "[8,    20] loss: 1.150\n",
            "[8,    40] loss: 1.112\n",
            "[9,    20] loss: 1.317\n",
            "[9,    40] loss: 1.157\n",
            "[10,    20] loss: 1.023\n",
            "[10,    40] loss: 0.954\n",
            "[11,    20] loss: 1.066\n",
            "[11,    40] loss: 0.894\n",
            "[12,    20] loss: 0.884\n",
            "[12,    40] loss: 0.842\n",
            "[13,    20] loss: 0.940\n",
            "[13,    40] loss: 0.831\n",
            "[14,    20] loss: 0.753\n",
            "[14,    40] loss: 0.735\n",
            "[15,    20] loss: 0.638\n",
            "[15,    40] loss: 0.668\n",
            "[16,    20] loss: 1.055\n",
            "[16,    40] loss: 0.845\n",
            "[17,    20] loss: 0.708\n",
            "[17,    40] loss: 0.613\n",
            "[18,    20] loss: 0.964\n",
            "[18,    40] loss: 0.745\n",
            "[19,    20] loss: 0.642\n",
            "[19,    40] loss: 0.511\n",
            "[20,    20] loss: 0.421\n",
            "[20,    40] loss: 0.408\n",
            "[21,    20] loss: 0.396\n",
            "[21,    40] loss: 0.383\n",
            "[22,    20] loss: 0.585\n",
            "[22,    40] loss: 0.448\n",
            "[23,    20] loss: 0.339\n",
            "[23,    40] loss: 0.288\n",
            "[24,    20] loss: 0.347\n",
            "[24,    40] loss: 0.286\n",
            "[25,    20] loss: 0.155\n",
            "[25,    40] loss: 0.133\n",
            "[26,    20] loss: 0.162\n",
            "[26,    40] loss: 0.135\n",
            "[27,    20] loss: 0.267\n",
            "[27,    40] loss: 0.262\n",
            "[28,    20] loss: 0.508\n",
            "[28,    40] loss: 0.393\n",
            "[29,    20] loss: 0.348\n",
            "[29,    40] loss: 0.270\n",
            "[30,    20] loss: 0.371\n",
            "[30,    40] loss: 0.267\n",
            "ITERATION:  0\n",
            "Accuracy of the network on the 0 iteration: 86 %\n",
            "Train the network, iteration:  1  on classes:  range(10, 20)\n",
            "[1,    20] loss: 6.636\n",
            "[1,    40] loss: 5.130\n",
            "[2,    20] loss: 4.434\n",
            "[2,    40] loss: 4.212\n",
            "[3,    20] loss: 4.031\n",
            "[3,    40] loss: 3.840\n",
            "[4,    20] loss: 3.695\n",
            "[4,    40] loss: 3.647\n",
            "[5,    20] loss: 3.558\n",
            "[5,    40] loss: 3.520\n",
            "[6,    20] loss: 3.353\n",
            "[6,    40] loss: 3.294\n",
            "[7,    20] loss: 3.167\n",
            "[7,    40] loss: 3.182\n",
            "[8,    20] loss: 3.097\n",
            "[8,    40] loss: 3.120\n",
            "[9,    20] loss: 3.131\n",
            "[9,    40] loss: 3.041\n",
            "[10,    20] loss: 2.985\n",
            "[10,    40] loss: 2.961\n",
            "[11,    20] loss: 2.894\n",
            "[11,    40] loss: 2.897\n",
            "[12,    20] loss: 2.824\n",
            "[12,    40] loss: 2.860\n",
            "[13,    20] loss: 2.791\n",
            "[13,    40] loss: 2.736\n",
            "[14,    20] loss: 2.612\n",
            "[14,    40] loss: 2.620\n",
            "[15,    20] loss: 2.643\n",
            "[15,    40] loss: 2.621\n",
            "[16,    20] loss: 2.625\n",
            "[16,    40] loss: 2.627\n",
            "[17,    20] loss: 2.853\n",
            "[17,    40] loss: 2.727\n",
            "[18,    20] loss: 2.681\n",
            "[18,    40] loss: 2.577\n",
            "[19,    20] loss: 2.538\n",
            "[19,    40] loss: 2.504\n",
            "[20,    20] loss: 2.529\n",
            "[20,    40] loss: 2.517\n",
            "[21,    20] loss: 2.476\n",
            "[21,    40] loss: 2.400\n",
            "[22,    20] loss: 2.347\n",
            "[22,    40] loss: 2.354\n",
            "[23,    20] loss: 2.388\n",
            "[23,    40] loss: 2.340\n",
            "[24,    20] loss: 2.274\n",
            "[24,    40] loss: 2.298\n",
            "[25,    20] loss: 2.292\n",
            "[25,    40] loss: 2.251\n",
            "[26,    20] loss: 2.251\n",
            "[26,    40] loss: 2.237\n",
            "[27,    20] loss: 2.282\n",
            "[27,    40] loss: 2.239\n",
            "[28,    20] loss: 2.317\n",
            "[28,    40] loss: 2.223\n",
            "[29,    20] loss: 2.216\n",
            "[29,    40] loss: 2.214\n",
            "[30,    20] loss: 2.254\n",
            "[30,    40] loss: 2.207\n",
            "ITERATION:  1\n",
            "Accuracy of the network on the 1 iteration: 54 %\n",
            "Train the network, iteration:  2  on classes:  range(20, 30)\n",
            "[1,    20] loss: 7.091\n",
            "[1,    40] loss: 5.467\n",
            "[2,    20] loss: 4.617\n",
            "[2,    40] loss: 4.211\n",
            "[3,    20] loss: 3.891\n",
            "[3,    40] loss: 3.909\n",
            "[4,    20] loss: 3.749\n",
            "[4,    40] loss: 3.674\n",
            "[5,    20] loss: 3.543\n",
            "[5,    40] loss: 3.450\n",
            "[6,    20] loss: 3.367\n",
            "[6,    40] loss: 3.344\n",
            "[7,    20] loss: 3.273\n",
            "[7,    40] loss: 3.190\n",
            "[8,    20] loss: 3.103\n",
            "[8,    40] loss: 3.099\n",
            "[9,    20] loss: 3.076\n",
            "[9,    40] loss: 3.102\n",
            "[10,    20] loss: 2.959\n",
            "[10,    40] loss: 2.994\n",
            "[11,    20] loss: 2.902\n",
            "[11,    40] loss: 2.869\n",
            "[12,    20] loss: 2.784\n",
            "[12,    40] loss: 2.762\n",
            "[13,    20] loss: 2.750\n",
            "[13,    40] loss: 2.775\n",
            "[14,    20] loss: 2.883\n",
            "[14,    40] loss: 2.852\n",
            "[15,    20] loss: 2.785\n",
            "[15,    40] loss: 2.766\n",
            "[16,    20] loss: 2.804\n",
            "[16,    40] loss: 2.678\n",
            "[17,    20] loss: 2.641\n",
            "[17,    40] loss: 2.648\n",
            "[18,    20] loss: 2.638\n",
            "[18,    40] loss: 2.672\n",
            "[19,    20] loss: 2.674\n",
            "[19,    40] loss: 2.663\n",
            "[20,    20] loss: 2.631\n",
            "[20,    40] loss: 2.587\n",
            "[21,    20] loss: 2.647\n",
            "[21,    40] loss: 2.587\n",
            "[22,    20] loss: 2.549\n",
            "[22,    40] loss: 2.604\n",
            "[23,    20] loss: 2.629\n",
            "[23,    40] loss: 2.520\n",
            "[24,    20] loss: 2.492\n",
            "[24,    40] loss: 2.442\n",
            "[25,    20] loss: 2.451\n",
            "[25,    40] loss: 2.420\n",
            "[26,    20] loss: 2.365\n",
            "[26,    40] loss: 2.448\n",
            "[27,    20] loss: 2.551\n",
            "[27,    40] loss: 2.430\n",
            "[28,    20] loss: 2.464\n",
            "[28,    40] loss: 2.448\n",
            "[29,    20] loss: 2.506\n",
            "[29,    40] loss: 2.510\n",
            "[30,    20] loss: 2.490\n",
            "[30,    40] loss: 2.443\n",
            "ITERATION:  2\n",
            "Accuracy of the network on the 2 iteration: 36 %\n",
            "Train the network, iteration:  3  on classes:  range(30, 40)\n",
            "[1,    20] loss: 7.569\n",
            "[1,    40] loss: 6.170\n",
            "[2,    20] loss: 5.306\n",
            "[2,    40] loss: 4.978\n",
            "[3,    20] loss: 4.544\n",
            "[3,    40] loss: 4.467\n",
            "[4,    20] loss: 4.173\n",
            "[4,    40] loss: 4.125\n",
            "[5,    20] loss: 3.946\n",
            "[5,    40] loss: 3.868\n",
            "[6,    20] loss: 3.771\n",
            "[6,    40] loss: 3.720\n",
            "[7,    20] loss: 3.476\n",
            "[7,    40] loss: 3.499\n",
            "[8,    20] loss: 3.365\n",
            "[8,    40] loss: 3.323\n",
            "[9,    20] loss: 3.148\n",
            "[9,    40] loss: 3.184\n",
            "[10,    20] loss: 3.446\n",
            "[10,    40] loss: 3.332\n",
            "[11,    20] loss: 3.227\n",
            "[11,    40] loss: 3.075\n",
            "[12,    20] loss: 3.019\n",
            "[12,    40] loss: 2.984\n",
            "[13,    20] loss: 2.962\n",
            "[13,    40] loss: 2.955\n",
            "[14,    20] loss: 2.921\n",
            "[14,    40] loss: 2.880\n",
            "[15,    20] loss: 2.804\n",
            "[15,    40] loss: 2.793\n",
            "[16,    20] loss: 2.863\n",
            "[16,    40] loss: 2.868\n",
            "[17,    20] loss: 2.903\n",
            "[17,    40] loss: 2.846\n",
            "[18,    20] loss: 2.891\n",
            "[18,    40] loss: 2.800\n",
            "[19,    20] loss: 2.684\n",
            "[19,    40] loss: 2.687\n",
            "[20,    20] loss: 2.605\n",
            "[20,    40] loss: 2.693\n",
            "[21,    20] loss: 2.902\n",
            "[21,    40] loss: 2.734\n",
            "[22,    20] loss: 2.544\n",
            "[22,    40] loss: 2.558\n",
            "[23,    20] loss: 2.541\n",
            "[23,    40] loss: 2.546\n",
            "[24,    20] loss: 2.559\n",
            "[24,    40] loss: 2.531\n",
            "[25,    20] loss: 2.575\n",
            "[25,    40] loss: 2.545\n",
            "[26,    20] loss: 2.559\n",
            "[26,    40] loss: 2.528\n",
            "[27,    20] loss: 2.519\n",
            "[27,    40] loss: 2.490\n",
            "[28,    20] loss: 2.488\n",
            "[28,    40] loss: 2.436\n",
            "[29,    20] loss: 2.486\n",
            "[29,    40] loss: 2.472\n",
            "[30,    20] loss: 2.506\n",
            "[30,    40] loss: 2.441\n",
            "ITERATION:  3\n",
            "Accuracy of the network on the 3 iteration: 27 %\n",
            "Train the network, iteration:  4  on classes:  range(40, 50)\n",
            "[1,    20] loss: 6.044\n",
            "[1,    40] loss: 5.088\n",
            "[2,    20] loss: 4.478\n",
            "[2,    40] loss: 4.278\n",
            "[3,    20] loss: 3.890\n",
            "[3,    40] loss: 3.830\n",
            "[4,    20] loss: 3.621\n",
            "[4,    40] loss: 3.607\n",
            "[5,    20] loss: 3.349\n",
            "[5,    40] loss: 3.411\n",
            "[6,    20] loss: 3.341\n",
            "[6,    40] loss: 3.300\n",
            "[7,    20] loss: 3.176\n",
            "[7,    40] loss: 3.191\n",
            "[8,    20] loss: 3.190\n",
            "[8,    40] loss: 3.176\n",
            "[9,    20] loss: 3.104\n",
            "[9,    40] loss: 3.062\n",
            "[10,    20] loss: 2.998\n",
            "[10,    40] loss: 3.024\n",
            "[11,    20] loss: 2.882\n",
            "[11,    40] loss: 2.828\n",
            "[12,    20] loss: 2.726\n",
            "[12,    40] loss: 2.826\n",
            "[13,    20] loss: 2.886\n",
            "[13,    40] loss: 2.804\n",
            "[14,    20] loss: 2.741\n",
            "[14,    40] loss: 2.774\n",
            "[15,    20] loss: 2.857\n",
            "[15,    40] loss: 2.807\n",
            "[16,    20] loss: 2.841\n",
            "[16,    40] loss: 2.795\n",
            "[17,    20] loss: 2.654\n",
            "[17,    40] loss: 2.704\n",
            "[18,    20] loss: 2.804\n",
            "[18,    40] loss: 2.791\n",
            "[19,    20] loss: 2.697\n",
            "[19,    40] loss: 2.587\n",
            "[20,    20] loss: 2.557\n",
            "[20,    40] loss: 2.529\n",
            "[21,    20] loss: 2.511\n",
            "[21,    40] loss: 2.605\n",
            "[22,    20] loss: 2.683\n",
            "[22,    40] loss: 2.632\n",
            "[23,    20] loss: 2.687\n",
            "[23,    40] loss: 2.571\n",
            "[24,    20] loss: 2.608\n",
            "[24,    40] loss: 2.579\n",
            "[25,    20] loss: 2.620\n",
            "[25,    40] loss: 2.524\n",
            "[26,    20] loss: 2.442\n",
            "[26,    40] loss: 2.546\n",
            "[27,    20] loss: 2.582\n",
            "[27,    40] loss: 2.560\n",
            "[28,    20] loss: 2.500\n",
            "[28,    40] loss: 2.577\n",
            "[29,    20] loss: 2.610\n",
            "[29,    40] loss: 2.573\n",
            "[30,    20] loss: 2.501\n",
            "[30,    40] loss: 2.579\n",
            "ITERATION:  4\n",
            "Accuracy of the network on the 4 iteration: 21 %\n",
            "Train the network, iteration:  5  on classes:  range(50, 60)\n",
            "[1,    20] loss: 6.565\n",
            "[1,    40] loss: 5.353\n",
            "[2,    20] loss: 4.555\n",
            "[2,    40] loss: 4.327\n",
            "[3,    20] loss: 3.988\n",
            "[3,    40] loss: 3.829\n",
            "[4,    20] loss: 3.577\n",
            "[4,    40] loss: 3.587\n",
            "[5,    20] loss: 3.378\n",
            "[5,    40] loss: 3.365\n",
            "[6,    20] loss: 3.141\n",
            "[6,    40] loss: 3.175\n",
            "[7,    20] loss: 3.073\n",
            "[7,    40] loss: 3.035\n",
            "[8,    20] loss: 2.980\n",
            "[8,    40] loss: 3.035\n",
            "[9,    20] loss: 2.849\n",
            "[9,    40] loss: 2.843\n",
            "[10,    20] loss: 2.856\n",
            "[10,    40] loss: 2.866\n",
            "[11,    20] loss: 2.907\n",
            "[11,    40] loss: 2.796\n",
            "[12,    20] loss: 2.677\n",
            "[12,    40] loss: 2.771\n",
            "[13,    20] loss: 2.796\n",
            "[13,    40] loss: 2.705\n",
            "[14,    20] loss: 2.645\n",
            "[14,    40] loss: 2.644\n",
            "[15,    20] loss: 2.567\n",
            "[15,    40] loss: 2.672\n",
            "[16,    20] loss: 2.661\n",
            "[16,    40] loss: 2.565\n",
            "[17,    20] loss: 2.510\n",
            "[17,    40] loss: 2.590\n",
            "[18,    20] loss: 2.553\n",
            "[18,    40] loss: 2.474\n",
            "[19,    20] loss: 2.405\n",
            "[19,    40] loss: 2.512\n",
            "[20,    20] loss: 2.423\n",
            "[20,    40] loss: 2.497\n",
            "[21,    20] loss: 2.484\n",
            "[21,    40] loss: 2.459\n",
            "[22,    20] loss: 2.530\n",
            "[22,    40] loss: 2.500\n",
            "[23,    20] loss: 2.564\n",
            "[23,    40] loss: 2.558\n",
            "[24,    20] loss: 2.497\n",
            "[24,    40] loss: 2.517\n",
            "[25,    20] loss: 2.483\n",
            "[25,    40] loss: 2.460\n",
            "[26,    20] loss: 2.338\n",
            "[26,    40] loss: 2.398\n",
            "[27,    20] loss: 2.333\n",
            "[27,    40] loss: 2.326\n",
            "[28,    20] loss: 2.318\n",
            "[28,    40] loss: 2.268\n",
            "[29,    20] loss: 2.334\n",
            "[29,    40] loss: 2.347\n",
            "[30,    20] loss: 2.369\n",
            "[30,    40] loss: 2.382\n",
            "ITERATION:  5\n",
            "Accuracy of the network on the 5 iteration: 15 %\n",
            "Train the network, iteration:  6  on classes:  range(60, 70)\n",
            "[1,    20] loss: 7.448\n",
            "[1,    40] loss: 5.608\n",
            "[2,    20] loss: 4.824\n",
            "[2,    40] loss: 4.474\n",
            "[3,    20] loss: 4.166\n",
            "[3,    40] loss: 4.147\n",
            "[4,    20] loss: 3.944\n",
            "[4,    40] loss: 3.867\n",
            "[5,    20] loss: 3.619\n",
            "[5,    40] loss: 3.566\n",
            "[6,    20] loss: 3.429\n",
            "[6,    40] loss: 3.476\n",
            "[7,    20] loss: 3.301\n",
            "[7,    40] loss: 3.358\n",
            "[8,    20] loss: 3.108\n",
            "[8,    40] loss: 3.172\n",
            "[9,    20] loss: 3.078\n",
            "[9,    40] loss: 3.053\n",
            "[10,    20] loss: 3.022\n",
            "[10,    40] loss: 3.013\n",
            "[11,    20] loss: 2.924\n",
            "[11,    40] loss: 2.883\n",
            "[12,    20] loss: 2.840\n",
            "[12,    40] loss: 2.864\n",
            "[13,    20] loss: 2.796\n",
            "[13,    40] loss: 2.753\n",
            "[14,    20] loss: 2.769\n",
            "[14,    40] loss: 2.810\n",
            "[15,    20] loss: 2.893\n",
            "[15,    40] loss: 2.713\n",
            "[16,    20] loss: 2.662\n",
            "[16,    40] loss: 2.689\n",
            "[17,    20] loss: 2.580\n",
            "[17,    40] loss: 2.758\n",
            "[18,    20] loss: 2.703\n",
            "[18,    40] loss: 2.714\n",
            "[19,    20] loss: 2.676\n",
            "[19,    40] loss: 2.651\n",
            "[20,    20] loss: 2.624\n",
            "[20,    40] loss: 2.640\n",
            "[21,    20] loss: 2.719\n",
            "[21,    40] loss: 2.630\n",
            "[22,    20] loss: 2.586\n",
            "[22,    40] loss: 2.554\n",
            "[23,    20] loss: 2.462\n",
            "[23,    40] loss: 2.482\n",
            "[24,    20] loss: 2.504\n",
            "[24,    40] loss: 2.537\n",
            "[25,    20] loss: 2.498\n",
            "[25,    40] loss: 2.509\n",
            "[26,    20] loss: 2.444\n",
            "[26,    40] loss: 2.474\n",
            "[27,    20] loss: 2.512\n",
            "[27,    40] loss: 2.393\n",
            "[28,    20] loss: 2.360\n",
            "[28,    40] loss: 2.420\n",
            "[29,    20] loss: 2.497\n",
            "[29,    40] loss: 2.417\n",
            "[30,    20] loss: 2.383\n",
            "[30,    40] loss: 2.358\n",
            "ITERATION:  6\n",
            "Accuracy of the network on the 6 iteration: 12 %\n",
            "Train the network, iteration:  7  on classes:  range(70, 80)\n",
            "[1,    20] loss: 7.444\n",
            "[1,    40] loss: 5.893\n",
            "[2,    20] loss: 4.880\n",
            "[2,    40] loss: 4.724\n",
            "[3,    20] loss: 4.185\n",
            "[3,    40] loss: 4.100\n",
            "[4,    20] loss: 3.776\n",
            "[4,    40] loss: 3.875\n",
            "[5,    20] loss: 3.546\n",
            "[5,    40] loss: 3.518\n",
            "[6,    20] loss: 3.273\n",
            "[6,    40] loss: 3.373\n",
            "[7,    20] loss: 3.140\n",
            "[7,    40] loss: 3.104\n",
            "[8,    20] loss: 3.106\n",
            "[8,    40] loss: 3.062\n",
            "[9,    20] loss: 3.049\n",
            "[9,    40] loss: 3.048\n",
            "[10,    20] loss: 3.009\n",
            "[10,    40] loss: 3.095\n",
            "[11,    20] loss: 3.009\n",
            "[11,    40] loss: 3.003\n",
            "[12,    20] loss: 2.808\n",
            "[12,    40] loss: 2.800\n",
            "[13,    20] loss: 2.800\n",
            "[13,    40] loss: 2.799\n",
            "[14,    20] loss: 2.835\n",
            "[14,    40] loss: 2.829\n",
            "[15,    20] loss: 2.714\n",
            "[15,    40] loss: 2.835\n",
            "[16,    20] loss: 2.934\n",
            "[16,    40] loss: 2.882\n",
            "[17,    20] loss: 2.689\n",
            "[17,    40] loss: 2.662\n",
            "[18,    20] loss: 2.559\n",
            "[18,    40] loss: 2.646\n",
            "[19,    20] loss: 2.570\n",
            "[19,    40] loss: 2.671\n",
            "[20,    20] loss: 2.673\n",
            "[20,    40] loss: 2.715\n",
            "[21,    20] loss: 2.470\n",
            "[21,    40] loss: 2.625\n",
            "[22,    20] loss: 2.542\n",
            "[22,    40] loss: 2.562\n",
            "[23,    20] loss: 2.683\n",
            "[23,    40] loss: 2.635\n",
            "[24,    20] loss: 2.631\n",
            "[24,    40] loss: 2.661\n",
            "[25,    20] loss: 2.617\n",
            "[25,    40] loss: 2.641\n",
            "[26,    20] loss: 2.566\n",
            "[26,    40] loss: 2.580\n",
            "[27,    20] loss: 2.537\n",
            "[27,    40] loss: 2.605\n",
            "[28,    20] loss: 2.584\n",
            "[28,    40] loss: 2.511\n",
            "[29,    20] loss: 2.477\n",
            "[29,    40] loss: 2.465\n",
            "[30,    20] loss: 2.474\n",
            "[30,    40] loss: 2.470\n",
            "ITERATION:  7\n",
            "Accuracy of the network on the 7 iteration: 11 %\n",
            "Train the network, iteration:  8  on classes:  range(80, 90)\n",
            "[1,    20] loss: 8.530\n",
            "[1,    40] loss: 6.696\n",
            "[2,    20] loss: 5.526\n",
            "[2,    40] loss: 5.216\n",
            "[3,    20] loss: 4.857\n",
            "[3,    40] loss: 4.607\n",
            "[4,    20] loss: 4.214\n",
            "[4,    40] loss: 4.255\n",
            "[5,    20] loss: 3.958\n",
            "[5,    40] loss: 3.943\n",
            "[6,    20] loss: 3.625\n",
            "[6,    40] loss: 3.636\n",
            "[7,    20] loss: 3.465\n",
            "[7,    40] loss: 3.521\n",
            "[8,    20] loss: 3.397\n",
            "[8,    40] loss: 3.401\n",
            "[9,    20] loss: 3.229\n",
            "[9,    40] loss: 3.231\n",
            "[10,    20] loss: 3.244\n",
            "[10,    40] loss: 3.093\n",
            "[11,    20] loss: 3.022\n",
            "[11,    40] loss: 3.090\n",
            "[12,    20] loss: 3.055\n",
            "[12,    40] loss: 3.042\n",
            "[13,    20] loss: 2.962\n",
            "[13,    40] loss: 2.956\n",
            "[14,    20] loss: 2.834\n",
            "[14,    40] loss: 2.894\n",
            "[15,    20] loss: 2.843\n",
            "[15,    40] loss: 2.812\n",
            "[16,    20] loss: 2.731\n",
            "[16,    40] loss: 2.735\n",
            "[17,    20] loss: 2.675\n",
            "[17,    40] loss: 2.840\n",
            "[18,    20] loss: 2.731\n",
            "[18,    40] loss: 2.697\n",
            "[19,    20] loss: 2.673\n",
            "[19,    40] loss: 2.671\n",
            "[20,    20] loss: 2.715\n",
            "[20,    40] loss: 2.646\n",
            "[21,    20] loss: 2.565\n",
            "[21,    40] loss: 2.564\n",
            "[22,    20] loss: 2.531\n",
            "[22,    40] loss: 2.651\n",
            "[23,    20] loss: 2.712\n",
            "[23,    40] loss: 2.790\n",
            "[24,    20] loss: 2.828\n",
            "[24,    40] loss: 2.870\n",
            "[25,    20] loss: 2.744\n",
            "[25,    40] loss: 2.818\n",
            "[26,    20] loss: 2.796\n",
            "[26,    40] loss: 2.786\n",
            "[27,    20] loss: 2.596\n",
            "[27,    40] loss: 2.678\n",
            "[28,    20] loss: 2.645\n",
            "[28,    40] loss: 2.545\n",
            "[29,    20] loss: 2.569\n",
            "[29,    40] loss: 2.548\n",
            "[30,    20] loss: 2.558\n",
            "[30,    40] loss: 2.487\n",
            "ITERATION:  8\n",
            "Accuracy of the network on the 8 iteration: 10 %\n",
            "Train the network, iteration:  9  on classes:  range(90, 100)\n",
            "[1,    20] loss: 9.132\n",
            "[1,    40] loss: 6.537\n",
            "[2,    20] loss: 5.499\n",
            "[2,    40] loss: 5.175\n",
            "[3,    20] loss: 4.623\n",
            "[3,    40] loss: 4.489\n",
            "[4,    20] loss: 4.169\n",
            "[4,    40] loss: 4.126\n",
            "[5,    20] loss: 3.743\n",
            "[5,    40] loss: 3.733\n",
            "[6,    20] loss: 3.520\n",
            "[6,    40] loss: 3.578\n",
            "[7,    20] loss: 3.357\n",
            "[7,    40] loss: 3.446\n",
            "[8,    20] loss: 3.378\n",
            "[8,    40] loss: 3.411\n",
            "[9,    20] loss: 3.259\n",
            "[9,    40] loss: 3.220\n",
            "[10,    20] loss: 3.151\n",
            "[10,    40] loss: 3.103\n",
            "[11,    20] loss: 3.022\n",
            "[11,    40] loss: 3.108\n",
            "[12,    20] loss: 3.105\n",
            "[12,    40] loss: 2.997\n",
            "[13,    20] loss: 2.970\n",
            "[13,    40] loss: 2.972\n",
            "[14,    20] loss: 2.938\n",
            "[14,    40] loss: 3.037\n",
            "[15,    20] loss: 3.098\n",
            "[15,    40] loss: 3.065\n",
            "[16,    20] loss: 2.911\n",
            "[16,    40] loss: 2.874\n",
            "[17,    20] loss: 2.887\n",
            "[17,    40] loss: 2.868\n",
            "[18,    20] loss: 2.863\n",
            "[18,    40] loss: 2.867\n",
            "[19,    20] loss: 2.936\n",
            "[19,    40] loss: 2.839\n",
            "[20,    20] loss: 2.791\n",
            "[20,    40] loss: 2.810\n",
            "[21,    20] loss: 2.703\n",
            "[21,    40] loss: 2.825\n",
            "[22,    20] loss: 2.751\n",
            "[22,    40] loss: 2.701\n",
            "[23,    20] loss: 2.709\n",
            "[23,    40] loss: 2.763\n",
            "[24,    20] loss: 2.729\n",
            "[24,    40] loss: 2.714\n",
            "[25,    20] loss: 2.743\n",
            "[25,    40] loss: 2.713\n",
            "[26,    20] loss: 2.676\n",
            "[26,    40] loss: 2.650\n",
            "[27,    20] loss: 2.618\n",
            "[27,    40] loss: 2.664\n",
            "[28,    20] loss: 2.611\n",
            "[28,    40] loss: 2.646\n",
            "[29,    20] loss: 2.559\n",
            "[29,    40] loss: 2.607\n",
            "[30,    20] loss: 2.571\n",
            "[30,    40] loss: 2.666\n",
            "ITERATION:  9\n",
            "Accuracy of the network on the 9 iteration: 9 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}