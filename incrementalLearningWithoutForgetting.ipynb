{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "incrementalLearningWithoutForgetting.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Gzrd6SoDNp6d",
        "QObN0rGNN2Be"
      ],
      "authorship_tag": "ABX9TyMN7Juiju2ZNMMLBCMsEZ+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoManca-FM/Project_MLDL/blob/main/incrementalLearningWithoutForgetting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUHNE0YdXMlR"
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms \n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from torch.nn.init import xavier_uniform_ "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzrd6SoDNp6d"
      },
      "source": [
        "### DATA LOADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qgo5-aj4ScF",
        "outputId": "d731bbd5-1dfe-4265-c589-411e0c0b0e42"
      },
      "source": [
        "# we build a transform to normalize images: Data normalization is an important step which ensures \n",
        "# each input parameter (pixel, in this case) has a similar data distribution. This makes convergence \n",
        "# faster while training the network.\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, \n",
        "                                         download=True, transform=transform)\n",
        "# DataLoader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
        "# batch_size = how many samples per batch to load\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXdZrK0zNwvw"
      },
      "source": [
        "### NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kAHBaHJ4Td1"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        \n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        self.inplanes = 16\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def addOutputNodes(self, num_new_outputs):\n",
        "        in_features = self.fc.in_features\n",
        "        out_features = self.fc.out_features\n",
        "        weight = self.fc.weight.data\n",
        "\n",
        "        self.fc = nn.Linear(in_features, out_features + num_new_outputs)\n",
        "        xavier_uniform_(self.fc.weight)\n",
        "        self.fc.weight.data[:out_features] = weight\n",
        "        \n",
        "\n",
        "\n",
        "def resnet20(pretrained=False, **kwargs):\n",
        "    n = 3\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet32(pretrained=False, **kwargs):\n",
        "    n = 5\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet56(pretrained=False, **kwargs):\n",
        "    n = 9\n",
        "    model = ResNet(Bottleneck, [n, n, n], **kwargs)\n",
        "    return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1JeZ9NB4ZxS",
        "outputId": "581f1445-7c07-465f-abea-0cfd4e016de2"
      },
      "source": [
        "net = resnet32()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
        "net.to(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QObN0rGNN2Be"
      },
      "source": [
        "### LOSS & PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUMJGDLO4cR-"
      },
      "source": [
        "lr = 0.01\n",
        "decay = 0.0001\n",
        "epochs = 2\n",
        "momentum = 0.9\n",
        "factor = 5"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLf5BCR-4c3X"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.SGD(net.parameters(), lr = lr, weight_decay=decay,momentum= momentum)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te6KvRNKN5k_"
      },
      "source": [
        "### TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IbM6obCwqXH"
      },
      "source": [
        "def training(trainloader, iteration, network, device, epochs, num_classes):\n",
        "\n",
        "  if (iteration != 0):\n",
        "    # add 10 output nodes to the network\n",
        "    network.addOutputNodes(num_classes)\n",
        "    network.to(device)\n",
        "\n",
        "  old_net = copy.deepcopy(network)\n",
        "  \n",
        "  #train the network\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "      inputs = data[0].to(device)\n",
        "      labels = data[1].to(device)\n",
        "      \n",
        "      # Sets the gradients of all optimized torch.Tensor to zero.\n",
        "      optimizer.zero_grad() \n",
        "\n",
        "      # forward: assign weights to each edge in each layer\n",
        "      outputs = network.forward(inputs) \n",
        "\n",
        "       # calculate the classification loss \n",
        "      classification_loss = criterion(outputs,labels) \n",
        "\n",
        "      # calculate the distillation loss\n",
        "      distillation_loss = dist_loss(outputs, old_net, inputs) \n",
        "      \n",
        "      loss = classification_loss + distillation_loss\n",
        "\n",
        "      # redesign the weights evaluating the performance of the network\n",
        "      loss.backward() \n",
        "\n",
        "      # update parameters\n",
        "      optimizer.step()  \n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # print every 20 mini-batches the average value of the loss accumulated in each batch\n",
        "      if i % 20 == 19:    \n",
        "        print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 20))\n",
        "        running_loss = 0.0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agoNfyi00-NF"
      },
      "source": [
        " def dist_loss(outputs, old_net, inputs):\n",
        "    T = 2\n",
        "    out_old = torch.sigmoid(old_net.forward(inputs))\n",
        "    prob_t = F.softmax(out_old/T, dim=1)\n",
        "    log_prob_s = F.log_softmax(outputs/T, dim=1)\n",
        "    distillation_loss = -(prob_t*log_prob_s).sum(dim=1).mean()\n",
        "    return distillation_loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHr8sBA_N-zJ"
      },
      "source": [
        "### TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cefaruFe3_DP"
      },
      "source": [
        "def test(testloader, iteration, network, acc):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  print(\"ITERATION: \", iteration)\n",
        "  \n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "          # calculate outputs by running images through the network\n",
        "          outputs = network.forward(images)\n",
        "\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "         \n",
        "        \n",
        "  acc.append(100*correct/total)\n",
        "  print(f'Accuracy of the network on the {iteration} iteration: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LxjrgyTOBWN"
      },
      "source": [
        "### EXECUTION "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bLZetGa4F_o"
      },
      "source": [
        "# divided our dataset into sample of 10 classes each\n",
        "# train the network on the first 10 classes\n",
        "# evaluate the network on the first 10 classes\n",
        "# train the network on the second 10 classes (adding 10 output layers)\n",
        "# evaluate the network on the first 20 classes\n",
        "iterations= 10 \n",
        "num_classes = 10\n",
        "test_set = [] #initialized here because we test over all the classes not only those one in which I train\n",
        "acc = []\n",
        "import random\n",
        "#indices = list(range(0,100))\n",
        "#random.shuffle(indices)\n",
        "for i in range(iterations):\n",
        "  classes_current_iter = range(i*num_classes, i*num_classes+num_classes)\n",
        "  train_iter = [] \n",
        "  for j in range(len(trainset)):\n",
        "    if(trainset[j][-1] in classes_current_iter):\n",
        "      test_set.append(trainset[j]) \n",
        "      train_iter.append(trainset[j])\n",
        "\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_iter, shuffle = True, batch_size=batch_size, num_workers=2)\n",
        "  valid_loader = torch.utils.data.DataLoader(test_set, shuffle = True, batch_size = batch_size, num_workers=2) \n",
        "  training(train_loader, i, net, device, epochs, num_classes) # Train the network with 10 classes at a time\n",
        "\n",
        "  test(valid_loader, i, net, acc) # Test the network with all classes seen until this iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjQZSybAOH9C"
      },
      "source": [
        "### CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXP0RxiQ50mm"
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "confusion_matrix = torch.zeros(100,100)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in valid_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net.forward(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "          confusion_matrix[t.long(),p.long()] += 1\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}