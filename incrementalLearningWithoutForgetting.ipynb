{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "incrementalLearningWithoutForgetting.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Gzrd6SoDNp6d",
        "QObN0rGNN2Be"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoManca-FM/Project_MLDL/blob/main/incrementalLearningWithoutForgetting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oLqIHr_uy-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699ca50f-7fd5-47aa-9b97-f3d005382c74"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun  3 07:35:29 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUHNE0YdXMlR"
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms \n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from torch.nn.init import xavier_uniform_ \n",
        "from torch.nn.init import kaiming_uniform_"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UivF_HGCTWYe"
      },
      "source": [
        "### DATA LOADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QleCp90XDa2a",
        "outputId": "ab76fa77-7483-4811-b498-fc3ee709a820"
      },
      "source": [
        "# we build a transform to normalize images: Data normalization is an important step which ensures \n",
        "# each input parameter (pixel, in this case) has a similar data distribution. This makes convergence \n",
        "# faster while training the network.\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "batch_size = 128\n",
        "\n",
        "trainset_raw = torchvision.datasets.CIFAR100(root='./data', train=True, \n",
        "                                         download=True, transform=transform)\n",
        "\n",
        "for i in range(len(trainset_raw)):\n",
        "  if(i==0):\n",
        "    trainset = [[trainset_raw[i][0], trainset_raw[i][1]]]\n",
        "  else:\n",
        "    trainset.append([trainset_raw[i][0], trainset_raw[i][1]])\n",
        "\n",
        "\n",
        "# DataLoader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
        "# batch_size = how many samples per batch to load\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset_raw = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "for i in range(len(testset_raw)):\n",
        "  if(i==0):\n",
        "    testset = [[testset_raw[i][0], testset_raw[i][1]]]\n",
        "  else:\n",
        "    testset.append([testset_raw[i][0], testset_raw[i][1]])\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXdZrK0zNwvw"
      },
      "source": [
        "### NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kAHBaHJ4Td1"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        \n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        self.inplanes = 16\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "\n",
        "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        \n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # def addOutputNodes(self, num_new_outputs):\n",
        "    #     in_features = self.fc.in_features\n",
        "    #     out_features = self.fc.out_features\n",
        "    #     weight = self.fc.weight.data\n",
        "\n",
        "    #     self.fc = nn.Linear(in_features, out_features + num_new_outputs)\n",
        "\n",
        "    #     #xavier initialization\n",
        "    #     xavier_uniform_(self.fc.weight)\n",
        "        \n",
        "    #     self.fc.weight.data[:out_features] = weight\n",
        "        \n",
        "def resnet32(pretrained=False, **kwargs):\n",
        "    n = 5\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH0rf0sTOB1r"
      },
      "source": [
        "class LWF(nn.Module):\n",
        "  def __init__(self, feature_size, n_classes):\n",
        "    super(LWF,self).__init__()\n",
        "    self.feature_extractor = resnet32()\n",
        "    self.feature_extractor.fc = nn.Linear(self.feature_extractor.fc.in_features, feature_size)\n",
        "    self.bn =  nn.BatchNorm1d(feature_size, momentum=0.9)\n",
        "    self.ReLU = nn.ReLU()\n",
        "\n",
        "    self.fc = nn.Linear(feature_size, n_classes, bias = False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.feature_extractor(x)\n",
        "    x = self.bn(x)\n",
        "    x = self.ReLU(x)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def increment_classes(self, n):\n",
        "    \"\"\"Add n classes in the final fc layer\"\"\"\n",
        "    in_features = self.fc.in_features\n",
        "    out_features = self.fc.out_features\n",
        "    weight = self.fc.weight.data\n",
        "\n",
        "    #xavier initialization\n",
        "    # xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    self.fc = nn.Linear(in_features, out_features + n, bias = False)\n",
        "    self.fc.weight.data[:out_features] = weight"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1JeZ9NB4ZxS",
        "outputId": "292fa50c-4610-4d19-94a5-c0e1f460035e"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
        "feature_size = 2048\n",
        "n_classes = 10\n",
        "net = LWF(feature_size, n_classes)\n",
        "net.to(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LWF(\n",
              "  (feature_extractor): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Linear(in_features=64, out_features=2048, bias=True)\n",
              "  )\n",
              "  (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "  (ReLU): ReLU()\n",
              "  (fc): Linear(in_features=2048, out_features=10, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QObN0rGNN2Be"
      },
      "source": [
        "### LOSS & PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUMJGDLO4cR-"
      },
      "source": [
        "lr = 0.01\n",
        "decay = 0.00001\n",
        "epochs = 20\n",
        "momentum = 0.9\n",
        "factor = 5"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLf5BCR-4c3X"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "bceLoss = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr = lr, weight_decay=decay,momentum= momentum)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te6KvRNKN5k_"
      },
      "source": [
        "### TRAINING BCE LOSS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-BsI39t38kM"
      },
      "source": [
        "# def training(trainloader, iteration, network, device, epochs, num_classes):\n",
        "\n",
        "#   num_classes_till_previous_step = iteration * num_classes\n",
        "#   num_current_classes = num_classes_till_previous_step + num_classes\n",
        "#   total_classes = 100\n",
        "\n",
        "#   distillation_loss = 0\n",
        "\n",
        "#   old_net = copy.deepcopy(network)\n",
        "#   old_net.eval()\n",
        "  \n",
        "#   if (iteration != 0):\n",
        "#     # add 10 output nodes to the network\n",
        "#     network.increment_classes(num_classes)\n",
        "#     network.to(device)\n",
        "\n",
        "#   optimizer = optim.SGD(network.parameters(), lr = lr, weight_decay=decay,momentum= momentum)\n",
        " \n",
        " \n",
        "#   #train the network\n",
        "#   for epoch in range(epochs):\n",
        "#     if (epoch == 49 or epoch == 63):\n",
        "#      optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / factor\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#       inputs = data[0].to(device)\n",
        "#       labels = data[1].to(device)\n",
        "#       optimizer.zero_grad()\n",
        "#       logits = network.forward(inputs)\n",
        "#       distilled_targets = get_one_hot(labels, num_current_classes, device)\n",
        "      \n",
        "#       if iteration > 0:\n",
        "#         logits_old = old_net(inputs)\n",
        "#         distilled_targets[:, 0:num_classes_till_previous_step] = torch.sigmoid(logits_old)\n",
        "\n",
        "#       loss = bceLoss(logits, distilled_targets) #* (num_current_classes / total_classes)\n",
        "\n",
        "#       # redesign the weights evaluating the performance of the network\n",
        "#       loss.backward()\n",
        "#       # update parameters\n",
        "#       optimizer.step()\n",
        "\n",
        "#       running_loss += loss.item()\n",
        "#       if i % 20 == 19:    \n",
        "#         print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 20))\n",
        "#         running_loss = 0.0\n",
        "\n",
        "#   return network"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFrwsyMN3_dO"
      },
      "source": [
        "# def get_one_hot(target,num_class, device):\n",
        "#   one_hot=torch.zeros(target.shape[0],num_class).to(device)\n",
        "#   one_hot=one_hot.scatter(dim=1,index=target.long().view(-1,1),value=1.)\n",
        "#   return one_hot"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOEE58LJWeAQ"
      },
      "source": [
        "### training (bce loss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXIC_IO6WgTB"
      },
      "source": [
        "def training(trainloader, iteration, network, device, epochs, num_classes):\n",
        "  \n",
        "  num_classes_till_previous_step = iteration * num_classes\n",
        "  num_current_classes = num_classes_till_previous_step + num_classes\n",
        "  total_classes = 100\n",
        "\n",
        "  distillation_loss = 0\n",
        "  \n",
        "  old_net = copy.deepcopy(network)\n",
        "  old_net.eval()\n",
        "  \n",
        "  if (iteration != 0):\n",
        "    # add 10 output nodes to the network\n",
        "    network.increment_classes(num_classes)\n",
        "    network.to(device)\n",
        "\n",
        "  # net = network.feature_extractor\n",
        "  # net = net.to(device)\n",
        "\n",
        "  optimizer = optim.SGD(network.parameters(), lr = lr, weight_decay=decay,momentum= momentum)\n",
        " \n",
        "  network.train()\n",
        "\n",
        "  #train the network\n",
        "  for epoch in range(epochs):\n",
        "    if (epoch == 49 or epoch == 63):\n",
        "     optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / factor\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      inputs = data[0].to(device)\n",
        "      labels = data[1].to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      \"\"\"logits = network.forward(inputs)\n",
        "\n",
        "      distilled_targets = get_one_hot(labels, num_current_classes, device)\n",
        "      \n",
        "      if iteration > 0:\n",
        "        logits_old = old_net(inputs)\n",
        "        distilled_targets[:, 0:num_classes_till_previous_step] = torch.sigmoid(logits_old)\n",
        "\n",
        "\n",
        "      loss = bceLoss(logits, distilled_targets) * (num_current_classes / total_classes)\"\"\"\n",
        "\n",
        "      #cross tentativo \n",
        "      outputs = network.forward(inputs)\n",
        "      loss = nn.CrossEntropyLoss()(outputs,labels)\n",
        "\n",
        "\n",
        "      # redesign the weights evaluating the performance of the network\n",
        "      loss.backward()\n",
        "      # update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      if i % 20 == 19:    \n",
        "        print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 20))\n",
        "        running_loss = 0.0\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZruRj2tBylu"
      },
      "source": [
        "def get_one_hot(target,num_class, device):\n",
        "  one_hot=torch.zeros(target.shape[0],num_class).to(device)\n",
        "  one_hot=one_hot.scatter(dim=1,index=target.long().view(-1,1),value=1.)\n",
        "  return one_hot"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHr8sBA_N-zJ"
      },
      "source": [
        "### TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cefaruFe3_DP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f57950cf-905f-4c15-90e2-42b829750035"
      },
      "source": [
        "\"\"\"\n",
        "def test(testloader, iteration, network, acc):\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  print(\"ITERATION: \", iteration)\n",
        "  \n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "          # calculate outputs by running images through the network\n",
        "          outputs = network.forward(images)\n",
        "\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "         \n",
        "        \n",
        "  acc.append(100*correct/total)\n",
        "  print(f'Accuracy of the network on the {iteration} iteration: %d %%' % (100 * correct / total))\n",
        "  \"\"\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef test(testloader, iteration, network, acc):\\n\\n  correct = 0\\n  total = 0\\n  print(\"ITERATION: \", iteration)\\n  \\n  # since we\\'re not training, we don\\'t need to calculate the gradients for our outputs\\n  with torch.no_grad():\\n      for data in testloader:\\n          images, labels = data[0].to(device), data[1].to(device)\\n\\n          # calculate outputs by running images through the network\\n          outputs = network.forward(images)\\n\\n          # the class with the highest energy is what we choose as prediction\\n          _, predicted = torch.max(outputs.data, 1)\\n          total += labels.size(0)\\n          correct += (predicted == labels).sum().item()\\n         \\n        \\n  acc.append(100*correct/total)\\n  print(f\\'Accuracy of the network on the {iteration} iteration: %d %%\\' % (100 * correct / total))\\n  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LxjrgyTOBWN"
      },
      "source": [
        "### EXECUTION "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bLZetGa4F_o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a26ee2a4-a1fd-4be2-cd38-38ba09281770"
      },
      "source": [
        "\"\"\"\n",
        "# divided our dataset into sample of 10 classes each\n",
        "# train the network on the first 10 classes\n",
        "# evaluate the network on the first 10 classes\n",
        "# train the network on the second 10 classes (adding 10 output layers)\n",
        "# evaluate the network on the first 20 classes\n",
        "iterations= 10 \n",
        "num_classes = 10\n",
        "test_set = [] #initialized here because we test over all the classes not only those one in which I train\n",
        "acc = []\n",
        "import random\n",
        "#indices = list(range(0,100))\n",
        "#random.shuffle(indices)\n",
        "for i in range(iterations):\n",
        "  classes_current_iter = range(i*num_classes, i*num_classes+num_classes)\n",
        "  train_iter = [] \n",
        "  for j in range(len(trainset)):\n",
        "    if(trainset[j][-1] in classes_current_iter):\n",
        "      test_set.append(trainset[j]) \n",
        "      train_iter.append(trainset[j])\n",
        "\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_iter, shuffle = True, batch_size=batch_size, num_workers=2)\n",
        "  valid_loader = torch.utils.data.DataLoader(test_set, shuffle = True, batch_size = batch_size, num_workers=2) \n",
        "  training(train_loader, i, net, device, epochs, num_classes) # Train the network with 10 classes at a time\n",
        "\n",
        "  test(valid_loader, i, net, acc) # Test the network with all classes seen until this iteration\n",
        "  \"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# divided our dataset into sample of 10 classes each\\n# train the network on the first 10 classes\\n# evaluate the network on the first 10 classes\\n# train the network on the second 10 classes (adding 10 output layers)\\n# evaluate the network on the first 20 classes\\niterations= 10 \\nnum_classes = 10\\ntest_set = [] #initialized here because we test over all the classes not only those one in which I train\\nacc = []\\nimport random\\n#indices = list(range(0,100))\\n#random.shuffle(indices)\\nfor i in range(iterations):\\n  classes_current_iter = range(i*num_classes, i*num_classes+num_classes)\\n  train_iter = [] \\n  for j in range(len(trainset)):\\n    if(trainset[j][-1] in classes_current_iter):\\n      test_set.append(trainset[j]) \\n      train_iter.append(trainset[j])\\n\\n\\n  train_loader = torch.utils.data.DataLoader(train_iter, shuffle = True, batch_size=batch_size, num_workers=2)\\n  valid_loader = torch.utils.data.DataLoader(test_set, shuffle = True, batch_size = batch_size, num_workers=2) \\n  training(train_loader, i, net, device, epochs, num_classes) # Train the network with 10 classes at a time\\n\\n  test(valid_loader, i, net, acc) # Test the network with all classes seen until this iteration\\n  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjQZSybAOH9C"
      },
      "source": [
        "### CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXP0RxiQ50mm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "846bd969-edc0-4e60-aa41-0e5aeab49100"
      },
      "source": [
        "\"\"\"\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "confusion_matrix = torch.zeros(100,100)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in valid_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net.forward(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "          confusion_matrix[t.long(),p.long()] += 1\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "\"\"\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom sklearn.metrics import plot_confusion_matrix\\nimport matplotlib.pyplot as plt\\n\\nconfusion_matrix = torch.zeros(100,100)\\n\\ncorrect = 0\\ntotal = 0\\n# since we\\'re not training, we don\\'t need to calculate the gradients for our outputs\\nwith torch.no_grad():\\n    for data in valid_loader:\\n        images, labels = data[0].to(device), data[1].to(device)\\n        # calculate outputs by running images through the network\\n        outputs = net.forward(images)\\n        # the class with the highest energy is what we choose as prediction\\n        _, predicted = torch.max(outputs.data, 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\n        for t, p in zip(labels.view(-1), predicted.view(-1)):\\n          confusion_matrix[t.long(),p.long()] += 1\\n\\nplt.figure()\\nplt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=plt.cm.Blues)\\nprint(\\'Accuracy of the network on the 10000 test images: %d %%\\' % (100 * correct / total))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TckH9XhBXBQb"
      },
      "source": [
        "### TEST (CONFUSION EACH STEP)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO7gpTu1XLl-"
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test(testloader, iteration, network, acc):\n",
        "  confusion_matrix = torch.zeros(iteration*10+10,iteration*10+10)\n",
        "  print(\"confusion matrix shape: \", confusion_matrix.shape)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  print(\"ITERATION: \", iteration)\n",
        "  network.eval()\n",
        "  \n",
        "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "          # calculate outputs by running images through the network\n",
        "          outputs = network.forward(images)\n",
        "\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          _, predicted = torch.max(torch.softmax(network.forward(images), dim=1), dim=1, keepdim=False)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "         \n",
        "          for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "            confusion_matrix[t.long(),p.long()] += 1\n",
        "\n",
        "  plt.figure()\n",
        "  plt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=plt.cm.jet)\n",
        "  plt.show()\n",
        "  acc.append(100*correct/total)\n",
        "  print(f'Accuracy of the network on the {iteration} iteration: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kxkDFX6DXo6"
      },
      "source": [
        "### RANDOM CLASSES\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jU3qySqDivk"
      },
      "source": [
        "EXECUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKHoN8W9EJBw"
      },
      "source": [
        "import random\n",
        "indices = list(range(0,100))\n",
        "random.shuffle(indices)\n",
        "dict_classes = dict(zip(indices,range(100)))\n",
        "for i in range(len(trainset)):\n",
        "  trainset[i][1] = dict_classes[trainset[i][1]]\n",
        "for j in range(len(testset)):\n",
        "  testset[j][1] = dict_classes[testset[j][1]]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INaUfRGBDkPP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50979018-657a-4ecd-c5f4-d8852b39471c"
      },
      "source": [
        "#TRYING TO RANDOMIZE CLASSES\n",
        "\n",
        "\n",
        "# divided our dataset into sample of 10 classes each\n",
        "# train the network on the first 10 classes\n",
        "# evaluate the network on the first 10 classes\n",
        "# train the network on the second 10 classes (adding 10 output layers)\n",
        "# evaluate the network on the first 20 classes\n",
        "iterations = 10 \n",
        "num_classes = 10 \n",
        "test_iter = [] #initialized here because we test over all the classes not only those one in which I train\n",
        "acc = []\n",
        "val_setL = []\n",
        "for i in range(iterations):\n",
        "  classes_current_iter = range(i*num_classes, i*num_classes+num_classes)\n",
        "  train_iter = []\n",
        "  for j in range(len(trainset)):\n",
        "    if(trainset[j][-1] in classes_current_iter):\n",
        "      train_iter.append(trainset[j])\n",
        "  for l in range(len(testset)):\n",
        "    if (testset[l][-1] in classes_current_iter):\n",
        "      test_iter.append(testset[l])\n",
        "\n",
        " \n",
        "  train_set, val_set = torch.utils.data.random_split(train_iter, [4000, 1000])\n",
        "\n",
        "  for z in range(len(val_set)):\n",
        "    val_setL.append(val_set[z])\n",
        "  \n",
        "\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_set, shuffle = True, batch_size=batch_size, num_workers=2)\n",
        "  valid_loader = torch.utils.data.DataLoader(val_setL,shuffle = True, batch_size=batch_size, num_workers=2)\n",
        "\n",
        " \n",
        "  test_loader = torch.utils.data.DataLoader(test_iter, shuffle = True, batch_size = batch_size, num_workers=2) \n",
        "  print(\"Train the network, iteration: \", i, \" on classes: \", classes_current_iter)\n",
        "  \n",
        "\n",
        "  print(\"train_set lenght: \", len(train_set))\n",
        "  print(\"val_set length: \", len(val_set))\n",
        "  print(\"val_setL length: \", len(val_setL))\n",
        "\n",
        "\n",
        "  training(train_loader, i, net, device, epochs, num_classes) # Train the network with 10 classes at a time\n",
        "  # print(\"after training: \", net.fc.weight.data)\n",
        "\n",
        "  # Test the network with all classes seen until this iteration on the validation set\n",
        "  test(valid_loader, i, net, acc)\n",
        "  \n",
        "  # Test the network with all classes seen until this iteration on the test set\n",
        "  # test(test_loader, i, net, acc) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train the network, iteration:  0  on classes:  range(0, 10)\n",
            "train_set lenght:  4000\n",
            "val_set length:  1000\n",
            "val_setL length:  1000\n",
            "[1,    20] loss: 2.135\n",
            "[2,    20] loss: 1.766\n",
            "[3,    20] loss: 1.493\n",
            "[4,    20] loss: 1.317\n",
            "[5,    20] loss: 1.206\n",
            "[6,    20] loss: 1.074\n",
            "[7,    20] loss: 0.901\n",
            "[8,    20] loss: 0.800\n",
            "[9,    20] loss: 0.760\n",
            "[10,    20] loss: 0.650\n",
            "[11,    20] loss: 0.583\n",
            "[12,    20] loss: 0.426\n",
            "[13,    20] loss: 0.336\n",
            "[14,    20] loss: 0.258\n",
            "[15,    20] loss: 0.226\n",
            "[16,    20] loss: 0.219\n",
            "[17,    20] loss: 0.210\n",
            "[18,    20] loss: 0.148\n",
            "[19,    20] loss: 0.269\n",
            "[20,    20] loss: 0.177\n",
            "confusion matrix shape:  torch.Size([10, 10])\n",
            "ITERATION:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 0 iteration: 51 %\n",
            "Train the network, iteration:  1  on classes:  range(10, 20)\n",
            "train_set lenght:  4000\n",
            "val_set length:  1000\n",
            "val_setL length:  2000\n",
            "[1,    20] loss: 2.544\n",
            "[2,    20] loss: 1.259\n",
            "[3,    20] loss: 1.049\n",
            "[4,    20] loss: 0.776\n",
            "[5,    20] loss: 0.565\n",
            "[6,    20] loss: 0.383\n",
            "[7,    20] loss: 0.281\n",
            "[8,    20] loss: 0.180\n",
            "[9,    20] loss: 0.148\n",
            "[10,    20] loss: 0.122\n",
            "[11,    20] loss: 0.145\n",
            "[12,    20] loss: 0.145\n",
            "[13,    20] loss: 0.095\n",
            "[14,    20] loss: 0.078\n",
            "[15,    20] loss: 0.082\n",
            "[16,    20] loss: 0.038\n",
            "[17,    20] loss: 0.040\n",
            "[18,    20] loss: 0.013\n",
            "[19,    20] loss: 0.018\n",
            "[20,    20] loss: 0.010\n",
            "confusion matrix shape:  torch.Size([20, 20])\n",
            "ITERATION:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 1 iteration: 31 %\n",
            "Train the network, iteration:  2  on classes:  range(20, 30)\n",
            "train_set lenght:  4000\n",
            "val_set length:  1000\n",
            "val_setL length:  3000\n",
            "[1,    20] loss: 5.803\n",
            "[2,    20] loss: 1.655\n",
            "[3,    20] loss: 1.448\n",
            "[4,    20] loss: 1.154\n",
            "[5,    20] loss: 0.931\n",
            "[6,    20] loss: 0.670\n",
            "[7,    20] loss: 0.470\n",
            "[8,    20] loss: 0.322\n",
            "[9,    20] loss: 0.182\n",
            "[10,    20] loss: 0.117\n",
            "[11,    20] loss: 0.066\n",
            "[12,    20] loss: 0.037\n",
            "[13,    20] loss: 0.036\n",
            "[14,    20] loss: 0.022\n",
            "[15,    20] loss: 0.012\n",
            "[16,    20] loss: 0.004\n",
            "[17,    20] loss: 0.003\n",
            "[18,    20] loss: 0.002\n",
            "[19,    20] loss: 0.004\n",
            "[20,    20] loss: 0.008\n",
            "confusion matrix shape:  torch.Size([30, 30])\n",
            "ITERATION:  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 2 iteration: 20 %\n",
            "Train the network, iteration:  3  on classes:  range(30, 40)\n",
            "train_set lenght:  4000\n",
            "val_set length:  1000\n",
            "val_setL length:  4000\n",
            "[1,    20] loss: 9.796\n",
            "[2,    20] loss: 1.969\n",
            "[3,    20] loss: 1.633\n",
            "[4,    20] loss: 1.623\n",
            "[5,    20] loss: 1.296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDJu2frr__VJ"
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "confusion_matrix = torch.zeros(100,100)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in testloader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    \n",
        "    # calculate outputs by running images through the network\n",
        "    outputs = net.forward(images)\n",
        "\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(torch.softmax(net.forward(images), dim=1), dim=1, keepdim=False)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "         \n",
        "    for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
        "      confusion_matrix[t.long(),p.long()] += 1\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=plt.cm.jet)\n",
        "plt.show()\n",
        "print(f'Accuracy of the network: %d %%' % (100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}